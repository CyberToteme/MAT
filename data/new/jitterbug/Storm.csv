projectname,classification,commenttext
Storm,SATD,// TODO support multiple levels of includes 
Storm,SATD,//  When all nimbodes go down and one or few of them come up   Unfortunately there might not be an exact way to know which one contains the most updated blob 
Storm,SATD,//  TODO: this assumes key is only from the one field   if not we need to have order of fields in PK 
Storm,SATD,//  the following code is duplicated in the constructor of MqttPublisher   we reproduce it here so we fail on the client side if SSL is misconfigured rather than when the topology   is deployed to the cluster 
Storm,SATD,//  should remove the third blob because the first has the reset timestamp 
Storm,SATD,//  Get rid of multiple '/' in url
Storm,SATD,//  Todo: Track file offsets instead of line number 
Storm,SATD,//  TODO support more configuration options for now we're defaulting to the hbase-*.xml files found on the classpath 
Storm,SATD,// This is a bit ugly but it works.  In order to maintain the same directory structure that existed before   we need to have storm conf storm jar and storm code in a shared directory and we need to set the   permissions for that entire directory but the tracking is on a per item basis so we are going to end   up running the permission modification code once for each blob that is downloaded (3 times in this case).   Because the permission modification code runs in a separate process we are doing a global lock to avoid   any races between multiple versions running at the same time.  Ideally this would be on a per topology   basis but that is a lot harder and the changes run fairly quickly so it should not be a big deal. 
Storm,SATD,// Could not find a "fully" compatible version.  Look to see if there is a possibly compatible version right below it 
Storm,SATD,// ResourceUtils.java is not a available on the classpath to let us parse out the resources we want. 
Storm,SATD,//  Files/move with non-empty directory doesn't work well on Windows   This is not atomic but it does work 
Storm,SATD,/*  Not thread safe. Have one instance per producer thread or synchronize externally  */
Storm,SATD,//  TODO substitution implementation is not exactly efficient or kind to memory... 
Storm,SATD,// TODO need a better way to do this 
Storm,SATD,//  TODO: Need to be able to set the tick tuple time to the message timeout ideally without parameterization 
Storm,SATD,/*              * Connection is unavailable. We will drop pending messages and let at-least-once message replay kick in.             *             * Another option would be to buffer the messages in memory.  But this option has the risk of causing OOM errors             * especially for topologies that disable message acking because we don't know whether the connection recovery will             * succeed  or not and how long the recovery will take.              */
Storm,SATD,//  TODO add metrics
Storm,SATD,//  TODO: consider adding a shuffle grouping after the spout to avoid so much routing of the args/return-info all over the place   (at least until its possible to just pack bolt logic into the spout itself) 
Storm,SATD,//  This is a hack for non-ras scheduler topology and worker resources 
Storm,SATD,// TODO move this logic to the model class 
Storm,SATD,// For some reason on the new code if ackers is null we get 0??? 
Storm,SATD,// This is a bit of a hack.  If it is a list then it is [component stream]  we want to format this as component:stream 
Storm,SATD,// TODO: DRY this code up with what's in ChainedAggregatorImpl 
Storm,SATD,//  should we fail the tuple or kill the worker? 
Storm,SATD,// TODO streams should be uniquely identifiable 
Storm,SATD,// TODO I would love to standardize this... 
Storm,SATD,//  FIXME: This class can be moved to webapp once UI porting is done. 
Storm,SATD,//  This isn't strictly necessary but it doesn't hurt and ensures that the machine stays up   to date even if callbacks don't all work exactly right 
Storm,SATD,//  To work around it we should get it an alternative (working) way. 
Storm,SATD,//  read s1 last....  This should cause s2 to be evicted on next put 
Storm,SATD,//  TODO more collection content type checking 
Storm,SATD,//  FIXME: We're using default config since it cannot be serialized   We still needs to provide some options externally 
Storm,SATD,// TODO this can only ever be null if someone is doing something odd with mocking   We should really fix the mocking and remove this 
Storm,SATD,// TODO put in some better exception mapping...  TODO move populateContext to a filter... 
Storm,SATD,//  should we fail the batch or kill the worker? 
Storm,SATD,// Getting the exact response code is a bit more complex.  TODO should use a better client 
Storm,SATD,//  TODO: de-duplicate this logic with the code in nimbus
Storm,SATD,/*      * TODO: should worker even take the topologyId as input? this should be deducible from cluster state (by searching through assignments)     * what about if there's inconsistency in assignments? -> but nimbus should guarantee this consistency.     *     * @param conf           - Storm configuration     * @param context        -     * @param topologyId     - topology id     * @param assignmentId   - assignment id     * @param supervisorPort - parent supervisor thrift server port     * @param port           - port on which the worker runs     * @param workerId       - worker id      */
Storm,SATD,// This is a bit ugly but it shows how this would happen in a worker so we will use the same APIs 
Storm,SATD,//  FIXME: stores map (topoConf) topologyContext and expose these to derived classes 
Storm,SATD,// Original implementation doesn't actually check if delete succeeded or not. 
Storm,SATD,//  it doesn't seem like you should have to do this but java serialization is wacky and doesn't call the default constructor. 
Storm,SATD,// This is a bit ugly The JSON we are expecting should be in the form   {"component": {"resource": value ...} ...}   But because value is coming from JSON it is going to be a Number and we want it to be a Double.   So the goal is to go through each entry and update it accordingly 
Storm,SATD,// We may (though unlikely) lose metering here if state transition is too frequent (less than a millisecond) 
Storm,SATD,//  do we want to throw an exception if path doesn't exist?? 
Storm,SATD,// TODO a better way to do this would be great. 
Storm,SATD,// TODO in the future this might be better in a common webapp location 
Storm,SATD,//  FIXME: it should be moved to storm-client when serialization-test.clj can be removed 
Storm,SATD,// TODO figure out how we want to deal with overrides. Users may want to add streams even when overriding other   properties. For now we just add them blindly which could lead to a potentially invalid topology. 
Storm,SATD,// More accurate that thread.sleep but still not great 
Storm,SATD,//  todo ignore the master batch coordinator ? 
Storm,SATD,// This test is rather ugly but it is the only way to see if the error messages are working correctly. 
Storm,SATD,//  TODO: need to be able to replace existing fields with the function fields (like Cascading Fields.REPLACE) 
Storm,SATD,// TODO: wrap this to set the stream name 
Storm,SATD,// TODO: this isn't right... it's not in the map anymore 
Storm,SATD,// TODO: add a method for drpc stream needs to know how to automatically do return results etc   is it too expensive to do a batch per drpc request? 
Storm,SATD,// TODO: do the inner join incrementally emitting the cross join with this tuple against all other sides  TODO: only do cross join if at least one tuple in each side 
Storm,SATD,// TODO once everything is in java this should not be possible any more 
Storm,SATD,// heck for backward compatibility we need to pass TOPOLOGY_AUTO_CREDENTIALS to hbase conf  the conf instance is instance of persistentMap so making a copy. 
Storm,SATD,//  TODO: handle regular rich spout without batches (need lots of updates to support this throughout)
Storm,SATD,// This is a hack to allow ZooKeeperMain to be called by this command. 
Storm,SATD,// Perhaps there is a better way to do this??? 
Storm,SATD,//  TODO: can optimize further by only querying backing map for keys not in the cache 
Storm,SATD,// TODO this is causing issues... 
Storm,SATD,/*  TODO: need to invoke a hook provided by the topology giving it a chance to create user resources.         * this would be part of the initialization hook         * need to separate workertopologycontext into WorkerContext and WorkerUserContext.         * actually just do it via interfaces. just need to make sure to hide setResource from tasks          */
Storm,SATD,/*  Not implemented  */
Storm,SATD,//  TODO: What would be a good test to ensure that RankableObjectWithFields is at least somewhat defensively copied? 
Storm,SATD,//  TODO: in the future want a way to include this logic in the spout itself   or make it unnecessary by having storm include metadata about which grouping a tuple   came from 
Storm,SATD,//  TODO: does this work well on windows? 
Storm,SATD,// Not enough guaranteed use the age of the topology instead.  TODO need a good way to only do this once... 
Storm,SATD,//  Use parseWithException instead of parse so we can capture deserialization errors in the log.   They are likely to be bugs in the spout code. 
Storm,SATD,// TODO: add logging that not all tuples were received
Storm,SATD,//  TODO: take away knowledge of storm's internals here 
Storm,SATD,// TODO we should handle '\n'. ref DelimitedRecordFormat 
Storm,SATD,/*  TODO: make sure test these two functions in manual tests  */
Storm,SATD,// TODO get this from type instead of hardcoding to Nimbus.  establish client-server transport via plugin 
Storm,SATD,// There is a bug in some versions that returns 0 for the uptime. 
Storm,SATD,//  need to set active false before calling onKill() - current implementation does not return. 
Storm,SATD,// this is kind of unnecessary for clojure 
Storm,SATD,//  TODO this class is reserved for supporting messages with different schemas.   current only one schema in the cache 
Storm,SATD,//  -- bad files dir config 
Storm,SATD,//  FIXME: we can filter by listKeys() with local blobstore when STORM-1986 is going to be resolved   as a workaround we call getBlobMeta() for all keys 
Storm,SATD,// 1 min.  This really means something is wrong.  Even on a very slow node 
Storm,SATD,// TODO check for null grouping args 
Storm,SATD,// TODO perhaps we can adjust the frequency later... 
Storm,SATD,// re-establish connection to eventhub servers using the right offset  TBD: might be optimized with cache. 
Storm,SATD,// TODO Handle the case where there may be no schema 
Storm,SATD,// It seems safer not to follow symlinks since we don't expect them here 
Storm,SATD,//  this is because json doesn't allow numbers as keys...   TODO: replace json with a better form of encoding 
Storm,SATD,//  Todo: optimize this computation... perhaps inner loop can be outside to avoid rescanning tuples 
Storm,SATD,// Likely it is because of a bug so try to get it another way 
Storm,WITHOUT_CLASSIFICATION,//  while disabling we retain the sampling pct. 
Storm,WITHOUT_CLASSIFICATION,//  hearbeat upon it 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_MEMONHEAP 
Storm,WITHOUT_CLASSIFICATION,//  test environment variable substitution 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNMENTS 
Storm,WITHOUT_CLASSIFICATION,// Verify that all messages are emitted ack all the messages 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTION_COMMAND 
Storm,WITHOUT_CLASSIFICATION,//  boolean to track deactivated state 
Storm,WITHOUT_CLASSIFICATION,/*              * track punctuation in non-batch mode so that the             * punctuation is acked after all the processors have emitted the punctuation downstream.              */
Storm,WITHOUT_CLASSIFICATION,//  local executors and localTaskIds running in this worker 
Storm,WITHOUT_CLASSIFICATION,//  only emit if we have declared fields. 
Storm,WITHOUT_CLASSIFICATION,//  all types of files included 
Storm,WITHOUT_CLASSIFICATION,// We want to capture the full time range so the target size is as   if we had one bucket less then we do 
Storm,WITHOUT_CLASSIFICATION,// use batch size that matches the default credit size 
Storm,WITHOUT_CLASSIFICATION,//  use the local setting for the login config rather than the topology's 
Storm,WITHOUT_CLASSIFICATION,//  Get the nimbodes with the latest version 
Storm,WITHOUT_CLASSIFICATION,//  destination taskId 
Storm,WITHOUT_CLASSIFICATION,//  component 
Storm,WITHOUT_CLASSIFICATION,// Fail the last emitted tuple and verify that the spout won't retry it because it's above the emit limit. 
Storm,WITHOUT_CLASSIFICATION,//  If we generate a null response then authentication has completed   (if not warn) and return without sending a response back to the   server. 
Storm,WITHOUT_CLASSIFICATION,// supervisor assignment id/supervisor id 
Storm,WITHOUT_CLASSIFICATION,//  the WindowManager scan loop early. 
Storm,WITHOUT_CLASSIFICATION,//  failed to send the JMS message fail the tuple fast
Storm,WITHOUT_CLASSIFICATION,// Set the first assignment 
Storm,WITHOUT_CLASSIFICATION,// com.mysql.jdbc.jdbc2.optional.MysqlDataSource  jdbc:mysql://localhost/test  root 
Storm,WITHOUT_CLASSIFICATION,/*                                                *  Update the count in the state. Here the first argument 0L is the initial value for the                                               *  count and                                               *  the second argument is a function that increments the count for each value received.                                                */
Storm,WITHOUT_CLASSIFICATION,//  test write 
Storm,WITHOUT_CLASSIFICATION,//  KEY 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_ID 
Storm,WITHOUT_CLASSIFICATION,// Start emitting right now 
Storm,WITHOUT_CLASSIFICATION,// format of date in worker logs 
Storm,WITHOUT_CLASSIFICATION,/*  Map  */
Storm,WITHOUT_CLASSIFICATION,//  use different blobstore dir so it doesn't conflict with other test 
Storm,WITHOUT_CLASSIFICATION,//  timestamp either in milliseconds or seconds at which this metric is occurred. 
Storm,WITHOUT_CLASSIFICATION,//  NO-OP 
Storm,WITHOUT_CLASSIFICATION,// Update the current count for this object 
Storm,WITHOUT_CLASSIFICATION,//  Ranked fourth since rack-3 has alot of memory but not cpu 
Storm,WITHOUT_CLASSIFICATION,/*      * Each ControlMessage is encoded as:     *  code (<0) ... short(2)     * Each TaskMessage is encoded as:     *  task (>=0) ... short(2)     *  len ... int(4)     *  payload ... byte[]     *      */
Storm,WITHOUT_CLASSIFICATION,//  This call returns immediately 
Storm,WITHOUT_CLASSIFICATION,//  For testing 
Storm,WITHOUT_CLASSIFICATION,//  Test2: test when no more workers are available due to topology worker max heap size limit but there is memory is still available   wordSpout2 is going to contain 5 executors that needs scheduling. Each of those executors has a memory requirement of 128.0 MB   The cluster contains 4 free WorkerSlots. For this topolology each worker is limited to a max heap size of 128.0   Thus one executor not going to be able to get scheduled thus failing the scheduling of this topology and no executors of this 
Storm,WITHOUT_CLASSIFICATION,//  set clean time really high so doesn't kick in 
Storm,WITHOUT_CLASSIFICATION,// Set it for nimbus only 
Storm,WITHOUT_CLASSIFICATION,//  1s   no lag 
Storm,WITHOUT_CLASSIFICATION,//  creates array[largestId+1] filled with nulls 
Storm,WITHOUT_CLASSIFICATION,//         builder.setStateSpout("stateSpout" mock(IRichStateSpout.class) 0);      } 
Storm,WITHOUT_CLASSIFICATION,//  close the input stream 
Storm,WITHOUT_CLASSIFICATION,//  Dependency uploading only makes sense for distributed mode 
Storm,WITHOUT_CLASSIFICATION,//  read 2nd line and ACK 
Storm,WITHOUT_CLASSIFICATION,//  USED_CPU 
Storm,WITHOUT_CLASSIFICATION,//  2) If no failed tuples to be retried then send tuples from hdfs 
Storm,WITHOUT_CLASSIFICATION,//  it happens when part is just '*' rather than denoting some directory 
Storm,WITHOUT_CLASSIFICATION,// Next verify that the blob store is correct before we start it up. 
Storm,WITHOUT_CLASSIFICATION,//  The connection is ready once the channel is active.   See:   - http://netty.io/wiki/new-and-noteworthy-in-4.0.html#wiki-h4-19 
Storm,WITHOUT_CLASSIFICATION,//  test for keylist to download 
Storm,WITHOUT_CLASSIFICATION,//  tuples should be available in store before they are added to window manager 
Storm,WITHOUT_CLASSIFICATION,//  memory required by topo-t0   memory required by topo-t1 
Storm,WITHOUT_CLASSIFICATION,//  This is a DSL (YAML etc.) topology... 
Storm,WITHOUT_CLASSIFICATION,//  check for required fields   check for sub-struct validity 
Storm,WITHOUT_CLASSIFICATION,//  periodically calls refreshLoad in 1 sec to simulate worker load update timer 
Storm,WITHOUT_CLASSIFICATION,//  [remoteTaskId] -> JCQueue. Some entries maybe null (if no emits to those tasksIds from this worker)
Storm,WITHOUT_CLASSIFICATION,//  Encoders 
Storm,WITHOUT_CLASSIFICATION,//  -- commit frequency - count 
Storm,WITHOUT_CLASSIFICATION,// check if iterable 
Storm,WITHOUT_CLASSIFICATION,//  set acl to below so that it can be shared by other users as well but allows only read 
Storm,WITHOUT_CLASSIFICATION,// Emit any remaining messages 
Storm,WITHOUT_CLASSIFICATION,// If AutoHDFS is specified do not attempt to login using keytabs only kept for backward compatibility. 
Storm,WITHOUT_CLASSIFICATION,//  ======== Ack ======= 
Storm,WITHOUT_CLASSIFICATION,//  task ids should be pulled first 
Storm,WITHOUT_CLASSIFICATION,//  referenced from a metric. 
Storm,WITHOUT_CLASSIFICATION,//  PULSE 
Storm,WITHOUT_CLASSIFICATION,//  add reference to one and then remove reference again so it has newer timestamp 
Storm,WITHOUT_CLASSIFICATION,//  writes multiple metric values into the database as a batch operation.  The tree map keeps the keys sorted 
Storm,WITHOUT_CLASSIFICATION,//  Get first and last block times for multiple runs and strategies 
Storm,WITHOUT_CLASSIFICATION,// The worker is up and running check for profiling requests 
Storm,WITHOUT_CLASSIFICATION,//  if the other config does not have it set. 
Storm,WITHOUT_CLASSIFICATION,//  MEM_OFF_HEAP 
Storm,WITHOUT_CLASSIFICATION,//  CAPACITY 
Storm,WITHOUT_CLASSIFICATION,//  new line is at beginning of each line (instead of end) for better recovery from 
Storm,WITHOUT_CLASSIFICATION,//  Exceptions are captured and thrown at the end of the batch by the executor 
Storm,WITHOUT_CLASSIFICATION,//  This thread will send out messages destined for remote tasks (on other workers) 
Storm,WITHOUT_CLASSIFICATION,// do another conversion (lets just make this all common) 
Storm,WITHOUT_CLASSIFICATION,//  Store the time at which the query started executing. The SQL   standard says that functions such as CURRENT_TIMESTAMP return the   same value throughout the query. 
Storm,WITHOUT_CLASSIFICATION,//  Maintain backward compatibility for 0.10 
Storm,WITHOUT_CLASSIFICATION,//  checkpoint the state every 5 seconds 
Storm,WITHOUT_CLASSIFICATION,// We cannot launch the container yet the resources may still be updating 
Storm,WITHOUT_CLASSIFICATION,// Use a bootstrap tuple to wait for topology to be running 
Storm,WITHOUT_CLASSIFICATION,//  AZE 
Storm,WITHOUT_CLASSIFICATION,// Spout - 500 MB 50% CPU 0 GPU  bolt-1 - 500 MB 50% CPU 0 GPU  bolt-3 500 MB 50% cpu 2 GPU  Total 1500 MB 150% CPU 2 GPU -> this node has 0 MB 0% CPU 0 GPU left 
Storm,WITHOUT_CLASSIFICATION,/*                         This case will arise in case of non-sequential offset being processed.                        So if the topic doesn't contain offset = nextCommitOffset (possible                        if the topic is compacted or deleted) the consumer should jump to                        the next logical point in the topic. Next logical offset should be the                        first element after nextCommitOffset in the ascending ordered emitted set.                      */
Storm,WITHOUT_CLASSIFICATION,// A topology can set resources in terms of CPU and Memory for each component 
Storm,WITHOUT_CLASSIFICATION,//  add with past ts 
Storm,WITHOUT_CLASSIFICATION,//  When moving to pacemaker workerbeats can be leaked too... 
Storm,WITHOUT_CLASSIFICATION,//  5 seconds 
Storm,WITHOUT_CLASSIFICATION,//  race condition with another thread and we lost. try again 
Storm,WITHOUT_CLASSIFICATION,// The producers are shut down first so keep going until the queue is empty. 
Storm,WITHOUT_CLASSIFICATION,//  we don't allow any cluster wide configuration 
Storm,WITHOUT_CLASSIFICATION,//  evicted metadata needs to be stored immediately.  Metadata lookups count on it being in the cache 
Storm,WITHOUT_CLASSIFICATION,//  will have the correct settings that cannot be overriden by the submitter. 
Storm,WITHOUT_CLASSIFICATION,//  A map describing which topologies are using which slots on this node.  The format of the map is the following: 
Storm,WITHOUT_CLASSIFICATION,//  track how many times each supervisor slot has been listed as bad 
Storm,WITHOUT_CLASSIFICATION,//  when they are successfully processed. 
Storm,WITHOUT_CLASSIFICATION,// all sent events are stored in pending 
Storm,WITHOUT_CLASSIFICATION,//  topo2 has 4 large tasks 
Storm,WITHOUT_CLASSIFICATION,// Ignore any exceptions we might be doing a test for authentication 
Storm,WITHOUT_CLASSIFICATION,// First failure is the initial delay so not interesting 
Storm,WITHOUT_CLASSIFICATION,/*      * Bolt-specific configuration for windowed bolts to specify the window length in time duration.      */
Storm,WITHOUT_CLASSIFICATION,//  RESOURCES_MAP 
Storm,WITHOUT_CLASSIFICATION,//  contains one Tuple per Stream being joined   refs to fields that will be part of output fields 
Storm,WITHOUT_CLASSIFICATION,// remote subject 
Storm,WITHOUT_CLASSIFICATION,/*          *  By default parser uses [ ] for quoting identifiers. Switching to DQID (double quoted identifiers)         *  is needed for array and map access (m['x'] = 1 or arr[2] = 10 etc) to work.          */
Storm,WITHOUT_CLASSIFICATION,// The second tuple should NOT be ack'd because the batch should be cleared and this will be
Storm,WITHOUT_CLASSIFICATION,// We need to adjust the throughput accordingly (so that it stays the same in aggregate) 
Storm,WITHOUT_CLASSIFICATION,/*      * test scheduling does not cause negative resources      */
Storm,WITHOUT_CLASSIFICATION,//  Return now *WITHOUT* sending upstream here since client   not authorized. 
Storm,WITHOUT_CLASSIFICATION,//  reader 
Storm,WITHOUT_CLASSIFICATION,//  Ranked first since rack-0 has the most balanced set of resources 
Storm,WITHOUT_CLASSIFICATION,//  Configs 
Storm,WITHOUT_CLASSIFICATION,// Waiting to be returned 
Storm,WITHOUT_CLASSIFICATION,//  validate path defined 
Storm,WITHOUT_CLASSIFICATION,//  Blostore launch command with topology blobstore map   Here we are giving it a local name so that we can read from the file   bin/storm jar examples/storm-starter/storm-starter-topologies-0.11.0-SNAPSHOT.jar   org.apache.storm.starter.BlobStoreAPIWordCountTopology bl -c 
Storm,WITHOUT_CLASSIFICATION,//  for some odd reason they are leaked.
Storm,WITHOUT_CLASSIFICATION,// get topology constraints 
Storm,WITHOUT_CLASSIFICATION,// Ensure the second file has a later modified timestamp as the spout should pick the first file first. 
Storm,WITHOUT_CLASSIFICATION,//  launch heartbeat threads immediately so that slow-loading tasks don't cause the worker to timeout 
Storm,WITHOUT_CLASSIFICATION,//  Tuple contains String Object in JSON format 
Storm,WITHOUT_CLASSIFICATION,/*      * returns list of Tuple3 (key val from table val from row)      */
Storm,WITHOUT_CLASSIFICATION,// bolt-3 - 500 MB 50% CPU 2 GPU  Total 500 MB 50% CPU 2 - GPU -> this node has 1000 MB 100% cpu 0 GPU left 
Storm,WITHOUT_CLASSIFICATION,// The priority of a topology describes the importance of the topology in decreasing importance   starting from 0 (i.e. 0 is the highest priority and the priority importance decreases as the priority number increases).  Recommended range of 0-29 but no hard limit set.   If there are not enough resources in a cluster the priority in combination with how far over a guarantees 
Storm,WITHOUT_CLASSIFICATION,// The second tuple is used to wait for the spout to rotate its pending map 
Storm,WITHOUT_CLASSIFICATION,//  StatefulBoltExecutor does the actual ack when the state is saved. 
Storm,WITHOUT_CLASSIFICATION,//  unknown version should be treated as "current version" which supports RPC heartbeat 
Storm,WITHOUT_CLASSIFICATION,// There are a few possible files that we would want to clean up  baseDir + "/" + "_tmp_" + baseName  baseDir + "/" + "_tmp_" + baseName + ".current"  baseDir + "/" + baseName.<VERSION>  baseDir + "/" + baseName.current  baseDir + "/" + baseName.version  In general we always want to delete the _tmp_ files if they are there. 
Storm,WITHOUT_CLASSIFICATION,// Check for : in case someone called their user "User Name"  This line contains the user name for the pid we're looking up  Example line: "User Name:    exampleDomain\exampleUser" 
Storm,WITHOUT_CLASSIFICATION,//  need to reverse the order of elements in PQ to delete files from oldest to newest
Storm,WITHOUT_CLASSIFICATION,// Ignored the file did not match
Storm,WITHOUT_CLASSIFICATION,// cleanup internal assignments 
Storm,WITHOUT_CLASSIFICATION,//  JMS options 
Storm,WITHOUT_CLASSIFICATION,//  has been acked 
Storm,WITHOUT_CLASSIFICATION,// check if exec satisfy spread 
Storm,WITHOUT_CLASSIFICATION,// if no event sent no checkpoint shall be created 
Storm,WITHOUT_CLASSIFICATION,//  dynamic fields 
Storm,WITHOUT_CLASSIFICATION,//  wait for more ACKs before proceeding 
Storm,WITHOUT_CLASSIFICATION,// get query filter 
Storm,WITHOUT_CLASSIFICATION,//  allocate another array to be switched 
Storm,WITHOUT_CLASSIFICATION,// JUMP 
Storm,WITHOUT_CLASSIFICATION,// verify that Offset 10 was last committed offset since this is the offset the spout should resume at 
Storm,WITHOUT_CLASSIFICATION,//   "modprinc -maxlife 3mins <principal>" in kadmin. 
Storm,WITHOUT_CLASSIFICATION,//  acquire lock on file1 
Storm,WITHOUT_CLASSIFICATION,//  now selecting from the full set should cause the fourth task to be chosen. 
Storm,WITHOUT_CLASSIFICATION,//  NODE 
Storm,WITHOUT_CLASSIFICATION,//  PREV_STATUS 
Storm,WITHOUT_CLASSIFICATION,//  configs 
Storm,WITHOUT_CLASSIFICATION,// The new partition should be discovered and the message should be emitted
Storm,WITHOUT_CLASSIFICATION,// error message returned is something went wrong
Storm,WITHOUT_CLASSIFICATION,//  The failed executions should not cause rotations and any new files 
Storm,WITHOUT_CLASSIFICATION,//  Producers 
Storm,WITHOUT_CLASSIFICATION,//  only log/set when there's been a change to the assignment 
Storm,WITHOUT_CLASSIFICATION,//  SingleRel 
Storm,WITHOUT_CLASSIFICATION,//  only if some data has arrived on each input stream 
Storm,WITHOUT_CLASSIFICATION,//  if an ack is for a message that failed once at least and was re-emitted then the record itself will be in   failedAndFetchedRecords. We use that to   determine if the FailedMessageRetryHandler needs to be told about it and then remove the record itself to 
Storm,WITHOUT_CLASSIFICATION,//  metric name 
Storm,WITHOUT_CLASSIFICATION,// bolt-1 - 500 MB 50% CPU 0 GPU  bolt-2 - 500 MB 50% CPU 1 GPU  bolt-2 - 500 MB 50% CPU 1 GPU  Total 1500 MB 150% CPU 2 GPU -> this node has 0 MB 0% CPU 0 GPU left 
Storm,WITHOUT_CLASSIFICATION,//  COMPONENT_EXECUTORS 
Storm,WITHOUT_CLASSIFICATION,//  The manually set STORM_WORKER_CGROUP_CPU_LIMIT config on supervisor will overwrite resources assigned by 
Storm,WITHOUT_CLASSIFICATION,//  When this master is not leader and get heartbeats report from supervisor/node just ignore it. 
Storm,WITHOUT_CLASSIFICATION,//  try locking another file2 at the same time 
Storm,WITHOUT_CLASSIFICATION,// Each time we try to schedule a new component simulate taking 1 second longer 
Storm,WITHOUT_CLASSIFICATION,// Now we need to build the array 
Storm,WITHOUT_CLASSIFICATION,// File 1 should be moved to archive
Storm,WITHOUT_CLASSIFICATION,// 1) create a couple files to consume 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_REGULAR_OFF_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  Current state 
Storm,WITHOUT_CLASSIFICATION,//  if # of workers requested is less than we took   then we know some workers we track died since we have more workers than we are supposed to have 
Storm,WITHOUT_CLASSIFICATION,// returns void if the event should continue false if the event should not be done 
Storm,WITHOUT_CLASSIFICATION,// user jerry submits another topology into a full cluster 
Storm,WITHOUT_CLASSIFICATION,//  Doing nothing (probably due to an oom issue) and hoping Utils.handleUncaughtException will handle it 
Storm,WITHOUT_CLASSIFICATION,//  should remain unchanged @ 2 
Storm,WITHOUT_CLASSIFICATION,// Just to be sure 
Storm,WITHOUT_CLASSIFICATION,//  LAUNCH_TIME_SECS 
Storm,WITHOUT_CLASSIFICATION,//  there won't be a BatchInfo for the success stream 
Storm,WITHOUT_CLASSIFICATION,//  Strategy to determine the fetch offset of the first realized by the spout upon activation 
Storm,WITHOUT_CLASSIFICATION,// start accepting requests 
Storm,WITHOUT_CLASSIFICATION,//  This could fail if a blob gets deleted by mistake.  Don't crash nimbus.
Storm,WITHOUT_CLASSIFICATION,//  STATUS 
Storm,WITHOUT_CLASSIFICATION,//  Create a default pipeline implementation. 
Storm,WITHOUT_CLASSIFICATION,//  public Object execute(Context) 
Storm,WITHOUT_CLASSIFICATION,// Scheduling changed after we killed all of the processes 
Storm,WITHOUT_CLASSIFICATION,//  this finds all active topologies blob keys from all local topology blob keys 
Storm,WITHOUT_CLASSIFICATION,// Sasl transport 
Storm,WITHOUT_CLASSIFICATION,// For cgroups no limit is max long. 
Storm,WITHOUT_CLASSIFICATION,// Not available 
Storm,WITHOUT_CLASSIFICATION,//  scheduled 
Storm,WITHOUT_CLASSIFICATION,//  config main.methods
Storm,WITHOUT_CLASSIFICATION,// if any error/exception thrown fetch it from zookeeper 
Storm,WITHOUT_CLASSIFICATION,//  initial state 
Storm,WITHOUT_CLASSIFICATION,//  error/exception thrown just skip 
Storm,WITHOUT_CLASSIFICATION,//  METRIC_NAME 
Storm,WITHOUT_CLASSIFICATION,// Verify the signature... 
Storm,WITHOUT_CLASSIFICATION,// Empty 
Storm,WITHOUT_CLASSIFICATION,//  set really small so will do cleanup 
Storm,WITHOUT_CLASSIFICATION,//  Look for deleted log timeouts 
Storm,WITHOUT_CLASSIFICATION,//  [ authz authn password ] 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_REGULAR_ON_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  But that is only if there is a bug in one of the password providers.
Storm,WITHOUT_CLASSIFICATION,//  NUM_USED_WORKERS 
Storm,WITHOUT_CLASSIFICATION,//  bolt3 should also receive from checkpoint streams of bolt1 bolt2 
Storm,WITHOUT_CLASSIFICATION,// It already points where we want it to 
Storm,WITHOUT_CLASSIFICATION,//  to really make this work well. 
Storm,WITHOUT_CLASSIFICATION,// Mapping of key->upstreamBolt->count 
Storm,WITHOUT_CLASSIFICATION,//  Equivalent create command on command line 
Storm,WITHOUT_CLASSIFICATION,// topoConf.put(Config.TOPOLOGY_STATE_PROVIDER_CONFIG "{\"keyClass\":\"String\"}"); 
Storm,WITHOUT_CLASSIFICATION,//  1) If there are any abandoned files pick oldest one 
Storm,WITHOUT_CLASSIFICATION,// assigning internally 
Storm,WITHOUT_CLASSIFICATION,// topology priority 
Storm,WITHOUT_CLASSIFICATION,//  TOTAL_SHARED_OFF_HEAP 
Storm,WITHOUT_CLASSIFICATION,//  retry till at least 1 element is drained 
Storm,WITHOUT_CLASSIFICATION,//  schedule mid block (10% - 90%) 
Storm,WITHOUT_CLASSIFICATION,//  key fields 
Storm,WITHOUT_CLASSIFICATION,// NO OP 
Storm,WITHOUT_CLASSIFICATION,// request from authorized hosts and group should be allowed. 
Storm,WITHOUT_CLASSIFICATION,// Now we need to free up some resources... 
Storm,WITHOUT_CLASSIFICATION,//  All internal state except for the current buckets are 
Storm,WITHOUT_CLASSIFICATION,// update the isLeader field for each nimbus summary 
Storm,WITHOUT_CLASSIFICATION,// has the streamid/outputFields of the node it's doing the partitioning on 
Storm,WITHOUT_CLASSIFICATION,//  https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.5 
Storm,WITHOUT_CLASSIFICATION,//  let's checkpoint so that we can get the last checkpoint when restarting. 
Storm,WITHOUT_CLASSIFICATION,// allow requesting slots number bigger than available slots 
Storm,WITHOUT_CLASSIFICATION,//  activation expired list should contain even the ones expired due to EXPIRE_EVENTS_THRESHOLD 
Storm,WITHOUT_CLASSIFICATION,//  Not sure what could cause this.
Storm,WITHOUT_CLASSIFICATION,//  SPECIFIC 
Storm,WITHOUT_CLASSIFICATION,//  flag indicating HiveWriter was closed 
Storm,WITHOUT_CLASSIFICATION,//  a stream of (number cube) pairs 
Storm,WITHOUT_CLASSIFICATION,//  default 128 K 
Storm,WITHOUT_CLASSIFICATION,//  run default scheduler on non-isolated topologies 
Storm,WITHOUT_CLASSIFICATION,//  baseDir/supervisor/usercache/user1 
Storm,WITHOUT_CLASSIFICATION,//  list all daemon logs 
Storm,WITHOUT_CLASSIFICATION,//  we already checked this 
Storm,WITHOUT_CLASSIFICATION,//  add any autocredential expiry metrics from the worker 
Storm,WITHOUT_CLASSIFICATION,//  this ensures that list of values is always written the same way regardless   of whether it's a java collection or one of clojure's persistent collections    (which have different serializers)   Doing this lets us deserialize as ArrayList and avoid writing the class here 
Storm,WITHOUT_CLASSIFICATION,//  1) create couple of input files to read 
Storm,WITHOUT_CLASSIFICATION,// We follow the model of service loaders (Even though it is not a service). 
Storm,WITHOUT_CLASSIFICATION,//  should be at 6   double ack on same msg   should still be at 6 
Storm,WITHOUT_CLASSIFICATION,//  do not process events beyond current ts 
Storm,WITHOUT_CLASSIFICATION,// generate some supervisors that are depleted of one resource 
Storm,WITHOUT_CLASSIFICATION,//  2) run spout 
Storm,WITHOUT_CLASSIFICATION,//  Await all results 
Storm,WITHOUT_CLASSIFICATION,//  worker slot which was never back to normal in tolerance period will be removed from cache 
Storm,WITHOUT_CLASSIFICATION,//  ATTENTION: whb can be null
Storm,WITHOUT_CLASSIFICATION,// Waiting for this also ensures that the first tuple gets failed if reset-timeout doesn't work 
Storm,WITHOUT_CLASSIFICATION,//  propagate interrupt 
Storm,WITHOUT_CLASSIFICATION,//  save the metadata for all types of strings it matches 
Storm,WITHOUT_CLASSIFICATION,// populate node to component Assignments 
Storm,WITHOUT_CLASSIFICATION,//  for serialization 
Storm,WITHOUT_CLASSIFICATION,//  no wildcard directory 
Storm,WITHOUT_CLASSIFICATION,/*      * Just output the word value with a count of 1.      */
Storm,WITHOUT_CLASSIFICATION,//  send a watermark event which should trigger three windows. 
Storm,WITHOUT_CLASSIFICATION,//  Tests for case when subject == null (security turned off) and 
Storm,WITHOUT_CLASSIFICATION,//  Both things are not expected and should not happen.
Storm,WITHOUT_CLASSIFICATION,// thenReturn always returns the same object which is already consumed by the time User3 tries to getBlob! 
Storm,WITHOUT_CLASSIFICATION,//  process all metadata 
Storm,WITHOUT_CLASSIFICATION,//  Update the key.current symlink. First create tmp symlink and do 
Storm,WITHOUT_CLASSIFICATION,// arbitrary message to be returned when scheduling is done 
Storm,WITHOUT_CLASSIFICATION,//  If resource is also present in resources map will overwrite the above 
Storm,WITHOUT_CLASSIFICATION,// heartbeats "stats" 
Storm,WITHOUT_CLASSIFICATION,//  can be 'local' or 'shuffle' 
Storm,WITHOUT_CLASSIFICATION,// request to impersonate users from unauthroized groups should be rejected.
Storm,WITHOUT_CLASSIFICATION,// Lets reread the children in STORMS as the source of truth and see if a new one was created in the background 
Storm,WITHOUT_CLASSIFICATION,/*  Naive implementation but it might be good enough  */
Storm,WITHOUT_CLASSIFICATION,// This is a special case where the jar was not uploaded so we will not download it (it is already on the classpath) 
Storm,WITHOUT_CLASSIFICATION,//  DEPENDENCY_ARTIFACTS 
Storm,WITHOUT_CLASSIFICATION,/*                 * update the word counts in the state.                * Here the first argument 0L is the initial value for the state                * and the second argument is a function that adds the count to the current value in the state.                 */
Storm,WITHOUT_CLASSIFICATION,// Parallelism is same 
Storm,WITHOUT_CLASSIFICATION,// No need to search more it is not going to help.
Storm,WITHOUT_CLASSIFICATION,//  Would only happen at 2 PB so we are OK for now 
Storm,WITHOUT_CLASSIFICATION,//  Test for replication. 
Storm,WITHOUT_CLASSIFICATION,//  DIRECT 
Storm,WITHOUT_CLASSIFICATION,//  -- lock dir config 
Storm,WITHOUT_CLASSIFICATION,// Once every UPDATE_RATE_PERIOD_NS 
Storm,WITHOUT_CLASSIFICATION,// Kill the container and restart it 
Storm,WITHOUT_CLASSIFICATION,// avoid system dependent things 
Storm,WITHOUT_CLASSIFICATION,// For this part of the test we interleve the differnt rotation types. 
Storm,WITHOUT_CLASSIFICATION,//  guarantees a list of unused string Ids exists.  Once the list is empty creates a new list 
Storm,WITHOUT_CLASSIFICATION,//  reached EOF didnt read anything 
Storm,WITHOUT_CLASSIFICATION,//  Set the shard iterator for last fetched sequence number to start from correct position in shard 
Storm,WITHOUT_CLASSIFICATION,/*          * Don't wait for Timetrigger to fire since this could lead to timing issues in unit tests.         * Set it to a large value and trigger manually.          */
Storm,WITHOUT_CLASSIFICATION,//  bolt that subscribes to the intermediate bolt and publishes to a JMS Topic 
Storm,WITHOUT_CLASSIFICATION,// Just go on and try to delte the others 
Storm,WITHOUT_CLASSIFICATION,/*  tuple payload serializer is specified via configuration  */
Storm,WITHOUT_CLASSIFICATION,//  also called from processLogConfigChange 
Storm,WITHOUT_CLASSIFICATION,//  load the first part of entries 
Storm,WITHOUT_CLASSIFICATION,//  If future got interrupted exception we want to interrupt parent thread itself. 
Storm,WITHOUT_CLASSIFICATION,// Ignore changes to scheduling while downloading the topology blobs   We don't support canceling the download through the future yet   because pending blobs may be shared by multiple workers and cancel it   may lead to race condition   To keep everything in sync just wait for all workers 
Storm,WITHOUT_CLASSIFICATION,//  DOUBLE_ARG 
Storm,WITHOUT_CLASSIFICATION,// Node ID and supervisor ID are the same. 
Storm,WITHOUT_CLASSIFICATION,//  3) read 6th line and see if another log entry was made 
Storm,WITHOUT_CLASSIFICATION,//  5) read initial lines in file then check if lock exists 
Storm,WITHOUT_CLASSIFICATION,// Now lets get the creds for the topos so we can verify those as well. 
Storm,WITHOUT_CLASSIFICATION,//  used to recognize the pattern of active log files we may remove the "current" from this list 
Storm,WITHOUT_CLASSIFICATION,//  baseDir/supervisor/usercache/user1/filecache/archives 
Storm,WITHOUT_CLASSIFICATION,/*  * This class consists exclusively of static factory main.methods that create instances that are essential to work with the *  Jpmml library.  */
Storm,WITHOUT_CLASSIFICATION,//  11 seconds passed by not timing out 
Storm,WITHOUT_CLASSIFICATION,//  it's null if one of:     a) a later transaction batch was emitted before this so we should skip this batch     b) if didn't exist and was created (in which case the StateInitializer was invoked and        it was emitted
Storm,WITHOUT_CLASSIFICATION,//  end executor summary 
Storm,WITHOUT_CLASSIFICATION,//  Test whether the integer is a power of 2. 
Storm,WITHOUT_CLASSIFICATION,// Release things that don't need to wait for us to finish downloading. 
Storm,WITHOUT_CLASSIFICATION,//  RocksDB should insert in sorted key order 
Storm,WITHOUT_CLASSIFICATION,//  Updating file few times every 5 seconds 
Storm,WITHOUT_CLASSIFICATION,// Dropping the parallelism of the bolts to 3 instead of 11 so we can find a solution in a reasonable amount of work when backtracking. 
Storm,WITHOUT_CLASSIFICATION,// Emit and ack the rest 
Storm,WITHOUT_CLASSIFICATION,//  Configs for memory enforcement done by the supervisor (not cgroups directly) 
Storm,WITHOUT_CLASSIFICATION,//  ICredentialsRenewer 
Storm,WITHOUT_CLASSIFICATION,//  Only need to keep track of failed tuples if commits to Kafka are controlled by   tuple acks which happens only for at-least-once processing semantics 
Storm,WITHOUT_CLASSIFICATION,// TODO: file rotation 
Storm,WITHOUT_CLASSIFICATION,// Copy it in case we want to modify it 
Storm,WITHOUT_CLASSIFICATION,//  this is to read default value for other configurations 
Storm,WITHOUT_CLASSIFICATION,//  This should trigger the scan to find   the next aligned window end ts but not produce any activations 
Storm,WITHOUT_CLASSIFICATION,//  ARGS_LIST 
Storm,WITHOUT_CLASSIFICATION,//  consume both files 
Storm,WITHOUT_CLASSIFICATION,//  JMS Topic spout 
Storm,WITHOUT_CLASSIFICATION,// set up nimbus-info to zk 
Storm,WITHOUT_CLASSIFICATION,// What we want... 
Storm,WITHOUT_CLASSIFICATION,//  Gets Nimbus Subject with NimbusPrincipal set on it 
Storm,WITHOUT_CLASSIFICATION,//  include partition id in the file name so that index for different partitions are independent. 
Storm,WITHOUT_CLASSIFICATION,//  go with known best input 
Storm,WITHOUT_CLASSIFICATION,// Too fast not reported 
Storm,WITHOUT_CLASSIFICATION,//  if a record is returned put the sequence number in the emittedPerShard to tie back with ack or fail 
Storm,WITHOUT_CLASSIFICATION,//  1 for point1 4 for point2 
Storm,WITHOUT_CLASSIFICATION,//  Null: log any unhandled errors to stderr. 
Storm,WITHOUT_CLASSIFICATION,//  TARGET_LOG_LEVEL 
Storm,WITHOUT_CLASSIFICATION,// Creates a MongoClient described by a URI. 
Storm,WITHOUT_CLASSIFICATION,// Empty (Still 500) 
Storm,WITHOUT_CLASSIFICATION,// 1) Retire writers 
Storm,WITHOUT_CLASSIFICATION,//  IPrincipalToLocal  
Storm,WITHOUT_CLASSIFICATION,//  SentenceSpout --> MyBolt 
Storm,WITHOUT_CLASSIFICATION,//  if the Resource Aware Scheduler is used 
Storm,WITHOUT_CLASSIFICATION,//  since user derek has exceeded his resource guarantee while user jerry has not topo-5 or topo-4 could be evicted because they have the same priority 
Storm,WITHOUT_CLASSIFICATION,// class Offset 
Storm,WITHOUT_CLASSIFICATION,// The connection is not sent unless a response is requested 
Storm,WITHOUT_CLASSIFICATION,//  Now that the root is fine we can start to look at the other paths under it. 
Storm,WITHOUT_CLASSIFICATION,//  Convenience data structure to speedup lookups 
Storm,WITHOUT_CLASSIFICATION,//  Returns the recorded throughput since the last call to getCurrentThroughput()       or since this meter was instantiated if being called for fisrt time. 
Storm,WITHOUT_CLASSIFICATION,// NOOP  We could add in configs through the web.xml if we wanted something stand alone here... 
Storm,WITHOUT_CLASSIFICATION,//  Storm 
Storm,WITHOUT_CLASSIFICATION,//  offset where processing will resume upon spout restart 
Storm,WITHOUT_CLASSIFICATION,//  File offset and byte offset should always be zero when searching multiple workers (multiple ports). 
Storm,WITHOUT_CLASSIFICATION,//  1 grab lock 
Storm,WITHOUT_CLASSIFICATION,// Gets a collection. 
Storm,WITHOUT_CLASSIFICATION,// Blobs are not supported in local mode.  Return nothing 
Storm,WITHOUT_CLASSIFICATION,// Filtered negative value 
Storm,WITHOUT_CLASSIFICATION,// NOOP we don't actually want to change log levels for tests 
Storm,WITHOUT_CLASSIFICATION,//  transfer encoding should be set as jersey sets it on by default. 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_SHARED_ON_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  agg spout stats 
Storm,WITHOUT_CLASSIFICATION,//  Keep this constructor for backward compatibility 
Storm,WITHOUT_CLASSIFICATION,// NOOP on purpose 
Storm,WITHOUT_CLASSIFICATION,//  3) deleting closed file  - should return true 
Storm,WITHOUT_CLASSIFICATION,//  stream name is specified 
Storm,WITHOUT_CLASSIFICATION,//  validate 4 metrics (aggregations) found for m4 for all agglevels when searching by port 
Storm,WITHOUT_CLASSIFICATION,// assume it is a topology id 
Storm,WITHOUT_CLASSIFICATION,//  With no principals in the subject ACL should always be set to WORLD_EVERYTHING 
Storm,WITHOUT_CLASSIFICATION,//  ERROR_TIME_SECS 
Storm,WITHOUT_CLASSIFICATION,//  it shouldn't be happen
Storm,WITHOUT_CLASSIFICATION,//  TOPO_IDS 
Storm,WITHOUT_CLASSIFICATION,// Generously adapted from:  https://github.com/kijiproject/kiji-express/blob/master/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization   /AvroSerializer.scala  Which has as an ASL2.0 license 
Storm,WITHOUT_CLASSIFICATION,//  just ignore the exception 
Storm,WITHOUT_CLASSIFICATION,//  Receives msgs from remote workers and feeds them to local executors. If any receiving local executor is under Back Pressure 
Storm,WITHOUT_CLASSIFICATION,//  to avoid reordering of emits stop at first failure 
Storm,WITHOUT_CLASSIFICATION,//  If we didn't take GPUs into account everything would fit under a single slot   But because there is only 1 GPU per node and each of the 2 spouts needs a GPU   It has to be scheduled on at least 2 nodes and hence 2 slots.   Because of this all of the bolts will be scheduled on a single slot with one of   the spouts and the other spout is on its own slot.  So everything that can be shared is   shared. 
Storm,WITHOUT_CLASSIFICATION,//         producerFwdConsumer();      // -- measurement 5 
Storm,WITHOUT_CLASSIFICATION,//  we try uploading second one and it should be failed throwing RuntimeException 
Storm,WITHOUT_CLASSIFICATION,// start the thread pool 
Storm,WITHOUT_CLASSIFICATION,// The spout must be able to reemit all retriable tuples even if the maxPollRecords is set to a low value compared to maxUncommittedOffsets. 
Storm,WITHOUT_CLASSIFICATION,//  {stream id -> metric -> value} note that sid->out-stats may contain both long and double values 
Storm,WITHOUT_CLASSIFICATION,// All the events should be expired when the next watermark is received 
Storm,WITHOUT_CLASSIFICATION,//  Test for subject with no principals and acls set to DEFAULT 
Storm,WITHOUT_CLASSIFICATION,//  Copy the 2nd half of the buffer to the first half. 
Storm,WITHOUT_CLASSIFICATION,// Remove something randomly... 
Storm,WITHOUT_CLASSIFICATION,// We found the topology lets get the conf 
Storm,WITHOUT_CLASSIFICATION,//  since we made sys components visible the component map has all system components 
Storm,WITHOUT_CLASSIFICATION,// noop 
Storm,WITHOUT_CLASSIFICATION,/*      * Computes tumbling window average      */
Storm,WITHOUT_CLASSIFICATION,//  check lock file contents 
Storm,WITHOUT_CLASSIFICATION,//  QueryPlanner on Streams mode configures the topology with compiled classes   so we need to add new classes into topology jar   Topology will be serialized and sent to Nimbus and deserialized and executed in workers. 
Storm,WITHOUT_CLASSIFICATION,// All done we can launch the worker now 
Storm,WITHOUT_CLASSIFICATION,// login and also update the subject field of this instance to 
Storm,WITHOUT_CLASSIFICATION,//  map from stream name to batch id 
Storm,WITHOUT_CLASSIFICATION,//  Create symbolic link relative to tar parent dir 
Storm,WITHOUT_CLASSIFICATION,// Set the metrics sample rate to 1 to force update the executor stats every time something happens  This is necessary because getAllTimeEmittedCount relies on the executor emit stats to be accurate 
Storm,WITHOUT_CLASSIFICATION,// check if node is alive 
Storm,WITHOUT_CLASSIFICATION,// Even if the topology is not valid we still need to remap it all 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_ON_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,// Translating the name (this call) happens in a different callback from validating   the user name and password. This has to be stateless though so we cannot save   the password provider away to be sure we got the same one that validated the password.   If the password providers are written correctly this should never happen   because if they cannot read the name they would return a null.   But on the off chance that something goes wrong with the translation because of a mismatch   we try to skip the bad one. 
Storm,WITHOUT_CLASSIFICATION,// Now rebalance and add a new partition 
Storm,WITHOUT_CLASSIFICATION,// shutdown server process since we could not handle Thrift requests any more 
Storm,WITHOUT_CLASSIFICATION,//  when worker bootup worker will start to setup initial connections to   other workers. When all connection is ready we will count down this latch   and spout and bolt will be activated assuming the topology is not deactivated. 
Storm,WITHOUT_CLASSIFICATION,//  json parsing fail -> error received
Storm,WITHOUT_CLASSIFICATION,//  MEMORY_USAGE 
Storm,WITHOUT_CLASSIFICATION,//  object handling interaction with kinesis 
Storm,WITHOUT_CLASSIFICATION,//  spout stats 
Storm,WITHOUT_CLASSIFICATION,//  Spout implementation 
Storm,WITHOUT_CLASSIFICATION,//  ======== emit  ========= 
Storm,WITHOUT_CLASSIFICATION,//  BOLT_OBJECT 
Storm,WITHOUT_CLASSIFICATION,//  Use custom class loader set in testing environment 
Storm,WITHOUT_CLASSIFICATION,//  topo1 has one single huge task that can not be handled by the small-super
Storm,WITHOUT_CLASSIFICATION,// user derek submits another topology into a full cluster  topo6 should not be able to scheduled initially but since topo6 has higher priority than topo5  topo5 will be evicted so that topo6 can be scheduled 
Storm,WITHOUT_CLASSIFICATION,//  Throws IOExceptions for 3rd & 4th call to next() succeeds on 5th thereafter
Storm,WITHOUT_CLASSIFICATION,//  remove reverse lookup from map 
Storm,WITHOUT_CLASSIFICATION,// Now rebalance 
Storm,WITHOUT_CLASSIFICATION,//  scans the database to look for a metadata string and returns the metadata info 
Storm,WITHOUT_CLASSIFICATION,//  NUM_TASKS 
Storm,WITHOUT_CLASSIFICATION,// deprecated in favor of non-threaded RotatingMap 
Storm,WITHOUT_CLASSIFICATION,//  we trust that the file exists 
Storm,WITHOUT_CLASSIFICATION,// this test works because mocking a spout splits up the tuples evenly among the tasks 
Storm,WITHOUT_CLASSIFICATION,//  Mapping: from storm tuple -> rocketmq Message 
Storm,WITHOUT_CLASSIFICATION,//  USED_MEM 
Storm,WITHOUT_CLASSIFICATION,//  =====================================================================================   key transformers   ===================================================================================== 
Storm,WITHOUT_CLASSIFICATION,//   If any events are scheduled sleep until   event generation. If any recurring events   are scheduled then we will always go   through this branch sleeping only the   exact necessary amount of time. We give   an upper bound e.g. 1000 millis to the   sleeping time to limit the response time   for detecting any new event within 1 secs. 
Storm,WITHOUT_CLASSIFICATION,// We don't want to override the client if there is a thrift server up and running or we would not test any   Of the actual thrift code 
Storm,WITHOUT_CLASSIFICATION,//  each drpc request is always a single attempt 
Storm,WITHOUT_CLASSIFICATION,//  add to the callers cache.  We can't add it to the stringMetadataCache since that could cause an eviction   database write which we want to only occur from the inserting DB thread.
Storm,WITHOUT_CLASSIFICATION,// This partition is new and should start at the committed offset 
Storm,WITHOUT_CLASSIFICATION,//  assuming sideFields are preserving its order 
Storm,WITHOUT_CLASSIFICATION,// Blocking call 
Storm,WITHOUT_CLASSIFICATION,//  above gives an extra empty string at the end. below   removes that
Storm,WITHOUT_CLASSIFICATION,// remote address 
Storm,WITHOUT_CLASSIFICATION,//  So far it matches.  Keep going... 
Storm,WITHOUT_CLASSIFICATION,// So if we fail we are forced to try again 
Storm,WITHOUT_CLASSIFICATION,//  attempt to find the string in the database 
Storm,WITHOUT_CLASSIFICATION,//  Setup a test message 
Storm,WITHOUT_CLASSIFICATION,// Prior to the org.apache change 
Storm,WITHOUT_CLASSIFICATION,//  delete one worker of r000s000 (failed) from topo1 assignment to enable actual schedule for testing 
Storm,WITHOUT_CLASSIFICATION,//  actual ret is Map<String Map<String Long/Double>> 
Storm,WITHOUT_CLASSIFICATION,//  time to sleep between retries in milliseconds 
Storm,WITHOUT_CLASSIFICATION,// Ranked fifth since rack-2 has not cpu resources 
Storm,WITHOUT_CLASSIFICATION,//  time of last flush on this writer 
Storm,WITHOUT_CLASSIFICATION,// This is an attempt to give all of the streams an equal opportunity to emit something. 
Storm,WITHOUT_CLASSIFICATION,//  this is the end key for whole scan 
Storm,WITHOUT_CLASSIFICATION,//  Transfers messages destined to other workers 
Storm,WITHOUT_CLASSIFICATION,//  Make sure that we have received at least an integer (length) 
Storm,WITHOUT_CLASSIFICATION,//  NAME 
Storm,WITHOUT_CLASSIFICATION,//  test write again 
Storm,WITHOUT_CLASSIFICATION,// Will be closed automatically when shutting down the DFS cluster 
Storm,WITHOUT_CLASSIFICATION,//  should remove key1 
Storm,WITHOUT_CLASSIFICATION,//  if Batch spout then id contains txId 
Storm,WITHOUT_CLASSIFICATION,//  Called with a bad port (not in the config) No searching should be done. 
Storm,WITHOUT_CLASSIFICATION,//  for partitionpersist 
Storm,WITHOUT_CLASSIFICATION,// The dir is empty so try to delete it may fail but that is OK
Storm,WITHOUT_CLASSIFICATION,//  Empty class 
Storm,WITHOUT_CLASSIFICATION,//  seems this can fail by returning false or throwing exception   convert false ret value to exception 
Storm,WITHOUT_CLASSIFICATION,//  found the next offset to commit 
Storm,WITHOUT_CLASSIFICATION,// This is only used for logging/metrics. Don't crash the process over it.
Storm,WITHOUT_CLASSIFICATION,//  counter for spout wait strategy   counter for back pressure wait strategy 
Storm,WITHOUT_CLASSIFICATION,// Now lets create a token and verify that we can connect... 
Storm,WITHOUT_CLASSIFICATION,//  TODO log 
Storm,WITHOUT_CLASSIFICATION,// this validates the structure of the topology 
Storm,WITHOUT_CLASSIFICATION,//  note that port1Dir is active worker containing active logs 
Storm,WITHOUT_CLASSIFICATION,//  we may output many tuples for a given input tuple 
Storm,WITHOUT_CLASSIFICATION,//  Now do multiple 
Storm,WITHOUT_CLASSIFICATION,/*                      * If the user wants to explicitly set an auto offset reset policy we should respect it but when the spout is                     * configured for at-least-once processing we should default to seeking to the earliest offset in case there's an offset                     * out of range error rather than seeking to the latest (Kafka's default). This type of error will typically happen                     * when the consumer requests an offset that was deleted.                      */
Storm,WITHOUT_CLASSIFICATION,//  hash 
Storm,WITHOUT_CLASSIFICATION,//  Bookkeeping 
Storm,WITHOUT_CLASSIFICATION,//  Instances of this type are sent from NettyWorker to upstream WorkerTransfer to indicate BackPressure situation 
Storm,WITHOUT_CLASSIFICATION,//  assumption: there're no put and delete for same target in parameter list 
Storm,WITHOUT_CLASSIFICATION,// by default no need to do this as a different user 
Storm,WITHOUT_CLASSIFICATION,//  set the default heap memory size for supervisor-test 
Storm,WITHOUT_CLASSIFICATION,//  COMMON 
Storm,WITHOUT_CLASSIFICATION,// This is the first time so initialize the resources. 
Storm,WITHOUT_CLASSIFICATION,// We need to do enforcement on a topology level not a single worker level...   Because in for cgroups each page in shared memory goes to the worker that touched it   first. We may need to make this more plugable in the future and let the resource   isolation manager tell us what to do 
Storm,WITHOUT_CLASSIFICATION,/*  headers  */
Storm,WITHOUT_CLASSIFICATION,// Update to set the second assignment 
Storm,WITHOUT_CLASSIFICATION,//  create reader and do some checks 
Storm,WITHOUT_CLASSIFICATION,//  PRINCIPAL 
Storm,WITHOUT_CLASSIFICATION,//  Non-static impl main.methods exist for mocking purposes.
Storm,WITHOUT_CLASSIFICATION,// topo-2 evicted since user bobby don't have any resource guarantees and topo-2 is the next lowest priority for user bobby 
Storm,WITHOUT_CLASSIFICATION,//  ==================   Evaluator   ================== 
Storm,WITHOUT_CLASSIFICATION,//  REPLICATION_COUNT 
Storm,WITHOUT_CLASSIFICATION,// 3 delete file and retry creation 
Storm,WITHOUT_CLASSIFICATION,//  given/when 
Storm,WITHOUT_CLASSIFICATION,//  If the latch is not started yet start it 
Storm,WITHOUT_CLASSIFICATION,//  Offsets emitted are 01234<void>89 
Storm,WITHOUT_CLASSIFICATION,//  but topo-4 was submitted earlier thus we choose that one to evict (somewhat arbitrary) 
Storm,WITHOUT_CLASSIFICATION,/* DON'T include sys */
Storm,WITHOUT_CLASSIFICATION,//  storm topology name 
Storm,WITHOUT_CLASSIFICATION,//  defaults to INFO level when the logger isn't found previously 
Storm,WITHOUT_CLASSIFICATION,//  no specific reason to mock... this is one of easiest ways to make dummy instance 
Storm,WITHOUT_CLASSIFICATION,//  [[remoteTaskId] -> true/false : indicates if remote task is under BP. 
Storm,WITHOUT_CLASSIFICATION,// use the default server port. 
Storm,WITHOUT_CLASSIFICATION,// play and ack 1 tuple 
Storm,WITHOUT_CLASSIFICATION,//  emit the averages downstream 
Storm,WITHOUT_CLASSIFICATION,// for reporting errors
Storm,WITHOUT_CLASSIFICATION,//  common aggregate 
Storm,WITHOUT_CLASSIFICATION,//  Bind and start to accept incoming connections. 
Storm,WITHOUT_CLASSIFICATION,//  package access for unit test 
Storm,WITHOUT_CLASSIFICATION,//  This is a test where we are configured to point right at a single artifact 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   required   required   required   required   required   required 
Storm,WITHOUT_CLASSIFICATION,//  assume filter choices have been made and since no selection was made all levels are valid 
Storm,WITHOUT_CLASSIFICATION,// open Sasl transport with the login credential 
Storm,WITHOUT_CLASSIFICATION,//  3 let go first lock 
Storm,WITHOUT_CLASSIFICATION,// 10 min values 
Storm,WITHOUT_CLASSIFICATION,//  validate search by port 
Storm,WITHOUT_CLASSIFICATION,// merge with existing assignments 
Storm,WITHOUT_CLASSIFICATION,//  initialize state for batch 
Storm,WITHOUT_CLASSIFICATION,// Offset 0 to maxUncommittedOffsets - 2 are pending maxUncommittedOffsets - 1 is failed but not retriable 
Storm,WITHOUT_CLASSIFICATION,// null isTimedOut means worker never reported any heartbeat 
Storm,WITHOUT_CLASSIFICATION,//  rw-r--r-- 
Storm,WITHOUT_CLASSIFICATION,// One time scheduling. 
Storm,WITHOUT_CLASSIFICATION,//  Regardless of TICKET_RENEW_WINDOW setting above and the ticket expiry time   thread will not sleep between refresh attempts any less than 1 minute (60*1000 milliseconds = 1 minute). 
Storm,WITHOUT_CLASSIFICATION,// and to wait for the message to get through to the spout (acks use the same path as timeout resets) 
Storm,WITHOUT_CLASSIFICATION,// subprocesses must send their pid first thing 
Storm,WITHOUT_CLASSIFICATION,//  null for __system bolt 
Storm,WITHOUT_CLASSIFICATION,//  assume that Get doesn't have any families defined. this is for not digging deeply...
Storm,WITHOUT_CLASSIFICATION,// The best way to force backtracking is to change the heuristic so the components are reversed so it is hard   to find an answer. 
Storm,WITHOUT_CLASSIFICATION,// in local mode there is no jar 
Storm,WITHOUT_CLASSIFICATION,//  Extract spout resource info 
Storm,WITHOUT_CLASSIFICATION,//  Class path entries that are neither directories nor archives (.zip or JAR files)   nor the asterisk (*) wildcard character are ignored.
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_VERSION 
Storm,WITHOUT_CLASSIFICATION,//  Convert sources to a JSON serializable format 
Storm,WITHOUT_CLASSIFICATION,//  CPU_GUARANTEE 
Storm,WITHOUT_CLASSIFICATION,//  Toplogy:  WorGenSpout -> FieldsGrouping -> CountBolt 
Storm,WITHOUT_CLASSIFICATION,//  Be careful about adding additional tests as the dfscluster will be shared 
Storm,WITHOUT_CLASSIFICATION,//  1) Start metastore 
Storm,WITHOUT_CLASSIFICATION,// get transaction id  if it already exists and attempt id is greater than the attempt there 
Storm,WITHOUT_CLASSIFICATION,//  TYPE 
Storm,WITHOUT_CLASSIFICATION,// Set up committed so it looks like some messages have been committed on each partition 
Storm,WITHOUT_CLASSIFICATION,//  should not be returned since this executor is not part of the topology's assignment
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   required   required   required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,// JPROFILE START 
Storm,WITHOUT_CLASSIFICATION,// 1) Find retirement candidates 
Storm,WITHOUT_CLASSIFICATION,//  Committed offset i.e. the offset where processing will resume upon spout restart. Initially it is set to fetchOffset. 
Storm,WITHOUT_CLASSIFICATION,//  allow freq_sec to expire 
Storm,WITHOUT_CLASSIFICATION,//  schedule topo2 
Storm,WITHOUT_CLASSIFICATION,//  If FlatMapFunction is aware of prepare let it handle preparation 
Storm,WITHOUT_CLASSIFICATION,//  Advance time and then trigger call to kafka consumer commit 
Storm,WITHOUT_CLASSIFICATION,// Verify that the tuple is not emitted again 
Storm,WITHOUT_CLASSIFICATION,//  use this instead of storm's built in one so that we can specify a singleemitbatchtopartition   without knowledge of storm's internals 
Storm,WITHOUT_CLASSIFICATION,//  2 -  Setup HFS Bolt   -------- 
Storm,WITHOUT_CLASSIFICATION,//  we wait for 3 seconds 
Storm,WITHOUT_CLASSIFICATION,//  put global stream id for spouts 
Storm,WITHOUT_CLASSIFICATION,//  by using a monotonically increasing attempt id downstream tasks   can be memory efficient by clearing out state for old attempts   as soon as they see a higher attempt id for a transaction 
Storm,WITHOUT_CLASSIFICATION,//  how to partition for second stage of aggregation 
Storm,WITHOUT_CLASSIFICATION,//  required   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  Try the loader plugin if configured 
Storm,WITHOUT_CLASSIFICATION,//  bail out 
Storm,WITHOUT_CLASSIFICATION,// The user is not set so lets see what the request context is. 
Storm,WITHOUT_CLASSIFICATION,//  i being the position in the fields of this seq the remainder of the seq is the size 
Storm,WITHOUT_CLASSIFICATION,// not_jump (closed in strict mode) 
Storm,WITHOUT_CLASSIFICATION,/*      * Asserts that commitSync has been called once     * that there are only commits on one topic     * and that the committed offset covers messageCount messages      */
Storm,WITHOUT_CLASSIFICATION,// Allow poll if the partition is not at the maxUncommittedOffsets limit 
Storm,WITHOUT_CLASSIFICATION,// When is this ever null?
Storm,WITHOUT_CLASSIFICATION,//  read error and input streams as this would free up the buffers 
Storm,WITHOUT_CLASSIFICATION,// Test for replication with NIMBUS as user 
Storm,WITHOUT_CLASSIFICATION,// 3 
Storm,WITHOUT_CLASSIFICATION,/*      The below field declarations are also used in common.clj to define the event logger output fields       */
Storm,WITHOUT_CLASSIFICATION,//  RESULT 
Storm,WITHOUT_CLASSIFICATION,//  IAutoCredentials 
Storm,WITHOUT_CLASSIFICATION,//  topology.getbolt (AKA sys tasks most specifically __acker tasks) 
Storm,WITHOUT_CLASSIFICATION,//  WINDOW_TO_TRANSFERRED 
Storm,WITHOUT_CLASSIFICATION,// There is at most one schedule per message id 
Storm,WITHOUT_CLASSIFICATION,// Make sure that even though nextTuple() doesn't receive valid data  the offset will be checkpointed after checkpointInterval seconds. 
Storm,WITHOUT_CLASSIFICATION,//  retrieve any existing aggregation matching this one and update the values 
Storm,WITHOUT_CLASSIFICATION,// private constructor 
Storm,WITHOUT_CLASSIFICATION,//  taskIds first 
Storm,WITHOUT_CLASSIFICATION,//  keeps track of in flight tuples) 
Storm,WITHOUT_CLASSIFICATION,// logger.info("emit for partition " + partition.getId() + " offset=" + offset); 
Storm,WITHOUT_CLASSIFICATION,// 4 
Storm,WITHOUT_CLASSIFICATION,// This is because the tests are checking that a hard cap of maxUncommittedOffsets + maxPollRecords - 1 uncommitted offsets exists 
Storm,WITHOUT_CLASSIFICATION,// Good so far check if we are in a CGroup 
Storm,WITHOUT_CLASSIFICATION,//  supervisor which was never back to normal in tolerance period will be removed from cache 
Storm,WITHOUT_CLASSIFICATION,//  maybe it needs a start phase (where it can do a retrieval an update phase and then a finish phase...?   shouldn't really be a one-at-a-time interface since we have all the tuples already?   TOOD: used for the new values stream   the list is needed to be able to get reduceragg and combineragg persistentaggregate   for grouped streams working efficiently
Storm,WITHOUT_CLASSIFICATION,//  in java classpath '*' is expanded to all jar/JAR files in the directory 
Storm,WITHOUT_CLASSIFICATION,// verify that offset 4 was committed for the given TopicPartition since processing should resume at 4. 
Storm,WITHOUT_CLASSIFICATION,//  could not lock so try another file. 
Storm,WITHOUT_CLASSIFICATION,//  use the hash index for prefix searches 
Storm,WITHOUT_CLASSIFICATION,// 3 4 compacted away 
Storm,WITHOUT_CLASSIFICATION,// DONE 
Storm,WITHOUT_CLASSIFICATION,// check if Map 
Storm,WITHOUT_CLASSIFICATION,// Tuples may be acked/failed after the spout deactivates so we have to be able to handle this too
Storm,WITHOUT_CLASSIFICATION,//  zkHostString for Solr gettingstarted example 
Storm,WITHOUT_CLASSIFICATION,/*  Not a Blocking call. If cannot emit will add 'tuple' to 'pendingEmits' and return 'false'. 'pendingEmits' can be null  */
Storm,WITHOUT_CLASSIFICATION,// Waiting to be fetched 
Storm,WITHOUT_CLASSIFICATION,//  a stream of (number square) pairs 
Storm,WITHOUT_CLASSIFICATION,//  OUTPUT_FIELDS 
Storm,WITHOUT_CLASSIFICATION,//  Timers 
Storm,WITHOUT_CLASSIFICATION,// 5 
Storm,WITHOUT_CLASSIFICATION,//  Do not use canonical file name here as we are using   symbolic links to read file data and performing atomic move   while updating files
Storm,WITHOUT_CLASSIFICATION,//  Initiate connection to remote destination 
Storm,WITHOUT_CLASSIFICATION,//  Assume that there could be a worker already on the node that is under the minWorkerCpu budget.   It's possible we could combine with it.  Let's disregard minWorkerCpu from the request   and validate that CPU as a rough fit. 
Storm,WITHOUT_CLASSIFICATION,//  by generating a list of random numbers and removing the ones that already are in use. 
Storm,WITHOUT_CLASSIFICATION,//  Client API to invoke blob store API functionality 
Storm,WITHOUT_CLASSIFICATION,/*  DELETE EVERYTHING IN HDFS  */
Storm,WITHOUT_CLASSIFICATION,//  do not send upstream to other handlers: no further action needs 
Storm,WITHOUT_CLASSIFICATION,//  Test class to override the write directory 
Storm,WITHOUT_CLASSIFICATION,// Retry once after a minute 
Storm,WITHOUT_CLASSIFICATION,//  update this only after writing to hdfs 
Storm,WITHOUT_CLASSIFICATION,//  Ranked second since rack-1 has a balanced set of resources but less than rack-0 
Storm,WITHOUT_CLASSIFICATION,// 6 
Storm,WITHOUT_CLASSIFICATION,//  comma separated list of topics   consumer group id for which the offset needs to be calculated   bootstrap brokers   security protocol to connect to kafka   properties file containing additional kafka consumer configs
Storm,WITHOUT_CLASSIFICATION,//  configs - hdfs bolt 
Storm,WITHOUT_CLASSIFICATION,//  aggregating metric did not exist don't look for further ones with smaller timestamps 
Storm,WITHOUT_CLASSIFICATION,//  rotate files when they reach 5MB 
Storm,WITHOUT_CLASSIFICATION,// create an authentication callback handler 
Storm,WITHOUT_CLASSIFICATION,//  port test-shuffle-load-even 
Storm,WITHOUT_CLASSIFICATION,//  given 
Storm,WITHOUT_CLASSIFICATION,//  consume file 1 
Storm,WITHOUT_CLASSIFICATION,// (Requested + Assigned - Guaranteed)/Available 
Storm,WITHOUT_CLASSIFICATION,// JPROFILE DUMP 
Storm,WITHOUT_CLASSIFICATION,// 7 
Storm,WITHOUT_CLASSIFICATION,// We are launching it now... 
Storm,WITHOUT_CLASSIFICATION,// Tests that isScheduled isReady and earliestRetriableOffsets are mutually consistent when there are multiple messages scheduled on a partition 
Storm,WITHOUT_CLASSIFICATION,//  can be UpdateStateByKey or StateQuery processors 
Storm,WITHOUT_CLASSIFICATION,//  These are mandatory parameters 
Storm,WITHOUT_CLASSIFICATION,/*              * scan the entire window to handle out of order events in             * the case of time based windows.              */
Storm,WITHOUT_CLASSIFICATION,//  Reader type config 
Storm,WITHOUT_CLASSIFICATION,//  WINDOW_TO_STATS 
Storm,WITHOUT_CLASSIFICATION,// find number of constraints per component  Key->Comp Value-># of constraints 
Storm,WITHOUT_CLASSIFICATION,//  Set up the in-memory filesystem. 
Storm,WITHOUT_CLASSIFICATION,// whether find all documents according to the query filter  updateBolt.withMany(true); 
Storm,WITHOUT_CLASSIFICATION,//  Metrics are off verify null 
Storm,WITHOUT_CLASSIFICATION,// Check that if one message fails repeatedly the retry cap limits how many times the message can be reemitted 
Storm,WITHOUT_CLASSIFICATION,// mock state store and receiver 
Storm,WITHOUT_CLASSIFICATION,//  test commit second creates properly 
Storm,WITHOUT_CLASSIFICATION,//  out of order events should be processed upto the lag 
Storm,WITHOUT_CLASSIFICATION,// Process has not terminated.  So check if it has completed  if not just destroy it. 
Storm,WITHOUT_CLASSIFICATION,//  don't update unless there are tuples   this helps out with things like global partition persist where multiple tasks may still   exist for this processor. Only want the global one to do anything 
Storm,WITHOUT_CLASSIFICATION,// Compute the stats for the different input streams 
Storm,WITHOUT_CLASSIFICATION,// Because the simple topology was scheduled first we want to be sure that it didn't put anything on   the GPU nodes. 
Storm,WITHOUT_CLASSIFICATION,//  wait for available queue 
Storm,WITHOUT_CLASSIFICATION,//  USER_NAME 
Storm,WITHOUT_CLASSIFICATION,// Make sure we can store the worker tokens even if no creds are provided. 
Storm,WITHOUT_CLASSIFICATION,//  this bolt does not emit anything 
Storm,WITHOUT_CLASSIFICATION,// Should only happen on a badly configured system 
Storm,WITHOUT_CLASSIFICATION,//  ignores invalid user/topo/key 
Storm,WITHOUT_CLASSIFICATION,// are cleared 
Storm,WITHOUT_CLASSIFICATION,//  return false to stop scan 
Storm,WITHOUT_CLASSIFICATION,//   Submit topology to storm cluster 
Storm,WITHOUT_CLASSIFICATION,// Prevent daemon log reads from pathing into worker logs 
Storm,WITHOUT_CLASSIFICATION,//  TOTAL_NODE_SHARED 
Storm,WITHOUT_CLASSIFICATION,// @{link org.apache.storm.trident.planner.Node} and several other trident classes inherit from DefaultResourceDeclarer   These classes are serialized out as part of the bolts and spouts of a topology often for each bolt/spout in the topology.   The following are marked as transient because they are never used after the topology is created so keeping them around just wastes   space in the serialized topology
Storm,WITHOUT_CLASSIFICATION,//  Read once. Since the first file is empty the spout should continue with file 2 
Storm,WITHOUT_CLASSIFICATION,// fail supervisor 
Storm,WITHOUT_CLASSIFICATION,//  update blob interface 
Storm,WITHOUT_CLASSIFICATION,//  default is aways "distributed" but here local cluster is being used. 
Storm,WITHOUT_CLASSIFICATION,// This is cheating a bit since maxPollRecords would normally spread this across multiple polls 
Storm,WITHOUT_CLASSIFICATION,//  Spy object that tries to mock the real object store 
Storm,WITHOUT_CLASSIFICATION,// Should not show files outside log root. 
Storm,WITHOUT_CLASSIFICATION,//  WORKER_HEARTBEATS 
Storm,WITHOUT_CLASSIFICATION,//  Zero-out the 2nd half to prevent accidental matches. 
Storm,WITHOUT_CLASSIFICATION,// Sleep a bit between emits to ensure that we don't reach the cap too quickly since this spout is used to test time based windows 
Storm,WITHOUT_CLASSIFICATION,//  transferred totals 
Storm,WITHOUT_CLASSIFICATION,//  consume file 2 
Storm,WITHOUT_CLASSIFICATION,//  test listkeys 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_TOTAL_MEMORY 
Storm,WITHOUT_CLASSIFICATION,// Congested we contract much more quickly 
Storm,WITHOUT_CLASSIFICATION,//  setup Broker 
Storm,WITHOUT_CLASSIFICATION,//  bolt aggregate 
Storm,WITHOUT_CLASSIFICATION,//  memory requirement is large enough so that two executors can not be fully assigned to one node 
Storm,WITHOUT_CLASSIFICATION,//  
Storm,WITHOUT_CLASSIFICATION,//  Now let's load w/o setting up gets and we should still get valid map back 
Storm,WITHOUT_CLASSIFICATION,// DISABLE_LOGIN_CACHE indicates whether or not to use the LoginCache.  So we exclude it from the keyString 
Storm,WITHOUT_CLASSIFICATION,//  if commands contains one or more null value spout is compiled with lower version of storm-kafka-client 
Storm,WITHOUT_CLASSIFICATION,//  NAMED_LOGGER_LEVEL 
Storm,WITHOUT_CLASSIFICATION,//  NOTE: the queue has to be thread safe.
Storm,WITHOUT_CLASSIFICATION,// Access foo to make it most recently used 
Storm,WITHOUT_CLASSIFICATION,//  Other scenario it covers is when max-seq-number and nimbus seq number are equal. 
Storm,WITHOUT_CLASSIFICATION,/*                 * compute the word counts in the last two second window                 */
Storm,WITHOUT_CLASSIFICATION,//  Dispatches to the right join method (inner/left/right/outer) based on the joinInfo.joinType 
Storm,WITHOUT_CLASSIFICATION,// partition info does not change in EventHub 
Storm,WITHOUT_CLASSIFICATION,//  TIMESTAMP 
Storm,WITHOUT_CLASSIFICATION,//  3 seconds 
Storm,WITHOUT_CLASSIFICATION,//  add all fetched records to the set of failed records if they are present in failed set 
Storm,WITHOUT_CLASSIFICATION,//  used by Stream#persistentAggregate 
Storm,WITHOUT_CLASSIFICATION,//  Create multiple copies of a test topology 
Storm,WITHOUT_CLASSIFICATION,// Now we need to map them all back again 
Storm,WITHOUT_CLASSIFICATION,//  Now compile! 
Storm,WITHOUT_CLASSIFICATION,//  EVENTLOG_PORT 
Storm,WITHOUT_CLASSIFICATION,// sync zk assignments/id-info to local 
Storm,WITHOUT_CLASSIFICATION,//  ensure 5 fields per tuple and no null fields 
Storm,WITHOUT_CLASSIFICATION,// Offset 0 is committed 1 to maxUncommittedOffsets-1 are failed maxUncommittedOffsets to maxUncommittedOffsets + maxPollRecords-1 are emitted  Fail the last tuples so only offset 0 is not failed.  Advance time so the failed tuples become ready for retry and check that the spout will emit retriable tuples  for all the failed tuples that are within maxUncommittedOffsets tuples of the committed offset 
Storm,WITHOUT_CLASSIFICATION,//  assume key is the first field 
Storm,WITHOUT_CLASSIFICATION,//  use redis for state persistence 
Storm,WITHOUT_CLASSIFICATION,// Found an existing token and it is not going to expire any time soon so don't bother adding in a new   token. 
Storm,WITHOUT_CLASSIFICATION,//  spout notified that message returned by us for retrying was actually emitted. hence remove it from set and   wait for its ack or fail   but still keep it in counts map to retry again on failure or remove on ack 
Storm,WITHOUT_CLASSIFICATION,//  current version supports RPC heartbeat 
Storm,WITHOUT_CLASSIFICATION,//  certain states might only accept one-tuple keys - those should just throw an error  
Storm,WITHOUT_CLASSIFICATION,// Set the size in case we are recovering an already downloaded object 
Storm,WITHOUT_CLASSIFICATION,//  find primary key from constructor 
Storm,WITHOUT_CLASSIFICATION,// sort executors based on component constraints 
Storm,WITHOUT_CLASSIFICATION,//  should be at 5 
Storm,WITHOUT_CLASSIFICATION,//  Config settings 
Storm,WITHOUT_CLASSIFICATION,//  Test1: Launch topo 1-3 together it should be able to use up either mem or cpu resource due to exact division 
Storm,WITHOUT_CLASSIFICATION,//  File offset should be zero since search-archived is false. 
Storm,WITHOUT_CLASSIFICATION,//  ACCESS 
Storm,WITHOUT_CLASSIFICATION,// The spout should now commit all the offsets since all offsets are either acked or were missing when retrying 
Storm,WITHOUT_CLASSIFICATION,//  Servers should be rotated **BEFORE** the old client is removed from clientForServer   or a race with getWriteClient() could cause it to be put back in the map. 
Storm,WITHOUT_CLASSIFICATION,//  now test regular updateBlob 
Storm,WITHOUT_CLASSIFICATION,//  (Re-)Cancel if current thread also interrupted 
Storm,WITHOUT_CLASSIFICATION,// Also fail the last tuple from partition two. Since the failed tuple is beyond the maxUncommittedOffsets limit it should not be retried until earlier messages are acked. 
Storm,WITHOUT_CLASSIFICATION,// Test3: "Launch topo5 only both mem and cpu should be exactly used up" 
Storm,WITHOUT_CLASSIFICATION,//  => field name does not match static field test if it matches dynamic field 
Storm,WITHOUT_CLASSIFICATION,// sleep to prevent race conditions 
Storm,WITHOUT_CLASSIFICATION,//  weak supervisor node 
Storm,WITHOUT_CLASSIFICATION,// default is ordered. 
Storm,WITHOUT_CLASSIFICATION,//  ensure the loggers are configured in the worker.xml before   trying to use them here 
Storm,WITHOUT_CLASSIFICATION,//  release lock on file1 and check 
Storm,WITHOUT_CLASSIFICATION,// Also put a generic resource with 0 value in the resources list to verify that it doesn't affect the sorting 
Storm,WITHOUT_CLASSIFICATION,//  OFF_HEAP_NODE 
Storm,WITHOUT_CLASSIFICATION,//  It's OK if new-byte-offset is negative.   This is normal if we are out of bytes to read from a small file. 
Storm,WITHOUT_CLASSIFICATION,//  don't bother blocking on a full queue just drop metrics in case we can't keep up 
Storm,WITHOUT_CLASSIFICATION,//  Update scenario 2 and 3 explain the code logic written here   especially when nimbus crashes and comes up after and before update 
Storm,WITHOUT_CLASSIFICATION,//  0 means DEFAULT_EVENT_LOOP_THREADS   https://github.com/netty/netty/blob/netty-4.1.24.Final/transport/src/main/java/io/netty/channel/MultithreadEventLoopGroup.java#L40 
Storm,WITHOUT_CLASSIFICATION,//  save the evicted key/value to the database immediately 
Storm,WITHOUT_CLASSIFICATION,//  it should be org.apache.storm:flux-core:jar:1.0.0 and commons-cli:commons-cli:jar:1.2 
Storm,WITHOUT_CLASSIFICATION,//  LOCAL_OR_SHUFFLE 
Storm,WITHOUT_CLASSIFICATION,// The sym link we are pointing to 
Storm,WITHOUT_CLASSIFICATION,// If CPU or memory is not set the values stored in topology.component.resources.onheap.memory.mb   topology.component.resources.offheap.memory.mb and topology.component.cpu.pcore.percent 
Storm,WITHOUT_CLASSIFICATION,//  initialize server-side SASL functionality if we haven't yet   (in which case we are looking at the first SASL message from the   client). 
Storm,WITHOUT_CLASSIFICATION,//  resend BP status in case prev notification was missed or reordered 
Storm,WITHOUT_CLASSIFICATION,//  slots will be null while supervisor has been removed from cached supervisors 
Storm,WITHOUT_CLASSIFICATION,// we just return the first metric we meet 
Storm,WITHOUT_CLASSIFICATION,//  boolean cache for local mode decision 
Storm,WITHOUT_CLASSIFICATION,// When authNid and authZid are not equal  authNId is attempting to impersonate authZid We 
Storm,WITHOUT_CLASSIFICATION,//  Kinesis Spout KinesisConfig object 
Storm,WITHOUT_CLASSIFICATION,//  SPOUTS 
Storm,WITHOUT_CLASSIFICATION,//  add new ports to cached supervisor.  We need a modifiable set to allow removing ports later. 
Storm,WITHOUT_CLASSIFICATION,//  BINARY_ARG 
Storm,WITHOUT_CLASSIFICATION,//  JMS Producer 
Storm,WITHOUT_CLASSIFICATION,//  for a failed message add it to failed set if it will be retried otherwise ack it; remove from emitted either way 
Storm,WITHOUT_CLASSIFICATION,//  if there are no trigger values in earlier attempts or this is a new batch emit pending triggers. 
Storm,WITHOUT_CLASSIFICATION,//  when 
Storm,WITHOUT_CLASSIFICATION,//  This code logic covers the update scenarios in 2 when the nimbus-1 goes down   before syncing the blob to nimbus-2 and an update happens.   If seq-num for nimbus-2 is 2 and max-seq-number is 3 then next sequence number is 4   (max-seq-number + 1). 
Storm,WITHOUT_CLASSIFICATION,//  We should be done now... 
Storm,WITHOUT_CLASSIFICATION,//  getters 
Storm,WITHOUT_CLASSIFICATION,//  trigger the window 
Storm,WITHOUT_CLASSIFICATION,//  offset 2   offset 6   offset 14   offset 18   offset 22   offset 26   offset 30   offset 34 
Storm,WITHOUT_CLASSIFICATION,//  Non Blocking call. If cannot emit to destination immediately such tuples will be added to `pendingEmits` argument 
Storm,WITHOUT_CLASSIFICATION,//  renewal thread's main loop. if it exits from here thread will exit. 
Storm,WITHOUT_CLASSIFICATION,//  validate 
Storm,WITHOUT_CLASSIFICATION,//  Shutdownable main.methods
Storm,WITHOUT_CLASSIFICATION,// new supervisor to cache 
Storm,WITHOUT_CLASSIFICATION,//  need to get ACL from meta 
Storm,WITHOUT_CLASSIFICATION,//  Test4: Scheduling a new topology does not disturb other assignments unnecessarily 
Storm,WITHOUT_CLASSIFICATION,//  WINDOW_TO_EMITTED 
Storm,WITHOUT_CLASSIFICATION,//  The encryption key must be hexadecimal. 
Storm,WITHOUT_CLASSIFICATION,// Execute and process latency... 
Storm,WITHOUT_CLASSIFICATION,// There should now be maxUncommittedOffsets + maxPollRecords emitted in all. 
Storm,WITHOUT_CLASSIFICATION,//  first just set the keys to null then flag to remove them at beginning of next commit when we know the current and last value   are both null 
Storm,WITHOUT_CLASSIFICATION,/*         Without fragmentation the cluster would be able to schedule both topologies on each node. Let's call each node        with both topologies scheduled as 100% scheduled.        We schedule the cluster in 3 blocks of topologies measuring the time to schedule the blocks. The first middle        and last blocks attempt to schedule the following 0-10% 10%-90% 90%-100%. The last block has a number of        scheduling failures due to cluster fragmentation and its time is dominated by attempting to evict topologies.        Timing results for scheduling are noisy. As a result we do multiple runs and use median values for FirstBlock        and LastBlock times. (somewhere a statistician is crying). The ratio of LastBlock / FirstBlock remains fairly constant.        TestLargeFragmentedClusterScheduling took 91118 ms        DefaultResourceAwareStrategy FirstBlock 249.0 LastBlock 1734.0 ratio 6.963855421686747        GenericResourceAwareStrategy FirstBlock 215.0 LastBlock 1673.0 ratio 7.78139534883721        ConstraintSolverStrategy FirstBlock 279.0 LastBlock 2200.0 ratio 7.885304659498208        TestLargeFragmentedClusterScheduling took 98455 ms        DefaultResourceAwareStrategy FirstBlock 266.0 LastBlock 1812.0 ratio 6.81203007518797        GenericResourceAwareStrategy FirstBlock 235.0 LastBlock 1802.0 ratio 7.6680851063829785        ConstraintSolverStrategy FirstBlock 304.0 LastBlock 2320.0 ratio 7.631578947368421        TestLargeFragmentedClusterScheduling took 97268 ms        DefaultResourceAwareStrategy FirstBlock 251.0 LastBlock 1826.0 ratio 7.274900398406374        GenericResourceAwareStrategy FirstBlock 220.0 LastBlock 1719.0 ratio 7.8136363636363635        ConstraintSolverStrategy FirstBlock 296.0 LastBlock 2469.0 ratio 8.341216216216216        TestLargeFragmentedClusterScheduling took 97963 ms        DefaultResourceAwareStrategy FirstBlock 249.0 LastBlock 1788.0 ratio 7.180722891566265        GenericResourceAwareStrategy FirstBlock 240.0 LastBlock 1796.0 ratio 7.483333333333333        ConstraintSolverStrategy FirstBlock 328.0 LastBlock 2544.0 ratio 7.7560975609756095        TestLargeFragmentedClusterScheduling took 93106 ms        DefaultResourceAwareStrategy FirstBlock 258.0 LastBlock 1714.0 ratio 6.6434108527131785        GenericResourceAwareStrategy FirstBlock 215.0 LastBlock 1692.0 ratio 7.869767441860465        ConstraintSolverStrategy FirstBlock 309.0 LastBlock 2342.0 ratio 7.5792880258899675        Choose the median value of the values above        DefaultResourceAwareStrategy    6.96        GenericResourceAwareStrategy    7.78        ConstraintSolverStrategy        7.75         */
Storm,WITHOUT_CLASSIFICATION,//  find the number of bytes with non-leading zeros 
Storm,WITHOUT_CLASSIFICATION,//  check avoids multiple log msgs when in a idle loop 
Storm,WITHOUT_CLASSIFICATION,//  schedule left over system tasks 
Storm,WITHOUT_CLASSIFICATION,//  this doesn't follow symlinks which is what we want 
Storm,WITHOUT_CLASSIFICATION,//  Build put query 
Storm,WITHOUT_CLASSIFICATION,//  window partitions 
Storm,WITHOUT_CLASSIFICATION,//  3) Take ownership of stale lock 
Storm,WITHOUT_CLASSIFICATION,//  We are capturing exceptions thrown in Blitzer's child threads into this data structure so that we can properly   pass/fail this test.  The reason is that Blitzer doesn't report exceptions which is a known bug in Blitzer 
Storm,WITHOUT_CLASSIFICATION,//  set size to cleanup another one 
Storm,WITHOUT_CLASSIFICATION,// Should not seek on the paused partition 
Storm,WITHOUT_CLASSIFICATION,// Verify digest is rejected... 
Storm,WITHOUT_CLASSIFICATION,// System bolt is not a part of backpressure. 
Storm,WITHOUT_CLASSIFICATION,//  check just the one port 
Storm,WITHOUT_CLASSIFICATION,//  add with current ts 
Storm,WITHOUT_CLASSIFICATION,//  Required 
Storm,WITHOUT_CLASSIFICATION,//  value of the metric 
Storm,WITHOUT_CLASSIFICATION,//  overflowQ size at the time the last BPStatus was sent 
Storm,WITHOUT_CLASSIFICATION,//  tests download of topology blobs in local mode on a topology without resources folder 
Storm,WITHOUT_CLASSIFICATION,//  Retry locking and verify 
Storm,WITHOUT_CLASSIFICATION,//  default 
Storm,WITHOUT_CLASSIFICATION,//  contents of the key starts with nimbus host port information 
Storm,WITHOUT_CLASSIFICATION,//  context is not used by the default implementation but is included   in the interface in case it is useful to subclasses 
Storm,WITHOUT_CLASSIFICATION,//  if we get more than one stateful operation we need to process the   current group so that we have one stateful operation per stateful bolt 
Storm,WITHOUT_CLASSIFICATION,//  do nothing for conf now 
Storm,WITHOUT_CLASSIFICATION,//  Add our messages and verify no metrics are recorded   
Storm,WITHOUT_CLASSIFICATION,// Looks like usage might not be supported 
Storm,WITHOUT_CLASSIFICATION,// Ensure Nimbus assigns topologies as quickly as possible 
Storm,WITHOUT_CLASSIFICATION,//  Put in legacy values 
Storm,WITHOUT_CLASSIFICATION,// Try to get the topology conf from nimbus so we can reuse it. 
Storm,WITHOUT_CLASSIFICATION,//  FUNC_ARGS 
Storm,WITHOUT_CLASSIFICATION,//  use "|" instead of "" for field delimiter 
Storm,WITHOUT_CLASSIFICATION,//  complexity is that of a linear scan on a TreeMap 
Storm,WITHOUT_CLASSIFICATION,// nextOffset is the last offset from last batch + 1 
Storm,WITHOUT_CLASSIFICATION,//  spout aggregate 
Storm,WITHOUT_CLASSIFICATION,// use storm's zookeeper servers if not specified. 
Storm,WITHOUT_CLASSIFICATION,//  clean up memory 
Storm,WITHOUT_CLASSIFICATION,//  If configs are present in Generic map and legacy - the legacy values will be overwritten 
Storm,WITHOUT_CLASSIFICATION,//  for each isolated topology:     compute even distribution of executors -> workers on the number of workers specified for the topology     compute distribution of workers to machines   determine host -> list of [slot topology id executors]   iterate through hosts and: a machine is good if:     1. only running workers from one isolated topology     2. all workers running on it match one of the distributions of executors for that topology     3. matches one of the # of workers   blacklist the good hosts and remove those workers from the list of need to be assigned workers   otherwise unassign all other workers for isolated topologies if assigned 
Storm,WITHOUT_CLASSIFICATION,// Now check that the spout will not emit anything else since nothing has been committed 
Storm,WITHOUT_CLASSIFICATION,// == 4 
Storm,WITHOUT_CLASSIFICATION,//  NONE 
Storm,WITHOUT_CLASSIFICATION,//  the metrics store is not critical to the operation of the cluster allow Nimbus to come up 
Storm,WITHOUT_CLASSIFICATION,// check the credential of our principal 
Storm,WITHOUT_CLASSIFICATION,//  System bolt doesn't call reportError() 
Storm,WITHOUT_CLASSIFICATION,//  join against diff stream compared to testThreeStreamLeftJoin_1 
Storm,WITHOUT_CLASSIFICATION,//  parsed Topology definition 
Storm,WITHOUT_CLASSIFICATION,//  compile parameters 
Storm,WITHOUT_CLASSIFICATION,//  check for all ports 
Storm,WITHOUT_CLASSIFICATION,// streaId indicates where tuple came from 
Storm,WITHOUT_CLASSIFICATION,// For the node we don't know if we have another one unless we look at the contents 
Storm,WITHOUT_CLASSIFICATION,//  WORKER_HOOKS 
Storm,WITHOUT_CLASSIFICATION,//  validate search by stream id 
Storm,WITHOUT_CLASSIFICATION,//  DEBUG_OPTIONS 
Storm,WITHOUT_CLASSIFICATION,//  Storm support to launch workers of older version.   If the config of TOPOLOGY_SCHEDULER_STRATEGY comes from the older version replace the package name. 
Storm,WITHOUT_CLASSIFICATION,// Update the watermark to this timestamp 
Storm,WITHOUT_CLASSIFICATION,//  create spouts 
Storm,WITHOUT_CLASSIFICATION,//  need to wait until sasl channel is also ready 
Storm,WITHOUT_CLASSIFICATION,//  perf critical path. would be nice to avoid iterator allocation here and below 
Storm,WITHOUT_CLASSIFICATION,//  all column families 
Storm,WITHOUT_CLASSIFICATION,// generate some that has alot of memory but little of cpu 
Storm,WITHOUT_CLASSIFICATION,//  baseDir/supervisor/usercache/user1/ 
Storm,WITHOUT_CLASSIFICATION,//  userSpout ==> jdbcBolt 
Storm,WITHOUT_CLASSIFICATION,//  all things are from dependencies 
Storm,WITHOUT_CLASSIFICATION,// do nothing 
Storm,WITHOUT_CLASSIFICATION,//  test exist with non-existent key 
Storm,WITHOUT_CLASSIFICATION,//  Testing whether acls are set to WORLD_EVERYTHING 
Storm,WITHOUT_CLASSIFICATION,//  iterate through all executor heartbeats 
Storm,WITHOUT_CLASSIFICATION,// There are free slots that we can take advantage of now. 
Storm,WITHOUT_CLASSIFICATION,// need more data 
Storm,WITHOUT_CLASSIFICATION,//  PORT 
Storm,WITHOUT_CLASSIFICATION,// register call back for blob-store 
Storm,WITHOUT_CLASSIFICATION,//  TASK_START 
Storm,WITHOUT_CLASSIFICATION,//  Tests for case when subject != null (security turned on) and   acls for the blob are set to WORLD_EVERYTHING 
Storm,WITHOUT_CLASSIFICATION,// RESTART 
Storm,WITHOUT_CLASSIFICATION,//  Performs projection on the tuples based on 'projectionFields' 
Storm,WITHOUT_CLASSIFICATION,// We are running so we should recover the blobs. 
Storm,WITHOUT_CLASSIFICATION,// Something happened and we couldn't find the file so ignore it for now. 
Storm,WITHOUT_CLASSIFICATION,//  1 -  Setup Spout   -------- 
Storm,WITHOUT_CLASSIFICATION,//  callback to caller 
Storm,WITHOUT_CLASSIFICATION,//  Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because   the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have 
Storm,WITHOUT_CLASSIFICATION,//  INPUTS 
Storm,WITHOUT_CLASSIFICATION,//  3) make sure 6 lines (3 from each file) were read in all 
Storm,WITHOUT_CLASSIFICATION,//  1st topology 
Storm,WITHOUT_CLASSIFICATION,//  look for a public instance variable 
Storm,WITHOUT_CLASSIFICATION,//  need one large set of all and then clean via LRU 
Storm,WITHOUT_CLASSIFICATION,//  In all other cases check for the latest update sequence of the blob on the nimbus   and assign the appropriate number. Check if all are have same sequence number 
Storm,WITHOUT_CLASSIFICATION,//  local worker heartbeat can be null cause some error/exception 
Storm,WITHOUT_CLASSIFICATION,//  because in local mode its not a separate   process. supervisor will register it in this case   if ConfigUtils.isLocalMode(conf) returns false then it is in distributed mode. 
Storm,WITHOUT_CLASSIFICATION,//  compare contents of files 
Storm,WITHOUT_CLASSIFICATION,//  True if this OffsetManager has made at least one commit to Kafka 
Storm,WITHOUT_CLASSIFICATION,//  minimum 0.x version of supporting STORM-2448 would be 0.10.3 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_CPU 
Storm,WITHOUT_CLASSIFICATION,//  Sleep before cleaning up files 
Storm,WITHOUT_CLASSIFICATION,//  ==== 
Storm,WITHOUT_CLASSIFICATION,//  state query is added to the existing stateful bolt 
Storm,WITHOUT_CLASSIFICATION,//  attempts to lookup the unique Id for a string that may not exist yet.  Returns INVALID_METADATA_STRING_ID 
Storm,WITHOUT_CLASSIFICATION,//  only used for TimedRotationPolicy 
Storm,WITHOUT_CLASSIFICATION,//  BOLT 
Storm,WITHOUT_CLASSIFICATION,// Error occurs but assignment has not changed 
Storm,WITHOUT_CLASSIFICATION,// We ensure there is only the default stream when configuring the spout so it should be safe to ignore the parameter here. 
Storm,WITHOUT_CLASSIFICATION,//  batch 1 is played with 25 tuples initially. 
Storm,WITHOUT_CLASSIFICATION,// Save it to try again later... 
Storm,WITHOUT_CLASSIFICATION,// This was 0 byte in test 
Storm,WITHOUT_CLASSIFICATION,//  pendingPrepare has no entries 
Storm,WITHOUT_CLASSIFICATION,//  Static State 
Storm,WITHOUT_CLASSIFICATION,//  We got at least one GET_PULSE_RESPONSE message. 
Storm,WITHOUT_CLASSIFICATION,//  if overrides are disabled we won't replace anything that already exists 
Storm,WITHOUT_CLASSIFICATION,// Fall through on purpose 
Storm,WITHOUT_CLASSIFICATION,//  mock the supervisor r000s000 as a failed supervisor 
Storm,WITHOUT_CLASSIFICATION,//  Restart topology with a different topology id 
Storm,WITHOUT_CLASSIFICATION,//  set null and get the old value 
Storm,WITHOUT_CLASSIFICATION,//  Test with integer value 
Storm,WITHOUT_CLASSIFICATION,// Schedule nimbus code sync thread to sync code from other nimbuses. 
Storm,WITHOUT_CLASSIFICATION,// Assignment has changed 
Storm,WITHOUT_CLASSIFICATION,//  Pretend to be storm. 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required 
Storm,WITHOUT_CLASSIFICATION,// Now verify that when it is switched we can recover 
Storm,WITHOUT_CLASSIFICATION,//  doesn't manipulate tuples (lists of stuff) so that things like aggregating into   cassandra is cleaner (don't need lists everywhere just store the single value there) 
Storm,WITHOUT_CLASSIFICATION,//  wrapper to hold global and window averages 
Storm,WITHOUT_CLASSIFICATION,//  Start at the part of the log file we are interested in. 
Storm,WITHOUT_CLASSIFICATION,//  Windows should set this to false cause symlink in compressed file doesn't work properly. 
Storm,WITHOUT_CLASSIFICATION,//  Set closing to true to prevent any further reconnection attempts. 
Storm,WITHOUT_CLASSIFICATION,// There are some different levels of accuracy here and we want to deal with all of them 
Storm,WITHOUT_CLASSIFICATION,// The spout should have emitted the tuple and must have committed it before emit 
Storm,WITHOUT_CLASSIFICATION,// populate with existing assignments 
Storm,WITHOUT_CLASSIFICATION,//  ignore ... if t is not a TupleImpl type .. faster than checking and then casting 
Storm,WITHOUT_CLASSIFICATION,//  store can be null during testing when mocking utils. 
Storm,WITHOUT_CLASSIFICATION,// Output the sum of all the known counts so for this key 
Storm,WITHOUT_CLASSIFICATION,//  read the short field 
Storm,WITHOUT_CLASSIFICATION,//  filesystem path to the resource 
Storm,WITHOUT_CLASSIFICATION,//  TOPO_HISTORY 
Storm,WITHOUT_CLASSIFICATION,//  should rewrite this to do a file move
Storm,WITHOUT_CLASSIFICATION,//  tests by subclassing. 
Storm,WITHOUT_CLASSIFICATION,//  Send response to client. 
Storm,WITHOUT_CLASSIFICATION,//  get trigger values only if they have more than zero 
Storm,WITHOUT_CLASSIFICATION,// All messages except the first acked message should have been emitted 
Storm,WITHOUT_CLASSIFICATION,//  There's enough bytes in the buffer. Read it.   
Storm,WITHOUT_CLASSIFICATION,//  get host -> all assignable worker slots for non-blacklisted machines (assigned or not assigned)   will then have a list of machines that need to be assigned (machine -> [topology list of list of executors])   match each spec to a machine (who has the right number of workers) free everything else on that machine and assign those slots   (do one topology at a time)   blacklist all machines who had production slots defined   log isolated topologies who weren't able to get enough slots / machines   run default scheduler on isolated topologies that didn't have enough slots + non-isolated topologies on remaining machines   set blacklist to what it was initially 
Storm,WITHOUT_CLASSIFICATION,// NOOP prepare should have already been called 
Storm,WITHOUT_CLASSIFICATION,// Demonstrate that the spout doesn't ack pending tuples when skipping compacted tuples. The pending tuples should be allowed to finish normally. 
Storm,WITHOUT_CLASSIFICATION,//  If we failed to get anything from Artifactory try to get it from our local cache 
Storm,WITHOUT_CLASSIFICATION,//  Test1: When a worker fails RAS does not alter existing assignments on healthy workers 
Storm,WITHOUT_CLASSIFICATION,//  Keep track of how many times we see each taskId 
Storm,WITHOUT_CLASSIFICATION,//  Make sure that we have received at least a short  
Storm,WITHOUT_CLASSIFICATION,//  previous code used this method to generate the string ensure the two match 
Storm,WITHOUT_CLASSIFICATION,//  should be ack-ed once 
Storm,WITHOUT_CLASSIFICATION,//  window length 
Storm,WITHOUT_CLASSIFICATION,//  need to add an empty string else it is nto added as query param. 
Storm,WITHOUT_CLASSIFICATION,// STORM-3141 regression test  Verify that remote worker can handle many tasks in one executor 
Storm,WITHOUT_CLASSIFICATION,//  Attempt to schedule multiple copies of 2 different topologies (topo-t0 and topo-t1) in 3 blocks.   Without fragmentation it is possible to schedule all topologies but fragmentation causes topologies to not   schedule for the last block. 
Storm,WITHOUT_CLASSIFICATION,/*                 * a two seconds tumbling window                 */
Storm,WITHOUT_CLASSIFICATION,// This verifies that partitions can't prevent each other from retrying tuples due to the maxUncommittedOffsets limit. 
Storm,WITHOUT_CLASSIFICATION,//  another thread could be writing out the metadata cache to the database. 
Storm,WITHOUT_CLASSIFICATION,// duplicate case 
Storm,WITHOUT_CLASSIFICATION,//  Needed to keep SimpleFileObject constructor happy. 
Storm,WITHOUT_CLASSIFICATION,// constraints and spreads 
Storm,WITHOUT_CLASSIFICATION,//  remove executor details assigned to the failed worker 
Storm,WITHOUT_CLASSIFICATION,//  COMPONENT_TO_SHARED_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTOR_INFO 
Storm,WITHOUT_CLASSIFICATION,// Otherwise don't bother them 
Storm,WITHOUT_CLASSIFICATION,//  Aggregate matching metrics over bucket timeframes.   We'll process starting with the longest bucket.  If the metric for this does not exist we don't have to 
Storm,WITHOUT_CLASSIFICATION,// this is the topology page so we know the user is authorized 
Storm,WITHOUT_CLASSIFICATION,//  verifyZeroInteractions(collector); 
Storm,WITHOUT_CLASSIFICATION,//  the offset of the last available message + 1. 
Storm,WITHOUT_CLASSIFICATION,//  We call fireMessageRead since the client is allowed to   perform this request. The client's request will now proceed   to the next pipeline component namely StormClientHandler. 
Storm,WITHOUT_CLASSIFICATION,/* with exhibitor */
Storm,WITHOUT_CLASSIFICATION,//  Creating nimbus hosts containing latest version of blob 
Storm,WITHOUT_CLASSIFICATION,//  JSON_CONF 
Storm,WITHOUT_CLASSIFICATION,//  check lock file presence 
Storm,WITHOUT_CLASSIFICATION,//  interval at which to commit offsets to zk in milliseconds 
Storm,WITHOUT_CLASSIFICATION,//  remove the slot from the existing assignments 
Storm,WITHOUT_CLASSIFICATION,//  spout with 5 parallel instances 
Storm,WITHOUT_CLASSIFICATION,//  Is the topology ZooKeeper authentication configuration unset?
Storm,WITHOUT_CLASSIFICATION,// Just ignore these for now.  We are going to throw it away anyways 
Storm,WITHOUT_CLASSIFICATION,//  holds remaining streams 
Storm,WITHOUT_CLASSIFICATION,//  Class that has the logic to handle tuple failure.
Storm,WITHOUT_CLASSIFICATION,//  Sets {@link SolrFieldsMapper} to use the default Solr collection if there is one defined 
Storm,WITHOUT_CLASSIFICATION,//  Adds the serialized and base64 file to the credentials map as a string with the filename as   the key. 
Storm,WITHOUT_CLASSIFICATION,//  works by emitting null to the collector. since the planner knows this is an ADD node with   no new output fields it just passes the tuple forward 
Storm,WITHOUT_CLASSIFICATION,//  get existing tuples and pending/unsuccessful triggers for this operator-component/task and add them to WindowManager 
Storm,WITHOUT_CLASSIFICATION,//  =========== Consumer Rebalance Listener - On the same thread as the caller =========== 
Storm,WITHOUT_CLASSIFICATION,//  if no truststore file assume the truststore is the keystore. 
Storm,WITHOUT_CLASSIFICATION,//  if we needed we could make config for update thread pool size 
Storm,WITHOUT_CLASSIFICATION,//  Updating a blacklist file periodically with random words 
Storm,WITHOUT_CLASSIFICATION,// all failed events are put in toResend which is sorted by event's offset 
Storm,WITHOUT_CLASSIFICATION,// anonymous user 
Storm,WITHOUT_CLASSIFICATION,// error
Storm,WITHOUT_CLASSIFICATION,//  PlannerImpl.transform() optimizes RelNode with ruleset 
Storm,WITHOUT_CLASSIFICATION,// kill or newly submit 
Storm,WITHOUT_CLASSIFICATION,// No task is under backpressure initially 
Storm,WITHOUT_CLASSIFICATION,// Enable metrics 
Storm,WITHOUT_CLASSIFICATION,//  5 partitions evicted to window state 
Storm,WITHOUT_CLASSIFICATION,// for each owner get resources configs and aggregate 
Storm,WITHOUT_CLASSIFICATION,//  ISOLATED_NODE_GUARANTEE 
Storm,WITHOUT_CLASSIFICATION,// get mapping of components to executors 
Storm,WITHOUT_CLASSIFICATION,//  indexed by id 
Storm,WITHOUT_CLASSIFICATION,//  configs - topo parallelism 
Storm,WITHOUT_CLASSIFICATION,//  next scheduled refresh is sooner than (now + MIN_TIME_BEFORE_LOGIN). 
Storm,WITHOUT_CLASSIFICATION,// fileOffset = one past last scanned file 
Storm,WITHOUT_CLASSIFICATION,// This partition was previously assigned so the consumer position shouldn't change 
Storm,WITHOUT_CLASSIFICATION,//  catch any runtime exceptions caused by eviction 
Storm,WITHOUT_CLASSIFICATION,//  this is ignored by javac currently but useJavaUtilZip should be 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  make sure the property was actually set 
Storm,WITHOUT_CLASSIFICATION,/*      * Bolt-specific configuration for windowed bolts to specify the sliding interval in time duration.      */
Storm,WITHOUT_CLASSIFICATION,//  sanity check of the provided test data 
Storm,WITHOUT_CLASSIFICATION,/*                 * split the sentences to words                 */
Storm,WITHOUT_CLASSIFICATION,//  Otherwise poll to see if any new event   was scheduled. This is in essence the   response time for detecting any new event   schedulings when there are no scheduled   events. 
Storm,WITHOUT_CLASSIFICATION,//  if lock file has been updated since last time then leave this lock file alone 
Storm,WITHOUT_CLASSIFICATION,//  Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple() 
Storm,WITHOUT_CLASSIFICATION,//  META 
Storm,WITHOUT_CLASSIFICATION,//  Sends the same tuple (list of scored/predicted values) to all the declared streams 
Storm,WITHOUT_CLASSIFICATION,//  always check recQ if ACKing enabled 
Storm,WITHOUT_CLASSIFICATION,//  package access for unit tests 
Storm,WITHOUT_CLASSIFICATION,//         JCQueue spoutQ = new JCQueue("spoutQ" 1024 100 0);          JCQueue ackQ = new JCQueue("ackQ" 1024 100 0);            final AckingProducer ackingProducer = new AckingProducer(spoutQ ackQ);          final Acker acker = new Acker(ackQ spoutQ);            runAllThds(ackingProducer acker); 
Storm,WITHOUT_CLASSIFICATION,//  task Ids experiencing BP. can be null   task Ids no longer experiencing BP. can be null 
Storm,WITHOUT_CLASSIFICATION,//  Instantiate the HdfsBolt 
Storm,WITHOUT_CLASSIFICATION,//  this method should return sequential numbers starting at 0 
Storm,WITHOUT_CLASSIFICATION,//  STATE_SPOUTS 
Storm,WITHOUT_CLASSIFICATION,//  So we have copied and pasted some of the needed main.methods here. (with a few changes to logging)
Storm,WITHOUT_CLASSIFICATION,//  GSID_TO_INPUT_STATS 
Storm,WITHOUT_CLASSIFICATION,// Only place we fall though to do the loop over again... 
Storm,WITHOUT_CLASSIFICATION,// Thread died before we could get the info skip 
Storm,WITHOUT_CLASSIFICATION,// Should not have flushed to file system yet 
Storm,WITHOUT_CLASSIFICATION,// Make sure the worker is down before we try to shoot any child processes 
Storm,WITHOUT_CLASSIFICATION,//  if not assign the highest sequence number. 
Storm,WITHOUT_CLASSIFICATION,// slotsUsed < origRequest 
Storm,WITHOUT_CLASSIFICATION,// Max heap size for a worker used by topology 
Storm,WITHOUT_CLASSIFICATION,// tmpDir will be handled separately 
Storm,WITHOUT_CLASSIFICATION,//  that store is passed to WindowStateUpdater to remove them after committing the batch. 
Storm,WITHOUT_CLASSIFICATION,/*              * This is an arbitrary choice to make the result consistent with calculateMin. Any value would be valid here becase there are             * no (non-zero) resources in the total set of resources so we're trying to average 0 values.              */
Storm,WITHOUT_CLASSIFICATION,//  if the component is a system (__*) component and we are hiding   them in UI keep going 
Storm,WITHOUT_CLASSIFICATION,//  ERRORS 
Storm,WITHOUT_CLASSIFICATION,// metaStoreURI = "jdbc:derby:;databaseName="+System.getProperty("java.io.tmpdir") +"metastore_db;create=true"; 
Storm,WITHOUT_CLASSIFICATION,/*  Ack the tuple and commit.         * Since the tuple is more than max poll records behind the most recent emitted tuple the consumer won't catch up in this poll.          */
Storm,WITHOUT_CLASSIFICATION,/*          * Load DriverManager first to avoid any race condition between         * DriverManager static initialization block and specific driver class's         * static initialization block. e.g. PhoenixDriver         *         * We should take this workaround since prepare() method is synchronized         * but an worker can initialize multiple AbstractJdbcBolt instances and         * they would make race condition.         *         * We just need to ensure that DriverManager class is always initialized         * earlier than provider so below line should be called first         * than initializing provider.          */
Storm,WITHOUT_CLASSIFICATION,//  class DirectInserter 
Storm,WITHOUT_CLASSIFICATION,//  Keys sorted in descending order 
Storm,WITHOUT_CLASSIFICATION,//  un-pin the last partition 
Storm,WITHOUT_CLASSIFICATION,// Now check for autoCreds that are missing from the command line but only if the   command line is used. 
Storm,WITHOUT_CLASSIFICATION,//  store inprocess triggers of a batch in store for batch retries for any failures 
Storm,WITHOUT_CLASSIFICATION,// Deleting this early does not hurt anything 
Storm,WITHOUT_CLASSIFICATION,// request from hosts that are not authorized should be rejected 
Storm,WITHOUT_CLASSIFICATION,// Ignored/expected... 
Storm,WITHOUT_CLASSIFICATION,// The user from the token is bob so verify that the name was set correctly... 
Storm,WITHOUT_CLASSIFICATION,//  package level for unit tests 
Storm,WITHOUT_CLASSIFICATION,// case 2: SaslTokenMessageRequest 
Storm,WITHOUT_CLASSIFICATION,//  2nd topology 
Storm,WITHOUT_CLASSIFICATION,// Gets a Database. 
Storm,WITHOUT_CLASSIFICATION,// server 
Storm,WITHOUT_CLASSIFICATION,//  hive principal storm-hive@WITZEN.COM   storm hive keytab /etc/security/keytabs/storm-hive.keytab   hive.metastore.uris : "thrift://pm-eng1-cluster1.field.hortonworks.com:9083" 
Storm,WITHOUT_CLASSIFICATION,//  SHARED_MEM_ON_HEAP 
Storm,WITHOUT_CLASSIFICATION,//  -- ignore file names config 
Storm,WITHOUT_CLASSIFICATION,//  Normal create update sync scenario returns the greatest sequence number in the set 
Storm,WITHOUT_CLASSIFICATION,//  for jackson 
Storm,WITHOUT_CLASSIFICATION,//  heartbeat ensure its no longer stale and read back the heartbeat data 
Storm,WITHOUT_CLASSIFICATION,// This cannot happen since we're using a standard charset 
Storm,WITHOUT_CLASSIFICATION,/*      * How often to poll Exhibitor cluster in millis.      */
Storm,WITHOUT_CLASSIFICATION,//  intended behavior 
Storm,WITHOUT_CLASSIFICATION,// expected 
Storm,WITHOUT_CLASSIFICATION,// index == null if it is memory or CPU 
Storm,WITHOUT_CLASSIFICATION,//  should the topology be active or inactive 
Storm,WITHOUT_CLASSIFICATION,// get mapping of execs to components 
Storm,WITHOUT_CLASSIFICATION,// play 1st tuple 
Storm,WITHOUT_CLASSIFICATION,// We expect the bolt to log exactly one decorated line per emit 
Storm,WITHOUT_CLASSIFICATION,// This must be defensively copied because a bolt probably has only one rotation policy object 
Storm,WITHOUT_CLASSIFICATION,//  test that the set-logger-level function was not called 
Storm,WITHOUT_CLASSIFICATION,//  required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  printTimeCostArray(totalPointsArray); 
Storm,WITHOUT_CLASSIFICATION,// cluster forgets about its previous status so if it is scheduled just leave it. 
Storm,WITHOUT_CLASSIFICATION,// Try again or go to empty if assignment has been nulled 
Storm,WITHOUT_CLASSIFICATION,//  Lastly the default servlet for root content (always needed to satisfy servlet spec) 
Storm,WITHOUT_CLASSIFICATION,//  baseDir/supervisor/usercache/user1/filecache/files 
Storm,WITHOUT_CLASSIFICATION,//  1) read 5 lines in file 
Storm,WITHOUT_CLASSIFICATION,//  Object associated with JSON field is already JSON 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   required   required 
Storm,WITHOUT_CLASSIFICATION,// check component is declared for spreading 
Storm,WITHOUT_CLASSIFICATION,// First we need some configs 
Storm,WITHOUT_CLASSIFICATION,//  trigger occurred create an aggregation and keep them in store 
Storm,WITHOUT_CLASSIFICATION,// Check if we can run a topology with that version of storm. 
Storm,WITHOUT_CLASSIFICATION,//  if re-partitioning is involved does a per-partition aggregate by key before emitting the results downstream 
Storm,WITHOUT_CLASSIFICATION,//  deletes metadata strings before the provided timestamp 
Storm,WITHOUT_CLASSIFICATION,// We assume that within 5% of the minimum congestion is still fine.  Not congested we grow (but slowly) 
Storm,WITHOUT_CLASSIFICATION,//  the worker to orphan   the worker that fails   the healthy worker 
Storm,WITHOUT_CLASSIFICATION,/*      * Same txid can be prepared again but the next txid cannot be prepared     * when previous one is not committed yet.      */
Storm,WITHOUT_CLASSIFICATION,// Topology Id -> executor ids -> component -> stats(...) 
Storm,WITHOUT_CLASSIFICATION,//  merge contents of `config` into topology config 
Storm,WITHOUT_CLASSIFICATION,/*      * Convenience method for registering CombinedMetric.      */
Storm,WITHOUT_CLASSIFICATION,//  map from topology id -> set of sets of executors 
Storm,WITHOUT_CLASSIFICATION,//  Make a key list to download 
Storm,WITHOUT_CLASSIFICATION,//  TOTAL_WORKERS 
Storm,WITHOUT_CLASSIFICATION,//  all tuples acked 
Storm,WITHOUT_CLASSIFICATION,// Update the size of the objects 
Storm,WITHOUT_CLASSIFICATION,//  should be at 7 
Storm,WITHOUT_CLASSIFICATION,//  COMPLETE_LATENCY_MS 
Storm,WITHOUT_CLASSIFICATION,// Copy the data 
Storm,WITHOUT_CLASSIFICATION,// get metric by name 
Storm,WITHOUT_CLASSIFICATION,//  If there are no remote outbound tasks don't start the thread. 
Storm,WITHOUT_CLASSIFICATION,//  Download missing blobs from potential nimbodes 
Storm,WITHOUT_CLASSIFICATION,// just ignore any error/exception. 
Storm,WITHOUT_CLASSIFICATION,//  avoid allocating SpoutAckInfo obj if not necessary 
Storm,WITHOUT_CLASSIFICATION,// NOT going to timeout for a while 
Storm,WITHOUT_CLASSIFICATION,// Try to emit all messages. Ensure only maxUncommittedOffsets are emitted 
Storm,WITHOUT_CLASSIFICATION,//  Only try reading once. 
Storm,WITHOUT_CLASSIFICATION,//  Jpmml evaluator 
Storm,WITHOUT_CLASSIFICATION,/*         For each partition the spout is allowed to retry all tuples between the committed offset and maxUncommittedOffsets ahead.        It is not allowed to retry tuples past that limit.        This makes the actual limit per partition maxUncommittedOffsets + maxPollRecords - 1        reached if the tuple at the maxUncommittedOffsets limit is the earliest retriable tuple        or if the spout is 1 tuple below the limit and receives a full maxPollRecords tuples in the poll.          */
Storm,WITHOUT_CLASSIFICATION,//  submit topology 
Storm,WITHOUT_CLASSIFICATION,//  poll metrics every minute then kill topology after specified duration 
Storm,WITHOUT_CLASSIFICATION,//  validate at least two agg level none metrics exist 
Storm,WITHOUT_CLASSIFICATION,// Allow the failed record to retry 
Storm,WITHOUT_CLASSIFICATION,/*  * Listens for all metrics dumps them as text to a configured host:port * * To use add this to your topology's configuration: * * ```java *   conf.registerMetricsConsumer(org.apache.storm.testing.ForwardingMetricsConsumer.class "<HOST>:<PORT>" 1); * ``` * * Or edit the storm.yaml config file: * * ```yaml *   topology.metrics.consumer.register: *     - class: "org.apache.storm.testing.ForwardingMetricsConsumer" *     - argument: "example.com:9999" *       parallelism.hint: 1 * ``` *  */
Storm,WITHOUT_CLASSIFICATION,//  For now we do not make a transaction when removing a topology assignment from local an overdue   assignment may be left on local disk.   So we should check if the local disk assignment is valid when initializing:   if topology files does not exist the worker[possibly alive] will be reassigned if it is timed-out;   if topology files exist but the topology id is invalid just let Supervisor make a sync;   if topology files exist and topology files is valid recover the container. 
Storm,WITHOUT_CLASSIFICATION,//  zk node under which to commit the sequence number of messages. e.g. /committed_sequence_numbers 
Storm,WITHOUT_CLASSIFICATION,// Just do a few polls to check that nothing more is emitted 
Storm,WITHOUT_CLASSIFICATION,// topo-3 should be evicted since its been up the longest 
Storm,WITHOUT_CLASSIFICATION,//  ack few 
Storm,WITHOUT_CLASSIFICATION,//  https://github.com/netty/netty/blob/netty-4.1.24.Final/transport/src/main/java/io/netty/channel/MultithreadEventLoopGroup.java#L40 
Storm,WITHOUT_CLASSIFICATION,//  clean up the profiler actions that are not being processed 
Storm,WITHOUT_CLASSIFICATION,//  Cassandra doesn't actually shut down until jvm shutdown so need to wait for that first. 
Storm,WITHOUT_CLASSIFICATION,//  Authenticate: Removed after authentication completes 
Storm,WITHOUT_CLASSIFICATION,// get a sorted list of unassigned executors based on number of constraints 
Storm,WITHOUT_CLASSIFICATION,// Now schedule GPU but with the simple topology in place. 
Storm,WITHOUT_CLASSIFICATION,//  I don't have anything 
Storm,WITHOUT_CLASSIFICATION,//  targetSize in Bytes 
Storm,WITHOUT_CLASSIFICATION,// Spout Settings 
Storm,WITHOUT_CLASSIFICATION,//  defaults 
Storm,WITHOUT_CLASSIFICATION,//  used for reporting used ports when heartbeating 
Storm,WITHOUT_CLASSIFICATION,// any change to this code must be serializable compatible or there will be problems
Storm,WITHOUT_CLASSIFICATION,/*                 * aggregate the count                 */
Storm,WITHOUT_CLASSIFICATION,//  no-op 
Storm,WITHOUT_CLASSIFICATION,//  make sure the timestamp on the metadata has the latest time 
Storm,WITHOUT_CLASSIFICATION,//  Some components might have different resource configs. 
Storm,WITHOUT_CLASSIFICATION,//  iterate again 
Storm,WITHOUT_CLASSIFICATION,// Just quit 
Storm,WITHOUT_CLASSIFICATION,// additional safety check to make sure that topologySubmitter is going to be a valid value 
Storm,WITHOUT_CLASSIFICATION,// com.mysql.jdbc.jdbc2.optional.MysqlDataSource  jdbc:mysql://localhost/test  root  password 
Storm,WITHOUT_CLASSIFICATION,// don't exit if not running unless it is an Error 
Storm,WITHOUT_CLASSIFICATION,// Unknown should only happen during compilation or some unit tests. 
Storm,WITHOUT_CLASSIFICATION,//  Convert targets to a JSON serializable format 
Storm,WITHOUT_CLASSIFICATION,//  a stream of words 
Storm,WITHOUT_CLASSIFICATION,//  insert child in-between parent and its current child nodes 
Storm,WITHOUT_CLASSIFICATION,//  SHUFFLE 
Storm,WITHOUT_CLASSIFICATION,// Check that only the tuple on the currently assigned partition is retried 
Storm,WITHOUT_CLASSIFICATION,//  optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,// just ignore the exception. 
Storm,WITHOUT_CLASSIFICATION,// From LocalizerTest 
Storm,WITHOUT_CLASSIFICATION,//  EXEC_STATS 
Storm,WITHOUT_CLASSIFICATION,//  first 50 should have expired due to expire events threshold 
Storm,WITHOUT_CLASSIFICATION,//  33 seconds passed by timed out 
Storm,WITHOUT_CLASSIFICATION,/*  If topic compaction is enabled in Kafka we sometimes need to commit past a gap of deleted offsets         * Since the Kafka consumer should return offsets in order we can assume that if a message is acked         * then any prior message will have been emitted at least once.         * If we see an acked message and some of the offsets preceding it were not emitted they must have been compacted away and should be skipped.          */
Storm,WITHOUT_CLASSIFICATION,//  since we made sys components hidden the component map is empty for this worker 
Storm,WITHOUT_CLASSIFICATION,//  NODE_HOST 
Storm,WITHOUT_CLASSIFICATION,/*      * Methods for validating confs      */
Storm,WITHOUT_CLASSIFICATION,// Also add in support for worker tokens 
Storm,WITHOUT_CLASSIFICATION,//  1) Build phase - Segregate tuples in the Window into streams.      First stream's tuples go into probe rest into HashMaps in hashedInputs 
Storm,WITHOUT_CLASSIFICATION,// Now check for overlap on the node 
Storm,WITHOUT_CLASSIFICATION,//  the window partition that holds the events 
Storm,WITHOUT_CLASSIFICATION,//  number of retry attempts for zk 
Storm,WITHOUT_CLASSIFICATION,/*      * Same txid can be committed again but the     * txid to be committed must be the last prepared one.      */
Storm,WITHOUT_CLASSIFICATION,//  Make every attempt to sync the data we have.  If it can't be done then kill the bolt with   a runtime exception.  The filesystem is presumably in a very bad state. 
Storm,WITHOUT_CLASSIFICATION,// Fall through if not supported 
Storm,WITHOUT_CLASSIFICATION,// When tuple tracking is enabled the spout must not commit acked tuples in at-most-once mode because they were committed before being emitted 
Storm,WITHOUT_CLASSIFICATION,// To avoid locking we will go through the map twice.  It should be small so it is probably not a big deal 
Storm,WITHOUT_CLASSIFICATION,/*                 * emit the count for the words that occurred                * at-least five times in the last two seconds                 */
Storm,WITHOUT_CLASSIFICATION,// For backwards compatability 
Storm,WITHOUT_CLASSIFICATION,//  nothing expires until threshold is hit 
Storm,WITHOUT_CLASSIFICATION,//  default 30 seconds. (we cache the size so it is cheap to do) 
Storm,WITHOUT_CLASSIFICATION,// Ack all emitted messages and commit them 
Storm,WITHOUT_CLASSIFICATION,// Normally this is a NOOP 
Storm,WITHOUT_CLASSIFICATION,//  perf critical path. don't use iterators. 
Storm,WITHOUT_CLASSIFICATION,//  do nothing 
Storm,WITHOUT_CLASSIFICATION,// Read the partitions transaction ids and offsets from the old storm-kafka /user path 
Storm,WITHOUT_CLASSIFICATION,//  max number of events to be cached in memory 
Storm,WITHOUT_CLASSIFICATION,// get document 
Storm,WITHOUT_CLASSIFICATION,//  required   required   optional   optional 
Storm,WITHOUT_CLASSIFICATION,// read from state store if not found use startingOffset 
Storm,WITHOUT_CLASSIFICATION,//  0 - validate args 
Storm,WITHOUT_CLASSIFICATION,//  The match at this candidate offset failed so start over with the   next candidate byte from the buffer. 
Storm,WITHOUT_CLASSIFICATION,// ensure always same order for registrations with TreeMap 
Storm,WITHOUT_CLASSIFICATION,//  Initialize spout using the same populated data (i.e same kafkaUnitRule) 
Storm,WITHOUT_CLASSIFICATION,//  Refresh the Ticket Granting Ticket (TGT) periodically. How often to refresh is determined by the   TGT's existing expiry date and the configured MIN_TIME_BEFORE_RELOGIN. For testing and development   you can decrease the interval of expiration of tickets (for example to 3 minutes) by running : 
Storm,WITHOUT_CLASSIFICATION,// populate worker to comp assignments 
Storm,WITHOUT_CLASSIFICATION,/*  In non error scenarios for the Azure Data Lake Store File System (adl://)                               the output stream must be closed before the file associated with it is deleted.                               For ADLFS deleting the file also removes any handles to the file hence out.close() will fail.  */
Storm,WITHOUT_CLASSIFICATION,//  no wildcard file 
Storm,WITHOUT_CLASSIFICATION,// 'groups username' command return is non-consistent across different unixes 
Storm,WITHOUT_CLASSIFICATION,// Cached topology -> executor ids used for deciding timeout workers of heartbeatsCache. 
Storm,WITHOUT_CLASSIFICATION,//  create a windowed stream of five seconds duration 
Storm,WITHOUT_CLASSIFICATION,// Spout should ack failed messages after they hit the retry limit 
Storm,WITHOUT_CLASSIFICATION,//  take one's complement' 
Storm,WITHOUT_CLASSIFICATION,//  environment variable substitution 
Storm,WITHOUT_CLASSIFICATION,// Don't modify the original 
Storm,WITHOUT_CLASSIFICATION,//  Do nothing. 
Storm,WITHOUT_CLASSIFICATION,// The frequency of reporting 
Storm,WITHOUT_CLASSIFICATION,//  sleep for 10 seconds 
Storm,WITHOUT_CLASSIFICATION,// early detection/early fail 
Storm,WITHOUT_CLASSIFICATION,//  ensures... overflowCount <= overflowLimit. if set to 0 disables overflow limiting. 
Storm,WITHOUT_CLASSIFICATION,//  initialize the hashedInputs data structure 
Storm,WITHOUT_CLASSIFICATION,// The current lat and count buckets are protected by a different lock   from the other buckets.  This is to reduce the lock contention   When doing complex calculations.  Never grab the instance object lock 
Storm,WITHOUT_CLASSIFICATION,//  ====== 
Storm,WITHOUT_CLASSIFICATION,//  found 
Storm,WITHOUT_CLASSIFICATION,// Emit and fail the same tuple until we've reached retry limit 
Storm,WITHOUT_CLASSIFICATION,// Don't pro-rate anything it is all approximate so an extra bucket is not that bad. 
Storm,WITHOUT_CLASSIFICATION,// parts[1] is empty for CGroups V2 else what is mapped that we are looking for 
Storm,WITHOUT_CLASSIFICATION,// Wrapping it makes it mutable 
Storm,WITHOUT_CLASSIFICATION,//  test deep equal 
Storm,WITHOUT_CLASSIFICATION,// Save the current user to help with recovery 
Storm,WITHOUT_CLASSIFICATION,//  [taskId]-> JCQueue :  initialized after local executors are initialized 
Storm,WITHOUT_CLASSIFICATION,//  if shard iterator not present for this message get it 
Storm,WITHOUT_CLASSIFICATION,//  this triggers java.lang.RuntimeException: Cannot convert null to int
Storm,WITHOUT_CLASSIFICATION,// This also tracks how many times worker transitioning out of a state 
Storm,WITHOUT_CLASSIFICATION,//  storm blobstore update --file blacklist.txt key 
Storm,WITHOUT_CLASSIFICATION,//  note that we delete the return value 
Storm,WITHOUT_CLASSIFICATION,//  WAIT_SECS 
Storm,WITHOUT_CLASSIFICATION,//  Get the list of keys from blobstore 
Storm,WITHOUT_CLASSIFICATION,//  the following are required if we're defining a core storm topology DAG in YAML etc. 
Storm,WITHOUT_CLASSIFICATION,//  we use this "weird" wrapper pattern temporarily for mocking in clojure test 
Storm,WITHOUT_CLASSIFICATION,//  -- archive dir config 
Storm,WITHOUT_CLASSIFICATION,//  remove 
Storm,WITHOUT_CLASSIFICATION,//  track resources - user to resourceSet  ConcurrentHashMap is explicitly used everywhere in this class because it uses locks to guarantee atomicity for compute and   computeIfAbsent where as ConcurrentMap allows for a retry of the function passed in and would require the function to have   no side effects. 
Storm,WITHOUT_CLASSIFICATION,// Write the mocking backwards so the actual method is not called on the spy object 
Storm,WITHOUT_CLASSIFICATION,//  a user is will decide which topologies are run and which ones are not. 
Storm,WITHOUT_CLASSIFICATION,// topo-3 evicted (lowest priority) 
Storm,WITHOUT_CLASSIFICATION,//  if the retry service returns a message that is not in failed set then ignore it. should never happen 
Storm,WITHOUT_CLASSIFICATION,// The key of the map is the worker id and the value is the corresponding workerslot object 
Storm,WITHOUT_CLASSIFICATION,//  specified via bolt.select() ... used in declaring Output fields      protected String[] dotSeparatedOutputFieldNames; // fieldNames in x.y.z format w/o stream name used for naming output fields 
Storm,WITHOUT_CLASSIFICATION,//  check lock file is gone 
Storm,WITHOUT_CLASSIFICATION,//  map from batchgroupid to coordspec 
Storm,WITHOUT_CLASSIFICATION,//  test another topology getting blob with updated version - it should update version now 
Storm,WITHOUT_CLASSIFICATION,// make sure all workers on scheduled in rack-1   The favored nodes would have put it on a different rack but because that rack does not have free space to run the   topology it falls back to this rack 
Storm,WITHOUT_CLASSIFICATION,//  Base for exponential function in seconds for retrying for second third and so on failures 
Storm,WITHOUT_CLASSIFICATION,//  {TopologyId -> {WorkerId -> {Executors}}} 
Storm,WITHOUT_CLASSIFICATION,//  timestamp to decide when to commit to zk again 
Storm,WITHOUT_CLASSIFICATION,//  HA Nimbus on being newly elected as leader. Change to a recurring pattern addresses these problems. 
Storm,WITHOUT_CLASSIFICATION,//  Authorize 
Storm,WITHOUT_CLASSIFICATION,//  wait for locks to expire 
Storm,WITHOUT_CLASSIFICATION,//  should have created blobDir 
Storm,WITHOUT_CLASSIFICATION,// free slot 
Storm,WITHOUT_CLASSIFICATION,//  Test2: Launch topo 1 2 and 4 they together request a little more mem than available so one of the 3 topos will not be 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTE_MS_AVG 
Storm,WITHOUT_CLASSIFICATION,//  CUSTOM_SERIALIZED 
Storm,WITHOUT_CLASSIFICATION,//  class Pair 
Storm,WITHOUT_CLASSIFICATION,//  try to modify the list which should fail 
Storm,WITHOUT_CLASSIFICATION,// default batch size 15000 
Storm,WITHOUT_CLASSIFICATION,//  add default user ACL when only empty user ACL is not present 
Storm,WITHOUT_CLASSIFICATION,// totalSpoutLag = totalLatestTimeOffset-totalLatestCompletedOffset 
Storm,WITHOUT_CLASSIFICATION,// Stats... 
Storm,WITHOUT_CLASSIFICATION,// Make the consumer return a single message for each partition 
Storm,WITHOUT_CLASSIFICATION,//  Adding metadata to avoid null pointer exception 
Storm,WITHOUT_CLASSIFICATION,// add a END_OF_BATCH indicator 
Storm,WITHOUT_CLASSIFICATION,//  works out 
Storm,WITHOUT_CLASSIFICATION,//  rebalance 
Storm,WITHOUT_CLASSIFICATION,//  wait and check for expiry again 
Storm,WITHOUT_CLASSIFICATION,/*      * The maximum number of states that will be searched looking for a solution in the constraint solver strategy      */
Storm,WITHOUT_CLASSIFICATION,//  length of parts should be greater than 0 
Storm,WITHOUT_CLASSIFICATION,//  Call prepare with our available taskIds 
Storm,WITHOUT_CLASSIFICATION,//  We consume the iterator by traversing and thus "emptying" it. 
Storm,WITHOUT_CLASSIFICATION,//  'stream' cannot be null 
Storm,WITHOUT_CLASSIFICATION,// taken care in finally block 
Storm,WITHOUT_CLASSIFICATION,//  3 - submit topology wait for a few min and terminate it 
Storm,WITHOUT_CLASSIFICATION,// since user jerry has enough resource guarantee 
Storm,WITHOUT_CLASSIFICATION,//  abandon file 
Storm,WITHOUT_CLASSIFICATION,//  this filter makes sure to receive only Key or row but not values associated with those rows. 
Storm,WITHOUT_CLASSIFICATION,/* without exhibitor */
Storm,WITHOUT_CLASSIFICATION,// for backward compatibility. 
Storm,WITHOUT_CLASSIFICATION,//  calc cid+sid->input_stats 
Storm,WITHOUT_CLASSIFICATION,// Emit a message on each partition and revoke the first partition 
Storm,WITHOUT_CLASSIFICATION,//  acls are not set for the blob (DEFAULT) 
Storm,WITHOUT_CLASSIFICATION,// Metrics related 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   optional   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,/*  IMeta  */
Storm,WITHOUT_CLASSIFICATION,// Routine level 
Storm,WITHOUT_CLASSIFICATION,// This should not happen because we checked for PUBLIC in isFieldAllowed 
Storm,WITHOUT_CLASSIFICATION,//  bolts 
Storm,WITHOUT_CLASSIFICATION,/*  Container of a collection of tuples  */
Storm,WITHOUT_CLASSIFICATION,// The maxUncommittedOffsets limit should not be enforced since it is only meaningful in at-least-once mode 
Storm,WITHOUT_CLASSIFICATION,// Need  more data 
Storm,WITHOUT_CLASSIFICATION,//  while 
Storm,WITHOUT_CLASSIFICATION,//  spouts that use them.  Or if only one uses it they can be created inline with the add 
Storm,WITHOUT_CLASSIFICATION,//  haven't found string keep searching 
Storm,WITHOUT_CLASSIFICATION,//  prevent issue when the implementation of fieldNames is not serializable   getRowType().getFieldNames() returns Calcite Pair$ which is NOT serializable 
Storm,WITHOUT_CLASSIFICATION,// the json_conf is populated by TopologyBuilder (e.g. boltDeclarer.setMemoryLoad) 
Storm,WITHOUT_CLASSIFICATION,//  group by node 
Storm,WITHOUT_CLASSIFICATION,//  ACL 
Storm,WITHOUT_CLASSIFICATION,//  Heartbeat here so that worker process dies if this fails   it's important that worker heartbeat to supervisor ASAP so that supervisor knows 
Storm,WITHOUT_CLASSIFICATION,//  UPTIME_SECS 
Storm,WITHOUT_CLASSIFICATION,//  if a HDFS keytab/principal have been supplied login otherwise assume they are   logged in already or running insecure HDFS. 
Storm,WITHOUT_CLASSIFICATION,//  Is this a directory or is it a file?
Storm,WITHOUT_CLASSIFICATION,//  producer 
Storm,WITHOUT_CLASSIFICATION,//  we might need to set the number of acker executors and eventlogger executors to be the estimated number of workers. 
Storm,WITHOUT_CLASSIFICATION,//  HBase has some entries 
Storm,WITHOUT_CLASSIFICATION,// We will adjust weights based off of the minimum load 
Storm,WITHOUT_CLASSIFICATION,// Tried all of the slots and none of them worked. 
Storm,WITHOUT_CLASSIFICATION,// No need to block the task run by the executor is safe to run even after metrics are closed 
Storm,WITHOUT_CLASSIFICATION,//  COMPONENT_DEBUG 
Storm,WITHOUT_CLASSIFICATION,//  should be throw
Storm,WITHOUT_CLASSIFICATION,// Nimbus Admin 
Storm,WITHOUT_CLASSIFICATION,// component overrides 
Storm,WITHOUT_CLASSIFICATION,//  Let us add the kerbClientPrinc and kerbTicket   We need to clone the ticket because java.security.auth.kerberos assumes TGT is unique for each subject   So sharing TGT with multiple subjects can cause expired TGT to never refresh. 
Storm,WITHOUT_CLASSIFICATION,// Stop emitting at a certain point because log rolling breaks the tests. 
Storm,WITHOUT_CLASSIFICATION,//  shard iterator corresponding to position in shard for new messages 
Storm,WITHOUT_CLASSIFICATION,//  level 3 - longer idling with Thread.sleep() 
Storm,WITHOUT_CLASSIFICATION,//  This does not have to be atomic worst case we update when one is not needed 
Storm,WITHOUT_CLASSIFICATION,//  if the windows should be persisted in state 
Storm,WITHOUT_CLASSIFICATION,//  Define our taskIds and loads 
Storm,WITHOUT_CLASSIFICATION,// Test for Supervisor Admin 
Storm,WITHOUT_CLASSIFICATION,//  3) read another line and see if another log entry was made 
Storm,WITHOUT_CLASSIFICATION,//  No control of cpu usage 
Storm,WITHOUT_CLASSIFICATION,//  Emitted Offsets List 
Storm,WITHOUT_CLASSIFICATION,// If there is an acked tupled after a compaction gap the spout should commit it immediately 
Storm,WITHOUT_CLASSIFICATION,//  main 
Storm,WITHOUT_CLASSIFICATION,//  To Commit to Solr and Ack according to the commit strategy 
Storm,WITHOUT_CLASSIFICATION,// Spouts 
Storm,WITHOUT_CLASSIFICATION,/*      * Even though normally bolts do not need to care about thread safety this particular bolt is different.     * It maintains a static field that is prepopulated before the topology starts is written into by the topology     * and is then read from after the topology is completed - all of this by potentially different threads.      */
Storm,WITHOUT_CLASSIFICATION,//  ======== Activate / Deactivate / Close / Declare Outputs ======= 
Storm,WITHOUT_CLASSIFICATION,// This will only ever grow so no need to worry about falling off the end 
Storm,WITHOUT_CLASSIFICATION,//  CPU_GUARANTEE_REMAINING 
Storm,WITHOUT_CLASSIFICATION,//  update streamState based on stateUpdates 
Storm,WITHOUT_CLASSIFICATION,//  Equivalent update command on command line 
Storm,WITHOUT_CLASSIFICATION,// The partition revocation hook must be called before the new partitions are assigned to the consumer 
Storm,WITHOUT_CLASSIFICATION,// new topology needs to be scheduled 
Storm,WITHOUT_CLASSIFICATION,//  A small control algorithm to adjust the amount of time that we sleep to make it more accurate 
Storm,WITHOUT_CLASSIFICATION,// Cached GlobalStreamId 
Storm,WITHOUT_CLASSIFICATION,// Special cases for storm... 
Storm,WITHOUT_CLASSIFICATION,//  E 
Storm,WITHOUT_CLASSIFICATION,//  Partition state path =   "/{prefix}/{topologyName}/{namespace}/{entityPath}/partitions/{partitionId}/state"; 
Storm,WITHOUT_CLASSIFICATION,//  Everything should fit in a single slot 
Storm,WITHOUT_CLASSIFICATION,//  last fetched sequence number corresponding to position in shard 
Storm,WITHOUT_CLASSIFICATION,//  sorted acked sequence numbers - needed to figure out what sequence number can be committed 
Storm,WITHOUT_CLASSIFICATION,// Check that null meta makes the spout seek to LATEST and that the returned meta is correct 
Storm,WITHOUT_CLASSIFICATION,//  tuple ts 
Storm,WITHOUT_CLASSIFICATION,//  TODO: jsonify StormTopology   at the minimum should send source info 
Storm,WITHOUT_CLASSIFICATION,// Timer 
Storm,WITHOUT_CLASSIFICATION,//  Don't allow topoConf to override various cluster-specific properties.   Specifically adding the cluster settings to the topoConf here will make sure these settings   also override the subsequently generated conf picked up locally on the classpath.     We will be dealing with 3 confs:   1) the submitted topoConf created here   2) the combined classpath conf with the topoConf added on top   3) the nimbus conf with conf 2 above added on top.     By first forcing the topology conf to contain the nimbus settings we guarantee all three confs 
Storm,WITHOUT_CLASSIFICATION,//  Switch to the new assignment even if localization hasn't completed or go to empty state 
Storm,WITHOUT_CLASSIFICATION,//  Calcite ensures that the value is structurized to the table definition   hence we can use PK index directly   To elaborate if table BAR is defined as ID INTEGER PK NAME VARCHAR DEPTID INTEGER   and query like INSERT INTO BAR SELECT NAME ID FROM FOO is executed   Calcite makes the projection ($1 <- ID $0 <- NAME null) to the value before INSERT. 
Storm,WITHOUT_CLASSIFICATION,// clearing assignments 
Storm,WITHOUT_CLASSIFICATION,// With earliest the spout should also resume where it left off rather than restart at the earliest offset. 
Storm,WITHOUT_CLASSIFICATION,// 0 no load to 1 fully loaded  0 no load to 1 fully loaded 
Storm,WITHOUT_CLASSIFICATION,//  2) Emit results 
Storm,WITHOUT_CLASSIFICATION,//  setup timer for commit elapse time tracking 
Storm,WITHOUT_CLASSIFICATION,// This additional check and download is for nimbus high availability in case you have more than one nimbus 
Storm,WITHOUT_CLASSIFICATION,//  write the tuple 
Storm,WITHOUT_CLASSIFICATION,//  Get the most recent artifact as a String and then parse the yaml 
Storm,WITHOUT_CLASSIFICATION,//  proposedRefresh is too far in the future: it's after ticket expires: simply return now. 
Storm,WITHOUT_CLASSIFICATION,// Cached CuratorFramework mainly used for BlobStore. 
Storm,WITHOUT_CLASSIFICATION,//  Construct a message containing the SASL response and send it to the server. 
Storm,WITHOUT_CLASSIFICATION,// Now check that the spout will emit another maxUncommittedOffsets messages 
Storm,WITHOUT_CLASSIFICATION,//  PROCESS_LATENCY_MS 
Storm,WITHOUT_CLASSIFICATION,//  thread safety: assumes Collector.emit*() calls are externally synchronized (if needed). 
Storm,WITHOUT_CLASSIFICATION,// Test for a user having WRITE or ADMIN privileges to change replication of a blob 
Storm,WITHOUT_CLASSIFICATION,//  message was acked after being retried. so clear the state for that message 
Storm,WITHOUT_CLASSIFICATION,//  reset commit timer such that commit happens on next call to nextTuple() 
Storm,WITHOUT_CLASSIFICATION,// The retry schedules for two messages should be unrelated 
Storm,WITHOUT_CLASSIFICATION,/*      * Bolt-specific configuration for windowed bolts to specify the time interval for generating     * watermark events. Watermark event tracks the progress of time when tuple timestamp is used.     * This config is effective only if {@link org.apache.storm.windowing.TimestampExtractor} is specified.      */
Storm,WITHOUT_CLASSIFICATION,//  group field 
Storm,WITHOUT_CLASSIFICATION,/*          * A stream of words emitted by the QuerySpout is used as         * the keys to query the state.          */
Storm,WITHOUT_CLASSIFICATION,// Add in supervisors that might have crashed but workers are still alive 
Storm,WITHOUT_CLASSIFICATION,// put [owner-> StormBase-list] mapping to ownerToBasesMap  if this owner (the input parameter) is null add all the owners with stormbase and guarantees  else add only this owner (the input paramter) to the map 
Storm,WITHOUT_CLASSIFICATION,// If the system is low on memory we cannot be kind and need to shoot something 
Storm,WITHOUT_CLASSIFICATION,// The child processes typically exit in < 1 sec.  If 2 mins later they are still around something is wrong 
Storm,WITHOUT_CLASSIFICATION,// Only log if the leader has changed.  It is not interesting otherwise. 
Storm,WITHOUT_CLASSIFICATION,// A race happened and it is probably not running
Storm,WITHOUT_CLASSIFICATION,//  required   required   optional 
Storm,WITHOUT_CLASSIFICATION,//  Authorize: client is allowed to doRequest() if and only if the client 
Storm,WITHOUT_CLASSIFICATION,//  This spout is added to test purpose so just failing fast doesn't hurt much 
Storm,WITHOUT_CLASSIFICATION,//  minimum 1.x version of supporting STORM-2448 would be 1.0.4 
Storm,WITHOUT_CLASSIFICATION,// Advance the time and replay the failed tuple.  
Storm,WITHOUT_CLASSIFICATION,/*      * Bolt-specific configuration for windowed bolts to specify the sliding interval as a count of number of tuples.      */
Storm,WITHOUT_CLASSIFICATION,//  Only need to keep track of acked tuples if commits to Kafka are controlled by   tuple acks which happens only for at-least-once processing semantics 
Storm,WITHOUT_CLASSIFICATION,//  do not close this InputStream in method: it will be used from jetty server 
Storm,WITHOUT_CLASSIFICATION,//  partition ids 0 .. 19 
Storm,WITHOUT_CLASSIFICATION,// if the current assignment is already running new assignment will never be promoted to currAssignment   because Timer is not being compared in #equals or #equivalent meaning newAssignment always equals to currAssignment. 
Storm,WITHOUT_CLASSIFICATION,// This happens if the min id is too small 
Storm,WITHOUT_CLASSIFICATION,//  config 
Storm,WITHOUT_CLASSIFICATION,// Should not show files outside worker log root.
Storm,WITHOUT_CLASSIFICATION,//  valid to delete before what's been committed since    those batches will never be accessed again 
Storm,WITHOUT_CLASSIFICATION,//  Client should not be sending other-than-SASL messages before   SaslServerHandler has removed itself from the pipeline. Such   non-SASL requests will be denied by the Authorize channel handler   (the next handler upstream in the server pipeline) if SASL   authentication has not completed. 
Storm,WITHOUT_CLASSIFICATION,//  window system state 
Storm,WITHOUT_CLASSIFICATION,//  used internally to merge values in groupByKeyAndWindow 
Storm,WITHOUT_CLASSIFICATION,// http://supervisor2:8000/download/DemoTest-26-1462229009%2F6703%2Fworker.log  http://supervisor2:8000/log?file=SlidingWindowCountTest-9-1462388349%2F6703%2Fworker.log 
Storm,WITHOUT_CLASSIFICATION,//  register the newly established channel 
Storm,WITHOUT_CLASSIFICATION,//  when storeTuplesInStore is false then the given windowStoreFactory is only used to store triggers and 
Storm,WITHOUT_CLASSIFICATION,// obtain a serializer object 
Storm,WITHOUT_CLASSIFICATION,// Verify correct unwrapping of partitions and delegation of assignment 
Storm,WITHOUT_CLASSIFICATION,// Meter declared here can be registered by any daemon and is currently used by Supervisor 
Storm,WITHOUT_CLASSIFICATION,//  send flush tuple to all local executors 
Storm,WITHOUT_CLASSIFICATION,/*          * Include the topology name & worker port in the file name so that         * multiple event loggers can log independently.          */
Storm,WITHOUT_CLASSIFICATION,//  Another KafkaSpout instance (of this topology) already committed therefore FirstPollOffsetStrategy does not apply. 
Storm,WITHOUT_CLASSIFICATION,// Adjust the divisor for the average to account for any skipped resources (those where the total was 0) 
Storm,WITHOUT_CLASSIFICATION,// bolt overrides 
Storm,WITHOUT_CLASSIFICATION,//  RATE 
Storm,WITHOUT_CLASSIFICATION,//  1) schedule the heartbeat on one thread in pool 
Storm,WITHOUT_CLASSIFICATION,// Ensure the commit timer has expired 
Storm,WITHOUT_CLASSIFICATION,//  if we didn't get the group - just return empty list; 
Storm,WITHOUT_CLASSIFICATION,// parts[0] == 0 for CGroup V2 else maps to hierarchy in /proc/cgroups 
Storm,WITHOUT_CLASSIFICATION,// Yes we are putting in a config that is not the same type we pulled out. 
Storm,WITHOUT_CLASSIFICATION,//  REPLICATION_FACTOR 
Storm,WITHOUT_CLASSIFICATION,//  cleanup 
Storm,WITHOUT_CLASSIFICATION,//  max number of window events in memory 
Storm,WITHOUT_CLASSIFICATION,//  Non Blocking. returns count of how many inserts succeeded 
Storm,WITHOUT_CLASSIFICATION,//  1 concurrent deletion - only one thread should succeed 
Storm,WITHOUT_CLASSIFICATION,//  Now let's update it but not advance time.  Should get old map again. 
Storm,WITHOUT_CLASSIFICATION,// Remove the unneeded entries from the graph  We want to keep all of our nodes and the nodes that they are connected directly to (parents and children). 
Storm,WITHOUT_CLASSIFICATION,//  we have enough resources now... 
Storm,WITHOUT_CLASSIFICATION,//  Don't emit anything .. allow configured spout wait strategy to kick in 
Storm,WITHOUT_CLASSIFICATION,//  Was scheduled for retry and re-emitted so remove from schedule. 
Storm,WITHOUT_CLASSIFICATION,//  ON_HEAP 
Storm,WITHOUT_CLASSIFICATION,//  no filter configured allow anyone 
Storm,WITHOUT_CLASSIFICATION,// Setup spout with mock consumer so we can get at the rebalance listener    
Storm,WITHOUT_CLASSIFICATION,//  NOTE: Variable used in lambda expression should be final or effectively final   (or it will cause compilation error)   and variable type should implement the Serializable interface if it isn't primitive type   (or it will cause not serializable exception). 
Storm,WITHOUT_CLASSIFICATION,// Hosts this user is authorized to impersonate from. 
Storm,WITHOUT_CLASSIFICATION,/*                  * If a worker crashes the states of all workers are rolled back and an initState message is sent across                 * the topology so that crashed workers can initialize their state.                 * The bolts that have their state already initialized need not be re-initialized.                  */
Storm,WITHOUT_CLASSIFICATION,//  ENABLE 
Storm,WITHOUT_CLASSIFICATION,//  note that sometimes the tuples active may be less than max_spout_pending e.g.   max_spout_pending = 3   tx 1 2 3 active tx 2 is acked. there won't be a commit for tx 2 (because tx 1 isn't committed yet)   and there won't be a batch for tx 4 because there's max_spout_pending tx active 
Storm,WITHOUT_CLASSIFICATION,//  Multivalue field split by non default token %   to match dynamic fields of the form "*_txt"   this field won't be indexed by solr 
Storm,WITHOUT_CLASSIFICATION,// A map consisting of all workers on the node. 
Storm,WITHOUT_CLASSIFICATION,//  if we had a timeout but the timeout is no longer active 
Storm,WITHOUT_CLASSIFICATION,//  sample fieldDescriptor = "stream1:x.y.z" 
Storm,WITHOUT_CLASSIFICATION,// This is an odd case for a rolling upgrade where the user on the old assignment may be null   but not on the new one.  Although in all other ways they are the same.   If this happens we want to use the assignment with the owner. 
Storm,WITHOUT_CLASSIFICATION,// Don't override the host name or everything looks like it is on nimbus 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_REGULAR_ON_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  this is also a helpful optimization that state implementations don't need to manually do 
Storm,WITHOUT_CLASSIFICATION,// sigar uses JNI and does not work in local mode 
Storm,WITHOUT_CLASSIFICATION,//  for mesos this is {hostname}-{topologyid} 
Storm,WITHOUT_CLASSIFICATION,// WritableByteChannel is a Channel which implements Closeable.   Hence although declared AutoCloseable super#close here should only throws IOException  We rethrow to conform the signature 
Storm,WITHOUT_CLASSIFICATION,// 2) Retire them 
Storm,WITHOUT_CLASSIFICATION,// create a factory class 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNMENT_ID 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_MEMOFFHEAP 
Storm,WITHOUT_CLASSIFICATION,//  a topology with multiple spouts 
Storm,WITHOUT_CLASSIFICATION,//  populate metric values using the provided key 
Storm,WITHOUT_CLASSIFICATION,//  Or done separately like with setting the 
Storm,WITHOUT_CLASSIFICATION,//  since 2 is the largest un-pinned entry before 3 is loaded 
Storm,WITHOUT_CLASSIFICATION,// Commits offsets during deactivation 
Storm,WITHOUT_CLASSIFICATION,//  no reason to try to execute a previous attempt than we've already seen 
Storm,WITHOUT_CLASSIFICATION,//  pipeline component. 
Storm,WITHOUT_CLASSIFICATION,//  Should have been re-emitted 
Storm,WITHOUT_CLASSIFICATION,//  MEMORY_GUARANTEE_REMAINING 
Storm,WITHOUT_CLASSIFICATION,//  Add the event logger details. 
Storm,WITHOUT_CLASSIFICATION,// To support topologies of older version to run we might have to loose the constraints so that  the configs of older version can pass the validation. 
Storm,WITHOUT_CLASSIFICATION,// There is a race on backpressure too... 
Storm,WITHOUT_CLASSIFICATION,//  free the error stream buffer 
Storm,WITHOUT_CLASSIFICATION,// no name 
Storm,WITHOUT_CLASSIFICATION,//  in partitioned example in case an emitter task receives a later transaction than it's emitted so far   when it sees the earlier txid it should know to emit nothing 
Storm,WITHOUT_CLASSIFICATION,//  Some times a Bolt or Spout will have some memory that is shared between the instances   These are typically caches but could be anything like a static database that is memory   mapped into the processes. These can be declared separately and added to the bolts and 
Storm,WITHOUT_CLASSIFICATION,// Creates a MongoURI from the given string. 
Storm,WITHOUT_CLASSIFICATION,// Verify that it only committed the message on the assigned partition 
Storm,WITHOUT_CLASSIFICATION,// hierarchy-ID:controller-list:cgroup-path 
Storm,WITHOUT_CLASSIFICATION,//  ID 
Storm,WITHOUT_CLASSIFICATION,// This can happen when a topology is first coming up   It's thrown by the blobstore code 
Storm,WITHOUT_CLASSIFICATION,// fall back to string which is already set 
Storm,WITHOUT_CLASSIFICATION,// There can be more then one line if cgroups are mounted in more then one place but we assume the first is good enough 
Storm,WITHOUT_CLASSIFICATION,// The resource that is not used should count as if it is being used 0% 
Storm,WITHOUT_CLASSIFICATION,//  could block 
Storm,WITHOUT_CLASSIFICATION,//  One tuple and one rotation should yield one file with data 
Storm,WITHOUT_CLASSIFICATION,//  if null acks every tuple 
Storm,WITHOUT_CLASSIFICATION,//  explicit delete for ephemeral node to ensure this session creates the entry. 
Storm,WITHOUT_CLASSIFICATION,// The spout must respect maxUncommittedOffsets even if some tuples have been acked but not committed 
Storm,WITHOUT_CLASSIFICATION,//  projection 
Storm,WITHOUT_CLASSIFICATION,//  search all metadata strings 
Storm,WITHOUT_CLASSIFICATION,// This should never happen because only the primary nimbus is active but just in case   declare the race safe even if we lose it. 
Storm,WITHOUT_CLASSIFICATION,//  should remove the blob since cache size set really small 
Storm,WITHOUT_CLASSIFICATION,//  if newReader is true and tuple is null then it is an empty reader 
Storm,WITHOUT_CLASSIFICATION,//  Populate user -> password map with JAAS configuration entries from the "Server" section.   Usernames are distinguished from other options by prefixing the username with a "user_" prefix. 
Storm,WITHOUT_CLASSIFICATION,//  3) read remaining lines in file then ensure lock is gone 
Storm,WITHOUT_CLASSIFICATION,//  We haven't received the entire object yet return and wait for more bytes. 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTOR_START_TIME_SECS 
Storm,WITHOUT_CLASSIFICATION,// Schedule the simple topology first 
Storm,WITHOUT_CLASSIFICATION,//  first offset of this batch 
Storm,WITHOUT_CLASSIFICATION,//  this tuple should be removed from emitted only inside the ack() method. This is to ensure   that the OffsetManager for that TopicPartition is updated and allows commit progression 
Storm,WITHOUT_CLASSIFICATION,// Add an element and bar should drop out 
Storm,WITHOUT_CLASSIFICATION,//  below calls shouldn't propagate any exceptions 
Storm,WITHOUT_CLASSIFICATION,//     private String timestampField; 
Storm,WITHOUT_CLASSIFICATION,//  we need to be able to lookup bolts by id then switch based 
Storm,WITHOUT_CLASSIFICATION,//  parse the output   clear the input stream buffer 
Storm,WITHOUT_CLASSIFICATION,// Treat it like a jar 
Storm,WITHOUT_CLASSIFICATION,// This is very private and does not need to be exposed 
Storm,WITHOUT_CLASSIFICATION,//  fetch records from kinesis starting at sequence number for message passed as argument. Any other messages fetched   and are in the failed queue will also 
Storm,WITHOUT_CLASSIFICATION,//  It's possible for target to have multiple tasks if it reads multiple sources 
Storm,WITHOUT_CLASSIFICATION,//  Tests for case when subject != null (security turned on) and 
Storm,WITHOUT_CLASSIFICATION,//  make sure samplingPct is within bounds. 
Storm,WITHOUT_CLASSIFICATION,// will fail since org.apache.storm.nimbus.NimbusInfo doesn't implement or extend org.apache.storm.networktopography   .DNSToSwitchMapping 
Storm,WITHOUT_CLASSIFICATION,// port range 
Storm,WITHOUT_CLASSIFICATION,//  2 try to grab another lock while dir is locked 
Storm,WITHOUT_CLASSIFICATION,// schedule tasks that are not part of components returned from topology.get_spout or 
Storm,WITHOUT_CLASSIFICATION,//  function to call when timer is killed 
Storm,WITHOUT_CLASSIFICATION,//  1 -  Setup Const Spout   -------- 
Storm,WITHOUT_CLASSIFICATION,// Now schedule all of the topologies that need to be scheduled 
Storm,WITHOUT_CLASSIFICATION,//  in critical path. don't use iterators. 
Storm,WITHOUT_CLASSIFICATION,// good 
Storm,WITHOUT_CLASSIFICATION,// this default ensures things expire at most 50% past the expiration time 
Storm,WITHOUT_CLASSIFICATION,// This should be the load metrics.   There will usually only be one message but if there are multiple we only process the latest one. 
Storm,WITHOUT_CLASSIFICATION,//  should remove the second blob first 
Storm,WITHOUT_CLASSIFICATION,//  get close enough 
Storm,WITHOUT_CLASSIFICATION,// comma separated offsets 
Storm,WITHOUT_CLASSIFICATION,// 2 close file and retry creation 
Storm,WITHOUT_CLASSIFICATION,//  Track serialized size of messages. 
Storm,WITHOUT_CLASSIFICATION,// Map the value 
Storm,WITHOUT_CLASSIFICATION,//  string does not exist 
Storm,WITHOUT_CLASSIFICATION,//  sync the filesystem after every 1k tuples 
Storm,WITHOUT_CLASSIFICATION,//  for kryo compatibility 
Storm,WITHOUT_CLASSIFICATION,//  Log median ratios for different strategies 
Storm,WITHOUT_CLASSIFICATION,// grouping assignment by node to see the nodes diff then notify nodes/supervisors to synchronize its owned assignment  because the number of existing assignments is small for every scheduling round 
Storm,WITHOUT_CLASSIFICATION,//  {metric -> value} 
Storm,WITHOUT_CLASSIFICATION,// is removed since that is what is going to trigger the retry for cleanup 
Storm,WITHOUT_CLASSIFICATION,//  level 1 - no waiting 
Storm,WITHOUT_CLASSIFICATION,//  topology.blobstore.map='{"key":{"localname":"blacklist.txt" "uncompress":"false"}}' 
Storm,WITHOUT_CLASSIFICATION,//  If this partition was previously assigned to this spout 
Storm,WITHOUT_CLASSIFICATION,// Add new records to Kafka and check that the next batch contains these records 
Storm,WITHOUT_CLASSIFICATION,// Scheduler histogram 
Storm,WITHOUT_CLASSIFICATION,// PRECONDITION: The new and current assignments must be equivalent 
Storm,WITHOUT_CLASSIFICATION,//  Submit to Storm cluster 
Storm,WITHOUT_CLASSIFICATION,// For example we have a three nodes(supervisor1 supervisor2 supervisor3) cluster:  slots before sort:  supervisor1:6700 supervisor1:6701  supervisor2:6700 supervisor2:6701 supervisor2:6702  supervisor3:6700 supervisor3:6703 supervisor3:6702 supervisor3:6701  slots after sort:  supervisor3:6700 supervisor2:6700 supervisor1:6700  supervisor3:6701 supervisor2:6701 supervisor1:6701  supervisor3:6702 supervisor2:6702  supervisor3:6703 
Storm,WITHOUT_CLASSIFICATION,// .. ignore 
Storm,WITHOUT_CLASSIFICATION,// This shouldn't throw on a Check because nothing is configured yet 
Storm,WITHOUT_CLASSIFICATION,//  drain 1 element and ensure BP is relieved (i.e tryPublish() succeeds) 
Storm,WITHOUT_CLASSIFICATION,// Save the private worker key away so we can test it too. 
Storm,WITHOUT_CLASSIFICATION,//  we found a spout   spout 
Storm,WITHOUT_CLASSIFICATION,//  [taskId-indexingBase] => queue : List of all recvQs local to this worker 
Storm,WITHOUT_CLASSIFICATION,//  SPOUT_OBJECT 
Storm,WITHOUT_CLASSIFICATION,//  In the short term the goal is to not shoot anyone unless we really need to.   The on heap should limit the memory usage in most cases to a reasonable amount   If someone is using way more than they requested this is a bug and we should   not allow it 
Storm,WITHOUT_CLASSIFICATION,//  For native protocol V3 or below all variables must be bound.   With native protocol V4 or above variables can be left unset   in which case they will be ignored server side (no tombstones will be generated). 
Storm,WITHOUT_CLASSIFICATION,//  Location of the file in the artifactory archive.  Also used to name file in cache. 
Storm,WITHOUT_CLASSIFICATION,// INT + SHORT 
Storm,WITHOUT_CLASSIFICATION,// Migrate the /coordinator currtx currattempts and meta directories.  The new spout expects the list of topic partitions as coordinator meta. 
Storm,WITHOUT_CLASSIFICATION,/* ========== Implementations ========== */
Storm,WITHOUT_CLASSIFICATION,// Emit maxUncommittedOffsets messages and fail all of them. Then ensure that the spout will retry them when the retry backoff has passed 
Storm,WITHOUT_CLASSIFICATION,//  Generate SASL response (but we only actually send the response if 
Storm,WITHOUT_CLASSIFICATION,//  Overloading the readInt method accomodate Subject in order to check for authorization (security turned on) 
Storm,WITHOUT_CLASSIFICATION,// Get and set the start time before getting current time in order to avoid potential race with the longest-scheduling-time-ms gauge 
Storm,WITHOUT_CLASSIFICATION,//  Make sure we don't process too frequently. 
Storm,WITHOUT_CLASSIFICATION,//  FIELDS 
Storm,WITHOUT_CLASSIFICATION,//  c.f. HADOOP-6559 
Storm,WITHOUT_CLASSIFICATION,//  boltMsgQueue should have at least one entry at the moment 
Storm,WITHOUT_CLASSIFICATION,//  intermediate bolt subscribes to jms spout anchors on tuples and auto-acks 
Storm,WITHOUT_CLASSIFICATION,//  map to track number of failures for each kinesis message that failed 
Storm,WITHOUT_CLASSIFICATION,/*                                                * Transform the stream of words to a stream of (word 1) pairs                                                */
Storm,WITHOUT_CLASSIFICATION,// Yes this should be a topo name but it makes this simpler... 
Storm,WITHOUT_CLASSIFICATION,// create a transport factory that will invoke our auth callback for digest 
Storm,WITHOUT_CLASSIFICATION,//  for the given processor node if we received punctuation from all tasks of its parent windowed streams 
Storm,WITHOUT_CLASSIFICATION,// Test for Nimbus Admin 
Storm,WITHOUT_CLASSIFICATION,//  max lag 
Storm,WITHOUT_CLASSIFICATION,//  bolt stats 
Storm,WITHOUT_CLASSIFICATION,//  STATE_SPOUT_OBJECT 
Storm,WITHOUT_CLASSIFICATION,//  or database. 
Storm,WITHOUT_CLASSIFICATION,// emit second batch 
Storm,WITHOUT_CLASSIFICATION,//  when there is no input field then the whole tuple is considered for comparison. 
Storm,WITHOUT_CLASSIFICATION,//  for each executor -> node+port pair 
Storm,WITHOUT_CLASSIFICATION,//  publish a retained message to the broker 
Storm,WITHOUT_CLASSIFICATION,//  this is to distinguish from TransactionAttempt 
Storm,WITHOUT_CLASSIFICATION,// Just go on and try to delete the others 
Storm,WITHOUT_CLASSIFICATION,// We created it so lets chmod it properly 
Storm,WITHOUT_CLASSIFICATION,// Stop searching if the message is known to be ready for retry 
Storm,WITHOUT_CLASSIFICATION,// Daemon common main.methods
Storm,WITHOUT_CLASSIFICATION,// bobby has no guarantee so topo-2 and topo-3 evicted 
Storm,WITHOUT_CLASSIFICATION,//  validate search by topology id and executor id 
Storm,WITHOUT_CLASSIFICATION,// hard coded max number of states to search 
Storm,WITHOUT_CLASSIFICATION,// Consumer. Sets up a topology that reads the given Kafka spouts and logs the received messages 
Storm,WITHOUT_CLASSIFICATION,//  this is to support things like persisting off of drpc stream which is inherently unreliable   and won't have a tx attempt 
Storm,WITHOUT_CLASSIFICATION,// convert NodePort to NodeInfo (again!!!). 
Storm,WITHOUT_CLASSIFICATION,//  effectively disable commits based on time 
Storm,WITHOUT_CLASSIFICATION,// If .current and .version do not match we roll back the .version file to match   what .current is pointing to. 
Storm,WITHOUT_CLASSIFICATION,// Any other failure result  The assumption is that the strategy set the status... 
Storm,WITHOUT_CLASSIFICATION,//  Shutdownable via INimbusCredentailPlugin 
Storm,WITHOUT_CLASSIFICATION,//  we do this since to concat a null String will actually concat a "null" which is not the expected: "" 
Storm,WITHOUT_CLASSIFICATION,//  Storm config 
Storm,WITHOUT_CLASSIFICATION,// Test for replication using SUPERVISOR access 
Storm,WITHOUT_CLASSIFICATION,//  its possible a string is used by multiple types of metadata strings 
Storm,WITHOUT_CLASSIFICATION,//  This avoids a race condition with cancel-timer. 
Storm,WITHOUT_CLASSIFICATION,// Resources missing from used are using none of that resource 
Storm,WITHOUT_CLASSIFICATION,//  idx - index of the type of this field in the FieldType list 
Storm,WITHOUT_CLASSIFICATION,//  Build get query 
Storm,WITHOUT_CLASSIFICATION,//  delete db and all tables in it 
Storm,WITHOUT_CLASSIFICATION,//  acls for the blob are set to DEFAULT (Empty ACL List) only for LocalFsBlobstore 
Storm,WITHOUT_CLASSIFICATION,//  ===== 
Storm,WITHOUT_CLASSIFICATION,/*  The output field of the spout ("lambda") is provided as the boltMessageField          so that this gets written out as the message in the kafka topic.          The tuples have no key field so the messages are written to Kafka without a key. */
Storm,WITHOUT_CLASSIFICATION,// MemFree:        14367072 kB  Buffers:          536512 kB  Cached:          1192096 kB   MemFree + Buffers + Cached 
Storm,WITHOUT_CLASSIFICATION,// Nothing 
Storm,WITHOUT_CLASSIFICATION,//  client). 
Storm,WITHOUT_CLASSIFICATION,// Right now this is only used for sending metrics to nimbus   but we may want to combine it with the heartbeatTimer at some point 
Storm,WITHOUT_CLASSIFICATION,//  default cache size 10GB converted to Bytes 
Storm,WITHOUT_CLASSIFICATION,//  it should never happened since we apply UUID 
Storm,WITHOUT_CLASSIFICATION,//  checkpoint spout should 've been added 
Storm,WITHOUT_CLASSIFICATION,//  locate expired lock files (if any). Try to take ownership (oldest lock first) 
Storm,WITHOUT_CLASSIFICATION,// This may or may not be reported depending on when process exits 
Storm,WITHOUT_CLASSIFICATION,//  sort by available slots size: from large to small 
Storm,WITHOUT_CLASSIFICATION,// case 1: plugin is a IContext class 
Storm,WITHOUT_CLASSIFICATION,//  able to delete the blob without checking meta's ACL   skip checking everything and continue deleting local files 
Storm,WITHOUT_CLASSIFICATION,// Ignored if cgroups is not setup don't do anything with it 
Storm,WITHOUT_CLASSIFICATION,// The spout must reemit failed messages waiting for retry even if it is not allowed to poll for new messages due to maxUncommittedOffsets being exceeded 
Storm,WITHOUT_CLASSIFICATION,// Check to see if we have enough slots before trying to get them 
Storm,WITHOUT_CLASSIFICATION,//  to the supervisor 
Storm,WITHOUT_CLASSIFICATION,//  go off to blobstore and get it   assume dir passed in exists and has correct permission 
Storm,WITHOUT_CLASSIFICATION,//  Share some common metadata strings to validate they do not get deleted 
Storm,WITHOUT_CLASSIFICATION,// simulate worker loss 
Storm,WITHOUT_CLASSIFICATION,//  a topology with two unconnected partitions 
Storm,WITHOUT_CLASSIFICATION,//  inputFields can be equal to outFields but multiple aggregators cannot have intersection outFields 
Storm,WITHOUT_CLASSIFICATION,//  Run only once. 
Storm,WITHOUT_CLASSIFICATION,//  Acknowledge all changing blobs as futures 
Storm,WITHOUT_CLASSIFICATION,// The resources are already normalized 
Storm,WITHOUT_CLASSIFICATION,//  used to recognize the pattern of some meta files in a worker log directory 
Storm,WITHOUT_CLASSIFICATION,//  name 
Storm,WITHOUT_CLASSIFICATION,/*      * Just output the word value with a count of 1.     * The HBaseBolt will handle incrementing the counter.      */
Storm,WITHOUT_CLASSIFICATION,// Have not moved to a java worker yet 
Storm,WITHOUT_CLASSIFICATION,//  update to the latest timestamp and add to the string cache 
Storm,WITHOUT_CLASSIFICATION,//  schedule first block (0% - 10%) 
Storm,WITHOUT_CLASSIFICATION,//  close the state to force flush 
Storm,WITHOUT_CLASSIFICATION,/*          * When partitions are reassigned the spout should seek with the first poll offset strategy for new partitions.         * Previously assigned partitions should be left alone since the spout keeps the emitted and acked state for those.          */
Storm,WITHOUT_CLASSIFICATION,//  OFF_HEAP_WORKER 
Storm,WITHOUT_CLASSIFICATION,//  JMS Topic provider 
Storm,WITHOUT_CLASSIFICATION,//  class DirLockingThread 
Storm,WITHOUT_CLASSIFICATION,// still need to return the first of pending list 
Storm,WITHOUT_CLASSIFICATION,//  timestamp to be used for shardIteratorType AT_TIMESTAMP - can be null 
Storm,WITHOUT_CLASSIFICATION,// get factory class name 
Storm,WITHOUT_CLASSIFICATION,//  blocking call that can be interrupted using Thread.interrupt() 
Storm,WITHOUT_CLASSIFICATION,//  Download updated blobs from potential nimbodes 
Storm,WITHOUT_CLASSIFICATION,//  consumer 
Storm,WITHOUT_CLASSIFICATION,//  Tell cassandra where the configuration files are. Use the test configuration file. 
Storm,WITHOUT_CLASSIFICATION,//  if maximum uncommitted records count has reached so dont emit any new records and return 
Storm,WITHOUT_CLASSIFICATION,//  5 min 
Storm,WITHOUT_CLASSIFICATION,//  check lock creation/deletion and contents 
Storm,WITHOUT_CLASSIFICATION,// Always have a space in between 
Storm,WITHOUT_CLASSIFICATION,//  window-state table should already be created with cf:tuples column 
Storm,WITHOUT_CLASSIFICATION,//  archive passed in must contain symlink named tmptestsymlink if not a zip file 
Storm,WITHOUT_CLASSIFICATION,// bolt 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGIES 
Storm,WITHOUT_CLASSIFICATION,/*                 * Join the squares and the cubes stream within the window.                * The values in the squares stream having the same key as that                * of the cubes stream within the window will be joined together.                 */
Storm,WITHOUT_CLASSIFICATION,//  acquire lock on file1 and verify if worked 
Storm,WITHOUT_CLASSIFICATION,// Scheduling changed 
Storm,WITHOUT_CLASSIFICATION,//  please pick small artifact which has small transitive dependency   and let's mark as Ignore if we want to run test even without internet or maven central is often not stable 
Storm,WITHOUT_CLASSIFICATION,//  last evaluated and last expired message ids per task stream (source taskid + stream-id) 
Storm,WITHOUT_CLASSIFICATION,// For some reasons we can not get supervisor port info eg: supervisor shutdown  Just skip for this scheduling round. 
Storm,WITHOUT_CLASSIFICATION,//  need to set memory.memsw.limit_in_bytes after setting memory.limit_in_bytes or error   might occur 
Storm,WITHOUT_CLASSIFICATION,//  CPU 
Storm,WITHOUT_CLASSIFICATION,//  required   required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  save a little typing 
Storm,WITHOUT_CLASSIFICATION,// no op. 
Storm,WITHOUT_CLASSIFICATION,//  The events should be put in a window when the first watermark is received 
Storm,WITHOUT_CLASSIFICATION,/*  * To change this template choose Tools | Templates * and open the template in the editor.  */
Storm,WITHOUT_CLASSIFICATION,//  only one of the multireducers will receive the tuples 
Storm,WITHOUT_CLASSIFICATION,//  merge stdout and stderr 
Storm,WITHOUT_CLASSIFICATION,//  no capacity for spout 
Storm,WITHOUT_CLASSIFICATION,// This spout owns 2 partitions: 6 and 14 
Storm,WITHOUT_CLASSIFICATION,//  CPU_USAGE 
Storm,WITHOUT_CLASSIFICATION,//  wait for the process to finish and check the exit code 
Storm,WITHOUT_CLASSIFICATION,//  the "old" trident kafka spout always returns true like this 
Storm,WITHOUT_CLASSIFICATION,//  utility main.methods
Storm,WITHOUT_CLASSIFICATION,//  late tuple stream 
Storm,WITHOUT_CLASSIFICATION,// When tuple tracking is enabled the spout must not replay tuples in no guarantee mode 
Storm,WITHOUT_CLASSIFICATION,//  Memory is the constraining resource. 
Storm,WITHOUT_CLASSIFICATION,//  tuple values are mapped with   metric timestamp value Map of tagK/tagV respectively. 
Storm,WITHOUT_CLASSIFICATION,//  adds addressedTuple to destination Q if it is not full. else adds to pendingEmits (if its not null) 
Storm,WITHOUT_CLASSIFICATION,//  then facing backpressure 
Storm,WITHOUT_CLASSIFICATION,//  hidden sys component 
Storm,WITHOUT_CLASSIFICATION,// Sleep a bit to avoid hogging the CPU. 
Storm,WITHOUT_CLASSIFICATION,/*                      * The position is behind the committed offset. This can happen in some cases e.g. if a message failed lots of (more                     * than max.poll.records) later messages were acked and the failed message then gets acked. The consumer may only be                     * part way through "catching up" to where it was when it went back to retry the failed tuple. Skip the consumer forward                     * to the committed offset.                      */
Storm,WITHOUT_CLASSIFICATION,//  for 
Storm,WITHOUT_CLASSIFICATION,//  INFO 
Storm,WITHOUT_CLASSIFICATION,// J_PROFILE_START is not used.  When you see a J_PROFILE_STOP   start profiling and save it away to stop when timeout happens 
Storm,WITHOUT_CLASSIFICATION,// register metrics 
Storm,WITHOUT_CLASSIFICATION,//  SERVER_PORT 
Storm,WITHOUT_CLASSIFICATION,// Try to maintain rolling upgrade compatible with 0.10 releases 
Storm,WITHOUT_CLASSIFICATION,//  Extract the field from tuple. Field may be nested field (x.y.z) 
Storm,WITHOUT_CLASSIFICATION,// 1 try to append to an open file 
Storm,WITHOUT_CLASSIFICATION,// until Ctrl-C 
Storm,WITHOUT_CLASSIFICATION,// for the topology which wants rebalance (specified by the scratchTopoId)   we exclude its assignment meaning that all the slots occupied by its assignment   will be treated as free slot in the scheduler code. 
Storm,WITHOUT_CLASSIFICATION,//  if not in blacklist then add it and set the resume time according to config 
Storm,WITHOUT_CLASSIFICATION,//  first topology should get evicted for higher priority (lower value) second topology to successfully schedule 
Storm,WITHOUT_CLASSIFICATION,//  strong supervisor node 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY STATE TRANSITIONS 
Storm,WITHOUT_CLASSIFICATION,//  all events since last clear 
Storm,WITHOUT_CLASSIFICATION,//  KEYS 
Storm,WITHOUT_CLASSIFICATION,//  Deny unsupported operations. 
Storm,WITHOUT_CLASSIFICATION,//  flush db to disk 
Storm,WITHOUT_CLASSIFICATION,/*                  * No events were found in the previous window interval.                 * Scan through the events in the queue to find the next                 * window intervals based on event ts.                  */
Storm,WITHOUT_CLASSIFICATION,//  if null then use zookeeper used   by Storm 
Storm,WITHOUT_CLASSIFICATION,// The other tuples are used to reset the first tuple's timeout 
Storm,WITHOUT_CLASSIFICATION,//  Builds a JSON list 
Storm,WITHOUT_CLASSIFICATION,//  this store is used only for storing triggered aggregated results but not tuples as storeTuplesInStore is set   as false int he below call. 
Storm,WITHOUT_CLASSIFICATION,//  graph with 3 kinds of nodes:   operation partition or spout   all operations have finishBatch and can optionally be committers 
Storm,WITHOUT_CLASSIFICATION,// Emit all remaining messages. Failed tuples retry immediately with current configuration so no need to wait. 
Storm,WITHOUT_CLASSIFICATION,// We are in the same process we cannot recover anything 
Storm,WITHOUT_CLASSIFICATION,//  sorted failed sequence numbers - needed to figure out what sequence number can be committed 
Storm,WITHOUT_CLASSIFICATION,// Skip special case if `storm kill_workers` is already invoked 
Storm,WITHOUT_CLASSIFICATION,//  WINDOW_TO_ACKED 
Storm,WITHOUT_CLASSIFICATION,// There is a race on logconfig where they can be leaked in some versions of storm. 
Storm,WITHOUT_CLASSIFICATION,// Expected... 
Storm,WITHOUT_CLASSIFICATION,//  common stats 
Storm,WITHOUT_CLASSIFICATION,//  Allow searching when start-byte-offset == file-len so it doesn't blow up on 0-length files 
Storm,WITHOUT_CLASSIFICATION,// NOOP no need to create links in local mode 
Storm,WITHOUT_CLASSIFICATION,//  We don't need to take care of sync cause we're always updating heartbeat 
Storm,WITHOUT_CLASSIFICATION,//  Producers. This is just to get some data in Kafka normally you would be getting this data from elsewhere 
Storm,WITHOUT_CLASSIFICATION,//  Get start/end indices for blocks 
Storm,WITHOUT_CLASSIFICATION,// There is no security so we are done. 
Storm,WITHOUT_CLASSIFICATION,// Log file permissions 
Storm,WITHOUT_CLASSIFICATION,// Security off  is admin  is in allowed group  is an allowed user 
Storm,WITHOUT_CLASSIFICATION,//  optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,// Prevent fileName from pathing into worker logs or outside daemon log root  
Storm,WITHOUT_CLASSIFICATION,/*  weight: 100-1000  */
Storm,WITHOUT_CLASSIFICATION,//  test commit creates properly 
Storm,WITHOUT_CLASSIFICATION,// Ignored... 
Storm,WITHOUT_CLASSIFICATION,// Get the nodes 
Storm,WITHOUT_CLASSIFICATION,// Some tests rely on reading the worker log. If there are too many emits and too much is logged the log might roll breaking the test.  Ensure the time based windowing tests can emit for 5 minutes 
Storm,WITHOUT_CLASSIFICATION,//  TOTAL_RESOURCES 
Storm,WITHOUT_CLASSIFICATION,//  [taskId-indexingBase] => queue some entries can be null. : outbound Qs for this executor instance 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_STATS 
Storm,WITHOUT_CLASSIFICATION,//  batch 1 is replayed with 50 tuples. 
Storm,WITHOUT_CLASSIFICATION,// We want to update longest scheduling time in real time in case scheduler get stuck   Get current time before startTime to avoid potential race with scheduler's Timer 
Storm,WITHOUT_CLASSIFICATION,/*          * atomically decrement the count if its greater than threshold and         * return if the event should be evicted          */
Storm,WITHOUT_CLASSIFICATION,//  We ran out of buffer for the search. 
Storm,WITHOUT_CLASSIFICATION,//  returns true if there was a change in the BP situation 
Storm,WITHOUT_CLASSIFICATION,// proxy timeout 
Storm,WITHOUT_CLASSIFICATION,// set topology name so that sample Trident topology can use it as stream name. 
Storm,WITHOUT_CLASSIFICATION,// look for available port 
Storm,WITHOUT_CLASSIFICATION,//  stores.length+1 as 2 users in Bengaluru 
Storm,WITHOUT_CLASSIFICATION,//  Add messages 
Storm,WITHOUT_CLASSIFICATION,// Always make sure to clean up everything else before worker directory 
Storm,WITHOUT_CLASSIFICATION,// Races are okay this is just to avoid extra work for each page load. 
Storm,WITHOUT_CLASSIFICATION,//  10th partition should not have been evicted 
Storm,WITHOUT_CLASSIFICATION,//  Note we could add support for setting the replication factor 
Storm,WITHOUT_CLASSIFICATION,//  delete anything older than an hour 
Storm,WITHOUT_CLASSIFICATION,// We only have a set amount of time we can wait for before looping around again 
Storm,WITHOUT_CLASSIFICATION,/*  LOOK AT LOCAL BLOBSTORE  */
Storm,WITHOUT_CLASSIFICATION,//  100 events with past ts should expire 
Storm,WITHOUT_CLASSIFICATION,/*  IFn  */
Storm,WITHOUT_CLASSIFICATION,//  The list of all executors (preferably sorted to make assignments simpler). 
Storm,WITHOUT_CLASSIFICATION,//  this is for backwards compatibility 
Storm,WITHOUT_CLASSIFICATION,//  see if lockTimeoutSec time has elapsed since we last selected the lock file 
Storm,WITHOUT_CLASSIFICATION,// The supervisor on the node down so add an orphaned slot to hold the unsupervised worker 
Storm,WITHOUT_CLASSIFICATION,// Not set if RECOVER_PARTIAL  Not set if RECOVER_PARTIAL 
Storm,WITHOUT_CLASSIFICATION,//  We can use the task index (starting from 0) as the partition ID 
Storm,WITHOUT_CLASSIFICATION,//  Read the length field. 
Storm,WITHOUT_CLASSIFICATION,// if authorizationId is not set set it to authenticationId. 
Storm,WITHOUT_CLASSIFICATION,// the first transaction in the new batch 
Storm,WITHOUT_CLASSIFICATION,//  end include processing 
Storm,WITHOUT_CLASSIFICATION,//  Reusing TupleInfo object as we directly call executor.ackSpoutMsg() & are not sending msgs. perf critical 
Storm,WITHOUT_CLASSIFICATION,//  ack 1 tuple 
Storm,WITHOUT_CLASSIFICATION,// Verify that the commit logic can handle offset voids due to log compaction 
Storm,WITHOUT_CLASSIFICATION,// The maximum number of state to search before stopping. 
Storm,WITHOUT_CLASSIFICATION,// so Kafka must be able to return more messages than that in order for the tests to be meaningful 
Storm,WITHOUT_CLASSIFICATION,// Log the user in and get the TGT 
Storm,WITHOUT_CLASSIFICATION,//  LONG_ARG 
Storm,WITHOUT_CLASSIFICATION,//  Make sure if there's enough bytes in the buffer. 
Storm,WITHOUT_CLASSIFICATION,//  Check to see if there are any existing files already localized. 
Storm,WITHOUT_CLASSIFICATION,//  generate a random storm id 
Storm,WITHOUT_CLASSIFICATION,//  A null value or a String value is acceptable 
Storm,WITHOUT_CLASSIFICATION,//  since this processor type is a committer this occurs in the commit phase 
Storm,WITHOUT_CLASSIFICATION,// Ignore the future 
Storm,WITHOUT_CLASSIFICATION,//  get the worker count back s.t. we can assert in each test function 
Storm,WITHOUT_CLASSIFICATION,//  MetricsCollectorConfig 
Storm,WITHOUT_CLASSIFICATION,//  22 seconds passed by still not timing out 
Storm,WITHOUT_CLASSIFICATION,// We are going to skip over CPU and Memory because they are captured elsewhere 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_ACTION_OPTIONS 
Storm,WITHOUT_CLASSIFICATION,//  this code here ensures that only one attempt is ever tracked for a batch so when 
Storm,WITHOUT_CLASSIFICATION,// Discard the pending records that are already committed 
Storm,WITHOUT_CLASSIFICATION,// We will not follow sym links 
Storm,WITHOUT_CLASSIFICATION,//  To Commit to Solr and Ack every tuple 
Storm,WITHOUT_CLASSIFICATION,// local node 
Storm,WITHOUT_CLASSIFICATION,//  calls this before actually killing the worker locally...   sends a "task finished" update 
Storm,WITHOUT_CLASSIFICATION,//  to keep track of free slots 
Storm,WITHOUT_CLASSIFICATION,//  should pass 
Storm,WITHOUT_CLASSIFICATION,//  optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  if there are no more pending consumed messages   and storm delivered ack for all 
Storm,WITHOUT_CLASSIFICATION,//  timer == null only if the processing guarantee is at-most-once 
Storm,WITHOUT_CLASSIFICATION,//  Give a hook chance to alter the clock. 
Storm,WITHOUT_CLASSIFICATION,// populating request context 
Storm,WITHOUT_CLASSIFICATION,// Allow poll if there are retriable tuples within the maxUncommittedOffsets limit 
Storm,WITHOUT_CLASSIFICATION,//  Build cluster and connect 
Storm,WITHOUT_CLASSIFICATION,//  2) check log file contents 
Storm,WITHOUT_CLASSIFICATION,//  create thread to process insertion of all metrics 
Storm,WITHOUT_CLASSIFICATION,//  2) deleting open file - should return true 
Storm,WITHOUT_CLASSIFICATION,//  Most of clojure tests currently try to access the blobs using getBlob. Since updateKeyForBlobStore   checks for updating the correct version of the blob as a part of nimbus ha before performing any   operation on it there is a necessity to stub several test cases to ignore this method. It is a valid   trade off to return if nimbusDetails which include the details of the current nimbus host port data are   not initialized as a part of the test. Moreover this applies to only local blobstore when used along with   nimbus ha. 
Storm,WITHOUT_CLASSIFICATION,// get spread components 
Storm,WITHOUT_CLASSIFICATION,//  shared off heap node memory 
Storm,WITHOUT_CLASSIFICATION,// re-emit second batch 
Storm,WITHOUT_CLASSIFICATION,/*  Thread safe. Same instance can be used across multiple threads  */
Storm,WITHOUT_CLASSIFICATION,//  a "topology source" is a class that can produce a `StormTopology` thrift object. 
Storm,WITHOUT_CLASSIFICATION,//  For the case that ack_fail message arrives before ack_init 
Storm,WITHOUT_CLASSIFICATION,//  properties file substitution 
Storm,WITHOUT_CLASSIFICATION,//  number of spout executors 
Storm,WITHOUT_CLASSIFICATION,//  JDK 7 tries to automatically drain the input streams for us   when the process exits but since close is not synchronized   it creates a race if we close the stream first and the same   fd is recycled.  the stream draining thread will attempt to   drain that fd!!  it may block OOM or cause bizarre behavior   see: https://bugs.openjdk.java.net/browse/JDK-8024521        issue is fixed in build 7u60 
Storm,WITHOUT_CLASSIFICATION,//  INITIAL_STATUS 
Storm,WITHOUT_CLASSIFICATION,/* solo */
Storm,WITHOUT_CLASSIFICATION,//  this is a sink and no result to emit. 
Storm,WITHOUT_CLASSIFICATION,//  yes it's just a test purpose 
Storm,WITHOUT_CLASSIFICATION,//  retrieving is encapsulated in Retrieval interface 
Storm,WITHOUT_CLASSIFICATION,//  MEMORY_GUARANTEE 
Storm,WITHOUT_CLASSIFICATION,// We don't really care too much about the scheduling of topology-gpu-0 because it was scheduled. 
Storm,WITHOUT_CLASSIFICATION,// Ignored will go with default timeout 
Storm,WITHOUT_CLASSIFICATION,// where state is stored in zookeeper (only for batch spout types) 
Storm,WITHOUT_CLASSIFICATION,// The blobstore is good now lets get the list of all topo Ids 
Storm,WITHOUT_CLASSIFICATION,//  Method which initializes nimbus admin 
Storm,WITHOUT_CLASSIFICATION,//  case 1: Control message 
Storm,WITHOUT_CLASSIFICATION,//  then (exception) 
Storm,WITHOUT_CLASSIFICATION,//  NUM_ERR_CHOICE 
Storm,WITHOUT_CLASSIFICATION,//  kinesis stream name to read from 
Storm,WITHOUT_CLASSIFICATION,// The spout must respect maxUncommittedOffsets after committing a set of records 
Storm,WITHOUT_CLASSIFICATION,//  other builder functions not exposed:    * createsObjectNamesWith(ObjectNameFactory onFactory)    * registerWith (MBeanServer)    * specificDurationUnits (Map<StringTimeUnit> specificDurationUnits)    * specificRateUnits(Map<StringTimeUnit> specificRateUnits) 
Storm,WITHOUT_CLASSIFICATION,//  Restart topology with the same topology id which mimics the behavior of partition reassignment 
Storm,WITHOUT_CLASSIFICATION,// Everything is scheduled correctly so no need to search any more. 
Storm,WITHOUT_CLASSIFICATION,/*              * Nodes in the descending order of priority.             * ProcessorNode has higher priority than partition and window nodes             * so that the topological order iterator will group as many processor nodes together as possible.             * UpdateStateByKeyProcessor has a higher priority than StateQueryProcessor so that StateQueryProcessor             * can be mapped to the same StatefulBolt that UpdateStateByKeyProcessor is part of.              */
Storm,WITHOUT_CLASSIFICATION,//  2) If no abandoned files then pick oldest file in sourceDirPath lock it and rename it 
Storm,WITHOUT_CLASSIFICATION,//  ack to not process the record again on restart and move on to next message 
Storm,WITHOUT_CLASSIFICATION,//  then 
Storm,WITHOUT_CLASSIFICATION,/*  validate cpu settings  */
Storm,WITHOUT_CLASSIFICATION,//  release both locks 
Storm,WITHOUT_CLASSIFICATION,// children is only ever null if topologyBasicBlobsRootDir does not exist.  This happens during unit tests   And because a non-existant directory is by definition clean we are ignoring it. 
Storm,WITHOUT_CLASSIFICATION,//  last partition is not evicted 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_SHARED_ON_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,// We have something to schedule... 
Storm,WITHOUT_CLASSIFICATION,// Support for worker tokens Similar to an IAutoCredentials implementation 
Storm,WITHOUT_CLASSIFICATION,//  unexpected error
Storm,WITHOUT_CLASSIFICATION,//  NUM_EXECUTORS 
Storm,WITHOUT_CLASSIFICATION,//  A blank result communicates that there are no more blobs. 
Storm,WITHOUT_CLASSIFICATION,//  Files/move with non-empty directory doesn't work well on Windows   FileUtils.moveDirectory is not atomic 
Storm,WITHOUT_CLASSIFICATION,// play 2nd tuple 
Storm,WITHOUT_CLASSIFICATION,// each thread will have its own request context 
Storm,WITHOUT_CLASSIFICATION,/*          * if the current processor preserves the key and is         * already partitioned on key skip the re-partition.          */
Storm,WITHOUT_CLASSIFICATION,// Test for Nimbus itself as a user 
Storm,WITHOUT_CLASSIFICATION,//  register most recent relogin attempt 
Storm,WITHOUT_CLASSIFICATION,//  Decoder 
Storm,WITHOUT_CLASSIFICATION,// This is allowed because the committed message brings the numUncommittedOffsets below the cap 
Storm,WITHOUT_CLASSIFICATION,//         ret.numSupervisors = clusterSummary.get_supervisors_size(); 
Storm,WITHOUT_CLASSIFICATION,//  only enable cleanup of blobstore on nimbus 
Storm,WITHOUT_CLASSIFICATION,//  disregard first line because it has header already read 
Storm,WITHOUT_CLASSIFICATION,// Sleep for 5 mins 
Storm,WITHOUT_CLASSIFICATION,//  It is imperative to not run the function   inside the timer lock. Otherwise it is   possible to deadlock if the fn deals with   other locks like the submit lock. 
Storm,WITHOUT_CLASSIFICATION,//  ID_TO_BOLT_AGG_STATS 
Storm,WITHOUT_CLASSIFICATION,// Lets use the number of actually scheduled workers as a way to bridge RAS and non-RAS 
Storm,WITHOUT_CLASSIFICATION,//  AcceptedBlockTimeRatios obtained by empirical testing (see comment block above) 
Storm,WITHOUT_CLASSIFICATION,// Verify if WorkerTokenManager recognizes the expired WorkerToken. 
Storm,WITHOUT_CLASSIFICATION,//  good to go increment # of tasks this component is being executed on 
Storm,WITHOUT_CLASSIFICATION,// order executors to be scheduled 
Storm,WITHOUT_CLASSIFICATION,//  if it does not exist. 
Storm,WITHOUT_CLASSIFICATION,//  Metadata information to commit to Kafka. It is unique per spout instance. 
Storm,WITHOUT_CLASSIFICATION,//  Number of times we had to backtrack. 
Storm,WITHOUT_CLASSIFICATION,/*                  * if a null tuple is not configured to be emitted it should be marked as emitted and acked immediately to allow its offset                 * to be commited to Kafka                  */
Storm,WITHOUT_CLASSIFICATION,// non impersonating request should be permitted. 
Storm,WITHOUT_CLASSIFICATION,// set the number of workers to be the same as partition number.  the idea is to have a spout and a partial count bolt co-exist in one  worker to avoid shuffling messages across workers in storm cluster. 
Storm,WITHOUT_CLASSIFICATION,// topology being null is used for tests  We probably should fix that at some point   but it is not trivial to do... 
Storm,WITHOUT_CLASSIFICATION,/* ==================================================     * Helper Classes     *================================================== */
Storm,WITHOUT_CLASSIFICATION,//  Stop searching as soon as passed current time 
Storm,WITHOUT_CLASSIFICATION,/*      * Register a IMetric instance.     *     * Storm will then call `getValueAndReset()` on the metric every `timeBucketSizeInSecs`     * and the returned value is sent to all metrics consumers.     *     * You must call this during `IBolt.prepare()` or `ISpout.open()`.     * @return The IMetric argument unchanged.      */
Storm,WITHOUT_CLASSIFICATION,//  1 expired 
Storm,WITHOUT_CLASSIFICATION,// Commit polled records immediately to ensure delivery is at-most-once. 
Storm,WITHOUT_CLASSIFICATION,// nimbus:num-blacklisted-supervisor + non-blacklisted supervisor = nimbus:num-supervisors 
Storm,WITHOUT_CLASSIFICATION,// In distributed mode send heartbeat directly to master if local supervisor goes down. 
Storm,WITHOUT_CLASSIFICATION,//  class BatchInserter 
Storm,WITHOUT_CLASSIFICATION,//  pause other topic-partitions to only poll from current topic-partition 
Storm,WITHOUT_CLASSIFICATION,// A map of assignments organized by node with the following format: 
Storm,WITHOUT_CLASSIFICATION,//  Earliest start 
Storm,WITHOUT_CLASSIFICATION,//  can be regular nodes (static state) or processor nodes 
Storm,WITHOUT_CLASSIFICATION,// rotateOutputFile(writer) has closed the writer. It's safe to remove the writer from the map here. 
Storm,WITHOUT_CLASSIFICATION,//  ensure that the first three tasks have been selected before 
Storm,WITHOUT_CLASSIFICATION,//  swapping two arrays 
Storm,WITHOUT_CLASSIFICATION,//  if the streamId is defined use it for the grouping otherwise assume storm's default stream 
Storm,WITHOUT_CLASSIFICATION,//  GROUPS 
Storm,WITHOUT_CLASSIFICATION,/*                 * print the results to stdout                 */
Storm,WITHOUT_CLASSIFICATION,// Verify that bob's token has expired 
Storm,WITHOUT_CLASSIFICATION,//  just make a note of the oldest expired lock now and check if its still unmodified after lockTimeoutSec 
Storm,WITHOUT_CLASSIFICATION,//  take the batched metric data and write to the database 
Storm,WITHOUT_CLASSIFICATION,//  assume message is immediately ACKed in non-ack mode 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTORS 
Storm,WITHOUT_CLASSIFICATION,/*  debug only! Once we have confidence can lose this.  */
Storm,WITHOUT_CLASSIFICATION,//  enable ACKing 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   required 
Storm,WITHOUT_CLASSIFICATION,//  Identify the join field for the stream and look it up in 'tuple'. field can be nested field:  outerKey.innerKey 
Storm,WITHOUT_CLASSIFICATION,// just skip when any error happens wait for next round assignments reassign 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_CPU 
Storm,WITHOUT_CLASSIFICATION,//  There's enough bytes in the buffer. Read it. 
Storm,WITHOUT_CLASSIFICATION,//  if cb returns false we are done with this section of rows 
Storm,WITHOUT_CLASSIFICATION,// For others using too much it is really a question of how much memory is free in the system 
Storm,WITHOUT_CLASSIFICATION,//  partition ids 
Storm,WITHOUT_CLASSIFICATION,// Topology may be deployed in deactivated mode wait for activation 
Storm,WITHOUT_CLASSIFICATION,// random number generator 
Storm,WITHOUT_CLASSIFICATION,//  JUnit ensures that the temporary folder is removed after   the test finishes 
Storm,WITHOUT_CLASSIFICATION,//  Instead of iterating again it would be possible to commit and update the state for each TopicPartition   in the prior loop but the multiple network calls should be more expensive than iterating twice over a small loop 
Storm,WITHOUT_CLASSIFICATION,//  -- source dir config 
Storm,WITHOUT_CLASSIFICATION,//  1) read initial lines in file then check if lock exists 
Storm,WITHOUT_CLASSIFICATION,//  consider all events for the initial window 
Storm,WITHOUT_CLASSIFICATION,// initialize assignment map 
Storm,WITHOUT_CLASSIFICATION,//  save metric key/value to be batched 
Storm,WITHOUT_CLASSIFICATION,//  SIGNATURE 
Storm,WITHOUT_CLASSIFICATION,//  allow blacklist scheduler to cache the supervisor 
Storm,WITHOUT_CLASSIFICATION,//  invoke setter 
Storm,WITHOUT_CLASSIFICATION,//  to be done for SASL_TOKEN_MESSAGE_REQUEST requests. 
Storm,WITHOUT_CLASSIFICATION,// This class assumes that there is at most one retry schedule per message id in this set at a time. 
Storm,WITHOUT_CLASSIFICATION,//  remove the executors cache to let it recompute. 
Storm,WITHOUT_CLASSIFICATION,//  numFails = 1 2 3 ... 
Storm,WITHOUT_CLASSIFICATION,// check if annotation is one of our 
Storm,WITHOUT_CLASSIFICATION,// return false if can't increment anymore 
Storm,WITHOUT_CLASSIFICATION,//  Thread Polling every 5 seconds to update the wordSet seconds which is   used in FilterWords bolt to filter the words 
Storm,WITHOUT_CLASSIFICATION,//  RECORDS 
Storm,WITHOUT_CLASSIFICATION,//  for tests reader will not be null 
Storm,WITHOUT_CLASSIFICATION,//  FULL_CLASS_NAME 
Storm,WITHOUT_CLASSIFICATION,//  Acked messages sorted by ascending order of offset 
Storm,WITHOUT_CLASSIFICATION,//  Advance time and then trigger first call to kafka consumer commit; the commit must progress to offset 9 
Storm,WITHOUT_CLASSIFICATION,//  get topology info 
Storm,WITHOUT_CLASSIFICATION,//  To make logic simple it assumes that all the tables have one PK (which it should be extended to support composed key) 
Storm,WITHOUT_CLASSIFICATION,// We are not really running anything so make this   simple to check for 
Storm,WITHOUT_CLASSIFICATION,//  select tasks once more than the number of tasks available 
Storm,WITHOUT_CLASSIFICATION,//  IAutoCredentials   ICredentialsRenewer   INimbusCredentialPlugin    IPrincipalToLocal    IGroupMappingServiceProvider  
Storm,WITHOUT_CLASSIFICATION,// Remove any configs that are specific to a host that might mess with the running topology. 
Storm,WITHOUT_CLASSIFICATION,//  ################# Subscribe Callback Implementation ###################### 
Storm,WITHOUT_CLASSIFICATION,//  Removing self so as not to create a deadlock where a nimbus is trying to download a missing blob   from itself 
Storm,WITHOUT_CLASSIFICATION,//  the states to be recovered 
Storm,WITHOUT_CLASSIFICATION,// topo-3 evicted since user bobby don't have any resource guarantees and topo-3 is the lowest priority for user bobby 
Storm,WITHOUT_CLASSIFICATION,//  always provide mocked HiveWriter 
Storm,WITHOUT_CLASSIFICATION,//  to be use. If we cannot calculate it assume that it is bad 
Storm,WITHOUT_CLASSIFICATION,//  Report messageSizes metric if enabled (non-null). 
Storm,WITHOUT_CLASSIFICATION,//  NODES 
Storm,WITHOUT_CLASSIFICATION,//  perf critical check to avoid unnecessary allocation 
Storm,WITHOUT_CLASSIFICATION,// Not symmetric difference. Performing A.entrySet() - B.entrySet() 
Storm,WITHOUT_CLASSIFICATION,//  and provides PairStream(KeyedStream) to consumer bolt. 
Storm,WITHOUT_CLASSIFICATION,/*                 * The elements having the same key within the window will be grouped                * together and the corresponding values will be merged.                *                * The result is a PairStream<String Iterable<Double>> with                * 'stock symbol' as the key and 'stock prices' for that symbol within the window as the value.                 */
Storm,WITHOUT_CLASSIFICATION,//  returns nil if doesn't exist 
Storm,WITHOUT_CLASSIFICATION,//  these triggers will be retried as part of batch retries 
Storm,WITHOUT_CLASSIFICATION,// Emit and ack some tuples ensure that some polled tuples remain cached in the spout by emitting less than maxPollRecords 
Storm,WITHOUT_CLASSIFICATION,//  nature of join   field for the current stream   field for the other (2nd) stream 
Storm,WITHOUT_CLASSIFICATION,// if a new document should be inserted if there are no matches to the query filter  updateBolt.withUpsert(true); 
Storm,WITHOUT_CLASSIFICATION,//  Schedule Nimbus inbox cleaner 
Storm,WITHOUT_CLASSIFICATION,//  2 - Setup Topology  -------- 
Storm,WITHOUT_CLASSIFICATION,//  Key has not been created yet and it is the first time it is being created 
Storm,WITHOUT_CLASSIFICATION,//  cleanup thread killing topology in b/w assignment and starting the topology 
Storm,WITHOUT_CLASSIFICATION,//  no offset commits have ever been done for this consumer group and topic-partition   so start at the beginning or end depending on FirstPollOffsetStrategy 
Storm,WITHOUT_CLASSIFICATION,//  The current executor we are trying to schedule 
Storm,WITHOUT_CLASSIFICATION,//  TOTAL_TASKS 
Storm,WITHOUT_CLASSIFICATION,//  METRIC_VALUE 
Storm,WITHOUT_CLASSIFICATION,// find the smallest offset in toResend list 
Storm,WITHOUT_CLASSIFICATION,//  if reached so far add it to the set of messages waiting to be retried with next retry time based on how many times it failed 
Storm,WITHOUT_CLASSIFICATION,// end of Test2 
Storm,WITHOUT_CLASSIFICATION,//  onheap and offheap memory requirement 
Storm,WITHOUT_CLASSIFICATION,// Emit the messages 
Storm,WITHOUT_CLASSIFICATION,//  Wrapper class handy for the client code to use the JSON parser to build to use with JSON parser 
Storm,WITHOUT_CLASSIFICATION,// <ExecutorDetails - Task Map<String - Type of resource Map<String - type of that resource Double - amount>>> 
Storm,WITHOUT_CLASSIFICATION,//  Remove it from failedPerShard anyway 
Storm,WITHOUT_CLASSIFICATION,//  SERIALIZED_PARTS 
Storm,WITHOUT_CLASSIFICATION,//  remove contiguous elements from the head of the heap 
Storm,WITHOUT_CLASSIFICATION,//  merge and push unions rules 
Storm,WITHOUT_CLASSIFICATION,//  test read 
Storm,WITHOUT_CLASSIFICATION,// find the smallest offset in pending list 
Storm,WITHOUT_CLASSIFICATION,//  create root directory if not exist 
Storm,WITHOUT_CLASSIFICATION,//  shard iterator type based on kinesis api - beginning of time latest at timestamp are only supported 
Storm,WITHOUT_CLASSIFICATION,// Log Writer Command... 
Storm,WITHOUT_CLASSIFICATION,//  COMPONENT_TO_NUM_TASKS 
Storm,WITHOUT_CLASSIFICATION,//  JSONAware not working for nested element on Map so write JSON format from here 
Storm,WITHOUT_CLASSIFICATION,//  2) Join the streams in order of streamJoinOrder 
Storm,WITHOUT_CLASSIFICATION,//  explicitly anchor emits to corresponding input tuples only as default window anchoring will anchor them to all tuples   in window 
Storm,WITHOUT_CLASSIFICATION,//  The following tests are run for both hdfs and local store to test the 
Storm,WITHOUT_CLASSIFICATION,//  It covers scenarios expalined in scenario 3 when nimbus-1 holding the latest   update goes down before it is downloaded by nimbus-2. Nimbus-2 gets elected as a leader 
Storm,WITHOUT_CLASSIFICATION,// end of Test3 
Storm,WITHOUT_CLASSIFICATION,//  can be null for things like partitionPersist occuring off a DRPC stream 
Storm,WITHOUT_CLASSIFICATION,//  key1 shouldn't be in iterator since it's marked as deleted 
Storm,WITHOUT_CLASSIFICATION,// defaults to seconds 
Storm,WITHOUT_CLASSIFICATION,//  the combination of the lock and the finished flag ensure that   an id is never timed out if it has been finished 
Storm,WITHOUT_CLASSIFICATION,// NOTE: @IsImplementationOfClass(implementsClass = IStrategy.class) is enforced in DaemonConf so   an error will be thrown by nimbus on topology submission and not by the client prior to submitting   the topology. 
Storm,WITHOUT_CLASSIFICATION,/*                 * convert the state back to a stream and print the results                 */
Storm,WITHOUT_CLASSIFICATION,//  make sure we've handled all supervisors on the host before we break 
Storm,WITHOUT_CLASSIFICATION,//  EXEC_SUMMARY 
Storm,WITHOUT_CLASSIFICATION,//  will be used instead 
Storm,WITHOUT_CLASSIFICATION,//  WORKERS 
Storm,WITHOUT_CLASSIFICATION,//  other members 
Storm,WITHOUT_CLASSIFICATION,// Worker Command... 
Storm,WITHOUT_CLASSIFICATION,// reset all the weights 
Storm,WITHOUT_CLASSIFICATION,// When nid and zid are not equal nid is attempting to impersonate zid We 
Storm,WITHOUT_CLASSIFICATION,//  offset was previously committed for this consumer group and topic-partition either by this or another topology. 
Storm,WITHOUT_CLASSIFICATION,// The user here from the jaas conf is bob.  No impersonation is done so verify that 
Storm,WITHOUT_CLASSIFICATION,//  Tar is not native to Windows. Use simple Java based implementation for   tests and simple tar archives 
Storm,WITHOUT_CLASSIFICATION,//  This means we are pointing at a file. 
Storm,WITHOUT_CLASSIFICATION,//  user configurable 
Storm,WITHOUT_CLASSIFICATION,//  wildcard is given in file 
Storm,WITHOUT_CLASSIFICATION,//  DETAILS 
Storm,WITHOUT_CLASSIFICATION,//  check adding reference to local resource with topology of same name 
Storm,WITHOUT_CLASSIFICATION,//  supervisor health check 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_RESOURCES_OVERRIDES 
Storm,WITHOUT_CLASSIFICATION,// check if exec can be on worker based on user defined component exclusions 
Storm,WITHOUT_CLASSIFICATION,// First verify that if something has a high load it's distribution will drop over time 
Storm,WITHOUT_CLASSIFICATION,//  daemons can only be 'nimbus' 'supervisor' or 'worker' 
Storm,WITHOUT_CLASSIFICATION,//  Ignore NoNodeExists exceptions because when sync() it may populate curr with stale data since   zookeeper reads are eventually consistent. 
Storm,WITHOUT_CLASSIFICATION,//  Spout internals 
Storm,WITHOUT_CLASSIFICATION,//  get tasks if the user is authorized for this topology 
Storm,WITHOUT_CLASSIFICATION,//  check for sub-struct validity 
Storm,WITHOUT_CLASSIFICATION,// this takes care of setting up coord streams for spouts and bolts 
Storm,WITHOUT_CLASSIFICATION,//  Note: We don't return from this method on ParseException to avoid triggering the   spout wait strategy (due to no emits). Instead we go back into the loop and   generate a tuple from next file 
Storm,WITHOUT_CLASSIFICATION,// This happens when the key is not found the cache loader returns a null and this exception is thrown.   because the cache cannot store a null. 
Storm,WITHOUT_CLASSIFICATION,//  Rack id to list of host names in that rack 
Storm,WITHOUT_CLASSIFICATION,//  1) First re-emit any previously failed tuples (from retryList) 
Storm,WITHOUT_CLASSIFICATION,// NImbus metrics distribution 
Storm,WITHOUT_CLASSIFICATION,//  authz = authn 
Storm,WITHOUT_CLASSIFICATION,// Check if the user is allowed to read this 
Storm,WITHOUT_CLASSIFICATION,//  Now parse it and return the map. 
Storm,WITHOUT_CLASSIFICATION,//  After this resources should contain all the kinds of resources   we can count for the group. If we see a kind of resource in another   node not in resources.keySet() we'll throw. 
Storm,WITHOUT_CLASSIFICATION,// When using the no guarantee mode the spout must commit tuples periodically regardless of whether they've been acked 
Storm,WITHOUT_CLASSIFICATION,// we expect to notify supervisors at almost the same time 
Storm,WITHOUT_CLASSIFICATION,//  MASTER_CODE_DIR 
Storm,WITHOUT_CLASSIFICATION,//  increment the fail count as we started with 0 
Storm,WITHOUT_CLASSIFICATION,// When reading the conf in nimbus we want to fall back to our own settings 
Storm,WITHOUT_CLASSIFICATION,// May be null if worker tokens are not supported by the thrift transport. 
Storm,WITHOUT_CLASSIFICATION,//  Read 1 line 
Storm,WITHOUT_CLASSIFICATION,// Expected 
Storm,WITHOUT_CLASSIFICATION,// TODO 
Storm,WITHOUT_CLASSIFICATION,// construct the final Assignments by adding start-times etc into it 
Storm,WITHOUT_CLASSIFICATION,//  SHARED_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  this is just in case supervisor is down so that disk doesn't fill up.   it shouldn't take supervisor 120 seconds between listing dir and reading it 
Storm,WITHOUT_CLASSIFICATION,//  components 
Storm,WITHOUT_CLASSIFICATION,//  For testing be careful as it doesn't clone 
Storm,WITHOUT_CLASSIFICATION,// If the write failed try to sync anything already written 
Storm,WITHOUT_CLASSIFICATION,// Schema change should have forced a rotation 
Storm,WITHOUT_CLASSIFICATION,//  These can be chained (like with setting the CPU requirement) 
Storm,WITHOUT_CLASSIFICATION,//  =============================================================================   ============================ getter main.methods =================================   =============================================================================
Storm,WITHOUT_CLASSIFICATION,// add the nid as the real user in reqContext's subject which will be used during authorization. 
Storm,WITHOUT_CLASSIFICATION,//  Always empty if processing guarantee is none or at-most-once 
Storm,WITHOUT_CLASSIFICATION,//  set to keep 2 blobs (each of size 34) 
Storm,WITHOUT_CLASSIFICATION,//  seek next offset after last offset from previous batch 
Storm,WITHOUT_CLASSIFICATION,// This is not atomic (so if something bad happens in the middle we need to be able to recover 
Storm,WITHOUT_CLASSIFICATION,// The unique topology id for the topology that created this metadata 
Storm,WITHOUT_CLASSIFICATION,//  locate oldest expired lock file (if any) and take ownership 
Storm,WITHOUT_CLASSIFICATION,// populating request context  
Storm,WITHOUT_CLASSIFICATION,//  Check last block scheduling time does not get significantly slower 
Storm,WITHOUT_CLASSIFICATION,//  null if it wasn't sampled 
Storm,WITHOUT_CLASSIFICATION,//  expecting AlreadyBeingCreatedException inside RemoteException 
Storm,WITHOUT_CLASSIFICATION,// now pending: [3] toResend: [12] 
Storm,WITHOUT_CLASSIFICATION,//  can't leave choices to be empty so initiate it similar as ShuffleGrouping 
Storm,WITHOUT_CLASSIFICATION,//  min-heap 
Storm,WITHOUT_CLASSIFICATION,//  Retries management 
Storm,WITHOUT_CLASSIFICATION,//  close all the created hTable instances 
Storm,WITHOUT_CLASSIFICATION,// The service must be able to remove retry schedules for unnecessary partitions 
Storm,WITHOUT_CLASSIFICATION,// the default is false.  the default is false. 
Storm,WITHOUT_CLASSIFICATION,//  use redis based state store for persistence 
Storm,WITHOUT_CLASSIFICATION,//  EXPIRATION_TIME_MILLIS 
Storm,WITHOUT_CLASSIFICATION,//  ackers==null when ackerCount not explicitly set on the topology 
Storm,WITHOUT_CLASSIFICATION,//  on whether they are IBasicBolt or IRichBolt instances 
Storm,WITHOUT_CLASSIFICATION,// This really should be impossible because we go off of the min load and inc anything within 5% of it.   But just to be sure it is never an issue especially with float rounding etc. 
Storm,WITHOUT_CLASSIFICATION,//  Therefore the timer in newAssignment won't be invoked 
Storm,WITHOUT_CLASSIFICATION,/*      * performs a hash-join by constructing a hash map of the smaller set iterating over the     * larger set and finding matching rows in the hash map.      */
Storm,WITHOUT_CLASSIFICATION,//  if all go down which is unlikely. Hence there might be a need to update the blob if all go down. 
Storm,WITHOUT_CLASSIFICATION,//  watermark interval 
Storm,WITHOUT_CLASSIFICATION,//  We can't move this to outside without breaking backward compatibility. 
Storm,WITHOUT_CLASSIFICATION,//  Tuples that were successfully acked/emitted. These tuples will be committed periodically when the commit timer expires 
Storm,WITHOUT_CLASSIFICATION,//  inner join - core implementation 
Storm,WITHOUT_CLASSIFICATION,//  This should throw AuthorizationException because auth failed 
Storm,WITHOUT_CLASSIFICATION,/*  IPersistentMap  */
Storm,WITHOUT_CLASSIFICATION,// race condition with delete 
Storm,WITHOUT_CLASSIFICATION,// The tick should cause tuple1 to be ack'd 
Storm,WITHOUT_CLASSIFICATION,// The consumer should not be seeking to retry the failed tuple it should just be continuing from the current position 
Storm,WITHOUT_CLASSIFICATION,// Interrupted is thrown when we are shutting down.   So just ignore it for now... 
Storm,WITHOUT_CLASSIFICATION,// Worker launched through external commands hence we count their exceptions toward shell exceptions 
Storm,WITHOUT_CLASSIFICATION,// JSTACK DUMP 
Storm,WITHOUT_CLASSIFICATION,/*                 * The result of aggregation is forwarded to                * the RedisStoreBolt. The forwarded tuple is a                * key-value pair of (word count) with ("key" "value")                * being the field names.                 */
Storm,WITHOUT_CLASSIFICATION,// Waiting for spout tuples isn't strictly necessary since we also wait for bolt emits but do it anyway  Allow two minutes for topology startup then wait for at most the time it should take to produce 10 windows 
Storm,WITHOUT_CLASSIFICATION,/*                  * NO-OP: the events are ack-ed in execute                  */
Storm,WITHOUT_CLASSIFICATION,//  list files 
Storm,WITHOUT_CLASSIFICATION,// worker.childopts validates 
Storm,WITHOUT_CLASSIFICATION,//  We ignore workers that are still bound to a slot which is monitored by a supervisor 
Storm,WITHOUT_CLASSIFICATION,// all events are sent successfully return last sent offset 
Storm,WITHOUT_CLASSIFICATION,//  metric timestamp value Map of tagK/tagV respectively. 
Storm,WITHOUT_CLASSIFICATION,// host-topoId-port-fileName 
Storm,WITHOUT_CLASSIFICATION,//  the first field is the batch id 
Storm,WITHOUT_CLASSIFICATION,//  STATS 
Storm,WITHOUT_CLASSIFICATION,// The writer must be closed before removed from the map.  If it failed we might lose some data. 
Storm,WITHOUT_CLASSIFICATION,//  realm is ignored 
Storm,WITHOUT_CLASSIFICATION,//  -- clocks in sync 
Storm,WITHOUT_CLASSIFICATION,// In some cases the new LocalAssignment may be equivalent to the old but   It is not equal.  In those cases we want to update the current assignment to   be the same as the new assignment 
Storm,WITHOUT_CLASSIFICATION,//  Validate a single task id return 
Storm,WITHOUT_CLASSIFICATION,// Now also check that no more tuples are polled for since both partitions are at their limits 
Storm,WITHOUT_CLASSIFICATION,//  Force a send error 
Storm,WITHOUT_CLASSIFICATION,//  Overloading the assertStoreHasExactly method accomodate Subject in order to check for authorization 
Storm,WITHOUT_CLASSIFICATION,// null worker id means generate one... 
Storm,WITHOUT_CLASSIFICATION,// There is a race on credentials where they can be leaked in some versions of storm. 
Storm,WITHOUT_CLASSIFICATION,// play all tuples 
Storm,WITHOUT_CLASSIFICATION,// this will fail the test since user derek does not have an entry for memory 
Storm,WITHOUT_CLASSIFICATION,//  5   0   0   4   1 
Storm,WITHOUT_CLASSIFICATION,//  MESSAGE_ID 
Storm,WITHOUT_CLASSIFICATION,//  REBALANCE_OPTIONS 
Storm,WITHOUT_CLASSIFICATION,//  512: for most scenes to avoid inner array resizing 
Storm,WITHOUT_CLASSIFICATION,//  early return as no shard is assigned - probably because number of executors > number of shards 
Storm,WITHOUT_CLASSIFICATION,//  acquire another lock on file1 and verify it failed 
Storm,WITHOUT_CLASSIFICATION,// Commit offsets 
Storm,WITHOUT_CLASSIFICATION,//  Catching and logging KeyNotFoundException because if   there is a subsequent update and delete the non-leader   nimbodes might throw an exception. 
Storm,WITHOUT_CLASSIFICATION,// Nimbus Compatibility 
Storm,WITHOUT_CLASSIFICATION,//  Returns either the source component name or the stream name for the tuple 
Storm,WITHOUT_CLASSIFICATION,// We want to be able to select the measurement interval   reporting window (We don't need 3 different reports)   We want to be able to specify format (and configs specific to the format)   With perhaps defaults overall 
Storm,WITHOUT_CLASSIFICATION,// Modifies justAssignedKeys 
Storm,WITHOUT_CLASSIFICATION,// This is called async so lets assume that it is something we care about 
Storm,WITHOUT_CLASSIFICATION,//  remove uploaded jars blobs not artifacts since they're shared across the cluster   Note that we don't handle TException to delete jars blobs   because it's safer to leave some blobs instead of topology not running 
Storm,WITHOUT_CLASSIFICATION,// get storm values and emit 
Storm,WITHOUT_CLASSIFICATION,//  Complete the send 
Storm,WITHOUT_CLASSIFICATION,// Initial delay for the commit and assignment refresh timers 
Storm,WITHOUT_CLASSIFICATION,//  TODO enable if setStateSpout gets implemented      @Test(expected = IllegalArgumentException.class) 
Storm,WITHOUT_CLASSIFICATION,/*                  * Here we don't set the tuples in windowedOutputCollector's context and emit un-anchored.                 * The checkpoint tuple will trigger a checkpoint in the receiver with the emitted tuples.                  */
Storm,WITHOUT_CLASSIFICATION,//  storm configuration 
Storm,WITHOUT_CLASSIFICATION,//  Looks for files in the directory with .current suffix 
Storm,WITHOUT_CLASSIFICATION,/*                  * This test sends a broadcast to all connected clients from the server so we need to wait until the server has registered                 * the client as connected before sending load metrics.                 *                 * It's not enough to wait until the client reports that the channel is open because the server event loop may not have                 * finished running channelActive for the new channel. If we send metrics too early the server will broadcast to no one.                 *                 * By waiting for the response here we ensure that the client will be registered at the server before we send load metrics.                  */
Storm,WITHOUT_CLASSIFICATION,//  Checks for assertion when we turn on security 
Storm,WITHOUT_CLASSIFICATION,//  ################# Listener Implementation ###################### 
Storm,WITHOUT_CLASSIFICATION,//  not required   not required 
Storm,WITHOUT_CLASSIFICATION,// This will only get updated once 
Storm,WITHOUT_CLASSIFICATION,//  determine how long to sleep from looking at ticket's expiry.   We should not allow the ticket to expire but we should take into consideration   MIN_TIME_BEFORE_RELOGIN. Will not sleep less than MIN_TIME_BEFORE_RELOGIN unless doing so   would cause ticket expiration. 
Storm,WITHOUT_CLASSIFICATION,// 2 try to append to a closed file 
Storm,WITHOUT_CLASSIFICATION,//  =====================================================================================   convert thrift stats to java maps   ===================================================================================== 
Storm,WITHOUT_CLASSIFICATION,//  SHARED_RESOURCES 
Storm,WITHOUT_CLASSIFICATION,// Make the spout commit any acked tuples 
Storm,WITHOUT_CLASSIFICATION,//  USERS 
Storm,WITHOUT_CLASSIFICATION,//  no heartbeat for this one should be 0 
Storm,WITHOUT_CLASSIFICATION,// clear up the kerberos state. But the tokens are not cleared! As per  the Java kerberos login module code only the kerberos credentials 
Storm,WITHOUT_CLASSIFICATION,//  copy constructor 
Storm,WITHOUT_CLASSIFICATION,// Writes the offsets in the new format to the /user partitions paths 
Storm,WITHOUT_CLASSIFICATION,// specify a configuration object to be used 
Storm,WITHOUT_CLASSIFICATION,// The spout should emit at most one message per call to nextTuple  This is necessary for Storm to be able to throttle the spout according to maxSpoutPending 
Storm,WITHOUT_CLASSIFICATION,// reserved for future 
Storm,WITHOUT_CLASSIFICATION,//  same set of events part of three windows 
Storm,WITHOUT_CLASSIFICATION,//         bd.shuffleGrouping(SPOUT_ID); 
Storm,WITHOUT_CLASSIFICATION,//  a stream of stock quotes 
Storm,WITHOUT_CLASSIFICATION,//  If MapFunction is aware of cleanup let it handle cleaning up 
Storm,WITHOUT_CLASSIFICATION,//  stream name unspecified 
Storm,WITHOUT_CLASSIFICATION,//  topo3 has 4 large tasks 
Storm,WITHOUT_CLASSIFICATION,//  SUCCESS 
Storm,WITHOUT_CLASSIFICATION,//  set watermark interval to a high value and trigger manually to fix timing issues 
Storm,WITHOUT_CLASSIFICATION,// fail 1st tuple 
Storm,WITHOUT_CLASSIFICATION,//  Tuples that have been emitted but that are "on the wire" i.e. pending being acked or failed. 
Storm,WITHOUT_CLASSIFICATION,// Ensure Nimbus has leadership otherwise topology submission will fail. 
Storm,WITHOUT_CLASSIFICATION,//  ignore... 
Storm,WITHOUT_CLASSIFICATION,//  if this latch is closed we need to create new instance. 
Storm,WITHOUT_CLASSIFICATION,//  actually Map<String Map<String Map<String Long/Double>>> 
Storm,WITHOUT_CLASSIFICATION,//  Objects are absent if they were zero both this iteration   and the last -- if only this one we need to report zero. 
Storm,WITHOUT_CLASSIFICATION,// with realm e.g. hdfs@WITZEND.COM   /etc/security/keytabs/storm.keytab 
Storm,WITHOUT_CLASSIFICATION,// We get a new random number and seed it to make sure that runs are consistent where possible. 
Storm,WITHOUT_CLASSIFICATION,//  if tuple arrives from a spout it can be passed as is   otherwise the value is in the first field of the tuple 
Storm,WITHOUT_CLASSIFICATION,//  TOTAL_TOPOLOGIES 
Storm,WITHOUT_CLASSIFICATION,//  simulate the time trigger by setting the reference time and invoking onTrigger() manually 
Storm,WITHOUT_CLASSIFICATION,//  returns paused topic-partitions. 
Storm,WITHOUT_CLASSIFICATION,//  The free pool never has anything running 
Storm,WITHOUT_CLASSIFICATION,// The time is now twice the message timeout the second tuple should expire since it was not acked 
Storm,WITHOUT_CLASSIFICATION,//  Define our taskIds 
Storm,WITHOUT_CLASSIFICATION,//  wrap KeyNotFoundException in an InvalidTopologyException 
Storm,WITHOUT_CLASSIFICATION,//  user class supplied...   this also provides a bridge to Trident... 
Storm,WITHOUT_CLASSIFICATION,// Trigger manually to avoid timing issues 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTOR_ID 
Storm,WITHOUT_CLASSIFICATION,// Create links to artifacts dir 
Storm,WITHOUT_CLASSIFICATION,//  other classes from config 
Storm,WITHOUT_CLASSIFICATION,//  0 means delegate batch size = trident batch size. 
Storm,WITHOUT_CLASSIFICATION,// Initial delay for the assignment refresh timer 
Storm,WITHOUT_CLASSIFICATION,//  names 
Storm,WITHOUT_CLASSIFICATION,//  wait for lock to expire 
Storm,WITHOUT_CLASSIFICATION,//  If that fails use config 
Storm,WITHOUT_CLASSIFICATION,// Verify simple is rejected... 
Storm,WITHOUT_CLASSIFICATION,//  filter ExecutorSummary's with empty stats 
Storm,WITHOUT_CLASSIFICATION,//  backward compatibility 
Storm,WITHOUT_CLASSIFICATION,//  Verify recorded messages size metrics  
Storm,WITHOUT_CLASSIFICATION,// 2 on first partition 0-2 on second partition 
Storm,WITHOUT_CLASSIFICATION,//  Ranked third since rack-4 has a lot of cpu but not a lot of memory 
Storm,WITHOUT_CLASSIFICATION,//  FRAGMENTED_CPU 
Storm,WITHOUT_CLASSIFICATION,/*              * Last batch meta is null but this is not the first batch emitted for this partition by this emitter instance. This is             * a replay of the first batch for this partition. Use the offset the consumer started at.              */
Storm,WITHOUT_CLASSIFICATION,//  Don't start new requests if there is an exception 
Storm,WITHOUT_CLASSIFICATION,// We require there to be both a topology and a component in this case so parse it out as such. 
Storm,WITHOUT_CLASSIFICATION,//  There is one task inside one executor for each worker of the topology.   TaskID is always -1 therefore you can only send-unanchored tuples to co-located SystemBolt.   This bolt was conceived to export worker stats via metrics api. 
Storm,WITHOUT_CLASSIFICATION,//  now scan all metadata and remove any matching string Ids from this list 
Storm,WITHOUT_CLASSIFICATION,//  Ignored 
Storm,WITHOUT_CLASSIFICATION,// Save the memory limit so we can enforce it less strictly 
Storm,WITHOUT_CLASSIFICATION,//  executor resources 
Storm,WITHOUT_CLASSIFICATION,// We could not recover container will be null. 
Storm,WITHOUT_CLASSIFICATION,// returns null if it's not a drpc group 
Storm,WITHOUT_CLASSIFICATION,//  pump more msgs than Q size & verify msg count is as expexted 
Storm,WITHOUT_CLASSIFICATION,//  check negative resource count 
Storm,WITHOUT_CLASSIFICATION,// ack 2nd tuple 
Storm,WITHOUT_CLASSIFICATION,//  Log any info sent on the error stream 
Storm,WITHOUT_CLASSIFICATION,//  Fill the 2nd half with new bytes from the stream. 
Storm,WITHOUT_CLASSIFICATION,// A DRPC token only works for the invocations transport not for the basic thrift transport. 
Storm,WITHOUT_CLASSIFICATION,//  (merge-with merge-agg-comp-stats-topo-page-bolt/spout (acc-stats comp-key) cid->statk->num) 
Storm,WITHOUT_CLASSIFICATION,// Map the key if needed 
Storm,WITHOUT_CLASSIFICATION,//  2 -  Setup DevNull Bolt   -------- 
Storm,WITHOUT_CLASSIFICATION,//  failed_with_exit_code is OK. We're mimicing Hadoop's health checks.   We treat non-zero exit codes as indicators that the scripts failed   to execute properly not that the system is unhealthy in which case   we don't want to start killing things. 
Storm,WITHOUT_CLASSIFICATION,// Fail both emitted tuples 
Storm,WITHOUT_CLASSIFICATION,//  there's not enough bytes in the buffer. 
Storm,WITHOUT_CLASSIFICATION,//         oneProducer1Consumer();          twoProducer1Consumer();          threeProducer1Consumer();          oneProducer2Consumers();          producerFwdConsumer(); 
Storm,WITHOUT_CLASSIFICATION,// Should be allowed to retry 3 times in addition to original try 
Storm,WITHOUT_CLASSIFICATION,//  check allowedWorkers only if the scheduler is not the Resource Aware Scheduler 
Storm,WITHOUT_CLASSIFICATION,//  check that tryPublish() & tryOverflowPublish() work as expected 
Storm,WITHOUT_CLASSIFICATION,// Put in a tuple to cause the first tuple to be acked 
Storm,WITHOUT_CLASSIFICATION,//  failures happen you don't get an explosion in memory usage in the tasks 
Storm,WITHOUT_CLASSIFICATION,//     httpServer.destroy();  } 
Storm,WITHOUT_CLASSIFICATION,// Seek directly to the earliest retriable message for each retriable topic partition 
Storm,WITHOUT_CLASSIFICATION,//  Hide the dead-ports from the all-ports   these dead-ports can be reused in next round of assignments 
Storm,WITHOUT_CLASSIFICATION,// Resets the last access time for key1 
Storm,WITHOUT_CLASSIFICATION,//  ACTION 
Storm,WITHOUT_CLASSIFICATION,//  A singleton instance allows us to mock delegated static main.methods in our
Storm,WITHOUT_CLASSIFICATION,/*          * When we click a link to the logviewer we expect the match line to be somewhere near the middle of the page. So we subtract half         * of the default page length from the offset at which we found the match.          */
Storm,WITHOUT_CLASSIFICATION,// generate topologies 
Storm,WITHOUT_CLASSIFICATION,// Allowing keytab based login for backward compatibility. 
Storm,WITHOUT_CLASSIFICATION,//  OWNER 
Storm,WITHOUT_CLASSIFICATION,//  map of tag value pairs 
Storm,WITHOUT_CLASSIFICATION,/*  * Just for testing purpose. After the migration of testing.clj. This class could be removed.  */
Storm,WITHOUT_CLASSIFICATION,// Only 1 rack is in use  r001 is the second rack with GPUs  r000 is the first rack with no GPUs 
Storm,WITHOUT_CLASSIFICATION,//  used in worker only keep it as a latch 
Storm,WITHOUT_CLASSIFICATION,//  throws ParseException. Effectively produces 3 lines (12 & 3) from each file read 
Storm,WITHOUT_CLASSIFICATION,//  RAS (Resource Aware Scheduler) 
Storm,WITHOUT_CLASSIFICATION,//  Starting empty 
Storm,WITHOUT_CLASSIFICATION,// no ack so return the first of pending list 
Storm,WITHOUT_CLASSIFICATION,// storm configuration 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_SHARED_OFF_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  highest sequence number that can be committed for this shard 
Storm,WITHOUT_CLASSIFICATION,// The batch size can be no larger than half the full recvQueue size to avoid contention issues. 
Storm,WITHOUT_CLASSIFICATION,// exact variable time that is added to the current bucket 
Storm,WITHOUT_CLASSIFICATION,//  COMPLETE_MS_AVG 
Storm,WITHOUT_CLASSIFICATION,//  events 8 9 10 should not be scanned at all since TimeEvictionPolicy lag 5s should break 
Storm,WITHOUT_CLASSIFICATION,// get it from cluster state/zookeeper every time to collect the UI stats may replace it with other StateStore later 
Storm,WITHOUT_CLASSIFICATION,//  Note that allSlotsAvailableForScheduling   only uses the supervisor-details. The rest of the arguments   are there to satisfy the INimbus interface. 
Storm,WITHOUT_CLASSIFICATION,//  the key and value of txIds are guaranteed to be converted to UTF-8 encoded String 
Storm,WITHOUT_CLASSIFICATION,//  clear workers off all hosts that are not blacklisted 
Storm,WITHOUT_CLASSIFICATION,//  Creating blob again before launching topology 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_MEMONHEAP 
Storm,WITHOUT_CLASSIFICATION,// Commit 
Storm,WITHOUT_CLASSIFICATION,// user jerry submits topo2 
Storm,WITHOUT_CLASSIFICATION,//  optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  10000000 records => 80MBs of memory footprint in the worst case 
Storm,WITHOUT_CLASSIFICATION,//  This is updated by the Worker and the topology has shared access to it 
Storm,WITHOUT_CLASSIFICATION,// we need to release resources associated with the worker event loop group 
Storm,WITHOUT_CLASSIFICATION,//  assertion 
Storm,WITHOUT_CLASSIFICATION,// no-op 
Storm,WITHOUT_CLASSIFICATION,//  protected using the Object Lock 
Storm,WITHOUT_CLASSIFICATION,//  distribution should be even for all nodes when loads are even 
Storm,WITHOUT_CLASSIFICATION,//  implementation for converting a Kinesis record to a storm tuple 
Storm,WITHOUT_CLASSIFICATION,//  A callback that does nothing. 
Storm,WITHOUT_CLASSIFICATION,// In case deactivate was called before 
Storm,WITHOUT_CLASSIFICATION,//  close the socket which releases connection if it has created any. 
Storm,WITHOUT_CLASSIFICATION,//  remove any entries in the cache 
Storm,WITHOUT_CLASSIFICATION,//  (acc-stats comp-key) ==> bolt2stats/spout2stats 
Storm,WITHOUT_CLASSIFICATION,//  Update current key list inside the blobstore if the version changes 
Storm,WITHOUT_CLASSIFICATION,//  lock protects against multiple topologies being submitted at once and 
Storm,WITHOUT_CLASSIFICATION,//  WINDOW_TO_FAILED 
Storm,WITHOUT_CLASSIFICATION,//  one or more column families 
Storm,WITHOUT_CLASSIFICATION,//  SUPERVISOR_ID 
Storm,WITHOUT_CLASSIFICATION,//  2) wait for all 3 locks to expire then heart beat on 2 locks 
Storm,WITHOUT_CLASSIFICATION,//  create sequence format instance. 
Storm,WITHOUT_CLASSIFICATION,//  blob key not specified use file 
Storm,WITHOUT_CLASSIFICATION,//  1 lock log entry every 2 tuples   Effectively disable commits based on time 
Storm,WITHOUT_CLASSIFICATION,//  TODO: finish 
Storm,WITHOUT_CLASSIFICATION,// this is to prevent the potential bug that  if the Login Cache is (1) enabled and then (2) disabled and then (3) enabled again  and if the LoginCacheKey remains unchanged (3) will use the Login cache from (1) which could be wrong  because the TGT cache (as well as the principle) could have been changed during (2) 
Storm,WITHOUT_CLASSIFICATION,//  Some other form of Unix 
Storm,WITHOUT_CLASSIFICATION,// element sojourn time in milliseconds 
Storm,WITHOUT_CLASSIFICATION,//  Suppressing exceptions as we don't care for errors on abort 
Storm,WITHOUT_CLASSIFICATION,// need to get the next node iterator 
Storm,WITHOUT_CLASSIFICATION,//  Creating a blacklist file to read from the disk 
Storm,WITHOUT_CLASSIFICATION,// TreeSet uses compareTo instead of equals() for the Set contract  Ensure that we can save two retry schedules with the same timestamp 
Storm,WITHOUT_CLASSIFICATION,//  thread object 'thread' will be null if a refresh thread is not needed. 
Storm,WITHOUT_CLASSIFICATION,//  Encoder 
Storm,WITHOUT_CLASSIFICATION,//  Don't ack tick tuples 
Storm,WITHOUT_CLASSIFICATION,// Don't let the user set who we launch as 
Storm,WITHOUT_CLASSIFICATION,//  acls for the blob are set to WORLD_EVERYTHING 
Storm,WITHOUT_CLASSIFICATION,//  4)  --- Create another input file and reverify same behavior --- 
Storm,WITHOUT_CLASSIFICATION,//  setup default Producer 
Storm,WITHOUT_CLASSIFICATION,//  pregenerate commonly used keys for scans 
Storm,WITHOUT_CLASSIFICATION,//  if SASL authentication is disabled saslChannelReady is initialized as true; otherwise false 
Storm,WITHOUT_CLASSIFICATION,//  number of values must be odd to compute median as below 
Storm,WITHOUT_CLASSIFICATION,// NOOP if windows gets support for run as user we will need to find a way to support this 
Storm,WITHOUT_CLASSIFICATION,//  wordSpout ==> countBolt ==> RedisBolt 
Storm,WITHOUT_CLASSIFICATION,//  RESET_LOG_LEVEL_TIMEOUT_EPOCH 
Storm,WITHOUT_CLASSIFICATION,//  storm blobstore create --file blacklist.txt --acl o::rwa key 
Storm,WITHOUT_CLASSIFICATION,//  3 - Setup Topology  -------- 
Storm,WITHOUT_CLASSIFICATION,//  EMITTED 
Storm,WITHOUT_CLASSIFICATION,// the body of the message is "message" + currentOffset e.g. "message123" 
Storm,WITHOUT_CLASSIFICATION,//  object capturing all zk related information for storing committed sequence numbers 
Storm,WITHOUT_CLASSIFICATION,//  there's a race condition with a delete: either blobstore or blobstoremaxsequence   this should be thrown to the caller to indicate that the key is invalid now 
Storm,WITHOUT_CLASSIFICATION,//  HOSTNAME 
Storm,WITHOUT_CLASSIFICATION,//  Stop services without killing the process instead. 
Storm,WITHOUT_CLASSIFICATION,//  $PATH should be defined on most systems 
Storm,WITHOUT_CLASSIFICATION,//  setup ZK 
Storm,WITHOUT_CLASSIFICATION,// No backoff for test retry service just check that messages will retry immediately 
Storm,WITHOUT_CLASSIFICATION,// just need an id 
Storm,WITHOUT_CLASSIFICATION,//  Javac option: remove these when the javac zip impl is fixed   (http://b/issue?id=1822932) 
Storm,WITHOUT_CLASSIFICATION,// Both assignments are null just wait 
Storm,WITHOUT_CLASSIFICATION,/*         For each partition the spout is allowed to retry all tuples between the committed offset and maxUncommittedOffsets ahead.        It must retry tuples within that limit even if more tuples were emitted.          */
Storm,WITHOUT_CLASSIFICATION,//  for local cluster 
Storm,WITHOUT_CLASSIFICATION,// Check that null meta makes the spout seek to EARLIEST and that the returned meta is correct 
Storm,WITHOUT_CLASSIFICATION,//  messages[next] == null can happen if we lost the connection and subsequently reconnected or timed out. 
Storm,WITHOUT_CLASSIFICATION,//  emits sliding window and global averages 
Storm,WITHOUT_CLASSIFICATION,// Topologies that were deemed to be invalid 
Storm,WITHOUT_CLASSIFICATION,// in addition add all the owners with guarantees 
Storm,WITHOUT_CLASSIFICATION,//  or after a consumer rebalance or during close/deactivate. Always empty if processing guarantee is none or at-most-once. 
Storm,WITHOUT_CLASSIFICATION,// for local test 
Storm,WITHOUT_CLASSIFICATION,//  window should be compacted and events should be expired. 
Storm,WITHOUT_CLASSIFICATION,//  add more events with current ts 
Storm,WITHOUT_CLASSIFICATION,//  The manually set STORM_WORKER_CGROUP_MEMORY_MB_LIMIT config on supervisor will overwrite 
Storm,WITHOUT_CLASSIFICATION,//  left join - core implementation 
Storm,WITHOUT_CLASSIFICATION,//  returns true if pendingEmits is empty 
Storm,WITHOUT_CLASSIFICATION,//  now ack msg 5 and check 
Storm,WITHOUT_CLASSIFICATION,// Race with delete  If it is not here the replication is 0  
Storm,WITHOUT_CLASSIFICATION,//  ignore 
Storm,WITHOUT_CLASSIFICATION,// This needs to be thread safe 
Storm,WITHOUT_CLASSIFICATION,//  In the worst case we will return a serialized name after a password provider said that the password   was okay.  In that case the ACLs are likely to prevent the request from going through anyways. 
Storm,WITHOUT_CLASSIFICATION,//  enable blobstore acl validation 
Storm,WITHOUT_CLASSIFICATION,//  NODE_INFO 
Storm,WITHOUT_CLASSIFICATION,//     Metrics 
Storm,WITHOUT_CLASSIFICATION,// Tests that isScheduled isReady and earliestRetriableOffsets are mutually consistent when there are messages from multiple partitions scheduled 
Storm,WITHOUT_CLASSIFICATION,// This might be a partial key grouping.. 
Storm,WITHOUT_CLASSIFICATION,//  make sure resources dir is created. 
Storm,WITHOUT_CLASSIFICATION,//  make sure we support different user reading same blob 
Storm,WITHOUT_CLASSIFICATION,//  validate search by metric id 
Storm,WITHOUT_CLASSIFICATION,// A map of node ids to node objects 
Storm,WITHOUT_CLASSIFICATION,//  pass cases 
Storm,WITHOUT_CLASSIFICATION,// construct a transport plugin 
Storm,WITHOUT_CLASSIFICATION,//  no reference to key1 
Storm,WITHOUT_CLASSIFICATION,//  initialize slots for this node 
Storm,WITHOUT_CLASSIFICATION,//  Quoting Javadoc in File.listFiles(FilenameFilter filter):   Returns {@code null} if this abstract pathname does not denote a directory or if an I/O error occurs. 
Storm,WITHOUT_CLASSIFICATION,//  we're making mock ignoring... 
Storm,WITHOUT_CLASSIFICATION,// Expire the token 
Storm,WITHOUT_CLASSIFICATION,// Create links to blobs 
Storm,WITHOUT_CLASSIFICATION,//  Maps transaction Ids to JMS message ids. 
Storm,WITHOUT_CLASSIFICATION,//  Testing whether acls are set to WORLD_EVERYTHING Here we are testing only for LocalFsBlobstore   as the HdfsBlobstore gets the subject information of the local system user and behaves as it is   always authenticated. 
Storm,WITHOUT_CLASSIFICATION,//  Define our taskIds - the test expects these to be incrementing by one up from zero 
Storm,WITHOUT_CLASSIFICATION,//  for kryo 
Storm,WITHOUT_CLASSIFICATION,// static to ensure eventhough the class is created using reflection we can still get  the topology to actions 
Storm,WITHOUT_CLASSIFICATION,/*      * Convenience method for registering ReducedMetric.      */
Storm,WITHOUT_CLASSIFICATION,//  one instance per executor avoids false sharing of CPU cache 
Storm,WITHOUT_CLASSIFICATION,//  FUNCTION_NAME 
Storm,WITHOUT_CLASSIFICATION,//  there's a race condition with a delete: blobstore   this should be thrown to the caller to indicate that the key is invalid now 
Storm,WITHOUT_CLASSIFICATION,//  deletes metrics matching the filter options 
Storm,WITHOUT_CLASSIFICATION,// must specify column schema when providing custom query. 
Storm,WITHOUT_CLASSIFICATION,//  case 4: task Message 
Storm,WITHOUT_CLASSIFICATION,//  populate a metric 
Storm,WITHOUT_CLASSIFICATION,//  The false parameter ensures overwriting the version file not appending 
Storm,WITHOUT_CLASSIFICATION,// Since the last tuple on the partition is more than maxPollRecords ahead of the failed tuple it shouldn't be emitted here 
Storm,WITHOUT_CLASSIFICATION,//  if ACLs have two user ACLs for empty user and principal discard empty user ACL 
Storm,WITHOUT_CLASSIFICATION,//  perform a scan given filter options and return results in either Metric or raw data. 
Storm,WITHOUT_CLASSIFICATION,//  function called on timer to reset log levels last set to DEBUG 
Storm,WITHOUT_CLASSIFICATION,//         oneProducer1Consumer(1000);  // -- measurement 1          twoProducer1Consumer(1000);    // -- measurement 2          threeProducer1Consumer(1);   // -- measurement 3 
Storm,WITHOUT_CLASSIFICATION,// $set operator: Sets the value of a field in a document. 
Storm,WITHOUT_CLASSIFICATION,//  schedule last block (90% to 100%) 
Storm,WITHOUT_CLASSIFICATION,//   Submit to Storm cluster 
Storm,WITHOUT_CLASSIFICATION,//  batch 3 replayed with 40 tuples 
Storm,WITHOUT_CLASSIFICATION,// add the authNid as the real user in reqContext's subject which will be used during authorization. 
Storm,WITHOUT_CLASSIFICATION,//  return null if it's not single emit 
Storm,WITHOUT_CLASSIFICATION,//  (merge-with (partial merge-with sum-or-0) acc-out spout-out) 
Storm,WITHOUT_CLASSIFICATION,//  0) config spout to log progress in lock file for each tuple 
Storm,WITHOUT_CLASSIFICATION,// defaults to 10 
Storm,WITHOUT_CLASSIFICATION,/*      * the value of m is as follows:     * <pre>     * #org.apache.storm.stats.CommonStats {     *  :executed {     *      ":all-time" {["split" "default"] 18727460}     *      "600" {["split" "default"] 11554}     *      "10800" {["split" "default"] 207269}     *      "86400" {["split" "default"] 1659614}}     *  :execute-latencies {     *      ":all-time" {["split" "default"] 0.5874528633354443}     *      "600" {["split" "default"] 0.6140350877192983}     *      "10800" {["split" "default"] 0.5864434687156971}     *      "86400" {["split" "default"] 0.5815376460556336}}     * }     * </pre>      */
Storm,WITHOUT_CLASSIFICATION,// These should match FixedAvroSerializer.config in the test resources 
Storm,WITHOUT_CLASSIFICATION,/*      * Write a String array as a Nework Int N followed by Int N Byte Array of     * compressed Strings. Handles also null arrays and null values.     * Could be generalised using introspection.     *      */
Storm,WITHOUT_CLASSIFICATION,//  NUM_WORKERS 
Storm,WITHOUT_CLASSIFICATION,//  Wait for 'ready' (channel connected and maybe authentication) 
Storm,WITHOUT_CLASSIFICATION,//  required for instantiation via reflection. must call prepare() thereafter 
Storm,WITHOUT_CLASSIFICATION,// this will do best effort flushing since the linger period   was set on creation 
Storm,WITHOUT_CLASSIFICATION,//  second predicate for or condition uses the fact that long addition over the limit circles back 
Storm,WITHOUT_CLASSIFICATION,// Yes eat the exception 
Storm,WITHOUT_CLASSIFICATION,// This technically does not conform with rfc1964 but should work so   long as you don't have any really odd names in your KDC. 
Storm,WITHOUT_CLASSIFICATION,//  invalidate the iterator 
Storm,WITHOUT_CLASSIFICATION,// only put this owner to the map 
Storm,WITHOUT_CLASSIFICATION,//  example. spout1: generate random strings   bolt1: get the first part of a string   bolt2: output the tuple 
Storm,WITHOUT_CLASSIFICATION,//  and try to renew the ticket. 
Storm,WITHOUT_CLASSIFICATION,//  Re-load from cached' file. 
Storm,WITHOUT_CLASSIFICATION,//  30 seconds 
Storm,WITHOUT_CLASSIFICATION,// Just throw it away in local mode 
Storm,WITHOUT_CLASSIFICATION,//  fail cases 
Storm,WITHOUT_CLASSIFICATION,// First off we want to verify that ROOT is good 
Storm,WITHOUT_CLASSIFICATION,//  -- max outstanding tuples 
Storm,WITHOUT_CLASSIFICATION,// Nimbus groups admin 
Storm,WITHOUT_CLASSIFICATION,// In almost all cases these should be the same but warn the user just in case something goes wrong... 
Storm,WITHOUT_CLASSIFICATION,// Fail all emitted messages except the last one. Try to commit. 
Storm,WITHOUT_CLASSIFICATION,//     public SimpleTridentHBaseMapper withTimestampField(String timestampField){          this.timestampField = timestampField;          return this;      } 
Storm,WITHOUT_CLASSIFICATION,// In some cases users will want to drop retrying old batches e.g. if the topology should start over from scratch.  If the FirstPollOffsetStrategy ignores committed offsets we should not retry batches for old topologies  The batch retry should be skipped entirely 
Storm,WITHOUT_CLASSIFICATION,// Run until Ctrl-C 
Storm,WITHOUT_CLASSIFICATION,//  automatically turn it into a batch spout should take parameters as to how much to batch      public Stream newStream(IRichSpout spout) {          Node n = new SpoutNode(getUniqueStreamId() TridentUtils.getSingleOutputStreamFields(spout) null spout SpoutNode   .SpoutType.BATCH);          return addNode(n);      } 
Storm,WITHOUT_CLASSIFICATION,// now login 
Storm,WITHOUT_CLASSIFICATION,/* Will only serialize AMQPValue type */
Storm,WITHOUT_CLASSIFICATION,//  It is necessary that this produce a deterministic assignment based on the key so seed the Random from the key 
Storm,WITHOUT_CLASSIFICATION,//  given (for this iteration) 
Storm,WITHOUT_CLASSIFICATION,//  check for required fields 
Storm,WITHOUT_CLASSIFICATION,// Global Grouping is fields with an empty list 
Storm,WITHOUT_CLASSIFICATION,//  -- spout id 
Storm,WITHOUT_CLASSIFICATION,//  offset and messageId are used interchangeably 
Storm,WITHOUT_CLASSIFICATION,//  O(Log N) 
Storm,WITHOUT_CLASSIFICATION,//  msec 
Storm,WITHOUT_CLASSIFICATION,//  4 - Print metrics every 30 sec kill topology after 20 min 
Storm,WITHOUT_CLASSIFICATION,// This histogram reflects the data distribution across only one ClusterSummary i.e.   data distribution across all entities of a type (e.g. data from all nimbus/topologies) at one moment.   Hence we use half of the CACHING_WINDOW time to ensure it retains only data from the most recent update 
Storm,WITHOUT_CLASSIFICATION,//  blocking call under the hood must invoke after launch cause some services must be initialized 
Storm,WITHOUT_CLASSIFICATION,// start the threads 
Storm,WITHOUT_CLASSIFICATION,/*          * The inputWindow gives a view of         * (a) all the events in the window         * (b) events that expired since last activation of the window         * (c) events that newly arrived since last activation of the window          */
Storm,WITHOUT_CLASSIFICATION,//  MEM_ON_HEAP 
Storm,WITHOUT_CLASSIFICATION,// The old token could not be deserialized.  This is bad but we are going to replace it anyways so just keep going. 
Storm,WITHOUT_CLASSIFICATION,//  then (for this iteration) 
Storm,WITHOUT_CLASSIFICATION,// in case we didn't fill in enough 
Storm,WITHOUT_CLASSIFICATION,// Cycle spout activation 
Storm,WITHOUT_CLASSIFICATION,//  Check for latest sequence number of a key inside zookeeper and return nimbodes containing the latest sequence number 
Storm,WITHOUT_CLASSIFICATION,//  we need to download to temp file and then unpack into the one requested 
Storm,WITHOUT_CLASSIFICATION,//  add identity partitions between groups 
Storm,WITHOUT_CLASSIFICATION,// Acking tuples for partitions that are no longer assigned is useless since the spout will not be allowed to commit them 
Storm,WITHOUT_CLASSIFICATION,//  read few lines from file1 dont ack 
Storm,WITHOUT_CLASSIFICATION,//  Check for Blobstore with authentication 
Storm,WITHOUT_CLASSIFICATION,/*      * key1 -> (val1 val2 ..)     * key2 -> (val3 val4 ..)      */
Storm,WITHOUT_CLASSIFICATION,//  1 -  Setup Hdfs Spout   -------- 
Storm,WITHOUT_CLASSIFICATION,//  read lines 3..7 don't ACK .. commit pos should remain same 
Storm,WITHOUT_CLASSIFICATION,// if playing from the repl and defining functions file won't exist 
Storm,WITHOUT_CLASSIFICATION,// Under RAS the number of workers is determined by the scheduler and the settings in the conf are ignored  conf.setNumWorkers(3); 
Storm,WITHOUT_CLASSIFICATION,// conf.put(Config.TOPOLOGY_STATE_PROVIDER "org.apache.storm.redis.state.RedisKeyValueStateProvider"); 
Storm,WITHOUT_CLASSIFICATION,//  check for null which can exist because of a race condition in which nimbus nodes in zk may have been   removed when connections are reconnected after getting children in the above line 
Storm,WITHOUT_CLASSIFICATION,//  SPECIFIC_STATS 
Storm,WITHOUT_CLASSIFICATION,//  Register file cleanup after jvm shutdown 
Storm,WITHOUT_CLASSIFICATION,//  TODO conditionally load properties from a file our resource 
Storm,WITHOUT_CLASSIFICATION,//  We don't need to sleep here because the IPartitionManager.receive() is   a blocked call so it's fine to call this function in a tight loop. 
Storm,WITHOUT_CLASSIFICATION,//  Triggers when an assignment should be refreshed 
Storm,WITHOUT_CLASSIFICATION,//  2 - if clocks are in sync then simply take ownership of the oldest expired lock 
Storm,WITHOUT_CLASSIFICATION,//  first sync assignments to local then sync processes. 
Storm,WITHOUT_CLASSIFICATION,//  SAMPLINGPCT 
Storm,WITHOUT_CLASSIFICATION,//  Check for new file every so often 
Storm,WITHOUT_CLASSIFICATION,// We waited for 1 second loop around and try again.... 
Storm,WITHOUT_CLASSIFICATION,//  topo3 should not be able to scheduled 
Storm,WITHOUT_CLASSIFICATION,//  create a new config. Make it additive (true) s.t. inherit parents appenders 
Storm,WITHOUT_CLASSIFICATION,// prints the total with low probability. 
Storm,WITHOUT_CLASSIFICATION,// if this task containing worker will be killed by a assignments sync  taskToNodePort will be an empty map which is refreshed by WorkerState 
Storm,WITHOUT_CLASSIFICATION,//  ALL 
Storm,WITHOUT_CLASSIFICATION,//  class JoinInfo 
Storm,WITHOUT_CLASSIFICATION,// Supervisor metrics distribution 
Storm,WITHOUT_CLASSIFICATION,//  inner join of 'age' and 'gender' records on 'id' field 
Storm,WITHOUT_CLASSIFICATION,// With uncommitted earliest the spout should pick up where it left off when reactivating. 
Storm,WITHOUT_CLASSIFICATION,//  Test1: If RAS spreads executors across multiple workers based on the set limit for a worker used by the topology 
Storm,WITHOUT_CLASSIFICATION,/* filter supervisor */
Storm,WITHOUT_CLASSIFICATION,//  DO NOT include the success stream as part of the batch. it should not trigger coordination tuples   and is just a metadata tuple to assist in cleanup should not trigger batch tracking 
Storm,WITHOUT_CLASSIFICATION,//  Load PMML Model from File 
Storm,WITHOUT_CLASSIFICATION,//  this code here handles a case where a previous commit failed and the partitions   changed since the last commit. This clears out any state for the removed partitions   for this txid.   we make sure only a single task ever does this. we're also guaranteed that   it's impossible for there to be another writer to the directory for that partition   because only a single commit can be happening at once. this is because in order for   another attempt of the batch to commit the batch phase must have succeeded in between.   hence all tasks for the prior commit must have finished committing (whether successfully or not) 
Storm,WITHOUT_CLASSIFICATION,//  SHELL 
Storm,WITHOUT_CLASSIFICATION,// Clean up some things the user should not set.  (Not a security issue just might confuse the topology) 
Storm,WITHOUT_CLASSIFICATION,//  prevent timer to check heartbeat based on last thing before activate 
Storm,WITHOUT_CLASSIFICATION,//  DATA 
Storm,WITHOUT_CLASSIFICATION,//  only holds msgs from other workers (via WorkerTransfer) when recvQueue is full 
Storm,WITHOUT_CLASSIFICATION,// HEAP DUMP 
Storm,WITHOUT_CLASSIFICATION,//  Wait for all tasks to complete 
Storm,WITHOUT_CLASSIFICATION,// ack rest 
Storm,WITHOUT_CLASSIFICATION,// Check that only two message ids were generated 
Storm,WITHOUT_CLASSIFICATION,//  Login will sleep until 80% of time from last refresh to   ticket's expiry has been reached at which time it will wake 
Storm,WITHOUT_CLASSIFICATION,// above for-loop has closed all the writers. It's safe to clear the map here. 
Storm,WITHOUT_CLASSIFICATION,//  get executor heartbeat 
Storm,WITHOUT_CLASSIFICATION,// name list is empty return an empty map 
Storm,WITHOUT_CLASSIFICATION,/* include sys (should not matter) */
Storm,WITHOUT_CLASSIFICATION,//  2- BP detected (i.e MainQ is full). So try adding to overflow 
Storm,WITHOUT_CLASSIFICATION,//  Finds the metadata string that matches the string Id and type provided.  The string should exist as it is 
Storm,WITHOUT_CLASSIFICATION,//  validate search by time 
Storm,WITHOUT_CLASSIFICATION,//  checks if the tasks which had back pressure are now free again. if so sends an update to other workers 
Storm,WITHOUT_CLASSIFICATION,//   Submit topology to Storm cluster 
Storm,WITHOUT_CLASSIFICATION,//  FRAGMENTED_MEM 
Storm,WITHOUT_CLASSIFICATION,//  Distributed mode 
Storm,WITHOUT_CLASSIFICATION,// We need to read a new one 
Storm,WITHOUT_CLASSIFICATION,//  SID_TO_OUTPUT_STATS 
Storm,WITHOUT_CLASSIFICATION,// return the smaller of pending and toResend 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_REGULAR_OFF_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,// The spout must respect maxUncommittedOffsets when requesting/emitting tuples 
Storm,WITHOUT_CLASSIFICATION,// 1 day values 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_CONF_OVERRIDES 
Storm,WITHOUT_CLASSIFICATION,//  get num_executors 
Storm,WITHOUT_CLASSIFICATION,// By default this is a NOOP 
Storm,WITHOUT_CLASSIFICATION,//  Normalize state 
Storm,WITHOUT_CLASSIFICATION,//  JAVA_OBJECT 
Storm,WITHOUT_CLASSIFICATION,//  for an acked message add it to acked set and remove it from emitted and failed 
Storm,WITHOUT_CLASSIFICATION,//  so we can track when it was last used for later deletion on database cleanup. 
Storm,WITHOUT_CLASSIFICATION,//  avoid buffering 
Storm,WITHOUT_CLASSIFICATION,//  the new-timeouts map now contains logger => timeout 
Storm,WITHOUT_CLASSIFICATION,//  4 try locking again 
Storm,WITHOUT_CLASSIFICATION,// construct THsHaServer 
Storm,WITHOUT_CLASSIFICATION,//  VERSION 
Storm,WITHOUT_CLASSIFICATION,//  if the acked message was in emittedPerShard that means we need to remove it from the emittedPerShard (which 
Storm,WITHOUT_CLASSIFICATION,//  Serializes a Java object to JSON 
Storm,WITHOUT_CLASSIFICATION,//  Resume polling at the last committed offset i.e. the first offset that is not marked as processed. 
Storm,WITHOUT_CLASSIFICATION,//  invalid key remove it from blobstore 
Storm,WITHOUT_CLASSIFICATION,//  =====================================================================================   thriftify stats main.methods   =====================================================================================
Storm,WITHOUT_CLASSIFICATION,//  wordSpout ==> countBolt ==> MongoUpdateBolt 
Storm,WITHOUT_CLASSIFICATION,//  size if IDENTIFIER 
Storm,WITHOUT_CLASSIFICATION,//  calc sid->output-stats 
Storm,WITHOUT_CLASSIFICATION,// NOOP 
Storm,WITHOUT_CLASSIFICATION,// Ranked last since rack-5 has neither CPU nor memory available 
Storm,WITHOUT_CLASSIFICATION,//  If FlatMapFunction is aware of cleanup let it handle cleaning up 
Storm,WITHOUT_CLASSIFICATION,//  Construct a message containing the SASL response and send it to the 
Storm,WITHOUT_CLASSIFICATION,//  this bolt does not emit tuples 
Storm,WITHOUT_CLASSIFICATION,// Don't try to move the JAR file in local mode it does not exist because it was not uploaded 
Storm,WITHOUT_CLASSIFICATION,// Nothing is scheduled here so throw away all of the profileActions 
Storm,WITHOUT_CLASSIFICATION,// logger.info("emitted new batches: " + listEvents.size()); 
Storm,WITHOUT_CLASSIFICATION,//  not used placeholder for GUI etc. 
Storm,WITHOUT_CLASSIFICATION,// Extend the config with defaults and the command line 
Storm,WITHOUT_CLASSIFICATION,//  setting value to any non-null string 
Storm,WITHOUT_CLASSIFICATION,// any stop profile actions that hadn't timed out yet we should restart after the worker is running again. 
Storm,WITHOUT_CLASSIFICATION,// Remove existing schedule for the message id 
Storm,WITHOUT_CLASSIFICATION,//  totals 
Storm,WITHOUT_CLASSIFICATION,/*      * Asserts that commitSync has been called once      * that there are only commits on one topic     * and that the committed offset covers messageCount messages      */
Storm,WITHOUT_CLASSIFICATION,//  STORM_ASSIGNMENT 
Storm,WITHOUT_CLASSIFICATION,//  find homedir 
Storm,WITHOUT_CLASSIFICATION,/*              * Delete the current index file and rename the tmp file to atomically             * replace the index file. Orphan .tmp files are handled in getTxnRecord.              */
Storm,WITHOUT_CLASSIFICATION,//  validate search by host id 
Storm,WITHOUT_CLASSIFICATION,// Avoid case of different blob version   when blob is not downloaded (first time download) 
Storm,WITHOUT_CLASSIFICATION,//  iterate the tuples 
Storm,WITHOUT_CLASSIFICATION,//  create thread to delete old metrics and metadata 
Storm,WITHOUT_CLASSIFICATION,//  we want to register a topo directory getChildren callback for all workers of this dir 
Storm,WITHOUT_CLASSIFICATION,//  e.g.:  123410111215  =>  410111215 
Storm,WITHOUT_CLASSIFICATION,//  expecting this exception 
Storm,WITHOUT_CLASSIFICATION,//  case 3: BackPressureStatus 
Storm,WITHOUT_CLASSIFICATION,//  METRIC_LIST 
Storm,WITHOUT_CLASSIFICATION,// topology.worker.childopts validates 
Storm,WITHOUT_CLASSIFICATION,// Give the topology time to come up without using it to wait for the spouts to complete 
Storm,WITHOUT_CLASSIFICATION,/* include sys */
Storm,WITHOUT_CLASSIFICATION,// Sometimes external things used with testing don't shut down all the way 
Storm,WITHOUT_CLASSIFICATION,// key is supervisor key value is supervisor ports 
Storm,WITHOUT_CLASSIFICATION,//  run aggregator to compute the result 
Storm,WITHOUT_CLASSIFICATION,// This is a NOOP 
Storm,WITHOUT_CLASSIFICATION,//  if re-partitioning is involved does a per-partition reduce by key before emitting the results downstream 
Storm,WITHOUT_CLASSIFICATION,//  if no configs from user-resource-pools.yaml get configs from conf 
Storm,WITHOUT_CLASSIFICATION,// password 
Storm,WITHOUT_CLASSIFICATION,//  set acl so user doesn't have read access 
Storm,WITHOUT_CLASSIFICATION,//  scans from key start to the key before end calling back until callback indicates not to process further 
Storm,WITHOUT_CLASSIFICATION,//  If unsuccesful fail the pending tuples 
Storm,WITHOUT_CLASSIFICATION,// This test is in two phases.  The first phase fills up the 10 buckets with 10 tuples each 
Storm,WITHOUT_CLASSIFICATION,//  reset property 
Storm,WITHOUT_CLASSIFICATION,//  version 
Storm,WITHOUT_CLASSIFICATION,//  PULSE_IDS 
Storm,WITHOUT_CLASSIFICATION,// This method enables the metrics to be accessed from outside of the JCQueue class 
Storm,WITHOUT_CLASSIFICATION,//  verify lock file location   verify lock filename 
Storm,WITHOUT_CLASSIFICATION,//  pass in componentToSortedTasks for the case of running tons of tasks in single executor 
Storm,WITHOUT_CLASSIFICATION,//  an acked message should not be in failed since if it fails and gets re-emitted it moves to emittedPerShard   from failedPerShard. Defensive coding. 
Storm,WITHOUT_CLASSIFICATION,// All null tuples should be commited meaning they were considered by to be emitted and acked 
Storm,WITHOUT_CLASSIFICATION,//  The start index is positioned to find any possible   occurrence search string that did not quite fit in the   buffer on the previous read. 
Storm,WITHOUT_CLASSIFICATION,// generate another rack of supervisors with less resources 
Storm,WITHOUT_CLASSIFICATION,// Scheduling changed while running... 
Storm,WITHOUT_CLASSIFICATION,//  load from state 
Storm,WITHOUT_CLASSIFICATION,//  boolean to indicate whether timer is active 
Storm,WITHOUT_CLASSIFICATION,//  a valid javac XD option which is another bug 
Storm,WITHOUT_CLASSIFICATION,//  A map of the worker to the components in the worker to be able to enforce constraints. 
Storm,WITHOUT_CLASSIFICATION,// any receive call after exceeding max pending messages results in null 
Storm,WITHOUT_CLASSIFICATION,// invoke service handler 
Storm,WITHOUT_CLASSIFICATION,//  Finally delete any baseName.<VERSION> files that are not pointed to by the current version 
Storm,WITHOUT_CLASSIFICATION,//  Choosing atmost 5 words to update the blacklist file for filtering 
Storm,WITHOUT_CLASSIFICATION,//  Assume the recvQueue is stable in which the arrival rate is equal to the consumption rate.   If this assumption does not hold the calculation of sojourn time should also consider   departure rate according to Queuing Theory. 
Storm,WITHOUT_CLASSIFICATION,//  SERIALIZED_JAVA 
Storm,WITHOUT_CLASSIFICATION,//  Generate SASL response to server using Channel-local SASL client. 
Storm,WITHOUT_CLASSIFICATION,// Gets Nimbus Subject with NimbusPrincipal set on it 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_STATUS 
Storm,WITHOUT_CLASSIFICATION,/*  We take the max of the default and whatever the user put in here.           Each node's resources can be the sum of several operations so the simplest           thing to do is get the max.           The situation we want to avoid is that the user sets low resources on one           node and when that node is combined with a bunch of others the sum is still           that low resource count. If any component isn't set we want to use the default.           Right now this code does not check that. It just takes the max of the summed           up resource counts for simplicity's sake. We could perform some more complicated           logic to be more accurate but the benefits are very small and only apply to some           very odd corner cases.  */
Storm,WITHOUT_CLASSIFICATION,/*  Indexed  */
Storm,WITHOUT_CLASSIFICATION,// Unblock downloading by accepting the futures. 
Storm,WITHOUT_CLASSIFICATION,//  this finds all dependency blob keys from active topologies from all local blob keys 
Storm,WITHOUT_CLASSIFICATION,//  NIMBUSES 
Storm,WITHOUT_CLASSIFICATION,// Emit all messages and check that they are emitted. Ack the messages too 
Storm,WITHOUT_CLASSIFICATION,// rest of jerry's running topologies 
Storm,WITHOUT_CLASSIFICATION,// Since we asked for tuples starting at seekOffset some retriable records must have been compacted away.  Ack up to the first offset received if the record is not already acked or currently in the topology 
Storm,WITHOUT_CLASSIFICATION,//  always retain resources in use 
Storm,WITHOUT_CLASSIFICATION,// Update worker tokens if needed 
Storm,WITHOUT_CLASSIFICATION,//  Consumer 
Storm,WITHOUT_CLASSIFICATION,/*  Not a Blocking call. If cannot emit will add 'tuple' to pendingEmits and return 'false'. 'pendingEmits' can be null  */
Storm,WITHOUT_CLASSIFICATION,// cannot be (cpuResourcePoolUtilization + memoryResourcePoolUtilization)/2  since memoryResourcePoolUtilization or cpuResourcePoolUtilization can be Double.MAX_VALUE  Should not return infinity in that case 
Storm,WITHOUT_CLASSIFICATION,// The failed tuples are ready for retry. Make it appear like 0 and 1 were compacted away. 
Storm,WITHOUT_CLASSIFICATION,//  helpful for debugging tests 
Storm,WITHOUT_CLASSIFICATION,//  WORKER_RESOURCES 
Storm,WITHOUT_CLASSIFICATION,//  else use the exponential backoff logic and handle long overflow 
Storm,WITHOUT_CLASSIFICATION,//  complete access to the blob 
Storm,WITHOUT_CLASSIFICATION,//  STREAMS 
Storm,WITHOUT_CLASSIFICATION,//  Reasonable size for a simple .class. 
Storm,WITHOUT_CLASSIFICATION,// In order to avoid going over _maxNodes I may need to steal from   myself even though other pools have free nodes. so figure out how   much each group should provide 
Storm,WITHOUT_CLASSIFICATION,//  =================  Factory Methods Declaring ModelOutputs to Default Stream  ================== 
Storm,WITHOUT_CLASSIFICATION,//  persist the window in state 
Storm,WITHOUT_CLASSIFICATION,//  Hdfs related settings 
Storm,WITHOUT_CLASSIFICATION,// race condition with a delete 
Storm,WITHOUT_CLASSIFICATION,//  TIME_SECS 
Storm,WITHOUT_CLASSIFICATION,// 3 hour values 
Storm,WITHOUT_CLASSIFICATION,//  No-op 
Storm,WITHOUT_CLASSIFICATION,//  gets min/max task pairs (executors): [1 1] [2 3] ... 
Storm,WITHOUT_CLASSIFICATION,// Tick should have flushed it 
Storm,WITHOUT_CLASSIFICATION,//  Test for subject with no principals and acls set to WORLD_EVERYTHING 
Storm,WITHOUT_CLASSIFICATION,//  create empty files in filesDir 
Storm,WITHOUT_CLASSIFICATION,// Initialize a worker slot for every port even if there is no assignment to it 
Storm,WITHOUT_CLASSIFICATION,//  explicit delete for ephmeral node to ensure this session creates the entry. 
Storm,WITHOUT_CLASSIFICATION,//  for global cleanup   for an active worker's dir make sure for the last "/" 
Storm,WITHOUT_CLASSIFICATION,//  repartition so that state query fields grouping works correctly. this can be optimized further 
Storm,WITHOUT_CLASSIFICATION,// The LogWriter in turn launches the actual worker. 
Storm,WITHOUT_CLASSIFICATION,//  If the config consists of a single key 'config' its values are used   instead. This means that the same config files can be used with Flux   and the ConfigurableTopology. 
Storm,WITHOUT_CLASSIFICATION,//  offset was not committed by this topology therefore FirstPollOffsetStrategy applies   (only when the topology is first deployed). 
Storm,WITHOUT_CLASSIFICATION,//  This is likely to happen when we try to commit something that   was cleaned up.  This is expected and acceptable. 
Storm,WITHOUT_CLASSIFICATION,// cred-update-lock is not needed here because creds are being added for the first time. 
Storm,WITHOUT_CLASSIFICATION,//  required   required   optional   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  ==================   Get PMML Model from Blobstore ================== 
Storm,WITHOUT_CLASSIFICATION,/*      * Bolt-specific configuration for windowed bolts to specify the name of the field in the tuple that holds     * the message id. This is used to track the windowing boundaries and avoid re-evaluating the windows     * during recovery of IStatefulWindowedBolt      */
Storm,WITHOUT_CLASSIFICATION,//  create metric for memory 
Storm,WITHOUT_CLASSIFICATION,//  Kafka 
Storm,WITHOUT_CLASSIFICATION,//  delete and recreate lock file   returns false if somebody else already deleted it (to take ownership) 
Storm,WITHOUT_CLASSIFICATION,//  Maps a storm tuple to redis key and value 
Storm,WITHOUT_CLASSIFICATION,// this will fail since jerry doesn't have either cpu or memory entries 
Storm,WITHOUT_CLASSIFICATION,// Remove/Clean up changed requests that are not for us 
Storm,WITHOUT_CLASSIFICATION,//  bolt that subscribes to the intermediate bolt and auto-acks 
Storm,WITHOUT_CLASSIFICATION,//  1) Delete absent file - should return false 
Storm,WITHOUT_CLASSIFICATION,//  TRANSFERRED 
Storm,WITHOUT_CLASSIFICATION,//  Test with a dummy test_subject for cases where subject !=null (security turned on) 
Storm,WITHOUT_CLASSIFICATION,//  should synchronize supervisor so it doesn't launch anything after being down (optimization) 
Storm,WITHOUT_CLASSIFICATION,//  default parallelism to 1 so if it's omitted the topology will still function. 
Storm,WITHOUT_CLASSIFICATION,//  pendingCommit has no entries 
Storm,WITHOUT_CLASSIFICATION,//  query the streamState for each input task stream and compute recoveryStates 
Storm,WITHOUT_CLASSIFICATION,//  required   optional   required 
Storm,WITHOUT_CLASSIFICATION,//  reset for next run 
Storm,WITHOUT_CLASSIFICATION,//  If that fails fall back on the multitenant-scheduler.yaml file 
Storm,WITHOUT_CLASSIFICATION,//  Back off 
Storm,WITHOUT_CLASSIFICATION,//  SETTABLE 
Storm,WITHOUT_CLASSIFICATION,//  if first retry then retry time  = current time  + initial delay 
Storm,WITHOUT_CLASSIFICATION,// on windows the host process still holds lock on the logfile 
Storm,WITHOUT_CLASSIFICATION,//  each evicted partition has MAX_EVENTS_PER_PARTITION 
Storm,WITHOUT_CLASSIFICATION,// The response should be empty since you should not be able to list files outside the worker log root. 
Storm,WITHOUT_CLASSIFICATION,//  TOTAL_EXECUTORS 
Storm,WITHOUT_CLASSIFICATION,// Try to create the parent directory may not work 
Storm,WITHOUT_CLASSIFICATION,// Simulate time starts out at 0 so we are going to just leave it here. 
Storm,WITHOUT_CLASSIFICATION,//  optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  required   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  disable it   log every 2 sec 
Storm,WITHOUT_CLASSIFICATION,//  Handles tuple events (emit ack etc.) 
Storm,WITHOUT_CLASSIFICATION,//  BOOLVAL 
Storm,WITHOUT_CLASSIFICATION,// Only log accesses that fetched something 
Storm,WITHOUT_CLASSIFICATION,//  make sure that the error thread exits 
Storm,WITHOUT_CLASSIFICATION,/*  LOOK AT HDFS BLOBSTORE AGAIN  */
Storm,WITHOUT_CLASSIFICATION,//  check avoids multiple log msgs when spinning in a idle loop 
Storm,WITHOUT_CLASSIFICATION,//  add 10 events 
Storm,WITHOUT_CLASSIFICATION,// create test DNSToSwitchMapping plugin 
Storm,WITHOUT_CLASSIFICATION,//  for the currently tested assignment a Map of the node to the components on it to be able to enforce constraints 
Storm,WITHOUT_CLASSIFICATION,//  otherwise tuples were emitted directly 
Storm,WITHOUT_CLASSIFICATION,//  expiry is before next scheduled refresh). 
Storm,WITHOUT_CLASSIFICATION,// If the system still has some free memory give them a grace period to 
Storm,WITHOUT_CLASSIFICATION,//  Tuple contains String Object in JSON format   Tuple contains Java object that must be serialized to JSON by SolrJsonMapper 
Storm,WITHOUT_CLASSIFICATION,//  topo4 has 12 small tasks whose mem usage does not exactly divide a node's mem capacity 
Storm,WITHOUT_CLASSIFICATION,//  bolt1 bolt2 should also receive from checkpoint spout 
Storm,WITHOUT_CLASSIFICATION,//  Cache the msgs grouped by destination node 
Storm,WITHOUT_CLASSIFICATION,//  Add unique identifier to each tuple which is helpful for debugging 
Storm,WITHOUT_CLASSIFICATION,//  The redis bolt (sink) 
Storm,WITHOUT_CLASSIFICATION,/*                  * Since this is a tumbling window calculation                 * we use all the tuples in the window to compute the avg.                  */
Storm,WITHOUT_CLASSIFICATION,//  respectively. 
Storm,WITHOUT_CLASSIFICATION,//  sleep for 10 seconds. 
Storm,WITHOUT_CLASSIFICATION,//  create an array of the right type 
Storm,WITHOUT_CLASSIFICATION,// Storm will try to get metrics from the spout even while deactivated the spout must be able to handle this 
Storm,WITHOUT_CLASSIFICATION,//  Initialization is only complete after the first call to  KafkaSpoutConsumerRebalanceListener.onPartitionsAssigned() 
Storm,WITHOUT_CLASSIFICATION,//  Mock failure 
Storm,WITHOUT_CLASSIFICATION,// update nextOffset; 
Storm,WITHOUT_CLASSIFICATION,//  Based on how Java handles the classpath   https://docs.oracle.com/javase/8/docs/technotes/tools/unix/classpath.html 
Storm,WITHOUT_CLASSIFICATION,//  ordered partition keys 
Storm,WITHOUT_CLASSIFICATION,//  This should mean that we were pointed at a directory. 
Storm,WITHOUT_CLASSIFICATION,//  renames files and returns the new file path 
Storm,WITHOUT_CLASSIFICATION,//  leave the acked offsets and consumer position as they were to resume where it left off 
Storm,WITHOUT_CLASSIFICATION,// Test for a user having read or write or admin access to read replication for a blob 
Storm,WITHOUT_CLASSIFICATION,// Instead the scheduler lets you set the maximum heap size for any worker. 
Storm,WITHOUT_CLASSIFICATION,/*      * Test time to schedule large cluster scheduling with fragmentation      */
Storm,WITHOUT_CLASSIFICATION,//  when (for this iteration) 
Storm,WITHOUT_CLASSIFICATION,//  can happen during shutdown of drpc while topology is still up 
Storm,WITHOUT_CLASSIFICATION,//  Map[StreamName -> JoinInfo] 
Storm,WITHOUT_CLASSIFICATION,//  size of the resource 
Storm,WITHOUT_CLASSIFICATION,//  Leadership coordination may be incomplete when launchServer is called. Previous behavior did a one time check   which could cause Nimbus to not process TopologyActions.GAIN_LEADERSHIP transitions. Similar problem exists for 
Storm,WITHOUT_CLASSIFICATION,//  should promote: only fetch storm bases of topologies that need scheduling. 
Storm,WITHOUT_CLASSIFICATION,//  for faster insertion to RocksDB. 
Storm,WITHOUT_CLASSIFICATION,//  Map[StreamName -> Map[Key -> List<Tuple>]  ] 
Storm,WITHOUT_CLASSIFICATION,//  move of tmp to current so that the operation is atomic. 
Storm,WITHOUT_CLASSIFICATION,//  SUPERVISOR_SUMMARIES 
Storm,WITHOUT_CLASSIFICATION,// This needs to be appropriately large to drown out any time advances performed during topology boot 
Storm,WITHOUT_CLASSIFICATION,//  not scheduled <=> never failed (i.e. never emitted) or scheduled and ready to be retried 
Storm,WITHOUT_CLASSIFICATION,// If the node does not exist then the version must be 0 
Storm,WITHOUT_CLASSIFICATION,//  Wait interfal for retrying after first failure 
Storm,WITHOUT_CLASSIFICATION,// Only add topologies that are not sharing nodes with other topologies 
Storm,WITHOUT_CLASSIFICATION,//  read and ack remaining lines 
Storm,WITHOUT_CLASSIFICATION,/*  CREATE THE BLOBSTORES  */
Storm,WITHOUT_CLASSIFICATION,/*  Seqable  */
Storm,WITHOUT_CLASSIFICATION,// new topology needs to be scheduled.  topo-4 should be evicted. Even though topo-1 from user jerry is older topo-1 will not be evicted 
Storm,WITHOUT_CLASSIFICATION,/* with auth */
Storm,WITHOUT_CLASSIFICATION,//  warm up 60 seconds 
Storm,WITHOUT_CLASSIFICATION,//  number of evicted events 
Storm,WITHOUT_CLASSIFICATION,//  add in spouts as groups so we can get parallelisms 
Storm,WITHOUT_CLASSIFICATION,//  1 -  Setup Kafka Spout   -------- 
Storm,WITHOUT_CLASSIFICATION,//  Offset management 
Storm,WITHOUT_CLASSIFICATION,// have the new credentials (pass it to the LoginContext constructor) 
Storm,WITHOUT_CLASSIFICATION,// Check to see if the CGroup is mounted at all 
Storm,WITHOUT_CLASSIFICATION,// emit cross-join of all emitted tuples 
Storm,WITHOUT_CLASSIFICATION,/*  COPY EVERYTHING FROM LOCAL BLOBSTORE TO HDFS  */
Storm,WITHOUT_CLASSIFICATION,// Check that the spout will reemit all 3 failed tuples and no other tuples 
Storm,WITHOUT_CLASSIFICATION,//  Every executor has an instance of this class 
Storm,WITHOUT_CLASSIFICATION,//  CUSTOM_OBJECT 
Storm,WITHOUT_CLASSIFICATION,// Test substitution where the target type is List 
Storm,WITHOUT_CLASSIFICATION,//  implementation for handling the failed messages retry logic 
Storm,WITHOUT_CLASSIFICATION,/*          * The KafkaConsumer commitSync API docs: "The committed offset should be the next message your application will consume i.e.         * lastProcessedMessageOffset + 1. "          */
Storm,WITHOUT_CLASSIFICATION,//  should not throw 
Storm,WITHOUT_CLASSIFICATION,//  Log the connection error only once 
Storm,WITHOUT_CLASSIFICATION,//  try to get BlobMeta   This will check if the key exists and if the subject has authorization 
Storm,WITHOUT_CLASSIFICATION,// TODO batch updating 
Storm,WITHOUT_CLASSIFICATION,//  Configured for achieving max throughput in single worker mode (empirically found).    For reference : numbers taken on MacBook Pro mid 2015      -- ACKer=0:  ~8 mill/sec (batchSz=2k & recvQsize=50k).  6.7 mill/sec (batchSz=1 & recvQsize=1k)      -- ACKer=1:  ~1 mill/sec   lat= ~1 microsec  (batchSz=1 & bolt.wait.strategy=Park bolt.wait.park.micros=0)      -- ACKer=1:  ~1.3 mill/sec lat= ~11 micros   (batchSz=1 & receive.buffer.size=1k bolt.wait & bp.wait =   Progressive[defaults])      -- ACKer=1:  ~1.6 mill/sec lat= ~300 micros  (batchSz=500 & bolt.wait.strategy=Park bolt.wait.park.micros=0) 
Storm,WITHOUT_CLASSIFICATION,// insure that if keytab is used only one login per process executed 
Storm,WITHOUT_CLASSIFICATION,//  0 means DEFAULT_EVENT_LOOP_THREADS 
Storm,WITHOUT_CLASSIFICATION,// test the happy path emit batches in sequence 
Storm,WITHOUT_CLASSIFICATION,//  print the values to stdout 
Storm,WITHOUT_CLASSIFICATION,//  wait strategy when the netty channel is not writable 
Storm,WITHOUT_CLASSIFICATION,// The following come from the JVm Specification table 4.4 
Storm,WITHOUT_CLASSIFICATION,//  Called by flush-tuple-timer thread 
Storm,WITHOUT_CLASSIFICATION,//  if an ack is received for a message then add it to the ackedPerShard TreeSet. TreeSet because while   committing we need to figure out what is the 
Storm,WITHOUT_CLASSIFICATION,//  get trigger count value from store 
Storm,WITHOUT_CLASSIFICATION,// Passed to workers in local clusters exposed by thrift server in distributed mode 
Storm,WITHOUT_CLASSIFICATION,//  Topology will not be able to be successfully scheduled: Config TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB=128.0 < 129.0   Largest memory requirement of a component in the topology). 
Storm,WITHOUT_CLASSIFICATION,//  IGroupMappingServiceProvider  
Storm,WITHOUT_CLASSIFICATION,//  netty TimerTask is already defined and hence a fully   qualified name 
Storm,WITHOUT_CLASSIFICATION,//  executor id is in form [start_task_id end_task_id] 
Storm,WITHOUT_CLASSIFICATION,//  get per task components 
Storm,WITHOUT_CLASSIFICATION,//  BOLTS 
Storm,WITHOUT_CLASSIFICATION,/*                  * Verify that some ticks are received. The interval between ticks is validated by the bolt.                 * Too few and the checks will time out. Too many and the bolt may crash (not reliably but the test should become flaky).                  */
Storm,WITHOUT_CLASSIFICATION,//         oneProducer2Consumers();     // -- measurement 4 
Storm,WITHOUT_CLASSIFICATION,//  add more events with a gap in ts 
Storm,WITHOUT_CLASSIFICATION,// Error should not be leaked according to the code but they are not important enough to fail the build if 
Storm,WITHOUT_CLASSIFICATION,//  SUPERVISORS 
Storm,WITHOUT_CLASSIFICATION,//  end of Test1 
Storm,WITHOUT_CLASSIFICATION,// A copy of cluster that we can modify but does not get committed back to cluster unless scheduling succeeds 
Storm,WITHOUT_CLASSIFICATION,//  The jitter allows the clients to get the data at different times and avoids thundering herd 
Storm,WITHOUT_CLASSIFICATION,// Ranked last since rack-2 has not cpu resources 
Storm,WITHOUT_CLASSIFICATION,//  read 1st line and ack 
Storm,WITHOUT_CLASSIFICATION,//  FileContext supports atomic rename whereas FileSystem doesn't
Storm,WITHOUT_CLASSIFICATION,// 3 seconds in milliseconds 
Storm,WITHOUT_CLASSIFICATION,//  Use default Storm-generated file names 
Storm,WITHOUT_CLASSIFICATION,//  nothing expired yet 
Storm,WITHOUT_CLASSIFICATION,//  should pass now 
Storm,WITHOUT_CLASSIFICATION,//  filtered out 
Storm,WITHOUT_CLASSIFICATION,//  sometimes Leader election indicates the current nimbus is leader but the host was recently restarted   and is currently not a leader. 
Storm,WITHOUT_CLASSIFICATION,//  show a progress bar so we know we're not stuck (especially on slow connections) 
Storm,WITHOUT_CLASSIFICATION,//  end of Test2 
Storm,WITHOUT_CLASSIFICATION,//  TODO timestamps 
Storm,WITHOUT_CLASSIFICATION,// ignore 
Storm,WITHOUT_CLASSIFICATION,// the timeout thread handling 
Storm,WITHOUT_CLASSIFICATION,// No slots to schedule for some reason so skip it. 
Storm,WITHOUT_CLASSIFICATION,//  end of Test3 
Storm,WITHOUT_CLASSIFICATION,//  PARALLELISM_HINT 
Storm,WITHOUT_CLASSIFICATION,//  Commit offsets that are ready to be committed for every topic partition 
Storm,WITHOUT_CLASSIFICATION,//  It's likely that Bolt is shutting down so no need to throw RuntimeException   just ignore 
Storm,WITHOUT_CLASSIFICATION,//  create empty file 
Storm,WITHOUT_CLASSIFICATION,//  Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because   the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have   complete access to the blob 
Storm,WITHOUT_CLASSIFICATION,//  this assumes that inFields and outFields are the same for combineragg   assumption also made above 
Storm,WITHOUT_CLASSIFICATION,//  converts a metadata string into a unique integer.  Updates the timestamp of the string 
Storm,WITHOUT_CLASSIFICATION,//  if there will be legacy values they will be in the outer conf 
Storm,WITHOUT_CLASSIFICATION,// wait until all workers supervisors and nimbus is waiting 
Storm,WITHOUT_CLASSIFICATION,//  Construct a groups mapping for the FixedGroupsMapping class 
Storm,WITHOUT_CLASSIFICATION,//  This is a test where we are configured to point right at an artifact dir 
Storm,WITHOUT_CLASSIFICATION,//  2nd cond prevents staying stuck with consuming overflow 
Storm,WITHOUT_CLASSIFICATION,/*                 * The elements having the same key within the window will be grouped                * together and their values will be reduced using the given reduce function.                *                * Here the result is a PairStream<String Double> with                * 'stock symbol' as the key and the maximum price for that symbol within the window as the value.                 */
Storm,WITHOUT_CLASSIFICATION,//  SECRET_VERSION 
Storm,WITHOUT_CLASSIFICATION,//  get the directory to put uncompressed archives in 
Storm,WITHOUT_CLASSIFICATION,// Expecting 4*2^(failCount-1) 
Storm,WITHOUT_CLASSIFICATION,// Now we know for sure that this is a bad id 
Storm,WITHOUT_CLASSIFICATION,//  Redis has a chunk but no more 
Storm,WITHOUT_CLASSIFICATION,//  File order of MD5 calculation is significant. Sorting is done on   unix-format names case-folded in order to get a platform-independent   sort and calculate the same MD5 on all platforms. 
Storm,WITHOUT_CLASSIFICATION,//  Result.create() states that "You must ensure that the keyvalues are already sorted." 
Storm,WITHOUT_CLASSIFICATION,//  if one group subscribes to the same stream with same partitioning multiple times   merge those together (otherwise can end up with many output streams created for that partitioning   if need to split into multiple output streams because of same input having different   partitioning to the group) 
Storm,WITHOUT_CLASSIFICATION,//  global variables only used internally in class 
Storm,WITHOUT_CLASSIFICATION,//  shard iterator corresponding to position in shard for failed messages 
Storm,WITHOUT_CLASSIFICATION,//  1 - acquire lock on dir 
Storm,WITHOUT_CLASSIFICATION,//  Find the most recent child and load that. 
Storm,WITHOUT_CLASSIFICATION,// At-most-once mode must commit tuples before they are emitted to the topology to ensure that a spout crash won't cause replays. 
Storm,WITHOUT_CLASSIFICATION,// cached supervisor doesn't show up 
Storm,WITHOUT_CLASSIFICATION,// else leader (NOOP) 
Storm,WITHOUT_CLASSIFICATION,//  val xor value 
Storm,WITHOUT_CLASSIFICATION,//  2) check log file content line count == tuples emitted + 1 
Storm,WITHOUT_CLASSIFICATION,//  return the first message to be retried from the set. It will return the message with the earliest retry time <= current time 
Storm,WITHOUT_CLASSIFICATION,//  amount of data written and rotation policies 
Storm,WITHOUT_CLASSIFICATION,// This is possibly lossy in the case where a value is deleted   because it has received no messages over the metrics collection   period and new messages are starting to come in.  This is   because I don't want the overhead of a synchronize just to have   the metric be absolutely perfect. 
Storm,WITHOUT_CLASSIFICATION,// locate login configuration  
Storm,WITHOUT_CLASSIFICATION,// The key was removed so we should delete it too. 
Storm,WITHOUT_CLASSIFICATION,//  namely the two eds on the orphaned worker and the healthy worker 
Storm,WITHOUT_CLASSIFICATION,//  No instantiation 
Storm,WITHOUT_CLASSIFICATION,// Compute the stats for these and save them 
Storm,WITHOUT_CLASSIFICATION,// Not for this topology so skip it 
Storm,WITHOUT_CLASSIFICATION,// Generate some that have neither resource to verify that the strategy will prioritize this last 
Storm,WITHOUT_CLASSIFICATION,// not_jump (open in not strict mode) 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_ON_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  Split up GZIP_MAGIC into readable bytes 
Storm,WITHOUT_CLASSIFICATION,//  6) check log file content line count == tuples emitted + 1 
Storm,WITHOUT_CLASSIFICATION,//  after which nimbus-1 comes back up and a read or update is performed. 
Storm,WITHOUT_CLASSIFICATION,//  get existing assignment (just the topologyToExecutorToNodePort map) -> default to {}   filter out ones which have a executor timeout   figure out available slots on cluster. add to that the used valid slots to get total slots. figure out how many executors   should be in each slot (e.g. 4 4 4 5)   only keep existing slots that satisfy one of those slots. for rest reassign them across remaining slots   edge case for slots with no executor timeout but with supervisor timeout... just treat these as valid slots that can be   reassigned to. worst comes to worse the executor will timeout and won't assign here next time around 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_OFF_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  first block (0% - 10%) 
Storm,WITHOUT_CLASSIFICATION,//  END Metrics 
Storm,WITHOUT_CLASSIFICATION,// if any error/exception thrown just ignore. 
Storm,WITHOUT_CLASSIFICATION,//  start threads after metadata cache created 
Storm,WITHOUT_CLASSIFICATION,// No node for basePath is OK nothing to remove 
Storm,WITHOUT_CLASSIFICATION,//  Test with long value 
Storm,WITHOUT_CLASSIFICATION,/*                 * create a stream of (word 1) pairs                 */
Storm,WITHOUT_CLASSIFICATION,// HBASE tokens are not renewable so we always have to get new ones. 
Storm,WITHOUT_CLASSIFICATION,// task to run 
Storm,WITHOUT_CLASSIFICATION,//  class 
Storm,WITHOUT_CLASSIFICATION,// initialize with storm configuration 
Storm,WITHOUT_CLASSIFICATION,// Kafka throws their own type of exception when interrupted.  Throw a new Java InterruptedException to ensure Storm can recognize the exception as a reaction to an interrupt. 
Storm,WITHOUT_CLASSIFICATION,//  {metric -> win -> value} ==> {win -> metric -> value} 
Storm,WITHOUT_CLASSIFICATION,// else local (NOOP) 
Storm,WITHOUT_CLASSIFICATION,// return null to mount options in string that is not part of cgroups 
Storm,WITHOUT_CLASSIFICATION,//  past limit quit 
Storm,WITHOUT_CLASSIFICATION,//  The whole bytes were not received yet - return null. 
Storm,WITHOUT_CLASSIFICATION,//  Executor summaries 
Storm,WITHOUT_CLASSIFICATION,// metrics rpc  
Storm,WITHOUT_CLASSIFICATION,//  net_clsns is not supported in ubuntu 
Storm,WITHOUT_CLASSIFICATION,/*      * Returns map from task -> componentId      */
Storm,WITHOUT_CLASSIFICATION,//  very stream name matches it stream name was specified 
Storm,WITHOUT_CLASSIFICATION,//  build new metadata based on emitted records 
Storm,WITHOUT_CLASSIFICATION,// Drop the change notifications we are not running anything right now 
Storm,WITHOUT_CLASSIFICATION,// Lets build a topology. 
Storm,WITHOUT_CLASSIFICATION,//  iterate over tuples in the current window 
Storm,WITHOUT_CLASSIFICATION,// ignored 
Storm,WITHOUT_CLASSIFICATION,// Supervisor admin 
Storm,WITHOUT_CLASSIFICATION,//  while holding currentLock to avoid deadlocks 
Storm,WITHOUT_CLASSIFICATION,// obtain a context object 
Storm,WITHOUT_CLASSIFICATION,//  protected using a lock on this counter 
Storm,WITHOUT_CLASSIFICATION,// Again we don't want to exit because of logging issues. 
Storm,WITHOUT_CLASSIFICATION,//  validate search by topology id 
Storm,WITHOUT_CLASSIFICATION,//  canonically the metrics data exported is time bucketed when doing counts.   convert the absolute values here into time buckets. 
Storm,WITHOUT_CLASSIFICATION,//  1) Simulate lock file lease expiring and getting closed by HDFS 
Storm,WITHOUT_CLASSIFICATION,//  tuple values are mapped with 
Storm,WITHOUT_CLASSIFICATION,//  refresh interval in msec   last time the command was performed   env for the command execution 
Storm,WITHOUT_CLASSIFICATION,//  max rounds of scanning the dirs 
Storm,WITHOUT_CLASSIFICATION,//  if no configs from loader try to read from user-resource-pools.yaml 
Storm,WITHOUT_CLASSIFICATION,//  REQUEST_ID 
Storm,WITHOUT_CLASSIFICATION,//  service is off now just interrupt it. 
Storm,WITHOUT_CLASSIFICATION,// Fail tuple 5 and 3 call nextTuple then fail tuple 2 
Storm,WITHOUT_CLASSIFICATION,//  this bolt dosen't emit to downstream bolts 
Storm,WITHOUT_CLASSIFICATION,// Check that a reemit emits exactly the same tuples as the last batch even if Kafka returns more messages 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTE_LATENCY_MS 
Storm,WITHOUT_CLASSIFICATION,// each context will have a single client channel worker event loop group 
Storm,WITHOUT_CLASSIFICATION,//  try to find a way to merge this code with what's already done in TridentBoltExecutor 
Storm,WITHOUT_CLASSIFICATION,//  {component id -> metric -> value} note that input may contain both long and double values 
Storm,WITHOUT_CLASSIFICATION,//  common fields 
Storm,WITHOUT_CLASSIFICATION,// 0-4 compacted away 
Storm,WITHOUT_CLASSIFICATION,//  A stream of words 
Storm,WITHOUT_CLASSIFICATION,//  this is fine b/c we still have a watch from the successful exists call 
Storm,WITHOUT_CLASSIFICATION,//  Now let's advance time. 
Storm,WITHOUT_CLASSIFICATION,//  IOException from reading the version files to be ignored 
Storm,WITHOUT_CLASSIFICATION,//  does not block 
Storm,WITHOUT_CLASSIFICATION,// Check that non-null meta makes the spout seek according to the provided metadata and that the returned meta is correct 
Storm,WITHOUT_CLASSIFICATION,//  class MockCollector 
Storm,WITHOUT_CLASSIFICATION,// The global stream id is this + the from component it must be a part of. 
Storm,WITHOUT_CLASSIFICATION,// Invalidate the cache as something on the node changed 
Storm,WITHOUT_CLASSIFICATION,//  Suppressing exceptions as we don't care for errors on connection close 
Storm,WITHOUT_CLASSIFICATION,// Ignored 
Storm,WITHOUT_CLASSIFICATION,//  STORM_ID 
Storm,WITHOUT_CLASSIFICATION,//  List to array conversion 
Storm,WITHOUT_CLASSIFICATION,// locate our thrift transport plugin 
Storm,WITHOUT_CLASSIFICATION,// Switch to CachedGauge if this starts to hurt performance 
Storm,WITHOUT_CLASSIFICATION,/*                  * Declare a separate 'punctuation' stream per output stream so that the receiving bolt                 * can subscribe to this stream with 'ALL' grouping and process the punctuation once it                 * receives from all upstream tasks.                  */
Storm,WITHOUT_CLASSIFICATION,//  sync sending will return a SendResult 
Storm,WITHOUT_CLASSIFICATION,//  ACKED 
Storm,WITHOUT_CLASSIFICATION,// DO NOT CHANGE UNLESS WE ADD IN STATE NOT STORED IN THE PARENT CLASS 
Storm,WITHOUT_CLASSIFICATION,// STORM-3372: Rotation policy other than TimedRotationPolicy causes NPE on cleanup 
Storm,WITHOUT_CLASSIFICATION,// user with no impersonation acl should be reject 
Storm,WITHOUT_CLASSIFICATION,//  this method expected to be thread safe 
Storm,WITHOUT_CLASSIFICATION,// We are done nothing that short is going to work here 
Storm,WITHOUT_CLASSIFICATION,// if it is is 0 or less it really is 1 per 10 seconds. 
Storm,WITHOUT_CLASSIFICATION,//  preCommit can be invoked during recovery before the state is initialized 
Storm,WITHOUT_CLASSIFICATION,// If it is not set a lot of things are not really going to work all that well 
Storm,WITHOUT_CLASSIFICATION,//  distributed mode 
Storm,WITHOUT_CLASSIFICATION,//  add special pathspec of static content mapped to the homePath 
Storm,WITHOUT_CLASSIFICATION,//  INimbusCredentialPlugin 
Storm,WITHOUT_CLASSIFICATION,// Ack both emitted tuples 
Storm,WITHOUT_CLASSIFICATION,//  RESET_LOG_LEVEL 
Storm,WITHOUT_CLASSIFICATION,// use JsonSerializer as the default serializer 
Storm,WITHOUT_CLASSIFICATION,//  We call fireChannelRead since the client is allowed to   perform this request. The client's request will now proceed   to the next pipeline component namely StormClientHandler. 
Storm,WITHOUT_CLASSIFICATION,//  sliding interval 
Storm,WITHOUT_CLASSIFICATION,// if DRPC spout then id contains function 
Storm,WITHOUT_CLASSIFICATION,//  partial writes of prior lines 
Storm,WITHOUT_CLASSIFICATION,//  Deletes the state inside the zookeeper for a key for which the 
Storm,WITHOUT_CLASSIFICATION,/*  ILookup  */
Storm,WITHOUT_CLASSIFICATION,//  Disconnects don't fail. 
Storm,WITHOUT_CLASSIFICATION,// Nimbus itself 
Storm,WITHOUT_CLASSIFICATION,//  make the new assignments for topologies 
Storm,WITHOUT_CLASSIFICATION,//  late tuple emitted 
Storm,WITHOUT_CLASSIFICATION,// Was a try-cause but I looked at the code around this and key not found is not wrapped in runtime   so it is not needed 
Storm,WITHOUT_CLASSIFICATION,// user jerry submits another topology 
Storm,WITHOUT_CLASSIFICATION,//  key1 shouldn't in iterator 
Storm,WITHOUT_CLASSIFICATION,//  blobstore directory is private! 
Storm,WITHOUT_CLASSIFICATION,//  the merged configs are only for the reset logic 
Storm,WITHOUT_CLASSIFICATION,// Now see if we can create a new token for bob and try again. 
Storm,WITHOUT_CLASSIFICATION,// create a socket with server 
Storm,WITHOUT_CLASSIFICATION,//  remove offsetManagers for all partitions that are no longer assigned to this spout 
Storm,WITHOUT_CLASSIFICATION,// We need to have 3 slots on 3 separate hosts. The topology needs 6 GPUs 3500 MB memory and 350% CPU   The bolt-3 instances must be on separate nodes because they each need 2 GPUs.   The bolt-2 instances must be on the same node as they each need 1 GPU   (this assumes that we are packing the components to avoid fragmentation).   The bolt-1 and spout instances fill in the rest. 
Storm,WITHOUT_CLASSIFICATION,//  when (empty) 
Storm,WITHOUT_CLASSIFICATION,// Remove any expired keys after possibly inserting new ones. 
Storm,WITHOUT_CLASSIFICATION,// We're either going to empty or starting fresh blob download. Either way the changing blob notifications are outdated. 
Storm,WITHOUT_CLASSIFICATION,/*  Verify that the following acked (now committed) tuples are not emitted again         * Since the consumer position was somewhere in the middle of the acked tuples when the commit happened         * this verifies that the spout keeps the consumer position ahead of the committed offset when committing          */
Storm,WITHOUT_CLASSIFICATION,// flushed the buffers completely 
Storm,WITHOUT_CLASSIFICATION,//  go to next file 
Storm,WITHOUT_CLASSIFICATION,//  subscribe to parent's punctuation stream 
Storm,WITHOUT_CLASSIFICATION,/*                      * set the current timestamp as the reference time for the eviction policy                     * to evict the events                      */
Storm,WITHOUT_CLASSIFICATION,//  RESOURCES 
Storm,WITHOUT_CLASSIFICATION,// We cannot connect if there is no client section in the jaas conf... 
Storm,WITHOUT_CLASSIFICATION,//  Metrics 
Storm,WITHOUT_CLASSIFICATION,// Skip any resources where the total is 0 the percent used for this resource isn't meaningful.  We fall back to prioritizing by cpu memory and any other resources by ignoring this value 
Storm,WITHOUT_CLASSIFICATION,// initialCapacity set to 11 since its the default inital capacity of PriorityBlockingQueue 
Storm,WITHOUT_CLASSIFICATION,//  Raw input data to be scored (predicted)   PMML Model read from file - null if using Blobstore   PMML Model downloaded from Blobstore - null if using File 
Storm,WITHOUT_CLASSIFICATION,//  ======== poll ========= 
Storm,WITHOUT_CLASSIFICATION,//         ackingProducerSimulation(); // -- measurement 6 
Storm,WITHOUT_CLASSIFICATION,//  if we can't find the resources directory in a resources jar or in the classpath just create an empty   resources directory. This way we can check later that the topology jar was fully downloaded. 
Storm,WITHOUT_CLASSIFICATION,//  1) acquire locks on file1file2file3 
Storm,WITHOUT_CLASSIFICATION,//  Successfully decoded a frame. 
Storm,WITHOUT_CLASSIFICATION,//  we'll assume the metadata was recently used if still in the cache. 
Storm,WITHOUT_CLASSIFICATION,//  number of events per window-partition 
Storm,WITHOUT_CLASSIFICATION,//  baseDir/supervisor/usercache/user1/filecache 
Storm,WITHOUT_CLASSIFICATION,//  reuse the retrieved iterator 
Storm,WITHOUT_CLASSIFICATION,//  Add cassandra cluster contact points 
Storm,WITHOUT_CLASSIFICATION,// Fail all emitted messages except the first. Commit the first. 
Storm,WITHOUT_CLASSIFICATION,//  string does not exist create using an unique string id and add to cache 
Storm,WITHOUT_CLASSIFICATION,//  this class should be combined with RegisteredGlobalState 
Storm,WITHOUT_CLASSIFICATION,/* Keeping it for backward compatibility */
Storm,WITHOUT_CLASSIFICATION,//  required   required   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  use zk servers as backup if they exist 
Storm,WITHOUT_CLASSIFICATION,// It's more likely to be a file read exception here so we don't differentiate 
Storm,WITHOUT_CLASSIFICATION,//  3) Select a new file if one is not open already 
Storm,WITHOUT_CLASSIFICATION,// In some cases we might be limiting memory in the supervisor and not in the cgroups 
Storm,WITHOUT_CLASSIFICATION,// It is possible that this component is already scheduled on this node or worker.  If so when we backtrack we cannot remove it 
Storm,WITHOUT_CLASSIFICATION,// test we can re-emit the second batch 
Storm,WITHOUT_CLASSIFICATION,//  outputsFields can be empty if this bolt acts like a sink in topology. 
Storm,WITHOUT_CLASSIFICATION,//  iterating multiple times should produce same events 
Storm,WITHOUT_CLASSIFICATION,//  executor0 resides one one worker (on one) executor1 and executor2 on another worker (on the other node) 
Storm,WITHOUT_CLASSIFICATION,//  RESET_LOG_LEVEL_TIMEOUT_SECS 
Storm,WITHOUT_CLASSIFICATION,//  optional 
Storm,WITHOUT_CLASSIFICATION,//  We are pretending to be nimbus here. 
Storm,WITHOUT_CLASSIFICATION,// Wait for a leader to be elected (or topology submission can be rejected) 
Storm,WITHOUT_CLASSIFICATION,//  a stateful processor immediately follows a window specification 
Storm,WITHOUT_CLASSIFICATION,/* user NOT authorized */
Storm,WITHOUT_CLASSIFICATION,//  STORM_VERSION 
Storm,WITHOUT_CLASSIFICATION,/*      * Validator definitions      */
Storm,WITHOUT_CLASSIFICATION,//  should fail 
Storm,WITHOUT_CLASSIFICATION,//  batch sync sending 
Storm,WITHOUT_CLASSIFICATION,//  don't emit anything .. allow configured spout wait strategy to kick in 
Storm,WITHOUT_CLASSIFICATION,// Downloading of all blobs finished. This is the precondition for all codes below. 
Storm,WITHOUT_CLASSIFICATION,// Initial timeout 1 second.  Workers commit suicide after this 
Storm,WITHOUT_CLASSIFICATION,//  since s2 read last it should be evicted s1 and s3 should exist 
Storm,WITHOUT_CLASSIFICATION,//  run() 
Storm,WITHOUT_CLASSIFICATION,//  numMatchesSought numMatchesFound expectedNextByteOffset 
Storm,WITHOUT_CLASSIFICATION,/*  validate memory settings  */
Storm,WITHOUT_CLASSIFICATION,//  Set principal in RebalanceOptions to nil because users are not suppose to set this 
Storm,WITHOUT_CLASSIFICATION,//  add tick tuple each second to force acknowledgement of pending tuples. 
Storm,WITHOUT_CLASSIFICATION,//  has successfully authenticated with this server. 
Storm,WITHOUT_CLASSIFICATION,//  if state found for this shard in zk then set the sequence number in fetchedSequenceNumber 
Storm,WITHOUT_CLASSIFICATION,// Failing tuples for partitions that are no longer assigned is useless since the spout will not be allowed to commit them if they later pass 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   required   required   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,// errors is a bit special because in older versions of storm the worker created the parent directories lazily   because of this it means we need to auto create at least the topo-id directory for all running topos. 
Storm,WITHOUT_CLASSIFICATION,//  look  Impt: HDFS timestamp may not reflect recent appends so we double check the   timestamp in last line of file to see when the last update was made 
Storm,WITHOUT_CLASSIFICATION,// 2) Clear cache 
Storm,WITHOUT_CLASSIFICATION,//  throwing an exception 
Storm,WITHOUT_CLASSIFICATION,//  set upper limit to how much cpu can be used by all workers running on supervisor node.   This is done so that some cpu cycles will remain free to run the daemons and other miscellaneous OS   operations. 
Storm,WITHOUT_CLASSIFICATION,//  this gets called repeatedly for no apparent reason don't do anything 
Storm,WITHOUT_CLASSIFICATION,//  join users and stores on city name 
Storm,WITHOUT_CLASSIFICATION,//  For testing only.. invoked via reflection 
Storm,WITHOUT_CLASSIFICATION,//  first stream's data goes into the probe 
Storm,WITHOUT_CLASSIFICATION,//  fail fast 
Storm,WITHOUT_CLASSIFICATION,//  hostport => WorkerSummary 
Storm,WITHOUT_CLASSIFICATION,//  files are world-wide readable and owner writable 
Storm,WITHOUT_CLASSIFICATION,//  Storm tuple to redis key-value mapper 
Storm,WITHOUT_CLASSIFICATION,//  add configs from resources like hdfs-site.xml 
Storm,WITHOUT_CLASSIFICATION,// / oldComponentDebug.keySet()/ newComponentDebug.keySet() maybe be APersistentSet which don't support addAll 
Storm,WITHOUT_CLASSIFICATION,//  since we made user not authorized component map is empty 
Storm,WITHOUT_CLASSIFICATION,//  create StateUpdater with the given windowStoreFactory to remove triggered aggregation results form store 
Storm,WITHOUT_CLASSIFICATION,//  get all metadata from the cache to put into the database   use a new map to prevent threading issues with writer thread 
Storm,WITHOUT_CLASSIFICATION,//  Avro strings are stored using a special Avro Utf8 type instead of using Java primitives 
Storm,WITHOUT_CLASSIFICATION,// Don't error if timer is shut down happens when the elector is closed. 
Storm,WITHOUT_CLASSIFICATION,//  with 20 tuples per second.  
Storm,WITHOUT_CLASSIFICATION,//  comma separated list of zk connect strings to connect to zookeeper e.g. localhost:2181 
Storm,WITHOUT_CLASSIFICATION,//  we lose the race but it doesn't matter 
Storm,WITHOUT_CLASSIFICATION,// fall throw on purpose 
Storm,WITHOUT_CLASSIFICATION,// Yes we can delete something that is not 0 because of races but that is OK for metrics 
Storm,WITHOUT_CLASSIFICATION,//  Remove old connections atomically 
Storm,WITHOUT_CLASSIFICATION,//  enum conversion 
Storm,WITHOUT_CLASSIFICATION,//  NOOP 
Storm,WITHOUT_CLASSIFICATION,// Don't permit path traversal for calls intended to read from the daemon logs 
Storm,WITHOUT_CLASSIFICATION,//  put bolt message first then put task ids 
Storm,WITHOUT_CLASSIFICATION,// Make sure the parent directory is there and ready to go 
Storm,WITHOUT_CLASSIFICATION,// Nothing is assigned yet should emit nothing 
Storm,WITHOUT_CLASSIFICATION,//  filter sys streams if necessary 
Storm,WITHOUT_CLASSIFICATION,//  calling chooseTasks should be finished before refreshing ring   adjusting groupingExecutionsPerThread might be needed with really slow machine   we allow race condition between refreshing ring and choosing tasks   so it will not make exact even distribution though diff is expected to be small   given that all threadTasks are finished before refreshing ring 
Storm,WITHOUT_CLASSIFICATION,// There could be collisions if keyToString returns only part of a result. 
Storm,WITHOUT_CLASSIFICATION,//  object handling zk interaction 
Storm,WITHOUT_CLASSIFICATION,//  search for the remaining bucket metrics. 
Storm,WITHOUT_CLASSIFICATION,// GAIN_LEADERSHIP is a system event so don't log an issue 
Storm,WITHOUT_CLASSIFICATION,// Only keep important conf keys 
Storm,WITHOUT_CLASSIFICATION,//  javax.jms objects 
Storm,WITHOUT_CLASSIFICATION,//  get the last successfully committed state from state store 
Storm,WITHOUT_CLASSIFICATION,// Since we only give up leadership if we're waiting for blobs to sync  it makes sense to wait a full sync cycle before trying for leadership again. 
Storm,WITHOUT_CLASSIFICATION,//  keys shouldn't appear twice 
Storm,WITHOUT_CLASSIFICATION,//  filter 
Storm,WITHOUT_CLASSIFICATION,// If any error/exception thrown report directly to nimbus. 
Storm,WITHOUT_CLASSIFICATION,// all time 
Storm,WITHOUT_CLASSIFICATION,//  This should not happen (localhost) but if it does we are still OK 
Storm,WITHOUT_CLASSIFICATION,/*                  * May be the task restarted in the middle and the state needs be initialized.                 * Fail fast and trigger recovery.                  */
Storm,WITHOUT_CLASSIFICATION,//  async sending 
Storm,WITHOUT_CLASSIFICATION,//  'generateOutputFields' enables us to avoid projection unless it is the final stream being joined 
Storm,WITHOUT_CLASSIFICATION,// when topology was launched 
Storm,WITHOUT_CLASSIFICATION,//  -- lock timeout 
Storm,WITHOUT_CLASSIFICATION,//  =====================================================================================   helper main.methods   =====================================================================================
Storm,WITHOUT_CLASSIFICATION,// Something odd happened try again later 
Storm,WITHOUT_CLASSIFICATION,// Groups this user is authorized to impersonate. 
Storm,WITHOUT_CLASSIFICATION,//  zk session timeout in milliseconds 
Storm,WITHOUT_CLASSIFICATION,// stream overrides 
Storm,WITHOUT_CLASSIFICATION,// We evicted enough topologies to have a hope of scheduling so try it now and don't evict more   than is needed 
Storm,WITHOUT_CLASSIFICATION,//  ERROR 
Storm,WITHOUT_CLASSIFICATION,//  attempt to find it in the string cache 
Storm,WITHOUT_CLASSIFICATION,//  validate search by executor id 
Storm,WITHOUT_CLASSIFICATION,//  DATA_SIZE 
Storm,WITHOUT_CLASSIFICATION,//  intended to not guarding with try-with-resource since otherwise test will fail 
Storm,WITHOUT_CLASSIFICATION,// since using StringWriter we do not need to close it. 
Storm,WITHOUT_CLASSIFICATION,// Waiting for spout tuples isn't strictly necessary since we also wait for bolt emits but do it anyway 
Storm,WITHOUT_CLASSIFICATION,// storm-core is needed here for backwards compatibility. 
Storm,WITHOUT_CLASSIFICATION,//  EVENTLOG_HOST 
Storm,WITHOUT_CLASSIFICATION,/*              Below regex uses negative lookahead to not split in the middle of json objects '{}'             or json arrays '[]'. This is needed to parse valid json object/arrays passed as options             via 'storm.cmd' in windows. This is not an issue while using 'storm.py' since it url-encodes             the options and the below regex just does a split on the commas that separates each option.             Note:- This regex handles only valid json strings and could produce invalid results             if the options contain un-encoded invalid json or strings with unmatched '[ ] { or }'. We can             replace below code with split("") once 'storm.cmd' is fixed to send url-encoded options.               */
Storm,WITHOUT_CLASSIFICATION,//  node -> topologyId -> double 
Storm,WITHOUT_CLASSIFICATION,//  Return a SaslTokenMessageRequest object 
Storm,WITHOUT_CLASSIFICATION,// The OutputStream is done 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_OFF_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,//  level 2 - parkNanos(1L) 
Storm,WITHOUT_CLASSIFICATION,// create a framed transport 
Storm,WITHOUT_CLASSIFICATION,//  Wait until time out 
Storm,WITHOUT_CLASSIFICATION,// clean up for resource isolation if enabled 
Storm,WITHOUT_CLASSIFICATION,//  2 -   DevNull Bolt   -------- 
Storm,WITHOUT_CLASSIFICATION,//  release the reference on all blobs associated with this topology. 
Storm,WITHOUT_CLASSIFICATION,//  LAST_ERROR 
Storm,WITHOUT_CLASSIFICATION,// Check that a blob can be deleted when a temporary file exists in the blob directory 
Storm,WITHOUT_CLASSIFICATION,//  Fields with embedded commas or double-quote characters 
Storm,WITHOUT_CLASSIFICATION,// This means 1 to maxUncommitteddOffsets but not maxUncommittedOffsets+1...maxUncommittedOffsets+maxPollRecords-1 
Storm,WITHOUT_CLASSIFICATION,//  MESSAGE_BLOB 
Storm,WITHOUT_CLASSIFICATION,//  no-op for zookeeper implementation 
Storm,WITHOUT_CLASSIFICATION,//  max number of files to delete for every round 
Storm,WITHOUT_CLASSIFICATION,//  print lookup result with low probability 
Storm,WITHOUT_CLASSIFICATION,// Only go off of the topology id for now. 
Storm,WITHOUT_CLASSIFICATION,//  never rollback 
Storm,WITHOUT_CLASSIFICATION,//  create another spout to take over processing and read a few lines 
Storm,WITHOUT_CLASSIFICATION,//  if delay or delay + current time are bigger than long max value 
Storm,WITHOUT_CLASSIFICATION,// In this case we are ignoring the coord stuff and only looking at 
Storm,WITHOUT_CLASSIFICATION,//  Maximum number of retries 
Storm,WITHOUT_CLASSIFICATION,//  2 -  Setup IdBolt & DevNullBolt   -------- 
Storm,WITHOUT_CLASSIFICATION,// {nodeId -> {topologyId -> {workerId -> {execs}}}} 
Storm,WITHOUT_CLASSIFICATION,//  all the JMS un-acked messages are going to be re-delivered   so clear the pendingAcks 
Storm,WITHOUT_CLASSIFICATION,//  add all the owners to the map 
Storm,WITHOUT_CLASSIFICATION,//  Prefer local tasks as target tasks if possible 
Storm,WITHOUT_CLASSIFICATION,// latest offset 
Storm,WITHOUT_CLASSIFICATION,//  if first failure add it to the count map 
Storm,WITHOUT_CLASSIFICATION,//  handle no input 
Storm,WITHOUT_CLASSIFICATION,/* user authorized */
Storm,WITHOUT_CLASSIFICATION,//  drop back down. 
Storm,WITHOUT_CLASSIFICATION,//  imperative that don't emit any tuples from here since output factory cannot be gotten until   preparation is done therefore receivers won't be ready to receive tuples yet   can't emit tuples from here anyway since it's not within a batch context (which is only   startBatch execute and finishBatch 
Storm,WITHOUT_CLASSIFICATION,//  topology to tracking of topology dir and resources 
Storm,WITHOUT_CLASSIFICATION,//  we skip uploading first one since we don't test upload for now 
Storm,WITHOUT_CLASSIFICATION,// This can happen if the supervisor crashed after launching a   worker that never came up. 
Storm,WITHOUT_CLASSIFICATION,// generate some that has alot of cpu but little of memory 
Storm,WITHOUT_CLASSIFICATION,//  loop through the arguments if we hit a list that has to be convered to an array   perform the conversion 
Storm,WITHOUT_CLASSIFICATION,// In the second part of the test the rate doubles to 20 per second but the rate tracker   increases its result slowly as we push the 10 tuples per second buckets out and relpace them 
Storm,WITHOUT_CLASSIFICATION,//  process stream definitions 
Storm,WITHOUT_CLASSIFICATION,//  FAILED 
Storm,WITHOUT_CLASSIFICATION,//  validate port 
Storm,WITHOUT_CLASSIFICATION,//  not configurable to prevent change between topology restarts 
Storm,WITHOUT_CLASSIFICATION,// No topologies running so NOOP 
Storm,WITHOUT_CLASSIFICATION,//  for windowed bolt windowed output collector will do the anchoring/acking 
Storm,WITHOUT_CLASSIFICATION,//  inject output bolt 
Storm,WITHOUT_CLASSIFICATION,//  Non Blocking. returns true/false indicating success/failure. Fails if full. 
Storm,WITHOUT_CLASSIFICATION,// scan.setCaching(1000); 
Storm,WITHOUT_CLASSIFICATION,// Fail only the last tuple 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   required   required   required   required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,//  consume file 1 partially 
Storm,WITHOUT_CLASSIFICATION,//  Generate SASL response (but we only actually send the response if   it's non-null. 
Storm,WITHOUT_CLASSIFICATION,//  generate random numbers 
Storm,WITHOUT_CLASSIFICATION,//  do nothing except validate heartbeat for now. 
Storm,WITHOUT_CLASSIFICATION,// retrieve authentication configuration  
Storm,WITHOUT_CLASSIFICATION,//  send expire command for hash only once   it expires key itself entirely so use it with caution 
Storm,WITHOUT_CLASSIFICATION,//  Retry forever 
Storm,WITHOUT_CLASSIFICATION,//  check is a minor optimization 
Storm,WITHOUT_CLASSIFICATION,//  All writes/syncs will fail so this should cause a RuntimeException 
Storm,WITHOUT_CLASSIFICATION,//  Now create a params map to put it in to our conf 
Storm,WITHOUT_CLASSIFICATION,// By convention each share corresponds to 1% of a CPU core   or 100 = 1 core full time. So the guaranteed number of ms   (approximately) should be ... 
Storm,WITHOUT_CLASSIFICATION,//  optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,// Returns messageIds in order of emission 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   required   required   optional 
Storm,WITHOUT_CLASSIFICATION,//  if fail count is greater than maxRetries discard or ack. for e.g. for maxRetries 3 4 failures are allowed at maximum 
Storm,WITHOUT_CLASSIFICATION,/*  * EventHubs bolt configurations * * Partition mode: * With partitionMode=true you need to create the same number of tasks as the number of * EventHubs partitions and each bolt task will only send data to one partition. * The partition ID is the task ID of the bolt. * * Event format: * The formatter to convert tuple to bytes for EventHubs. * if null the default format is common delimited tuple fields.  */
Storm,WITHOUT_CLASSIFICATION,//  guard NPE 
Storm,WITHOUT_CLASSIFICATION,//  replaces normal aggregation here with a global grouping because it needs to be consistent across batches  
Storm,WITHOUT_CLASSIFICATION,//  Add to our collection. 
Storm,WITHOUT_CLASSIFICATION,// login our principal 
Storm,WITHOUT_CLASSIFICATION,// find document from mongodb 
Storm,WITHOUT_CLASSIFICATION,//  MSG 
Storm,WITHOUT_CLASSIFICATION,// NO-OP 
Storm,WITHOUT_CLASSIFICATION,// login our user 
Storm,WITHOUT_CLASSIFICATION,// First check that maxUncommittedOffsets is respected when emitting from scratch 
Storm,WITHOUT_CLASSIFICATION,//  1) Perform Join 
Storm,WITHOUT_CLASSIFICATION,//  only host3 should be returned given filter 
Storm,WITHOUT_CLASSIFICATION,// Verify correct wrapping/unwrapping of partition and delegation of partition assignment 
Storm,WITHOUT_CLASSIFICATION,// for ChainedAggregator 
Storm,WITHOUT_CLASSIFICATION,// Have a crummy cache to show off shared memory accounting 
Storm,WITHOUT_CLASSIFICATION,//  if key already exists while creating the blob else update it 
Storm,WITHOUT_CLASSIFICATION,// ensure that serializations are same for all tasks no matter what's on 
Storm,WITHOUT_CLASSIFICATION,//  zk connection timeout in milliseconds 
Storm,WITHOUT_CLASSIFICATION,// The following are required for backwards compatibility with clojure code 
Storm,WITHOUT_CLASSIFICATION,//  spawn tar utility to untar archive for full fledged unix behavior such   as resolving symlinks in tar archives 
Storm,WITHOUT_CLASSIFICATION,//        The contract of Rankable#copy() returns a Rankable value not a RankableObjectWithFields. 
Storm,WITHOUT_CLASSIFICATION,//  unauthorized 
Storm,WITHOUT_CLASSIFICATION,// 1 create an already existing open file w/o override flag 
Storm,WITHOUT_CLASSIFICATION,//  we could also make this static but not to do it due to mock 
Storm,WITHOUT_CLASSIFICATION,//  blobstore state 
Storm,WITHOUT_CLASSIFICATION,//  key dir needs to be number 0 to number of buckets choose one so we know where to look 
Storm,WITHOUT_CLASSIFICATION,//  compute window length adjustment (delta) to account for time drift 
Storm,WITHOUT_CLASSIFICATION,//  PROCESS_MS_AVG 
Storm,WITHOUT_CLASSIFICATION,//  Backtracking algorithm does not take into account the ordering of executors in worker to reduce traversal space 
Storm,WITHOUT_CLASSIFICATION,//  Ignore. 
Storm,WITHOUT_CLASSIFICATION,//  Spout/Bolt object 
Storm,WITHOUT_CLASSIFICATION,//  "************ Sampling Metrics ***************** 
Storm,WITHOUT_CLASSIFICATION,// InputStream is done 
Storm,WITHOUT_CLASSIFICATION,//  check if processor has specific priority first 
Storm,WITHOUT_CLASSIFICATION,//  we skip uploading first one since we want to test rollback not upload 
Storm,WITHOUT_CLASSIFICATION,//  BITS 
Storm,WITHOUT_CLASSIFICATION,//  Bolt implementation 
Storm,WITHOUT_CLASSIFICATION,// read all the topologies 
Storm,WITHOUT_CLASSIFICATION,//  required   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,/*                  * Split the stream of numbers into streams of                 * even and odd numbers. The first stream contains even                 * and the second contains odd numbers.                  */
Storm,WITHOUT_CLASSIFICATION,//  has been emitted and it is pending ack or fail 
Storm,WITHOUT_CLASSIFICATION,//  if value is greater than Long.MAX_VALUE it truncates to Long.MAX_VALUE 
Storm,WITHOUT_CLASSIFICATION,// use JsonScheme as the default scheme 
Storm,WITHOUT_CLASSIFICATION,//  build components that may be referenced by spouts bolts etc.   the map will be a String --> Object where the object is a fully   constructed class instance 
Storm,WITHOUT_CLASSIFICATION,//  If MapFunction is aware of prepare let it handle preparation 
Storm,WITHOUT_CLASSIFICATION,// two constructor signatures used to initialize validators.  One constructor takes input a Map of arguments the other doesn't take any arguments (default constructor)  If validator has a constructor that takes a Map as an argument call that constructor 
Storm,WITHOUT_CLASSIFICATION,// connect 
Storm,WITHOUT_CLASSIFICATION,//  Configure for achieving max throughput in single worker mode (empirically found).       -- Expect ~5.3 mill/sec (3.2 mill/sec with batchSz=1)       -- ~1 mill/sec lat= ~20 microsec  with acker=1 & batchSz=1 
Storm,WITHOUT_CLASSIFICATION,//  Return a TaskMessage object 
Storm,WITHOUT_CLASSIFICATION,//  static fields 
Storm,WITHOUT_CLASSIFICATION,//  METRICS 
Storm,WITHOUT_CLASSIFICATION,//  if maxRetries is 0 dont retry and return false as per interface contract 
Storm,WITHOUT_CLASSIFICATION,//  find the number of data bytes + length byte 
Storm,WITHOUT_CLASSIFICATION,//  SHARED_MEM_OFF_HEAP 
Storm,WITHOUT_CLASSIFICATION,//  Add new connections atomically 
Storm,WITHOUT_CLASSIFICATION,//  1 -  Setup StringGen Spout   -------- 
Storm,WITHOUT_CLASSIFICATION,//  get num_tasks 
Storm,WITHOUT_CLASSIFICATION,//  watermark events are not added to the queue. 
Storm,WITHOUT_CLASSIFICATION,// Some mount options (i.e. rw and relatime) in type are not cgroups related 
Storm,WITHOUT_CLASSIFICATION,// Bolts 
Storm,WITHOUT_CLASSIFICATION,/*                          * Update histograms based on the new summary. Most common implementation of Reporter reports Gauges before                         * Histograms. Because DerivativeGauge will trigger cache refresh upon reporter's query histogram will also be                         * updated before query                          */
Storm,WITHOUT_CLASSIFICATION,//  employed when no incoming data   employed when outbound path is congested 
Storm,WITHOUT_CLASSIFICATION,// Update the OffsetManager for each committed partition and update numUncommittedOffsets 
Storm,WITHOUT_CLASSIFICATION,//  Create keyspace not supported in the current datastax driver 
Storm,WITHOUT_CLASSIFICATION,//  Suppressing exceptions as we don't care for errors on heartbeats 
Storm,WITHOUT_CLASSIFICATION,// Schema changes should have forced file rotations 
Storm,WITHOUT_CLASSIFICATION,//  Test3: When a supervisor and a worker on it fails RAS does not alter existing assignments 
Storm,WITHOUT_CLASSIFICATION,//  finish up reading the file 
Storm,WITHOUT_CLASSIFICATION,//  baseDir/supervisor/usercache/ 
Storm,WITHOUT_CLASSIFICATION,/*      * Passing a factory rather than the actual object to avoid enforcing the strong     * requirement of having to have ModelRunner to be Serializable      */
Storm,WITHOUT_CLASSIFICATION,//  SCRIPT 
Storm,WITHOUT_CLASSIFICATION,// Initial Setup 
Storm,WITHOUT_CLASSIFICATION,//  string does not exist in database 
Storm,WITHOUT_CLASSIFICATION,//  We don't use the classpath part of this so just an empty list 
Storm,WITHOUT_CLASSIFICATION,// Treat reassign as remove and add 
Storm,WITHOUT_CLASSIFICATION,//  Get latest sequence number of the blob present in the zookeeper --> possible to refactor this piece of code 
Storm,WITHOUT_CLASSIFICATION,// Throttle this spout a bit to avoid maxing out CPU 
Storm,WITHOUT_CLASSIFICATION,//  whatever remains in the tab are non matching left rows. 
Storm,WITHOUT_CLASSIFICATION,//  explicit anchoring emits to corresponding input tuples only as default window anchoring will anchor them to all   tuples in window 
Storm,WITHOUT_CLASSIFICATION,// Scheduling Status set to FAIL_OTHER so no eviction policy will be attempted to make space for this topology 
Storm,WITHOUT_CLASSIFICATION,//  banner 
Storm,WITHOUT_CLASSIFICATION,// We didn't find it but there are races so we want to check again after a sync 
Storm,WITHOUT_CLASSIFICATION,//  empty if no factoryArgs 
Storm,WITHOUT_CLASSIFICATION,//  Make the assignment null to let slot clean up the disk assignment. 
Storm,WITHOUT_CLASSIFICATION,//  If authentication of client is complete we will also send a   SASL-Complete message to the client. 
Storm,WITHOUT_CLASSIFICATION,// race condition with another thread and we lost   try again 
Storm,WITHOUT_CLASSIFICATION,//  Convenience alternative to prepare() for use in Tests 
Storm,WITHOUT_CLASSIFICATION,//  Use while loop try to decode as more messages as possible in single call 
Storm,WITHOUT_CLASSIFICATION,/*          * Windows should be aligned to the slide size starting at firstWindowEndTime - windowSec.         * Because all windows are aligned to the slide size we can partition the spout emitted timestamps by which window they should fall in.         * This checks that the partitioned spout emits fall in the expected windows based on the logs from the spout and bolt.          */
Storm,WITHOUT_CLASSIFICATION,//  it shouldn't propagate any exceptions 
Storm,WITHOUT_CLASSIFICATION,/* Get one message at a time for backward compatibility behaviour */
Storm,WITHOUT_CLASSIFICATION,// 3 seconds from now 
Storm,WITHOUT_CLASSIFICATION,// Spout 
Storm,WITHOUT_CLASSIFICATION,//  update the shard iterator to next one in case this fetch does not give the message. 
Storm,WITHOUT_CLASSIFICATION,//  if it is local mode just get the local nimbus instance and set the heartbeats 
Storm,WITHOUT_CLASSIFICATION,//  ensure that choice1 and choice2 are not the same task 
Storm,WITHOUT_CLASSIFICATION,// No need to extract it it is not what we are looking for. 
Storm,WITHOUT_CLASSIFICATION,/*                 * Queries the state and emits the                * matching (key value) as results. The stream state returned                * by the updateStateByKey is passed as the argument to stateQuery.                 */
Storm,WITHOUT_CLASSIFICATION,//  keep committing when topology is deactivated since ack and fail keep getting called on deactivated topology 
Storm,WITHOUT_CLASSIFICATION,//  =====================================================================================   heartbeats related   ===================================================================================== 
Storm,WITHOUT_CLASSIFICATION,//  Join helper to concat fields to the record 
Storm,WITHOUT_CLASSIFICATION,//  convert all strings to numeric Ids for the metric key and add to the metadata cache 
Storm,WITHOUT_CLASSIFICATION,// Once Storm is baselined to Java 11 we can use URLDecoder.decode(String Charset) instead which obsoletes this method. 
Storm,WITHOUT_CLASSIFICATION,// make sure that defined key is string in case wrong stuff got put into Config.java 
Storm,WITHOUT_CLASSIFICATION,//  Writing random words to be blacklisted 
Storm,WITHOUT_CLASSIFICATION,//  more ACLs can be added here 
Storm,WITHOUT_CLASSIFICATION,//  2) wait for all 3 locks to expire then heart beat on 2 locks and verify stale lock 
Storm,WITHOUT_CLASSIFICATION,//  ID_TO_SPOUT_AGG_STATS 
Storm,WITHOUT_CLASSIFICATION,// Emit all messages and fail the first one while acking the rest 
Storm,WITHOUT_CLASSIFICATION,//  =====================================================================================   aggregation stats main.methods   =====================================================================================
Storm,WITHOUT_CLASSIFICATION,//  1 - parse cmd line args 
Storm,WITHOUT_CLASSIFICATION,//  schedule 1st topology 
Storm,WITHOUT_CLASSIFICATION,//  A singleton instance allows us to mock delegated static main.methods in our   tests by subclassing.
Storm,WITHOUT_CLASSIFICATION,//  We purposely simulate a 1 second bucket size so the rate will always be 10 per second. 
Storm,WITHOUT_CLASSIFICATION,// wordspout -> lookupbolt 
Storm,WITHOUT_CLASSIFICATION,//  STRING_ARG 
Storm,WITHOUT_CLASSIFICATION,//  you cant define a topologySource and a DSL topology at the same time... 
Storm,WITHOUT_CLASSIFICATION,//  can be null;   nested field "x.y.z"  becomes => String["x""y""z"]   either "stream1:x.y.z" or "x.y.z" depending on whether stream name is present. 
Storm,WITHOUT_CLASSIFICATION,//  Task Id not used so just pick a static value 
Storm,WITHOUT_CLASSIFICATION,// make sure all workers on scheduled in rack-0 
Storm,WITHOUT_CLASSIFICATION,//  REQUESTED_MEMOFFHEAP 
Storm,WITHOUT_CLASSIFICATION,// For testing... 
Storm,WITHOUT_CLASSIFICATION,// Parallelism is double 
Storm,WITHOUT_CLASSIFICATION,// Two distinct schema should result in only two files 
Storm,WITHOUT_CLASSIFICATION,//  initialize server-side SASL functionality if we haven't yet   (in which case we are looking at the first SASL message from the 
Storm,WITHOUT_CLASSIFICATION,// This spout owns 1 partitions: 6 
Storm,WITHOUT_CLASSIFICATION,//  Then get it and return the file as string. 
Storm,WITHOUT_CLASSIFICATION,// Sliding windows should produce one window every slideSize tuples  Wait for the spout to emit at least enough tuples to get minBoltEmit windows and at least one full window 
Storm,WITHOUT_CLASSIFICATION,// Add in all of the components 
Storm,WITHOUT_CLASSIFICATION,// Monitor for assignment changes as often as possible so e.g. shutdown happens as fast as possible. 
Storm,WITHOUT_CLASSIFICATION,//  User defined Callback 
Storm,WITHOUT_CLASSIFICATION,// Let worker tokens work on insecure ZK... 
Storm,WITHOUT_CLASSIFICATION,// We were rescheduled while waiting for the resources to be updated   but the container is already not running. 
Storm,WITHOUT_CLASSIFICATION,// nodeHost is not null here as newConnections is only non-empty if assignment was not null above.   Host   Port 
Storm,WITHOUT_CLASSIFICATION,//  supervisor port should be only presented to worker which supports RPC heartbeat 
Storm,WITHOUT_CLASSIFICATION,//  update the value in state 
Storm,WITHOUT_CLASSIFICATION,//  The first seek offset for each topic partition i.e. the offset this spout instance started processing at. 
Storm,WITHOUT_CLASSIFICATION,//  Cassandra daemon calls System.exit() on windows which kills the test. 
Storm,WITHOUT_CLASSIFICATION,// Rotate once per timeout period that has passed since last time this was called.  This is necessary since this method may be called at arbitrary intervals. 
Storm,WITHOUT_CLASSIFICATION,// Timer is discarded after the initial launch of an assignment 
Storm,WITHOUT_CLASSIFICATION,// check resources 
Storm,WITHOUT_CLASSIFICATION,//  later on this will be joined back with return-info and all the results 
Storm,WITHOUT_CLASSIFICATION,//  based on transactional topologies 
Storm,WITHOUT_CLASSIFICATION,// Thread.sleep(120); 
Storm,WITHOUT_CLASSIFICATION,//  JMS Queue Provider 
Storm,WITHOUT_CLASSIFICATION,// remove resources 
Storm,WITHOUT_CLASSIFICATION,// This is to verify that a low maxPollRecords does not interfere with reemitting failed tuples 
Storm,WITHOUT_CLASSIFICATION,// If the node does not exist it is fine/expected... 
Storm,WITHOUT_CLASSIFICATION,//  default is a sliding window of count 1 
Storm,WITHOUT_CLASSIFICATION,/*           Verify that failed offsets will only retry if the corresponding message exists.           When log compaction is enabled in Kafka it is possible that a tuple can fail           and then be impossible to retry because the message in Kafka has been deleted.          The spout needs to quietly ack such tuples to allow commits to progress past the deleted offset.          */
Storm,WITHOUT_CLASSIFICATION,//  BOOL_ARG 
Storm,WITHOUT_CLASSIFICATION,//  distribution should be exactly even 
Storm,WITHOUT_CLASSIFICATION,//  attempt to find it in callers cache 
Storm,WITHOUT_CLASSIFICATION,//  benchmark label 
Storm,WITHOUT_CLASSIFICATION,//  messages. 
Storm,WITHOUT_CLASSIFICATION,// Merge the old credentials so creds nimbus created are not lost.   And in case the user forgot to upload something important this time. 
Storm,WITHOUT_CLASSIFICATION,//  generated code will be not compilable since return type of MYPLUS and type of 'x' are different 
Storm,WITHOUT_CLASSIFICATION,//  examine the response message from server 
Storm,WITHOUT_CLASSIFICATION,//  force triggers building ring 
Storm,WITHOUT_CLASSIFICATION,//  Nothing here could not get message id 
Storm,WITHOUT_CLASSIFICATION,//  required 
Storm,WITHOUT_CLASSIFICATION,//  try to use zookeeper secret 
Storm,WITHOUT_CLASSIFICATION,/*      * Check for uncaught exceptions during the execution     * of the trigger and fail fast.     * The uncaught exceptions will be wrapped in     * ExecutionException and thrown when future.get() is invoked.      */
Storm,WITHOUT_CLASSIFICATION,// We need to re-login some other thread might have logged into hadoop using   their credentials (e.g. AutoHBase might be also part of nimbu auto creds) 
Storm,WITHOUT_CLASSIFICATION,//  When this master is not leader and get a sync request from node   just return nil which will cause client/node to get an unknown error   the node/supervisor will sync it as a timer task. 
Storm,WITHOUT_CLASSIFICATION,//  size matches check if the streams are expected 
Storm,WITHOUT_CLASSIFICATION,//  emulate the call of withLateTupleStream method 
Storm,WITHOUT_CLASSIFICATION,// We set the stddev of the skewed keys to be 1/5 of the length but then we use the absolute value   of that so everything is skewed towards 0 
Storm,WITHOUT_CLASSIFICATION,//  storm hbase keytab /etc/security/keytabs/storm-hbase.keytab 
Storm,WITHOUT_CLASSIFICATION,//  if there are no windowed/batched processors we would ack immediately 
Storm,WITHOUT_CLASSIFICATION,// The spout should not emit any more tuples. 
Storm,WITHOUT_CLASSIFICATION,//  Helper static vars for each platform 
Storm,WITHOUT_CLASSIFICATION,//  TASK_END 
Storm,WITHOUT_CLASSIFICATION,//  DO NOT CHANGE THIS TO SYSOUT 
Storm,WITHOUT_CLASSIFICATION,//  1- try adding to main queue if its overflow is not empty 
Storm,WITHOUT_CLASSIFICATION,// bolt-1  bolt-2  bolt-3 
Storm,WITHOUT_CLASSIFICATION,//  if no new assignment. 
Storm,WITHOUT_CLASSIFICATION,//  Generally used to compare how files were actually written and compare to expectations based on total 
Storm,WITHOUT_CLASSIFICATION,// do retries if the connect fails 
Storm,WITHOUT_CLASSIFICATION,//  then stop running it 
Storm,WITHOUT_CLASSIFICATION,//  COMMON_STATS 
Storm,WITHOUT_CLASSIFICATION,//  Find the task the events from this component route to. 
Storm,WITHOUT_CLASSIFICATION,//  handle Ctrl-C 
Storm,WITHOUT_CLASSIFICATION,//  if the empty partition was not invalidated by flush but evicted from cache 
Storm,WITHOUT_CLASSIFICATION,//  componentExecutors maybe be APersistentMap which don't support "put" 
Storm,WITHOUT_CLASSIFICATION,// It is possible to get asked about eviction before we have a context due to WindowManager.compactWindow.  In this case we should hold on to all the events. When the first watermark is received the context will be set  and the events will be reevaluated for eviction 
Storm,WITHOUT_CLASSIFICATION,//  add tuple to the batch state 
Storm,WITHOUT_CLASSIFICATION,//  Redis config parameters for the RedisStoreBolt 
Storm,WITHOUT_CLASSIFICATION,//  this is fine because the only time this is shared is when it's a local context 
Storm,WITHOUT_CLASSIFICATION,//  USED_PORTS 
Storm,WITHOUT_CLASSIFICATION,//  SCHEDULER_META 
Storm,WITHOUT_CLASSIFICATION,//  the oldest pq_size files in this directory will be placed in PQ with the newest at the root 
Storm,WITHOUT_CLASSIFICATION,//  required   optional 
Storm,WITHOUT_CLASSIFICATION,// make sure all workers on scheduled in rack-1 
Storm,WITHOUT_CLASSIFICATION,//  7) read remaining lines in file then ensure lock is gone 
Storm,WITHOUT_CLASSIFICATION,//  get a currently unused unique string id 
Storm,WITHOUT_CLASSIFICATION,//  Use streamId source component name OR field in tuple to distinguish incoming tuple streams 
Storm,WITHOUT_CLASSIFICATION,// The spout must reemit retriable tuples even if they fail out of order.  The spout should be able to skip tuples it has already emitted when retrying messages even if those tuples are also retries. 
Storm,WITHOUT_CLASSIFICATION,//  business logic. 
Storm,WITHOUT_CLASSIFICATION,// user jerry submits another topology but this one should be scheduled since it has higher priority than than the 
Storm,WITHOUT_CLASSIFICATION,// The last slot fill it up 
Storm,WITHOUT_CLASSIFICATION,// Operation level IO Exceptions 
Storm,WITHOUT_CLASSIFICATION,//  Extract bolt resource info 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTED 
Storm,WITHOUT_CLASSIFICATION,//  sort by port: from small to large 
Storm,WITHOUT_CLASSIFICATION,// Sort comps by number of constraints 
Storm,WITHOUT_CLASSIFICATION,// Build new metadata based on the consumer position.  We want the next emit to start at the current consumer position  so make a meta that indicates that position - 1 is the last emitted offset  This helps us avoid cases like STORM-3279 and simplifies the seek logic. 
Storm,WITHOUT_CLASSIFICATION,// Emit 10 empty batches simulating no new records being present in Kafka 
Storm,WITHOUT_CLASSIFICATION,//  In at-most-once mode the offsets are committed after every poll and not periodically as controlled by the timer 
Storm,WITHOUT_CLASSIFICATION,// Test if topology is already partially scheduled on one rack 
Storm,WITHOUT_CLASSIFICATION,//  ASSIGNED_SHARED_OFF_HEAP_MEMORY 
Storm,WITHOUT_CLASSIFICATION,/*          * total tuples should be         * recovered (batch-1 + batch-2) + replayed (batch-3)          */
Storm,WITHOUT_CLASSIFICATION,//  It's possible that permissions were not set properly on the directory and   the user who is *supposed* to own the dir does not. In this case try the   delete as the supervisor user. 
Storm,WITHOUT_CLASSIFICATION,// Create the directory no matter what. This is so we can check if it was downloaded in the future. 
Storm,WITHOUT_CLASSIFICATION,// Busy wait... 
Storm,WITHOUT_CLASSIFICATION,// short 
Storm,WITHOUT_CLASSIFICATION,// Check that the emitter can handle emitting empty batches on a new partition.  If the spout is configured to seek to LATEST or the partition is empty the initial batches may be empty 
Storm,WITHOUT_CLASSIFICATION,//  String metastoreDBLocation = "jdbc:derby:databaseName=/tmp/metastore_db;create=true";   conf.set("javax.jdo.option.ConnectionDriverName""org.apache.derby.jdbc.EmbeddedDriver");   conf.set("javax.jdo.option.ConnectionURL"metastoreDBLocation); 
Storm,WITHOUT_CLASSIFICATION,// This is here to avoid an overridable call in the constructor 
Storm,WITHOUT_CLASSIFICATION,// pending is empty return the smallest in toResend 
Storm,WITHOUT_CLASSIFICATION,//  attempt scheduling both topologies.   this triggered negative resource event as the second topology incorrectly scheduled with the first in place 
Storm,WITHOUT_CLASSIFICATION,// Ignored the file system we are on does not support this so don't do it. 
Storm,WITHOUT_CLASSIFICATION,//  kill process when timeout 
Storm,WITHOUT_CLASSIFICATION,// find login file configuration from Storm configuration 
Storm,WITHOUT_CLASSIFICATION,//  compareAndSet added because of https://issues.apache.org/jira/browse/STORM-1535 
Storm,WITHOUT_CLASSIFICATION,// Test for subject with no principals 
Storm,WITHOUT_CLASSIFICATION,//  ################# Connect Callback Implementation ###################### 
Storm,WITHOUT_CLASSIFICATION,//  in other case just set this to 0 to trigger re-sync later 
Storm,WITHOUT_CLASSIFICATION,//  force process to be terminated 
Storm,WITHOUT_CLASSIFICATION,//  Acquire a slot 
Storm,WITHOUT_CLASSIFICATION,//  returns list of list of slots reverse sorted by number of slots 
Storm,WITHOUT_CLASSIFICATION,// play and fail 1 tuple 
Storm,WITHOUT_CLASSIFICATION,//  delete 
Storm,WITHOUT_CLASSIFICATION,//  doing below because its affecting storm metrics most likely   had to make tick tuple a mandatory argument since its positional 
Storm,WITHOUT_CLASSIFICATION,// Thread safety is mostly around acl.  If acl needs to be updated it is changed atomically  More then one thread may be trying to update it at a time but that is OK because the  change is atomic 
Storm,WITHOUT_CLASSIFICATION,// Could be replaced when metrics support remove all functions   https://github.com/dropwizard/metrics/pull/1280 
Storm,WITHOUT_CLASSIFICATION,//  required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
Storm,WITHOUT_CLASSIFICATION,// else we assume it is the same as teh SUPER_ACL which is covered by CREATOR_ALL 
Storm,WITHOUT_CLASSIFICATION,//  topo5 has 40 small tasks it should be able to exactly use up both the cpu and mem in the cluster 
Storm,WITHOUT_CLASSIFICATION,//  Ensure that the original batch1 is discarded and new one is persisted. 
Storm,WITHOUT_CLASSIFICATION,// It was not there before so we need to run it. 
Storm,WITHOUT_CLASSIFICATION,//  A stream of random sentences 
Storm,WITHOUT_CLASSIFICATION,//  no reference to archive1 
Storm,WITHOUT_CLASSIFICATION,// By default all of the values are 0 
Storm,WITHOUT_CLASSIFICATION,//  object representing information on paramaters to use while connecting to kinesis using kinesis client 
Storm,WITHOUT_CLASSIFICATION,//  1) read 5 lines in file 
Storm,WITHOUT_CLASSIFICATION,//  A no-op grouper 
Storm,WITHOUT_CLASSIFICATION,// Release things that don't need to wait for us 
Storm,WITHOUT_CLASSIFICATION,//  trigger manually to avoid timing issues 
Storm,WITHOUT_CLASSIFICATION,//  actual value of m is: Map<String Map<String/GlobalStreamId Long/Double>> ({win -> stream -> value}) 
Storm,WITHOUT_CLASSIFICATION,// File 2 should be read 
Storm,WITHOUT_CLASSIFICATION,//  check to see if any shard has already fetched records waiting to be emitted in which case dont fetch more 
Storm,WITHOUT_CLASSIFICATION,//  for testing 
Storm,WITHOUT_CLASSIFICATION,// In local mode the main process should never exit on it's own 
Storm,WITHOUT_CLASSIFICATION,// First delete everything that no longer exists... 
Storm,WITHOUT_CLASSIFICATION,//  KILL_OPTIONS 
Storm,WITHOUT_CLASSIFICATION,//  additional tests that go beyond what this test is primarily about 
Storm,WITHOUT_CLASSIFICATION,// Old Buckets and their length are only touched when rotating or gathering the metrics which should not be that frequent   As such all access to them should be protected by synchronizing with the RateTracker instance 
Storm,WITHOUT_CLASSIFICATION,// a null tuple should be added to the ack list since by definition is a direct ack 
Storm,WITHOUT_CLASSIFICATION,// Don't know a better way to validate that it failed. 
Storm,WITHOUT_CLASSIFICATION,// The tick should NOT cause any acks since the batch was empty except for acking itself 
Storm,WITHOUT_CLASSIFICATION,//  We have three mutually disjoint treesets per shard at any given time to keep track of what sequence number   can be committed to zookeeper.   emittedPerShard ackedPerShard and failedPerShard. Any record starts by entering emittedPerShard. On ack   it moves from emittedPerShard to   ackedPerShard and on fail if retry service tells us to retry then it moves from emittedPerShard to   failedPerShard. The failed records will move from   failedPerShard to emittedPerShard when the failed record is emitted again as a retry.   Logic for deciding what sequence number to commit is find the highest sequence number from ackedPerShard   called X such that there is no sequence   number Y in emittedPerShard or failedPerShard that satisfies X > Y. For e.g. if ackedPerShard is 145   emittedPerShard is 26 and   failedPerShard is 37 then we can only commit 1 and not 4 because 2 is still pending and 3 has failed 
Storm,WITHOUT_CLASSIFICATION,//  use exhibitor servers 
Storm,WITHOUT_CLASSIFICATION,//  reserved for saving topology data 
Storm,WITHOUT_CLASSIFICATION,//  Map of records  that were fetched from kinesis as a result of failure and are waiting to be emitted 
Storm,WITHOUT_CLASSIFICATION,//  this is so we can do things like have simple DRPC that doesn't need to use batch processing 
Storm,WITHOUT_CLASSIFICATION,//  Stop iterating in the middle of the 10th partition 
Storm,WITHOUT_CLASSIFICATION,//  COMPONENT_TYPE 
Storm,WITHOUT_CLASSIFICATION,//  Configure the server. 
Storm,WITHOUT_CLASSIFICATION,// Assign partitions to the spout 
Storm,WITHOUT_CLASSIFICATION,//  ok if its return null 
Storm,WITHOUT_CLASSIFICATION,// Should assert file size 
Storm,WITHOUT_CLASSIFICATION,//  APPROVED_WORKERS 
Storm,WITHOUT_CLASSIFICATION,//  JMS Queue Spout 
Storm,WITHOUT_CLASSIFICATION,//  First make the cache dir 
Storm,WITHOUT_CLASSIFICATION,// Emit some more messages 
Storm,WITHOUT_CLASSIFICATION,// Just in case we get something we are confused about   we can continue processing the rest of the tasks 
Storm,WITHOUT_CLASSIFICATION,//  We need to add the new futures to the existing ones 
Storm,WITHOUT_CLASSIFICATION,//  Logging an exception while client is connecting 
Storm,WITHOUT_CLASSIFICATION,// The scheduler generally will try to pack executors into workers until the max heap size is met but   this can vary depending on the specific scheduling strategy selected.   The reason for this is to try and balance the maximum pause time GC might take (which is larger for larger heaps)   against better performance because of not needing to serialize/deserialize tuples. 
Storm,WITHOUT_CLASSIFICATION,//  wordSpout ==> countBolt ==> MongoInsertBolt 
Storm,WITHOUT_CLASSIFICATION,//  IS_LEADER 
Storm,WITHOUT_CLASSIFICATION,// unknown error just skip 
Storm,WITHOUT_CLASSIFICATION,//  =================  Factory Methods Declaring ModelOutputs to Multiple Streams  ================== 
Storm,WITHOUT_CLASSIFICATION,//  SESSION 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTOR_NODE_PORT 
Storm,WITHOUT_CLASSIFICATION,//  index and fieldIndex are precomputed delegates built up over many operations using persistent data structures 
Storm,WITHOUT_CLASSIFICATION,//  VALIDATION ONLY CONFIGS   Some configs inside Config.java may reference classes we don't want to expose in storm-client but we still want to validate   That they reference a valid class.  To allow this to happen we do part of the validation on the client side with annotations on   static final members of the Config class and other validations here.  We avoid naming them the same thing because clojure code   walks these two classes and creates clojure constants for these values. 
Storm,WITHOUT_CLASSIFICATION,//  PULSES 
Storm,WITHOUT_CLASSIFICATION,//  common 
Storm,WITHOUT_CLASSIFICATION,//  required for jackson serialization 
Storm,WITHOUT_CLASSIFICATION,//  timestamp in millisecond 0 means   disabling filter 
Storm,WITHOUT_CLASSIFICATION,// TODO batch finding 
Storm,WITHOUT_CLASSIFICATION,//  perf critical loop. dont use iterators 
Storm,WITHOUT_CLASSIFICATION,//  COMPONENT_ID 
Storm,WITHOUT_CLASSIFICATION,//  -- commit frequency - seconds 
Storm,WITHOUT_CLASSIFICATION,// There is a race here that we can still lose 
Storm,WITHOUT_CLASSIFICATION,// JPROFILE STOP 
Storm,WITHOUT_CLASSIFICATION,// class 
Storm,WITHOUT_CLASSIFICATION,/*      * aggWorkerStats tests      */
Storm,WITHOUT_CLASSIFICATION,// Break the code if out of sync to thrift protocol 
Storm,WITHOUT_CLASSIFICATION,//  server. 
Storm,WITHOUT_CLASSIFICATION,// status of scheduling the topology e.g. success or fail? 
Storm,WITHOUT_CLASSIFICATION,// with realm e.g. storm@WITZEND.COM 
Storm,WITHOUT_CLASSIFICATION,//  calc sid->output_stats 
Storm,WITHOUT_CLASSIFICATION,//  MS 
Storm,WITHOUT_CLASSIFICATION,//  The removed writer should have been closed 
Storm,WITHOUT_CLASSIFICATION,//  (JMOCK-263).  See https://github.com/jmock-developers/jmock-library/issues/22 for more information. 
Storm,WITHOUT_CLASSIFICATION,//  convert each record into a HashMap using fieldNames as keys 
Storm,WITHOUT_CLASSIFICATION,// used for local cluster heartbeating 
Storm,WITHOUT_CLASSIFICATION,//  SERVICE_TYPE 
Storm,WITHOUT_CLASSIFICATION,//  apply new log settings we just received 
Storm,WITHOUT_CLASSIFICATION,//  g1 being null means the source is a spout node 
Storm,WITHOUT_CLASSIFICATION,/*  LOOK AT HDFS BLOBSTORE  */
Storm,WITHOUT_CLASSIFICATION,// The first tuple should be acked and should not have failed 
Storm,WITHOUT_CLASSIFICATION,// This section simply put the formatted log filename and corresponding port in the matching. 
Storm,WITHOUT_CLASSIFICATION,//  TOPO_OWNER 
Storm,WITHOUT_CLASSIFICATION,//  filter configured should fail all users 
Storm,WITHOUT_CLASSIFICATION,//  Validating class implementation could fail on non-Nimbus Daemons.  Nimbus will catch the class not found on startup   and log an error message so just validating this as a String for now. 
Storm,WITHOUT_CLASSIFICATION,//  3 - if clocks are not in sync .. 
Storm,WITHOUT_CLASSIFICATION,//  JDK_VERSION 
Storm,WITHOUT_CLASSIFICATION,//  4) Read record from file emit to collector and record progress 
Storm,WITHOUT_CLASSIFICATION,//  This should always be set to digest. 
Storm,WITHOUT_CLASSIFICATION,// We were rescheduled while waiting for the worker to come up 
Storm,WITHOUT_CLASSIFICATION,//  get from DB and add to lookup cache 
Storm,WITHOUT_CLASSIFICATION,//  list all files for this topology 
Storm,WITHOUT_CLASSIFICATION,// examine the response message from server 
Storm,WITHOUT_CLASSIFICATION,// Alice has no digest jaas section at all... 
Storm,WITHOUT_CLASSIFICATION,//  test basic substitution 
Storm,WITHOUT_CLASSIFICATION,//  monotonically increasing id for correlating sent/recvd msgs. ok if restarts from 0 on crash. 
Storm,WITHOUT_CLASSIFICATION,//  for unit tests 
Storm,WITHOUT_CLASSIFICATION,//  just mkdir STORM_ZOOKEEPER_ROOT dir 
Storm,WITHOUT_CLASSIFICATION,// Nothing else should be emitted all tuples are acked except for the final tuple which is pending. 
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_NAME 
Storm,WITHOUT_CLASSIFICATION,//  ensure checkpoint interval is not less than a sane low value. 
Storm,WITHOUT_CLASSIFICATION,//  END TOPOLOGY STATE TRANSITIONS 
Storm,WITHOUT_CLASSIFICATION,// get metric name 
Storm,WITHOUT_CLASSIFICATION,//  matches and matchCount is not changed 
Storm,WITHOUT_CLASSIFICATION,// But throughput is the same 
Storm,WITHOUT_CLASSIFICATION,/* without Auth */
Storm,WITHOUT_CLASSIFICATION,//  CREDS 
Storm,WITHOUT_CLASSIFICATION,//  tracks order in which msg came in 
Storm,WITHOUT_CLASSIFICATION,// The first tuple wil be used to check timeout reset 
Storm,WITHOUT_CLASSIFICATION,//  hbase principal storm-hbase@WITZEN.COM 
Storm,WITHOUT_CLASSIFICATION,//  write the tuple to a JMS destination... 
Storm,WITHOUT_CLASSIFICATION,// call updateMetricFromRPC with params 
Storm,WITHOUT_CLASSIFICATION,// This is here only for testing. 
Storm,WITHOUT_CLASSIFICATION,//  Do nothing 
Storm,WITHOUT_CLASSIFICATION,//  Preserve interrupt status 
Storm,WITHOUT_CLASSIFICATION,//  How many states searched so far. 
Storm,WITHOUT_CLASSIFICATION,//  NOOP no need to create links in local mode 
Storm,WITHOUT_CLASSIFICATION,//  Usage: Let it and then explicitly terminate.   Metrics will be printed when application is terminated. 
Storm,WITHOUT_CLASSIFICATION,/*                      * propagate it so that task gets canceled and the exception                     * can be retrieved from executorFuture.get()                      */
Storm,WITHOUT_CLASSIFICATION,//  allow blacklist scheduler to cache the supervisor with an added port 
Storm,WITHOUT_CLASSIFICATION,//  flushes local and remote messages 
Storm,WITHOUT_CLASSIFICATION,// to allow the revocation hook to commit offsets for the revoked partitions. 
Storm,WITHOUT_CLASSIFICATION,//  roll to next 
Storm,WITHOUT_CLASSIFICATION,//  topology will be scheduled 
Storm,WITHOUT_CLASSIFICATION,// Ensure filename doesn't contain ../ parts  
Storm,WITHOUT_CLASSIFICATION,//  re-init cache and partitions 
Storm,WITHOUT_CLASSIFICATION,//  Boilerplate overrides to cast result from base type to JoinBolt so user doesn't have to   down cast when invoking these main.methods
Storm,WITHOUT_CLASSIFICATION,// case 2: Non-IContext plugin must have a makeContext(topoConf) method that returns IContext object 
Storm,WITHOUT_CLASSIFICATION,// metrics has just been collected let's also log it 
Storm,WITHOUT_CLASSIFICATION,//  Not exposed:   * withClock(Clock) 
Storm,WITHOUT_CLASSIFICATION,//  validate search by component id 
Storm,WITHOUT_CLASSIFICATION,//  remove any previously created cache instance 
Storm,WITHOUT_CLASSIFICATION,// release earliest blacklist - but release all supervisors on a given blacklisted host. 
Storm,WITHOUT_CLASSIFICATION,// Stop counting when past current time 
Storm,WITHOUT_CLASSIFICATION,//  this can happen when multiple clients doing mkdir at same time 
Storm,WITHOUT_CLASSIFICATION,//  remove uploaded jars blobs not artifacts since they're shared across the cluster 
Storm,WITHOUT_CLASSIFICATION,// creates brand new tuples with brand new fields 
Storm,WITHOUT_CLASSIFICATION,// Topology metrics distribution 
Storm,WITHOUT_CLASSIFICATION,//  it's a method with zero args 
Storm,WITHOUT_CLASSIFICATION,// If not call default constructor 
Storm,WITHOUT_CLASSIFICATION,//  it's non-null. 
Storm,WITHOUT_CLASSIFICATION,// STORM-3087. 
Storm,WITHOUT_CLASSIFICATION,// Done capturing topology information... 
Storm,WITHOUT_CLASSIFICATION,//  ======== Next Tuple ======= 
Storm,WITHOUT_CLASSIFICATION,//  the metrics processor is not critical to the operation of the cluster allow Supervisor to come up 
Storm,WITHOUT_CLASSIFICATION,//  add more events with past ts 
Storm,WITHOUT_CLASSIFICATION,/*      * force create a windowed bolt with identity nodes so that we don't     * have a stateful processor inside a windowed bolt.      */
Storm,WITHOUT_CLASSIFICATION,//  INT_ARG 
Storm,WITHOUT_CLASSIFICATION,//  configs - kafka spout 
Storm,WITHOUT_CLASSIFICATION,//  It's likely that Bolt is shutting down so no need to die.   just ignore and loop will be terminated eventually 
Storm,WITHOUT_CLASSIFICATION,//  get all the tuples in a batch and add it to trident-window-manager 
Storm,WITHOUT_CLASSIFICATION,//  do not process current timestamp since tuples might arrive while the trigger is executing 
Storm,WITHOUT_CLASSIFICATION,// When tuple tracking is enabled the spout must not replay tuples in at-most-once mode 
Storm,WITHOUT_CLASSIFICATION,//  if there are no records do not call flush 
Storm,WITHOUT_CLASSIFICATION,//  add a null for missing fields (usually in case of outer joins) 
Storm,WITHOUT_CLASSIFICATION,// We don't want to run the test is CGroups are not setup 
Storm,WITHOUT_CLASSIFICATION,// Initialize the network topography 
Storm,WITHOUT_CLASSIFICATION,//  read remaining lines 
Storm,WITHOUT_CLASSIFICATION,//  resolve references 
Storm,WITHOUT_CLASSIFICATION,//  for completeness 
Storm,WITHOUT_CLASSIFICATION,// The topology we are scheduling 
Storm,WITHOUT_CLASSIFICATION,//  if the node is already deleted do nothing
Storm,WITHOUT_CLASSIFICATION,//  should still fail 
Storm,WITHOUT_CLASSIFICATION,//  ============  Factories  ============ 
Storm,WITHOUT_CLASSIFICATION,//  Test2: When a supervisor fails RAS does not alter existing assignments 
Storm,WITHOUT_CLASSIFICATION,//  start trigger once the initialization is done. 
Storm,WITHOUT_CLASSIFICATION,//  reference to key2 
Storm,WITHOUT_CLASSIFICATION,// backtracking (If we ever get here there really isn't a lot of hope that we will find a scheduling) 
Storm,WITHOUT_CLASSIFICATION,//  Kafka consumer configuration 
Storm,WITHOUT_CLASSIFICATION,//  ensure 1 instance per producer thd. 
Storm,WITHOUT_CLASSIFICATION,//  WINDOW_TO_COMPLETE_LATENCIES_MS 
Storm,WITHOUT_CLASSIFICATION,// Ignored it is not set 
Storm,WITHOUT_CLASSIFICATION,// Now fail a tuple on partition one and verify that it is allowed to retry because the failed tuple is below the maxUncommittedOffsets limit 
Storm,WITHOUT_CLASSIFICATION,/*  Ack the tuple and commit.         *          * The waiting to emit list should now be cleared and the next emitted tuple should be the last tuple on the partition         * which hasn't been emitted yet          */
Storm,WITHOUT_CLASSIFICATION,//  be kept in memory to avoid going to kinesis again for retry 
Storm,WITHOUT_CLASSIFICATION,/*  Counted  */
Storm,WITHOUT_CLASSIFICATION,//  isset id assignments 
Storm,WITHOUT_CLASSIFICATION,//  the supervisors. this also allows you to declare the serializations as a sequence 
Storm,WITHOUT_CLASSIFICATION,// Should not show files outside daemon log root. 
Storm,WITHOUT_CLASSIFICATION,//  class FileLockingThread 
Storm,WITHOUT_CLASSIFICATION,//  Closing the channel and reconnecting should be done before handling the messages. 
Storm,WITHOUT_CLASSIFICATION,//  All internal state except for the count of the current bucket are 
Storm,WITHOUT_CLASSIFICATION,//  if # of workers requested is more than we currently have 
Storm,WITHOUT_CLASSIFICATION,//  STREAM_ID 
Storm,WITHOUT_CLASSIFICATION,//  PATH 
Storm,WITHOUT_CLASSIFICATION,// Now add some more events and a new watermark and check that the previous events are expired 
Storm,WITHOUT_CLASSIFICATION,// tasks figure out what tasks to talk to by looking at topology at runtime 
Storm,WITHOUT_CLASSIFICATION,/*          * Instead of iterating over all the tuples in the window to compute         * the sum the values for the new events are added and old events are         * subtracted. Similar optimizations might be possible in other         * windowing computations.          */
Storm,WITHOUT_CLASSIFICATION,// Answer when we ask for a private key... 
Storm,WITHOUT_CLASSIFICATION,//  ======== Fail ======= 
Storm,WITHOUT_CLASSIFICATION,//  SCHED_STATUS 
Storm,WITHOUT_CLASSIFICATION,//  pick a worker to mock as failed 
Storm,WITHOUT_CLASSIFICATION,//  batch 1 
Storm,WITHOUT_CLASSIFICATION,//  no data 
Storm,WITHOUT_CLASSIFICATION,// Offset 0 is committed 1 to maxUncommittedOffsets - 1 are failed but not retriable  The spout should now emit another maxPollRecords messages 
Storm,WITHOUT_CLASSIFICATION,//  only start those requested 
Storm,WITHOUT_CLASSIFICATION,// set to use the default resource aware strategy when using the MultitenantResourceAwareBridgeScheduler 
Storm,WITHOUT_CLASSIFICATION,/*      * Bolt-specific configuration for windowed bolts to specify the window length as a count of number of tuples     * in the window.      */
Storm,WITHOUT_CLASSIFICATION,//  TOPOLOGY_CONF 
Storm,WITHOUT_CLASSIFICATION,// check format 
Storm,WITHOUT_CLASSIFICATION,//  remove the port from the supervisor and make sure the blacklist scheduler can remove the port without 
Storm,WITHOUT_CLASSIFICATION,// create a wrap transport factory so that we could apply user credential during connections 
Storm,WITHOUT_CLASSIFICATION,//  component id -> stats 
Storm,WITHOUT_CLASSIFICATION,//  invalidate after releasing the lock   if the parition is pinned before we could invalidate   it will get invalidated in the next flush or when the entry gets evicted from the cache. 
Storm,WITHOUT_CLASSIFICATION,// Link the components together 
Storm,WITHOUT_CLASSIFICATION,//  Mark the current buffer position before reading task/len field   because the whole frame might not be in the buffer yet.   We will reset the buffer position to the marked position if 
Storm,WITHOUT_CLASSIFICATION,//  attempt to find it in the string cache this will update the LRU 
Storm,WITHOUT_CLASSIFICATION,//  Kafka spout configuration 
Storm,WITHOUT_CLASSIFICATION,//  json record doesn't need columns to be in the same order   as table in hive. 
Storm,WITHOUT_CLASSIFICATION,/*      * The interval between retries of an Exhibitor operation.      */
Storm,WITHOUT_CLASSIFICATION,//  value fields 
Storm,WITHOUT_CLASSIFICATION,// Running in daemon mode we would pass Error to calling thread. 
Storm,WITHOUT_CLASSIFICATION,//  batch 2 
Storm,WITHOUT_CLASSIFICATION,//  resources assigned by RAS (Resource Aware Scheduler) 
Storm,WITHOUT_CLASSIFICATION,//  c.f. org.apache.hadoop.security.UserGroupInformation. 
Storm,WITHOUT_CLASSIFICATION,// We found a good result we are done. 
Storm,WITHOUT_CLASSIFICATION,// pending is still empty 
Storm,WITHOUT_CLASSIFICATION,//  informs other workers about back pressure situation. Runs in the NettyWorker thread. 
Storm,WITHOUT_CLASSIFICATION,// Sleep for 50 mins 
Storm,WITHOUT_CLASSIFICATION,//  Legacy resource parsing 
Storm,WITHOUT_CLASSIFICATION,//  required   required 
Storm,WITHOUT_CLASSIFICATION,//  last offset of this batch 
Storm,WITHOUT_CLASSIFICATION,// given processornodes and static state nodes 
Storm,WITHOUT_CLASSIFICATION,/*  CREATE THE BLOBSTORE  */
Storm,WITHOUT_CLASSIFICATION,// The window boundaries are )windowStart windowEnd) 
Storm,WITHOUT_CLASSIFICATION,//  batch 3 
Storm,WITHOUT_CLASSIFICATION,//  e.g. [\"a\" \"b\" \"a\"]) => [\"a\" \"b\" \"a#2\"]" 
Storm,WITHOUT_CLASSIFICATION,// STORM-3059 
Storm,WITHOUT_CLASSIFICATION,// We could make this configurable in the future... 
Storm,WITHOUT_CLASSIFICATION,//  sub process used to execute the command 
Storm,WITHOUT_CLASSIFICATION,//  that worker is running and moves on 
Storm,WITHOUT_CLASSIFICATION,//  map to track next retry time for each kinesis message that failed 
Storm,WITHOUT_CLASSIFICATION,//  EXECUTOR_STATS 
Storm,WITHOUT_CLASSIFICATION,//  rwx-------- 
Storm,WITHOUT_CLASSIFICATION,//     long crc32(Tuple tuple); 
Storm,WITHOUT_CLASSIFICATION,//  SPOUT 
Storm,WITHOUT_CLASSIFICATION,//  need more data 
Storm,WITHOUT_CLASSIFICATION,//  wait so all events expire 
Storm,WITHOUT_CLASSIFICATION,// Create a new session id if the user gave an empty session string.   This is the use case when the user wishes to list blobs   starting from the beginning. 
Storm,WITHOUT_CLASSIFICATION,//  HOST 
Storm,WITHOUT_CLASSIFICATION,//  to be called after all Executor objects in the worker are created and before this object is used 
Storm,WITHOUT_CLASSIFICATION,//  Redis has three chunks which last chunk only has entries 
Storm,WITHOUT_CLASSIFICATION,//  Read property file for extra consumer properties 
Storm,WITHOUT_CLASSIFICATION,// If debug logging is turned on we should just log the leader all the time.... 
Storm,WITHOUT_CLASSIFICATION,//  If lock file has expired then own it 
Storm,WITHOUT_CLASSIFICATION,// iterate of WorkerSummary and find the one with the port 
Storm,WITHOUT_CLASSIFICATION,// Now we can calculate a percentage 
Storm,WITHOUT_CLASSIFICATION,//  Queue of records per shard fetched from kinesis and are waiting to be emitted 
Storm,WITHOUT_CLASSIFICATION,//  Change the '1' to e.g. 5 to change this to 5 minutes. 
Storm,WITHOUT_CLASSIFICATION,//  TIME_STAMP 
Storm,WITHOUT_CLASSIFICATION,// spout overrides 
Storm,WITHOUT_CLASSIFICATION,// merge with existing statuses 
Storm,WITHOUT_CLASSIFICATION,//  at least 4K 
Storm,WITHOUT_CLASSIFICATION,//  sorted set of records to be retrued based on retry time. earliest retryTime record comes first 
Storm,WITHOUT_CLASSIFICATION,// check that only subscribed to one component/stream for statespout  setsubscribedstate appropriately 
Storm,WITHOUT_CLASSIFICATION,//  Factory main.methods
Storm,WITHOUT_CLASSIFICATION,//  Get sequence number details from latest sequence number of the blob 
Storm,WITHOUT_CLASSIFICATION,/*                  * Create a stream of random numbers from a spout that                 * emits random integers by extracting the tuple value at index 0.                  */
Storm,WITHOUT_CLASSIFICATION,//  Schedule topology history cleaner 
Storm,WITHOUT_CLASSIFICATION,//  will commit progress into lock file if commit threshold is reached 
Storm,WITHOUT_CLASSIFICATION,//  2 - Create and configure topology 
Storm,WITHOUT_CLASSIFICATION,// map to hold partition level and topic level metrics 
Storm,WITHOUT_CLASSIFICATION,//  setup some files/dirs to emulate supervisor restart 
Storm,WITHOUT_CLASSIFICATION,// The versions are different so roll back to whatever current is 
Storm,WITHOUT_CLASSIFICATION,// only for test 
Storm,WITHOUT_CLASSIFICATION,// supervisor contains bad slots 
Storm,WITHOUT_CLASSIFICATION,//  could/should use readFully(buffer0length)?
Storm,WITHOUT_CLASSIFICATION,// Save the current state for recovery 
Storm,WITHOUT_CLASSIFICATION,//  sub directories to store either files or uncompressed archives respectively 
Storm,WITHOUT_CLASSIFICATION,//  DEPENDENCY_JARS 
Storm,WITHOUT_CLASSIFICATION,//  K/sec 
Storm,WITHOUT_CLASSIFICATION,// The subset of earliest retriable offsets that are on pollable partitions 
Storm,WITHOUT_CLASSIFICATION,// Convert to millis 
Storm,WITHOUT_CLASSIFICATION,//  WORKER_SUMMARIES 
Storm,WITHOUT_CLASSIFICATION,//  class Configs 
Storm,WITHOUT_CLASSIFICATION,//  We call fireChannelRead since the client is allowed to perform   this request. The client's request will now proceed to the next 
Storm,WITHOUT_CLASSIFICATION,//  -- enable/disable ACKing 
Storm,WITHOUT_CLASSIFICATION,//  if no TGT do not bother with ticket management. 
Storm,WITHOUT_CLASSIFICATION,// fileOffset = the index of last scanned file 
Storm,WITHOUT_CLASSIFICATION,// The failed tuples are ready for retry. Make it appear like 0 and 1 on the first partition were compacted away.  In this case the second partition acts as control to verify that we only skip past offsets that are no longer present. 
Storm,WITHOUT_CLASSIFICATION,// Worker dirs 
Storm,WITHOUT_CLASSIFICATION,// At this point there is nothing to do.  In all likelihood any filesystem operations will fail.  The next tuple will almost certainly fail to write and/or sync which force a rotation.  That  will give rotateAndReset() a chance to work which includes creating a fresh file handle. 
Storm,WITHOUT_CLASSIFICATION,//  required   required   required   required   required   required 
Storm,WITHOUT_CLASSIFICATION,//     final StormTopology topology; 
Storm,WITHOUT_CLASSIFICATION,// logLevel 
Storm,WITHOUT_CLASSIFICATION,//  in which case it's a noop 
Storm,WITHOUT_CLASSIFICATION,// Verify that the poll started at the earliest retriable tuple offset 
Storm,WITHOUT_CLASSIFICATION,//  now check memory only 
Storm,WITHOUT_CLASSIFICATION,//  continue without idling 
Storm,WITHOUT_CLASSIFICATION,//  init the writer once the cache is setup 
Storm,WITHOUT_CLASSIFICATION,//  this is because can't currently merge splitting logic into a spout   not the most kosher algorithm here since the grouper indexes are being trounced via the adding of nodes to random groups but it 
Storm,WITHOUT_CLASSIFICATION,//  Load PMML Model from Blob store 
Storm,WITHOUT_CLASSIFICATION,// add to nimbuses 
Storm,WITHOUT_CLASSIFICATION,// On heap memory is used to help calculate the heap of the java process for the worker   off heap memory is for things like JNI memory allocated off heap or when using the   ShellBolt or ShellSpout.  In this case the 16 MB of off heap is just as an example   as we are not using it. 
Storm,WITHOUT_CLASSIFICATION,//  expectedOwner being null means that security is disabled (which why are we uploading credentials with security disabled??? 
Storm,WITHOUT_CLASSIFICATION,// AllowedId is null in the constructor so it can assign what it needs/etc. 
Storm,WITHOUT_CLASSIFICATION,// Verify that no more tuples are emitted and all tuples are committed 
Storm,WITHOUT_CLASSIFICATION,/*                  * Emitted messages for partitions that are no longer assigned to this spout can't be acked and should not be retried hence                 * remove them from emitted collection.                  */
Storm,WITHOUT_CLASSIFICATION,//  treat supervisor as bad only if all of its slots matched the cached supervisor   track how many times a cached supervisor has been marked bad 
Storm,WITHOUT_CLASSIFICATION,//  spouts 
Storm,WITHOUT_CLASSIFICATION,//  ack-ed once 
Storm,WITHOUT_CLASSIFICATION,//  Get or load from the cache optionally pinning the entry   so that it wont get evicted from the cache 
Storm,WITHOUT_CLASSIFICATION,//  key for hdfs Kerberos configs 
