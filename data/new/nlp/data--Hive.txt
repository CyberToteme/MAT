SATD	 fixme probably this should also integrated with issame logics 
SATD	 wait for block just sure not ideal 
SATD	 hive not sure this correct dont think gets wrapped udftointeger 
SATD	 expect this never happen practice can pool paths even have angled braces 
SATD	 todo this does not work correctly none the partitions created but the folder for the first two created because when going through the partitions the first two are already put and started the thread pool when the exception occurs the third one when the exception occurs the finally part but the map can empty depends the progress the other threads the folders wont deleted pathtablelocation year 
SATD	 todo remove this after stashing only rqd pieces from opconverter 
SATD	 todo should try make giant array for one cache call avoid overhead 
SATD	 forward the row reducer discard the row vectorized may forward the row not sure yet 
SATD	 handle any move session requests the way move session works right now sessions get moved destination pool there capacity destination pool there capacity destination pool the session gets killed since cannot pause query todo future this the process killing can delayed until the point where session actually required could consider delaying the move when destination capacity full until there claim src pool may change command support delayed move etl which will run under src cluster fraction long 
SATD	 not threadsafe 
SATD	 would better had errorcode field 
SATD	 todo this can probably replaced with much less code via dynamic dispatch andor templates 
SATD	 todo handle this properly 
SATD	 todo need maxlength checking 
SATD	 todo prewarm and update can probably merged 
SATD	 deprecated favour link removed hive 
SATD	 todo disabling this test tez publishes counters only after task completion which will cause write side counters not validated correctly dag will completed before validation testtimeout 
SATD	 todo might want increase the default batch size viable gets oom too high 
SATD	 todo remove when method relmdsize 
SATD	 store all caches variables change the main one based config this not thread safe between different split generations and wasnt anyway 
SATD	 todo once shuffle out this can use apis convert between app and job 
SATD	 todo does this need the finaldestination 
SATD	 todo add limit instead count should more efficient 
SATD	 todo not support listing resources command beeline list jar 
SATD	 note scheduler will call this based lack sources schedule time and set this true theres easy way work around this need better classes 
SATD	 todo will this also fix windowing try 
SATD	 but lets make little bit more explicit 
SATD	 todo this should inherit from volcanocost and should just override isle method 
SATD	 todo check for duplicates assume clause values present ndv which may not correct range check can find assume values ndv set uniformly distributed over col values account for skewness histogram 
SATD	 todo didnt care about the column order could switch join sides here 
SATD	 todo could remember its unsupported and stop sending calls although might bad idea for hsstandalone metastore that could updated with support maybe should just remember this for some time 
SATD	 deprecated favour link removed hive 
SATD	 todo why this like that 
SATD	 todo escape handling may changed follow the largest issue which are treated statement terminators for the cli once the cli fixed this code should reinvestigated 
SATD	 delete data fail table doesnt exist need results back 
SATD	 todo this does not work correctly none the partitions created but the folder for the first two created because when going through the partitions the first two are already put and started the thread pool when the exception occurs the third one when the exception occurs the finally part but the map can empty depends the progress the other threads the folders wont deleted 
SATD	 note assume that this isnt already malformed query dont check for that here will fail later anyway 
SATD	 todo not clear should cache these not for now dont bother 
SATD	 todo this needs removed see comments 
SATD	 bug this will not work remote mode hive 
SATD	 todo change this method make the output easier parse parse programmatically 
SATD	 todo remove 
SATD	 wouldnt make more sense return the first element the list returned the previous call 
SATD	 better logic would find the alias 
SATD	 need enforce the size here even the types are the same 
SATD	 this method may not safe can throw npe key value null 
SATD	 todo this ugly hack because tez plugin isolation does not make sense for llap plugins are going register threadlocal here for now that the scheduler initializing the same thread after the communicator will pick the other way around 
SATD	 todo mssplit uncomment once move eventmessage over 
SATD	 follow hives rules for type inference oppose calcites for return type todo perhaps should this for all functions not just 
SATD	 todo hive while getting stats from metastore currently only get one col time this could improved get all necessary columns advance then use local todo hive aggregations could done directly metastore hive over mysql 
SATD	 todo see planindexreading this not needed here 
SATD	 todo why stored both table and dpctx 
SATD	 todo use threadpool for more concurrency todo checkset all files only directories 
SATD	 todo hive handle additional reasons like launch failed 
SATD	 todo handle agg func name translation correct add func 
SATD	 todo both these are texception why need these separate clauses 
SATD	 todo two hss start exactly the same time which could happen during coordinated restart they could start generating the same ids should store the starttime 
SATD	 todo given the specific data and lookups perhaps the nested thing should not map fact cslm has slow singlethreaded operation and one file probably often read just one few threads much more simple with locking might better lets use cslm for now since its available 
SATD	 todo ideally this should done independent whether setup not 
SATD	 this not modeled before because needs parameterized pertest there better way this should 
SATD	 todo hive 
SATD	 make sure nullvalued confvar properties not override the hadoop configuration note comment out the following test case for now until better way test found this test case cannot reliably tested the reason for this that hive does overwrite fsdefaultname hiveconf the property set system properties coresitexml null coresitexml 
SATD	 deprecated favour link removed hive 
SATD	 todo due value this probably should throw exception 
SATD	 deprecated favour link removed hive 
SATD	 hmm not good the only type expected here struct which maps hcatrecord anything else error return null the inspector 
SATD	 todo should these rather arrays 
SATD	 todo change all this based regular interface instead relying the proto service exception signatures cannot controlled without this for the moment 
SATD	 will implemented later 
SATD	 todo actually need this reader the caller just extracts child readers 
SATD	 todo hive what mergework and why not part the regular operator chain the call further down can block and will not receive information about abort request 
SATD	 todo hive handle cases where nodes down and come back the same port historic information 
SATD	 todo most protocol exceptions are probably unrecoverable throw 
SATD	 todo also need remove the mapjoin from the list rss children 
SATD	 todo why there tezsession execdriver 
SATD	 replace insert overwrite merge equivalent rewriting here need this complex ast rewriting that generates the same plan that merge clause would generate because cbo does not support merge yet todo support merge first class member cbo simplify this logic 
SATD	 todo current implementation replication will result droppartition under replication scope being called perpartition instead multiple partitions however robust must still handle the case multiple partitions case this assumption changes the future however this assumption changes will not very performant fetch each partition onebyone and then decide inspection whether not this candidate for dropping thus need way push this filter the metastore allow drop partition not depending predicate the parameter key values 
SATD	 todo should pass curr instead null 
SATD	 really not sure this should here will have see how the storage mechanism evolves 
SATD	 todo force file setup staging dir confsetfsdefaultfs file tmp 
SATD	 todo why not just use getrootdir 
SATD	 todo lossy conversion 
SATD	 todo remove the copy after orc and orc 
SATD	 todo the name doesnt match with the metadata from dump then need rewrite the original and expanded texts using new name currently refers the source database name 
SATD	 todo remove this constructor 
SATD	 todo add the ability extractfiletail read from multiple buffers 
SATD	 todo instead simply restricting message format should eventually move jdbcdriverstype registering message format and picking message factory per event decode for now however since all messages have the same factory restricting message format effectively guard against older leftover data that would cause problems 
SATD	 todo when hive moves java make updatetimezone default method 
SATD	 not strictly necessary noone will look 
SATD	 exponent part scaling down while scale scaling now its tricky unscaledvalue significand scale twoscaledown 
SATD	 todo cat fairly certain that most calls this are error this should only used when the database location unset which should never happen except when new database being created once have confirmation this change calls this getdatabasepath since does the right thing also merge this with duplicates much the logic 
SATD	 todo should this the concern the mutator 
SATD	 todo provide support for reporting errors this should never happen server always returns valid status success 
SATD	 todo ideally when the splits udf made proper api coordinator should not managed global should create and then pass around 
SATD	 todo convert sqlstate etc 
SATD	 todo include externalpreemption this list 
SATD	 workaround avroutf 
SATD	 todo needs initializeinput part inputjobinfo 
SATD	 note that this logic may drop some the tables the database even the drop database fail for any reason todo fix this 
SATD	 some walkers extending defaultgraphwalker forwardwalker not use opqueue and rely uniquely the towalk structure thus store the results produced the dispatcher here todo rewriting the logic those walkers use opqueue 
SATD	 this corner case where have extract time unit like daymonth pushed extraction todo the best way fix this add explicit output druid types calcite extraction functions impls 
SATD	 workaround for hadoop remove when hadoop longer supported 
SATD	 could pass the number nodes that expect instead also single concurrent request per node currently hardcoded 
SATD	 todo strangely the default parametrization ignore missing tables 
SATD	 fixme should clean testpath but not doing now for debuggings sake 
SATD	 this will throw error close pout early 
SATD	 not very clean way and should modified later due compatibility reasons user sees the results json for custom scripts and has way for specifying that right now hardcoded the code 
SATD	 todo this test passes fine locally but fails linux not sure why 
SATD	 todo eventually calls hadoop method that used buggy and also anyway just does copy for direct buffer copy here 
SATD	 todo expand other functions needed what about types other than primitive 
SATD	 set footer cache for current split generation see field comment not thread safe 
SATD	 todo the below seem like they should just combined into partitiondesc 
SATD	 callablewithndc inherits from ndc only when call invoked callablewithndc has extended provide access its ndcstack that cloned during creation until then will use reflection access the private field fixme hive follow remove this reflection 
SATD	 todo this brittle who said everyone has upgrade using upgrade process 
SATD	 check whether task can run completion may end blocking its sources this currently happens via looking source state todo eventually this should lookup the hive processor figure out whether its reached state where can finish especially cases failures after data has been fetched return true the task can finish false otherwise 
SATD	 undone how look for all nulls multikey let nulls through for now 
SATD	 note this can still conflict with parallel transactions not currently handle parallel changes from two admins design 
SATD	 todo time good enough for now well likely improve this may also work something the equivalent pid thrid and move nanos ensure uniqueness 
SATD	 fixme somehow place pointers that reexecution compilation have failed the query have been successfully compiled before 
SATD	 undone havent finished isrepeated 
SATD	 hack instead figure out way get the paths 
SATD	 todo support complex types for complex type simply return 
SATD	 todo the control flow for this needs defined hive supposed threadlocal 
SATD	 todo should the top the file 
SATD	 get rid tokselexpr 
SATD	 clear the work map after build todo remove caching instead 
SATD	 donotupdatestats supposed transient parameter that only passed via rpc want avoid this property from being persistent note this property set table property will remove which incorrect but cant distinguish between these two cases this problem was introduced hive better approach would pass the property 
SATD	 this using the payload from the corresponding inputname ideally should using its own configuration class but that 
SATD	 like cannot used with this todo 
SATD	 deprecated favour link removed hive 
SATD	 todo once hbase completed use that api switch using mapreduce version the apis rather than mapred copied from hbases tablemapreduceutil since not public api 
SATD	 todo fetch partitions batches todo threadpool process partitions 
SATD	 todo simple wrap rethrow for now clean with error codes 
SATD	 todo add method only get current skip history more efficient 
SATD	 simple wrapper object with objectinspector todo need redefine the hashcode and equals methods that can put into hashmap key this class also serves facility for function that returns both object and objectinspector 
SATD	 the ordering types here used determine which numeric types are commonconvertible one another probably better rely the ordering explicitly defined here than assume that the enum values that were arbitrarily assigned primitivecategory work for our purposes 
SATD	 something else wrong 
SATD	 todo comments rexshuttlevisitcall mention other types this category need resolve those together and preferably the base class rexshuttle 
SATD	 todo verify this needed why cant always nullempty 
SATD	 the outer join that saw most recently right outer join 
SATD	 todo this relies hdfs not changing the format assume could get inode this still going work otherwise file ids can turned off later should use public utility method hdfs obtain the inodebased path 
SATD	 todo should rather prefix 
SATD	 todo this method temporary ideally hive should only need pass tez the amount memory requires the map join and tez should take care figuring out how much allocate adjust the percentage memory reserved for the processor from tez based the actual requested memory the map join return the adjusted percentage 
SATD	 hack verify that authorization check passed exception can thrown cause the functions are not being called with valid params verify that exception has come from objectstore code which means that the 
SATD	 todo should this use 
SATD	 todo move this calcite 
SATD	 todo add this metadatareader orc metadata buffer not just metadata 
SATD	 todo this should come from type system currently there definition 
SATD	 todo the moment theres way knowing whether query running not race possible between dagcomplete and registerfragment where the registerfragment processed after dagcompletes may need keep track completed dags for certain time duration avoid this alternately send explicit dag start message before any other message processed multiple threads communicating from single gets the way this 
SATD	 should allow writing nontransactional tables explicit transaction the user may issue rollback but these tables wont rollback can this checking determine whether its readingwriting any non acid and raise appropriate error driveracidsinks and can used any acid the query 
SATD	 todo implement 
SATD	 todo what this checking 
SATD	 think this wrong the drop table statement should come the table topic not the topic alan 
SATD	 todo rather tez sessions should not depend sessionstate 
SATD	 todo support tezvectorization 
SATD	 todo read this somewhere useful like the task scheduler 
SATD	 todo add support for serialization values here 
SATD	 todo convert genincludedcolumns and setsearchargument use typedescription 
SATD	 todo when this code little less hot change most logs debug will determine what under lock and then stuff outside the lock the approach statebased consider the task have duck when have decided give one the sends below merely fix the discrepancy with the actual state may add the ability wait for llaps positively ack the revokes future the procedural approach requires that track the ducks traveling network concurrent terminations etc while more precise its much more complex 
SATD	 todo why doesnt this use context 
SATD	 note this redundant with types 
SATD	 the following lines are exactly what mysql does todo why this 
SATD	 temporary variable for testing this added just turn off this feature case bug deployment has not been documented hivedefaultxml intentionally this should removed 
SATD	 todo need session handle 
SATD	 this kind hacky the read entity contains the old table whereas the write entity contains the new table this needed for rename both the old and the new table names are passed 
SATD	 undone for now dont add more small keys 
SATD	 todo can this ever happen 
SATD	 todo theres potential problem here some table uses external schema like avro with very large type name seems like the view does not derive the serde from the table wont able just get the type from the deserializer like the table does wont able properly store the type the rdbms metastore 
SATD	 todo this should depends input format and map something 
SATD	 todo can actually consider storing all the delta encoded row offsets not lot overhead compared the data itself and with row offsets could use columnar blocks for inconsistent splits are not optimizing for inconsistent splits for now 
SATD	 todo should this just use physical ids 
SATD	 todo wtf why this this method this has nothing with anything 
SATD	 this class isnt used and suspect does totally the wrong thing its only here that can provide some output format the tables and partitions create actually write 
SATD	 todo does arg need type cast 
SATD	 what 
SATD	 todo not sure this the right exception 
SATD	 need convert the hive type the sql type name todo this would better handled enum 
SATD	 todo npe should not thrown 
SATD	 consider allocate larger initial size 
SATD	 good way find out may even have app 
SATD	 hardcode sasl here zkdtsm only supports none sasl and never want none 
SATD	 support listobject setobject and object have differently 
SATD	 still null after just having initialized bail out somethings very wrong 
SATD	 todo convert this assertfail once hive fixed 
SATD	 todo should able enable caches separately 
SATD	 for dynamic partitioned hash join the big table will also coming from reducesinkoperator check for this condition todo use indexof parentrsgettag 
SATD	 todo following hivesubqueryfinder has been copied from since there bug there calcite once calcite fixed should get rid the following code 
SATD	 todo extend rule can applied for these cases 
SATD	 todo support case date 
SATD	 todo figure out better way set repeat for binary type 
SATD	 todo are with breaking compatibility existing party storagehandlers this method could moved the hivestoragehandler interface 
SATD	 todo this global lock may not necessary all concurrent methods are synchronized 
SATD	 todo should this make sure set jobs 
SATD	 builder for relational expressions todo note that this copied from calcites relbulder because calcite hasnt been fixed yet this should deleted and replaced with relbuilder subqueryremoverule once calcite fixed edit although calcite has been fixed and released but hive now has special handling join gets flag see semi join created not still can not replace this with calcites relbuilder pcode relbuilder does not make possible anything that you could not also accomplish calling the factory methods the particular relational expression but makes common tasks more straightforward and concise pcode relbuilder uses factories create relational expressions default uses the default factories which create logical relational expressions link link and forth but you could override those factories that say code filter creates instead code hivefilter pit not threadsafe 
SATD	 todo setting autocommit should not generate exception long set false 
SATD	 todo precision and scale would practically invalid for string conversion 
SATD	 todo should this really default fetchnext 
SATD	 todo not sure about the use this should instead use workeridentity sessionid 
SATD	 deprecated favour link removed hive 
SATD	 todo trace ranges here between data cache and incomplete cache 
SATD	 fixme retain old error create new one 
SATD	 dont remove the work from the sparkwork here the removal done later 
SATD	 todo need move from python java for the rest the script 
SATD	 check input pruning possible todo this code buggy relies having one file per bucket support design 
SATD	 todo strictly speaking you can commit empty txn thus conjunct wrong but only possible for for multistmt txns 
SATD	 view ddl alter view add partition does not work because the nature implementation the ddl hive hive will internally invoke another driver the select statement and hcat does not let select statement through cannot find way get around without modifying hive code just leave unsupported 
SATD	 todo test these need link against libs maven profiles table tflat int int stored orc table tflattext int int stored textfile 
SATD	 note this hacky and this section code fragile depending code varnames its likely stop working some time the future especially upgrade versions actively need find better way make sure the leak doesnt happen instead just clearing out the cache after every call 
SATD	 todo should never happen 
SATD	 undone dont know why hive causes this return assertequals readergetprogress 
SATD	 deprecated favour link removed hive 
SATD	 not support windows will revisit this really need for windows 
SATD	 pass empty list columns will written the file todo should able make this work for update 
SATD	 todo throw exception 
SATD	 todo move dynamicserde when its ready 
SATD	 todo replace with direct call progresshelper when reliably available 
SATD	 todo this the only place that uses keeptmpdir why 
SATD	 todo use when the typo fixed 
SATD	 indicates whether node had recent communication failure this primarily for tracking and logging purposes for the moment todo some point treat task rejection and communication failures differently 
SATD	 todo move other protocols use this too 
SATD	 todo move the following properties out configuration constant 
SATD	 creates size estimators for java objects the estimators attempt most the reflection work initialization time and also take some shortcuts minimize the amount work done during the actual estimation todo clean 
SATD	 todo move base class 
SATD	 since the mapjoin has had all its other parents removed this point would bad here tries anything 
SATD	 todo should this done for use with 
SATD	 todo this should changed evaluated lazily especially for single segment case 
SATD	 todo disable blacklisting tez when using llap until this properly supported blacklisting can cause containers move terminating state which can cause attempt marked failed this becomes problematic when set allowedfailures todo hive what happens when try scheduling task node that tez this point thinks blacklisted 
SATD	 todo should this check conformtoacid 
SATD	 todo split count not same buckets 
SATD	 todo cols that come through ptf should retain virtualcolumness 
SATD	 todo verify any quoting needed for keys 
SATD	 todo this should check that the job actually completed and likely use completion time 
SATD	 todo move this logicalequals 
SATD	 todobr could use combined instead list use column processing from why not use expr 
SATD	 todo temporary for debugging doesnt interfere with mtt failures unlike logdebug 
SATD	 todo deprecation reason does not seem reflect the config the ordering important case keys which are also deprecated unset will unset the deprecated keys and all its variants 
SATD	 grpset col already part input todo cant just copy the exprnodedesc from input need explicitly set table alias null false 
SATD	 todo dataoperationtype set conservatively here wed really want distinguish updatedelete and insertselect and resource that written acid not 
SATD	 todo gap ctas may currently broken used work see the old code and why isctas isnt used 
SATD	 todo add partitioned table that needs conversion mmacid 
SATD	 todo really need all this nonsense 
SATD	 this bad but have sort the keys the maps order commutative 
SATD	 todo this seems the same really need both 
SATD	 todo npe should not thrown 
SATD	 tests for the worker thread and its jobs todo most delta files this test suite use txn range nnm that means that they all look like they were created compaction streaming api delta files created sql should have range and suffix and later need change some these have better test coverage 
SATD	 not good reach here this was initialized setmetastorehandler time this means handlergetwh returning null error out 
SATD	 the processor context for partition pruner this contains the table alias that being currently processed todo this class may not useful 
SATD	 deprecated favour link removed hive 
SATD	 todo enable this for production debug switching between two small buffers new caslog 
SATD	 helper class isolate newer hbase features from users running against older versions hbase that dont provide those features todo remove this class when its okay drop support for earlier version hbase 
SATD	 note for uniform hash bucketspartitions when the key empty will use the instead 
SATD	 todo verify this needed 
SATD	 todo this class completely unnecessary mapping with parent 
SATD	 moving across different filesystems differnent encryption zone need file copy instead rename todo consider need this for different file authority throws hiveexception 
SATD	 todo support propagation for windowing 
SATD	 named columns join todo can also the same for semi join but seems that other dbms does not support yet 
SATD	 helper class that generates sql queries with syntax specific target todo why throw metaexception 
SATD	 can continue todo need check that this the same that are rebuilding 
SATD	 guess commonjoinresolver will work commonjoinresolver may convert join operation correlation optimizer will not merge that join todo for joinoperator that has both intermediate tables and query input tables input tables should able guess this joinoperator will converted mapjoin based 
SATD	 todo could log from ticket cache instead good method ugi right now 
SATD	 todo support for binary spec presumably wed parse somewhere earlier 
SATD	 redundant todo callers this often get partvals out name for reason 
SATD	 todo mssplit for now keep copy hiveconf around need call other methods with this should changed configuration once everything that this calls that requires hiveconf moved the standalone metastore 
SATD	 closes the client releasing any link imetastoreclient meta store connections held does not notify any open transactions todo perhaps should 
SATD	 todo will this work 
SATD	 dont have the entire part copy both whatever intended cache and the rest allocated buffer could try optimize bit have contiguous buffers with gaps but its probably not needed 
SATD	 todo hack fix until hive addressed nonexact type shouldnt promoted exact type might this corrects that 
SATD	 fixme old implementation returned null exception maybe 
SATD	 optimize the scenario when there are grouping keys only reducer needed 
SATD	 todo decimal exact numeric approximate numeric 
SATD	 todo why doesnt this use one the existing options implementations 
SATD	 ensure theres threadlocal dont expect one dont ever want create key paths with world visibility why that even option 
SATD	 todo when job complete should print the msgcount table log 
SATD	 todo explain should use fetchtask for reading 
SATD	 this detail not desired 
SATD	 todo use the other hdfsutils here 
SATD	 first check the registry has been updated since the error and skip the error have received new valid registry info todo externally add grace period for this 
SATD	 todo calculate from cached values 
SATD	 array size not big enough 
SATD	 undone needed longtest 
SATD	 todo reduce the number lookups that happen here this shouldnt hdfs for each call 
SATD	 data read for this stripe check have some included indexonly columns todo there may bug here could there partial filtering indexonly column 
SATD	 fixme the right thing luke 
SATD	 todo make can randomize the column order 
SATD	 todo right now treat each slice stripe with single and never bother with indexes phase need add indexing and filtering 
SATD	 todo pattern from curator better error handling 
SATD	 thread pool listening used for unhandled errors for now todo remove 
SATD	 now need look for any values that the user set that metastoreconf doesnt know about todo commenting this out for now breaks because the conf values arent getting properly interpolated case variables see hive 
SATD	 fixme including this the signature will almost certenly differ even the operator doing the same there might conflicting usages logicalcompare 
SATD	 multimrinput may not fix once tez resolved 
SATD	 preempt only there are pending preemptions the same host when the premption registers the request the highest priority will given the slot even the initial preemption was caused some other task todo maybe register which task the preemption was for avoid bad nonlocal allocation 
SATD	 todo this wrong this test sets dummy txn manager and cannot create acid tables this used work accident now this works due test flag the test needs fixed also applies for couple more tests 
SATD	 todo why this needed could just save the host and port 
SATD	 todo there needs mechanism figure out different attempts for the same task delays could potentially changed based this 
SATD	 its not wrong take all delete events for bucketed tables but its more efficient only take those that belong the bucket assuming trust the file name unbucketed table get all files 
SATD	 determine initial input vector expression note may have convert later from decimal regular decimal 
SATD	 note this little bit confusing the special treatment stripelevel buffers because run the columnstreamdata refcount one ahead specified above may look like this would release the buffers too many times one release from the consumer one from below and one here however this merely handling special case where all the batches that are sharing the stripe level stream have been processed before got here they have all decrefed the csd but have not released the buffers because that extra refcount this essentially the consumer refcount being released here 
SATD	 this doesnt always work since some jdbc drivers oracles return blank string from gettablename 
SATD	 todo can this moved out the main callback path 
SATD	 todo add more expected test result here 
SATD	 todo mssplit for now have construct this reflection because imetastoreclient cant moved until after hivemetastore moved which cant moved until this moved 
SATD	 todo need test where actually have more than file 
SATD	 here comes the ugly part 
SATD	 removed hive 
SATD	 optimization for later 
SATD	 there way provide char length here might actually long there object inspector with char length receiving this value 
SATD	 this command exists solely output this message todo can error 
SATD	 todo add support for and clauses under clauses firstcut takes known minimal tree and others expr expr and 
SATD	 todo set this tree instead flat list 
SATD	 operator factory for predicate pushdown processing operator graph each operator determines the pushdown predicates walking the expression tree each operator merges its own pushdown predicates with those its children finally the tablescan operator gathers all the predicates and inserts filter operator after itself todo further optimizations multiinsert case create filter operator for those predicates that couldnt pushed the previous operators the data flow merge multiple sequential filter predicates into that plans are more readable remove predicates from filter operators that have been pushed currently these pushed predicates are evaluated twice 
SATD	 hive call currently ineffective 
SATD	 note location check the buffer always locked for move here 
SATD	 todo mssplit for now cannot load the default class since its from load the which just throws this allows existing hive instances work but also allows instantiate the metastore stand alone for testing not sure this the best long term solution 
SATD	 todo this very brittle given that hive supports nested directories the tables the caller should pass flag explicitly telling the directories the input are data parent data for now retain this for backward compat 
SATD	 todo these appear always called under write lock they need sync 
SATD	 todo could perhaps reuse the same directory for hiveresources 
SATD	 theres mismatch between static and object name mismatch between vector and nonvector operator name the optimizer doenst work correctly 
SATD	 not really sure how refer this can todo could find different from branch for the union that might have alias could add alias here refer but that might break other branches 
SATD	 todo instantiating objects are generally costly refactor 
SATD	 todo when upgraded method should used here 
SATD	 all the code paths below propagate nulls even neither arg nor arg have nulls this reduce the number code paths and shorten the code the expense maybe doing unnecessary work neither input has nulls this could improved the future expanding the number code paths 
SATD	 deprecated favour link removed hive 
SATD	 todo remove this once calcite can take rule operand 
SATD	 todo could fall back trying one one and only ignore the failed ones 
SATD	 deprecated favour link removed hive 
SATD	 first try temp table todo cat think the right thing here always put temp tables the current catalog but dont yet have notion current catalog well have hold until 
SATD	 todo set other table properties needed 
SATD	 dont pass the pool set not thread safe the user trying force use nonexistent pool want fail anyway will fail later during get 
SATD	 todo since operationlog moved package oahhqlsession may add enum there and map fetchorientation 
SATD	 todo ideally this only needs called the result type will also change however since that requires support from type inference rules tell whether rule decides return type based input types for now all operators will recreated with new type any operand changed unless the operator has builtin type 
SATD	 todo ideally should store shortened representation only the necessary fields hbase will probably require custom sarg application code 
SATD	 this massive hack the compactor threads have access packages such acidinputformat depends metastore cant directly access those deal with this the compactor thread classes have been put and they are instantiated here dyanmically this not ideal but avoids massive refactoring hive packages wrap the start the threads catch throwable loop that any failures dont doom the rest the metastore 
SATD	 fixme there were afterclass methodsi guess this the right ordermaybe not 
SATD	 todo hive get all these properties from the registry this will need take care different instances publishing potentially different values when support changing configurations dynamically 
SATD	 todo replace with withtimeout after get the relevant guava upgrade 
SATD	 todo need turn rules thats commented out and add more necessary 
SATD	 todo handle renaming files somewhere 
SATD	 deprecated favour link removed hive 
SATD	 todo this doesnt include superclass 
SATD	 fixme move this colstat related part 
SATD	 todo not sure that this the correct behavior doesnt make sense create the partition without column info this should investigated later 
SATD	 note implies but but here not sufficient enough info set 
SATD	 fixme this should changeto valueof that will also kill that fallback none which think more like problem than feature 
SATD	 todo setup set threads process incoming requests make sure requests for single dagquery are handled the same thread 
SATD	 todo add alter database support hcat 
SATD	 todo add tests for partitions other catalogs 
SATD	 todo suspect could skip much the stuff above this the function the case update and delete but dont understand all the side effects the above code and dont want skip over yet 
SATD	 todo expireafteraccess locks cache segments put and expired get doesnt look too bad but find some perf issues might good idea remove this are probably not caching that many constructors note that weakkeys causes used for key compare this will only work for classes the same classloader should this case 
SATD	 todo create and init session sets queue isdefault but does not initialize the configuration 
SATD	 deprecated favour link string removed hive 
SATD	 todo handle query hints currently ignore them 
SATD	 todo currently only support equal operator two references might extend the logic support other orderpreserving udfs here 
SATD	 the right was the left side right outer join 
SATD	 todo why does this only kill nondefault sessions nothing for workload management since that only deals with default ones 
SATD	 todo should this use thats what other code uses 
SATD	 todo fix this 
SATD	 fixme doing the multiline handling down here means higherlevel logic never sees the extra lines for example script being saved wont include the continuation lines this logged sfnet bug 
SATD	 review jhyde oct this rule nonstatic depends the state members reldecorrelator and has sideeffects the decorrelator this breaks the contract planner rule and the rule will not reusable other planners 
SATD	 can loop thru all the tables check they are acid first and then perform cleanup but its more efficient unconditionally perform cleanup for the database especially when there are lot tables 
SATD	 todo implement this 
SATD	 todo allow using unsafe optionally bounds check first trigger bugs whether the first byte matches not 
SATD	 todo this doesnt appear used anywhere 
SATD	 for auto convert mapjoins not safe dedup here todo 
SATD	 were scanning tree from roots leaf this not technically correct demux and mux operators might form diamond shape but will only scan one path and ignore the others because the diamond shape always contained single vertex the scan depth first and because remove parents when pack pipeline into vertex will never visit any node twice but because that might have situation where need connect work that comes after the work were currently looking also note the concept leaf and root reversed hive for historical 
SATD	 the following parameters are not supported yet todo add support 
SATD	 todo this should unique 
SATD	 todo concurrent insertupdate same partition should pass 
SATD	 deprecated favour link removed hive 
SATD	 todo this invalid for acid tables and cannot access acidutils here 
SATD	 after each major compaction stats need updated each column the tablepartition which previously had stats create bucketed orc backed table orc currently required acid populate partitions with data compute stats insert some data into the table using streamingapi trigger major compaction which should update stats check that stats have been updated throws exception todo add nonpartitioned test add test with sorted table 
SATD	 hive delta file names changed deltaxxxxyyyyzzzz prior that the name was deltaxxxxyyyy want run compaction tests such that both formats are used since new code has able read old files 
SATD	 todo should call 
SATD	 todo could try get superclass generic interfaces 
SATD	 todo this actually calls the metrics system and getmetrics that may expensive for now looks like should thread 
SATD	 todo get rid the builders they serve purpose just call ctors directly 
SATD	 todo verify having not separate filter shouldnt introduce derived table 
SATD	 why dont lock the snapshot here instead having client make explicit call whenever chooses want rely locks for transaction scheduling must get the snapshot after lock acquisition relying locks pessimistic strategy which works better under high contention 
SATD	 todo temporary need expose from orc utils note the difference null checks 
SATD	 todo maybe use stack estobj pairs instead recursion 
SATD	 todo shortcut for last col below length 
SATD	 temporary order avoid new version storageapi the conversion here 
SATD	 todo these constraints should supported for partition columns 
SATD	 have nested setcolref process that and start from scratch todo use stack 
SATD	 todo this correct based the same logic hive 
SATD	 todo refactor the following into the pipeline 
SATD	 todo change fileinputformat field after mapreduce 
SATD	 todo ever use this endpoint for anything else refactor cycling into separate class 
SATD	 something seriously wrong this happening 
SATD	 todo ordering seems affect the distinctness needs checking disabling 
SATD	 todo avoid put working directly outstream 
SATD	 cant fetch prefix colqual must pull the entire qualifier todo use iterator the filter serverside 
SATD	 these tests inherently cause exceptions written the test output logs this undesirable since you might appear someone looking the test output logs something failing when isnt not sure 
SATD	 return the above line should have been all the implementation that need but due bug that impl which recognizes only singledigit columns need another impl here 
SATD	 deprecated favour link removed hive 
SATD	 todo verify need use unwrap data 
SATD	 generate relnode for lateral view todo support different functions not only inline with lateral view join 
SATD	 todo this actually not adding anything since lockcomponent uses trie promote lock except accident when have partitioned target table have readentity and writeentity for the table mark readentity and then delete writeentity replace with partition entries dbtxnmanager skips read lock the readentity inputnolockneeded 
SATD	 todo cleanup once parquet support timestamp type annotation 
SATD	 todo buffers are accounted for allocation time but ideally should report the memory overhead from the java objects memory manager and remove when discarding file 
SATD	 todo cleanup this 
SATD	 todo hive need maxlength checking 
SATD	 addbatchtowriter have passed the batch both orc and operator pipeline neither ever changes the vectors wed need set vectors batch write todo for now create this from scratch ideally should return the vectors from ops could also have the orc thread create for its spare time 
SATD	 todo enforce max length 
SATD	 todo would make sense return buffers asynchronously 
SATD	 todo the best solution support nan expression reduction 
SATD	 todo ideally this should moved outside hivemetastore shared between all the rawstores right now theres method create pool 
SATD	 todo hive abort handling 
SATD	 todo update search once hive done 
SATD	 note type param not available here 
SATD	 todo when function privileges are implemented they should deleted here 
SATD	 todo perhaps move orc instream 
SATD	 todo should not throw different exceptions for different hms deployment types 
SATD	 todo clean all the other paths that are created 
SATD	 todo ideally when colstatsaccurate stuff stored some sane structure this should retrieve partstoupdate single query checking partition params java 
SATD	 undone missing datetime interval data types 
SATD	 deprecated favour link hcattablegetcols removed hive 
SATD	 todo refactor this out 
SATD	 todo this needs looked map map map made concurrent for now since split generation can happen parallel 
SATD	 todo fix the expressions later 
SATD	 deprecated favour link createhcattable removed hive 
SATD	 todo handle 
SATD	 push down semi joins todo enable this later 
SATD	 todo add txnscomment filed and set aborted system due timeout easier read logs 
SATD	 todo riven switch this back package level when can move into riven 
SATD	 todo change this over just store local dir indices instead the entire path far more efficient 
SATD	 todo might revisit this createdroprecreate cases needs some thinking 
SATD	 todo why this text formatter 
SATD	 note this rule replicated from calcites subqueryremoverule transform that converts exists and scalar subqueries into joins todo reason this replicated instead using calcites calcite creates null literal with null type but hive needs properly typed psubqueries are represented link rexsubquery expressions subquery may may not correlated subquery correlated the wrapped link relnode will contain link rexcorrelvariable before the rewrite and the product the rewrite will link correlate the correlate can removed using link reldecorrelator 
SATD	 truncate todo posixfallocate 
SATD	 note this code tries get all keyvalue pairs out the map its not very efficient the more efficient way should let mapoi return iterator this currently not supported mapoi yet 
SATD	 undone why need specify binarysortableserde explicitly here 
SATD	 todo last param bogus why this hardcoded 
SATD	 todo move this common method note this gets ids name assume indices dont need adjusted for acid 
SATD	 ends decorrelating expressions corcol value generator not generated further simplified false this wrong and messes the whole tree prevent this visitcall overridden rewritesimply such predicates not null also need take care that this only for correlated predicates and not user specified explicit predicates todo this code should removed once calcite fixed and there support not equal 
SATD	 todo need proper clone meanwhile lets least keep this horror one place 
SATD	 note that pass job config the record reader but use global config for llap todo add tracing serde reader 
SATD	 deprecated favour link removed hive 
SATD	 fixme support template types currently has conflict with 
SATD	 this rule copy link that regenerates hive specific aggregate operators todo when calcite completed should able remove much this code and just override the relevant methods planner rule that reduces aggregate functions link simpler forms prewrites liavgx rarr sumx countx listddevpopx rarr sqrt sumx sumx sumx countx countx listddevsampx rarr sqrt sumx sumx sumx countx case countx when then null else countx end livarpopx rarr sumx sumx sumx countx countx livarsampx rarr sumx sumx sumx countx case countx when then null else countx end 
SATD	 todo handle more than inputs for setop 
SATD	 todo separate model needed for compressedoops which can guessed from memory size 
SATD	 todo should convert multijoin child hivejoin 
SATD	 todo should doing security check here users should not able see each others locks 
SATD	 handle anything but for now again need real client for this api todo handle and return new connection nothing for now 
SATD	 todo close basically resets the object bunch nulls should ideally not reuse the object because its pointless and errorprone 
SATD	 todo assumes its isoriginal why 
SATD	 todo required due why wasnt required before 
SATD	 todo change this not serialize the entire configuration minor 
SATD	 todo for object inspector fields assigning for now better estimate the memory size every object inspectors have implement memoryestimate interface which lot change with little benefit compared 
SATD	 undone add support for date timestamp intervalyearmonth intervaldaytime 
SATD	 todo there was code here create guessestimate for collection wrt how usage changes when removing elements however its too errorprone for anything involving preallocated capacity was discarded 
SATD	 extract the buckedid from pathfilesmap this more accurate method however may not work certain cases where buckets are named after files used while loading data such case fallback old potential inaccurate method the accepted file names are such copy 
SATD	 this bit hackish fix mismatch between sarg and hive types for timestamp and date todo move those types storageapi 
SATD	 todo hive move using loop and timed wait once tez fixed 
SATD	 todo add configurable option skip the history and just drop 
SATD	 todo refactor this into utility llap tests use this pattern lot 
SATD	 todo make aliases unique otherwise needless rewriting takes place 
SATD	 derive additional attributes rendered explain todo this method relied upon custom input formats set jobconf properties this madness this hive storage handlers 
SATD	 connects the link imetastoreclient meta store that will used manage link transaction lifecycles also checks that the tables destined receive mutation events are able the client should only hold one open transaction any given time todo enforce this 
SATD	 todo this should have option for directory inherit from the parent table including bucketing and list bucketing for the use compaction when the latter runs inside transaction 
SATD	 undone parameterize for implementation variation 
SATD	 todo ifexists could moved metastore fact already supports that check for now since get parts for output anyway can get the error message earlier get rid output can get rid this 
SATD	 fixme moved default value herefor now think this features never really used from the command line 
SATD	 todo should moved out 
SATD	 todo types need checked 
SATD	 todo handle multi joins 
SATD	 todo ugly hack because java doesnt have dtors and tez input hangs shutdown 
SATD	 todo make script output prefixing configurable had disable this since results lots test diffs 
SATD	 todo also support filekey splits like orcsplit does 
SATD	 todo when txn stats are implemented use writeids determine stats accuracy 
SATD	 hive pretty simple read stupid writing out values via the serializer were just going through matching indices hive formats normally handle mismatches with null dont have that option instead well end throwing exception for invalid records 
SATD	 todo handle negations 
SATD	 todo handle task container map events case hard failures 
SATD	 todo partition names are not case insensitive 
SATD	 todo this should configured serde 
SATD	 todo have put the support for clause 
SATD	 ideally should use hiverelnode convention however since volcano planner throws that case because druidquery does not implement the interface set bindable currently not use convention hive hence that should fine todo want make use convention while directly generating operator tree instead ast this should changed 
SATD	 call getsplit the inputformat create hcatsplit for each underlying split when the desired number input splits missing use default number denoted zero todomalewicz currently each partition split independently into desired number however want the union all partitions split into desired number while maintaining balanced sizes input 
SATD	 todo need handle the this what mysql does here 
SATD	 deprecated favour link and removed hive 
SATD	 todo watches the output dirs need cancelled some point for now via the expiry 
SATD	 todo use the hard link feature hdfs once done 
SATD	 todo allow port per host 
SATD	 note this for generating the internal path name for partitions users should always use the metastore api get the path name for partition users should not directly take partition values and turn into path name themselves because the logic below may change the future the future its add new chars the escape list and old data wont corrupt because the full path name metastore stored that case hive will continue read the old data but when creates new partitions will use new names edit there are some use cases for which adding new chars does not seem backward compatible partition was created with name having special char that you want start escaping and then you try dropping the partition with hive version that now escapes the special char using the list below then the drop partition fails work 
SATD	 todo can more precise than stringstring 
SATD	 todo this wrong this test sets dummy txn manager and cannot create acid tables change use proper txn manager the setup for some tests hangs this used work accident now this works due test flag the test needs fixed create table 
SATD	 todo currently way test alter partition hcatclient doesnt support 
SATD	 note some these scenarios could handled but they are not supported right now the reason that bind query appuser using the signed token information and dont want bother figuring out which one use case ambiguity use case ambiguous user ambiguous user ambiguous user ambiguous app 
SATD	 only support bulkload when hfilefamilypath has been specified todo support detecting cfs from column mapping todo support loading into multiple cfs time 
SATD	 todo why this changed from the default hiveconf 
SATD	 todo need set catalog parameter 
SATD	 todo should have check the server side embedded metastore throws remote throws ttransportexception 
SATD	 todo this will probably send message that needed here 
SATD	 theres full hash code stored front the key could check that first keylength obviously doesnt make sense less bytes check key then theres match check vain but what the proportion matches for writes could all keys are unique for reads hope its really high then theres mismatch what probability there that key mismatches bytes just checking the key faster 
SATD	 given rexcall tablescan find max nulls currently picks the col with max nulls todo improve this param call param return 
SATD	 note beelineopts uses reflector extensive way call getters and setters itself you want add any getters setters this class but not have interfere with saved variables beelineproperties careful use this marker needed also possible get this naming these functions obtainblah instead getblah and but that not explicit and will likely surprise people looking the code the future better explicit intent 
SATD	 todo should this currentdirs 
SATD	 todo what else required this environment map 
SATD	 fixme hive should probably move this method somewhere else 
SATD	 todo these bytes should versioned 
SATD	 todo minicluster slow this test times out make work 
SATD	 send dropped table notifications subscribers can receive these notifications for dropped tables listening topic hcat with message selector string value value todo datanucleus currently used the hivemetastore for persistence has been found throw npe when serializing objects that contain null for this reason override some fields the storagedescriptor this notification this should fixed after hive upgrade datanucleus from resolved 
SATD	 this utility designed help with upgrading hive ondisk layout for transactional tables has changed and require preprocessing before upgrade ensure they are readable hive some transactional tables identified this utility require major compaction run them before upgrading once this compaction starts more updatedeletemerge statements may executed these tables until upgrade finished additionally new type transactional tables was added insertonly tables these tables support acid semantics and work with any inputoutputformat any managed tables may made insertonly transactional table these tables dont support updatedeletemerge commands this utility works modes preupgrade and postupgrade preupgrade mode has have hive jars the classpath will perform analysis existing transactional tables determine which require compaction and generate set sql commands launch all these compactions note that depending the number tablespartitions and amount data them compactions may take significant amount time and resources the script output this utility includes some heuristics that may help estimate the time required script produced action needed for compactions run instance standalone hive metastore must running please make sure sufficiently high this specifies the limit concurrent compactions that may run each compaction job mapreduce job may used set yarn queue ame where all compaction jobs will submitted postupgrade mode hive jarshivesitexml should the classpath this utility will find all the tables that may made transactional with ful crud support and generate alter table commands will also find all tables that may not support full crud but can made insertonly transactional tables and generate corresponding alter table commands todo rename files execute option may supplied both modes have the utility automatically execute the equivalent the generated commands location option may supplied followed path set the location for the generated scripts 
SATD	 srcs new filestatus why this needed 
SATD	 todo should probably throw exception here 
SATD	 convert agg args and type args calcite todo does hql allows expressions aggregate args can only 
SATD	 todo note that the token not renewable right now and will last for weeks default 
SATD	 todo should this also topdown 
SATD	 todo most the code this class ripped from zookeeper tests instead redoing should contribute updates their code which let more easily access testing helper objects xxx copied from the only used class qtestutil from hbasetests 
SATD	 its constant constant column column cant fetch any ranges todo can try smarter and push the value some node which 
SATD	 todo implement 
SATD	 todo implement implicit asyncrddactions conversion instead jcmonitor todo how handle stage failures 
SATD	 todo should check isassignablefrom 
SATD	 for replication addptns need follow insertifnotexist alterifexists scenario todo ideally should push this mechanism the metastore because otherwise have choice but iterate over the partitions here 
SATD	 todo should create the batch from vrbctx and reuse the vectors like below future work 
SATD	 this hacky way doing the quotes since will match any these hello this something split would considered quoted 
SATD	 todo the type checking the expressions 
SATD	 make tree out the filter todo this all pretty ugly the only reason need all these transformations maintain support for simple filters for hcat users that query metastore forcing everyone use thick client out the question maybe could parse the filter into standard hive expressions and not all this separate tree 
SATD	 not safe continue for rsgbygbylim kind pipelines see hive for more 
SATD	 hack for tables with columns treat table with single column called col 
SATD	 todo expect one dir why dont enforce 
SATD	 post serialization separators are automatically inserted between different fields the struct currently there not way disable that the work around here pad the 
SATD	 only support limited unselected column following order todo support unselected columns genericudtf and windowing functions examine the order this query block and adds column needed order select list 
SATD	 todo want explicit about this dump not being replication dump can uncomment this else section but currently unneeded will require lot golden file regen 
SATD	 todo refactor this hive 
SATD	 todo and not null can potentially folded earlier into noop 
SATD	 todo remove this for now adding because has assertion about column counts that not true for semijoins 
SATD	 todo this seems indicate that priorities change too little perhaps need adjust the policy 
SATD	 note need srcfs rather than because possible that the files lists files which are from different filesystem than the where the files file itself was loaded from currently possible for repl load hdfsipdir and for the files contain hdfsname entries andor viceversa and this causes errors might also possible that there will mix them given files file todo revisit close the end replv dev see our assumption now still holds and not optimize 
SATD	 note with some trickery could add logic for each type confvars for now the potential spurious mismatches and for float should easy work around 
SATD	 fixme hiveserversiteurl not settable 
SATD	 the following check only guard against failures todo knowing which expr constant gbys aggregation function arguments could better done using metadata provider calcite check the corresponding expression exprs see literal 
SATD	 todo remove hive this required only support the deprecated interfaces 
SATD	 todo partitions are loaded lazily via the iterator then will have avoid conversion everything here defeats the purpose 
SATD	 figures out the aliases for whom safe push predicates based ansi sql semantics the join conditions are left associative right outer join left outer join inner join interpreted right outer join left outer join inner join for inner joins both the left and right join subexpressions are considered for pushing down aliases for the right outer join the right subexpression considered and the left ignored and for the left outer join the left subexpression considered and the left ignored here aliases and are eligible pushed todo further optimization opportunity for the case and and are first joined and then the result with but the second join currently treats and separate aliases and thus disallowing predicate expr containing both tables and such such predicates also can pushed just above the second join and below the first join param join operator param row resolver return set qualified aliases 
SATD	 todo not stopping umbilical explicitly some taskkill requests may get scheduled during querycomplete which will using the umbilical hive should fix this until then leave umbilical open and wait for closed after max idle timeout default 
SATD	 todo gap design noone seems use tables they will work but not convert its possible work around this recreating and reinserting the table 
SATD	 todo should moved out 
SATD	 todo threadpool could here one thread per stripe for now linear 
SATD	 optionally some filtering rows undone 
SATD	 todo should have check the server side embedded metastore throws remote throws 
SATD	 fixme support pruning dynamic partitioning 
SATD	 fixme sideeffect will leave the last query set the session level 
SATD	 todo use faster nonsync inputstream 
SATD	 todo should this rather use threadlocal for numa affinity 
SATD	 todo this object created once call one method and then immediately destroyed its basically just roundabout way pass arguments static method simplify 
SATD	 todopc implement max 
SATD	 todo make sure this method eventually used find the prep batch scripts 
SATD	 this doesnt throw any exceptions because dont want the compaction appear failed stats gathering fails since this prevents cleaner from doing its job and there are multiple failures auto initiated compactions will stop which leads problems that are much worse than stale stats todo longer term should write something this binary field need figure out the msg format and how surface show compactions etc 
SATD	 todo transitive dependencies warning 
SATD	 are get the token locally todo coordinator should passed hive must initialized for now 
SATD	 todo ideally acidutils class and various constants should common 
SATD	 hack actually need but that not available 
SATD	 todo hive handling dummyops and propagating abort information them 
SATD	 todo this least for the session pool will always the hive user how does doas above this affect things 
SATD	 todo this currently broken need set memory manager bogus implementation avoid problems with memory manager actually tracking the usage 
SATD	 todo for oneblock case could move notification for the last block out the loop 
SATD	 todo figure out better data structure for node list 
SATD	 why isnt ppd working working but storage layer doesnt row level filtering only row group level 
SATD	 note currently this implementation does not fall back regular copy distcp tried and fails depend upon that behaviour cases like replication wherein distcp fails there good reason not plod along with trivial implementation and fail instead 
SATD	 todo this never used 
SATD	 todo versions could also picked build time 
SATD	 hack refactor once the metadata apis with types are ready 
SATD	 todo this cannot evict enough will spin infinitely terminate some point 
SATD	 but not convert the join 
SATD	 todo ideally should have test for session itself 
SATD	 todo needed verify that recordschema entry for fieldname matches appropriate type 
SATD	 fixme null value treated differently the other endwhen those filter will 
SATD	 todobr change the output colexprnodecolumn names external namesbr verify need use the keyvalue cols switch external names possiblebr exprnode columninfo the specified differently for different gbrs pipeline remove the different treatments virtualcolmap needs maintained 
SATD	 replace the entire current diskrange with new cached range case inexact match either the below may throw not currently support the case where the caller requests single cache buffer via multiple smaller subranges that happens this may throw noone does now though todo should actively assert here for cache buffers larger than range 
SATD	 todo fill when partitiondoneevent supported 
SATD	 todo for now this affects non broadcast unsorted cases well make use the edge property when its available 
SATD	 todo most other options are probably unrecoverable throw 
SATD	 this temporary hack fix things that are not fixed the compiler 
SATD	 should fixed accumulo and 
SATD	 this hackery but having hivecommon depend standalonemetastore really bad because will pull all the metastore code into every module need check that arent using the standalone metastore are should treat the same 
SATD	 todo fix this has run since tables may unbucketed 
SATD	 not sure why this method doesnt throw any exceptions but since the interface doesnt allow well just swallow them and move this okish since releaselocks only called for roac queries would really bad eat exceptions here for write operations 
SATD	 todo implement this when tez upgraded tez 
SATD	 todo there should better way this code just needs modified 
SATD	 todo session reuse completely disabled for doastrue always launches new session 
SATD	 todo case failure heartbeat tasks for the specific dag should ideally killed 
SATD	 todo check all required tables are allowed get from cache 
SATD	 todo only the qualified name should left here 
SATD	 fixme this secret contract reusein getaggrkey creates more closer relation the statsgatherer 
SATD	 note its not quite clear why this done inside this seems like should the top level 
SATD	 todo why this synchronized 
SATD	 todo local cache created once the configs for future queries will not honored 
SATD	 support for dynamic partitions can added later the following not optimized insert overwrite table tds select key value from where where and are bucketed the same keys and partitioned 
SATD	 fixme using real scaling newold ration might yield better results 
SATD	 todo try this with acid default seem making table acid listener too late 
SATD	 context class for operator tree walker for partition pruner todo this class may not useful 
SATD	 thrift cannot write readonly buffers well todo actually thrift never writes the buffer could use reflection unset the unnecessary readonly flag allocationcopy perf becomes problem 
SATD	 todo should replaced cliserviceclient 
SATD	 deprecated favour link builderhcattable boolean removed hive 
SATD	 todo change exprnodeconverter independent partition expr 
SATD	 are not going verify for each partition just verify for the table todo need verify the partition column instead 
SATD	 todo doesnt the right thing hive 
SATD	 todo can columns retain virtualness out union 
SATD	 todo why invent our own error path top the one from futureget 
SATD	 todo there more correct way get the literal value for the object 
SATD	 todo this need review thread safety various places see callsers link pass sessionstate forked threads currently looks like those threads only read metadata but this fragile also maps sessionstate where tempt table metadata stored are concurrent and any putget crosses memory barrier and does using most code javautilconcurrent the readers the objects these maps should have the most recent view the object but again could fragile 
SATD	 arguments then can use more efficient form 
SATD	 todo something preventing the process from terminating after main adding exit hacky solution 
SATD	 todo write error the channel theres mechanism for that now 
SATD	 todo reuse columnvectors hasbatch save the array column take apart each list 
SATD	 todo hold onto this predicate that dont add the filter operator 
SATD	 todo ideally remove elements from this once its known that tasks are linked the instance all deallocated 
SATD	 todo can blindly copy sort trait what inputs changed and are now sorting different cols 
SATD	 todo this works different remote and embedded mode embedded mode exception happens 
SATD	 undone need copy the object 
SATD	 did remove those and gave cbo the proper ast that kinda hacky 
SATD	 fixme consider other operator info wellnot just conf 
SATD	 check todo add option skip this number partitions checks done triggers via counter 
SATD	 vertex started but not complete 
SATD	 todo need the description how these maps are kept consistent 
SATD	 undone need copy the object 
SATD	 todo filterexpr todo functioncache todo constraintcache todo need nested copy todo string intern todo monitor event queue todo initial load slow todo size estimation 
SATD	 todo not sure about this this call doesnt set the compression type the conf file the way gethiverecordwriter does orc appears read the value for itself not sure this correct not 
SATD	 todo refactor there upcoming patch that refactors this bit code currently the idea the following default replcopywork will behave similarly copywork and simply copy along data from the source destination the flag readsrcasfileslist set changes the source behaviour this copytask and instead copying explicit files this will then fall back behaviour wherein files read from the source and the files specified the files are then copied the destination this allows lazycopyonsource and pullfrom destination semantic that want use from replication 
SATD	 scratch variable created here this could optimized the future perhaps using threadlocal storage allocate this scratch field 
SATD	 ideally there should better way determine that the followingwork contains dynamic partitioned hash join but some cases createreducework looks like the work must createdconnected first before the gentezproccontext can updated with the mapjoinwork relationship 
SATD	 future thought this may expensive consider having thread pool run parallel 
SATD	 number rows processed between checks for factor todo there overlap between and checkinterval 
SATD	 todo llapnodeid just hostport pair could make this class more generic 
SATD	 note assume here that plan has been validated beforehand dont verify 
SATD	 implement future needed 
SATD	 todo move this into ctor would need create cachewriter then 
SATD	 after spark only use jobmetricslistener get job metrics todo remove when the new api provides equivalent functionality 
SATD	 todo could this only the actually used 
SATD	 todo may add app name etc later 
SATD	 todo across catalogs 
SATD	 todo this does not work because materialized views need the creation metadata updated case tables used were replicated different database runcreate materialized view dbname matview select from dbname ptned where driver verifysetupselect from dbname matview ptndata driver 
SATD	 fixme oss seems contain duplicates 
SATD	 todo danger stack overflow needs retry limit 
SATD	 cannot drop because uses one its tables todo error message coming from metastore currently not very concise foreign key violation should make easily understandable 
SATD	 todo ideally querytracker should have fragmenttoquery mapping 
SATD	 todo wtf the old code seems just drop the ball here 
SATD	 hack initialize cache with expiry time causing return new hive client every time otherwise the cache doesnt play well with the second test method with the client gets closed the teardown the previous test 
SATD	 deprecated favour link removed hive 
SATD	 todo hive make use progress notifications once hive starts sending them out progressnotified 
SATD	 todo evil need figure out way remove this sleep 
SATD	 todo all the extrapolation logic should moved out this class 
SATD	 fixme this add seems suspicious lines below the value returned this method used betterds 
SATD	 this hack for now handle the group case 
SATD	 all other distinct keys will just forwarded this could optimized 
SATD	 this workaround for derby and oracle bug pretty horrible 
SATD	 todo this fetching all the rows once from broker multiple historical nodes move use scan query avoid back pressure the nodes 
SATD	 todo strictly speaking there bug here heartbeat commits but both heartbeat and checklock are the same retry block checklock throws heartbeat also retired 
SATD	 deprecated favour link removed hive 
SATD	 wow somethings really wrong 
SATD	 undone does this random range need high 
SATD	 todo java support using string with switches but ides dont all seem know that casing fine for now but should eventually remove this also didnt want create another enum just for this 
SATD	 have found invalid decimal value while enforcing precision and scale ideally would replace with null here which what hive does however need plumb this thru somehow because otherwise having different expression type ast causes the plan generation fail after cbo probably due some residual state saqb for now will not run cbo the presence invalid decimal 
SATD	 code initially inspired google objectexplorer todo roll the directonly estimators from fields various other optimizations possible 
SATD	 dirty hack this will throw away spaces and other things find better 
SATD	 todo should local cache also fileid preserve the original logic for now 
SATD	 todo need get child 
SATD	 todo refactor with cache impl has the same merge logic 
SATD	 dirty hack set the environment variables using reflection code this method for testing purposes only and should not used elsewhere 
SATD	 todo decorelation subquery should done before attempting partition pruning otherwise expression evaluation may try execute corelated sub query 
SATD	 todo issamplingpred sampledesc issortedfilter 
SATD	 todo will nice refactor 
SATD	 todo this should not throw todo this should take comment parameter set ccmetainfo provide some context for the failure 
SATD	 todo replace this with map 
SATD	 todo remove some these fields needed 
SATD	 todo this not valid function names for builtin udfs are specified functionregistry and only happen match annotations for user udfs the name what user specifies creation time annotation can absent 
SATD	 todo verify that this correct 
SATD	 todo could try get the declaring object and infer argument stupid java 
SATD	 closedestroy used seq coupling most the time the difference either not clear not relevant remove 
SATD	 todo this doesnt check compaction already running even though initiator does but 
SATD	 this little bit weird well the call outside the lock our caller calls under lock wed preserve the lock state for them their finally block will release the 
SATD	 todo really need some comments explain exactly why each these removed 
SATD	 fixme this broken for multiline sql 
SATD	 not implemented 
SATD	 slice boundaries may not match split boundaries due torn rows either direction this counter may not consistent with splits this also why increment requested bytes here instead based the split dont want the metrics inconsistent with each other matter what determine here least well account for both the same manner 
SATD	 this kinda hacky know these are llaserdedatabuffers 
SATD	 todo perhaps can made more efficient creating byte directly 
SATD	 this not strictly accurate but type cannot null 
SATD	 todo for non columnar dont need this might well update all stats 
SATD	 todo this should ideally not create addpartitiondesc per partition 
SATD	 todo should also whitelist input formats here from 
SATD	 not public since must have the deserialize read object 
SATD	 todo checking children useless compare already does that 
SATD	 todo why does the original code not just use datastream that passes stream 
SATD	 this bogus hack because copies the contents the sql file intended for creating derby databases and thus will inexorably get out date with open any suggestions how make this read the file build friendly way 
SATD	 this ifelse chain looks ugly the inner loop but given that will the same for given operator branch prediction should work quite nicely recordupdateer expects get the actual row not serialized version thus 
SATD	 todo this has find better home its also hardcoded default hive would nice 
SATD	 deprecated favour link removed hive 
SATD	 todo add upstream 
SATD	 todo should have check the server side embedded metastore throws remote throws metaexception 
SATD	 load the list partitions and return the list partition specs todo followup hive should refactor use get the list full partspecs after that check the number dps created not exceed the limit and iterate over and call loadpartition here the reason dont inside hive the latter large and 
SATD	 there are options for this conditionaltask merge the partitions move the partitions dont merge the partitions merge some partitions and move other partitions merge some partitions and dont merge others this case the merge done first followed the move prevent conflicts todo are not dealing with concatenate ddl should not create mergemove path 
SATD	 todo this would more flexible doing sql select statement rather than using inputformat directly see link long long int string string param numsplitsexpected return throws exception 
SATD	 deprecated favour link hcattablegetdbname removed hive 
SATD	 todo hive differentiate between 
SATD	 todo pointless 
SATD	 this kind not pretty but this how detect whether buffer was cached would always set this for lookups put time 
SATD	 todo move these test parameters more specific places theres need have them here 
SATD	 todo checksum only available hdfs need find solution for other local etc 
SATD	 fixme possible alternative move both into under some class nested ones and that way this factory level caching can made transparent 
SATD	 neither expired nor olderthan criteria selected this better not attempt delete tokens 
SATD	 fixme hadoop made the incompatible change for while spark still using hadoop spark requires hive support hadoop first then spark can start working hadoop support remove this after spark supports hadoop 
SATD	 variables used llap daemons todo eventually autopopulate this based prefixes the conf variables will need renamed for this 
SATD	 todo setfilemetadata could just create schema called two places clean later 
SATD	 deprecated favour link hcattablecomment removed hive 
SATD	 this workaround for hadoop libjars are not added classpath the 
SATD	 todo need some sort validation phase over original ast make things user friendly for example original command refers column that doesnt exist this will caught when processing the rewritten query but the errors will point locations that the user cant map anything values clause must have the same number values target table including partition cols part cols last select clause insert select todo care preserve comments original sql todo check identifiers are propertly escapedquoted the generated sql its currently inconsistent look does unescape unparse todo consider when not matched source then update set targettablecol sourcetablecol what happens when source empty this should runtime error maybe not the outer side roj empty the join produces rows supporting when not matched source then this should runtime error 
SATD	 todo wtf this doesnt anything 
SATD	 todo why this needed 
SATD	 note this whole logic replicated from calcites reldecorrelator and exteneded make suitable for hive should get rid this and replace with calcites reldecorrelator once that works with join project etc instead join project this point this has differed from calcites version significantly cannot get rid this reldecorrelator replaces all correlated expressions corexp relational expression relnode tree with noncorrelated expressions that are produced from joining the relnode that produces the corexp with the relnode that references ptodop lireplace code corelmap constructor parameter with relnode limake link currentrel immutable would require fresh reldecorrelator for each node being decorrelatedli limake fields code corelmap immutableli limake subclass rules static and have them create their own decorrelatorli 
SATD	 deprecated favour link string removed hive 
SATD	 concern leaking scratch column 
SATD	 todo this ugly hack see the same for discussion 
SATD	 derby and oracle not interpret filters ansiproperly some cases and need workaround 
SATD	 todo currently not testing the following scenarios multidb whlevel repl load need add that insert into tables quite few cases need enumerated there including dyn adds 
SATD	 todo ppd needs get pushed param scanrel return 
SATD	 convert nonacidorctbl acid table todo remove transprop after hive 
SATD	 fixme possibly the distinction between tablepartition not need however was like this beforewill change later 
SATD	 dont take directories into account for quick stats todo wtf 
SATD	 fixup the children and parents new vector child add new vector child the vector parents children list copy and fixup the parent list the original child instead just assuming relationship when the child mapjoinoperator will have extra parent for the mapjoinoperators small table needs fixed too 
SATD	 todo the copy data unnecessary but there workaround 
SATD	 get the list partitions that need update statistics todo should reuse the partitions generated compile time since getting the list partitions quite expensive return list partitions that need update statistics throws hiveexception 
SATD	 todo this test should removed once acid tables replication supported 
SATD	 todo there easy and reliable way compute the memory used the executor threads and onheap cache 
SATD	 todo normally the result not necessary might make sense pass false 
SATD	 remove newalloc flag first use full unlock after that would imply forcediscarding this buffer acceptable this kind ugly compact between the cache and 
SATD	 only attempt this cmd was successful fixme would probably better move this afterexecution 
SATD	 todo cleanup pending tasks etc that the next dag not affected 
SATD	 todo does this include partition columns 
SATD	 work bytescolumnvector output columns 
SATD	 two reducesinkoperators are correlated means that they have same sorting columns key columns same partitioning columns same sorting orders and conflict the numbers reducers todo should relax this condition todo need handle aggregation functions with distinct keyword this case distinct columns will added the key columns 
SATD	 fixme move testjsonserde from hcat serde 
SATD	 todo confirm this safe 
SATD	 todo execute errors like this currently dont return good error 
SATD	 this not working workaround set part java opts dusertimezoneutc 
SATD	 simply get the next day and back half day this not ideal but seems work 
SATD	 todo maybe should throw this asis too thriftcliservice currently catches exception the combination determines what would kill the executor thread for now lets only allow oom propagate 
SATD	 todo may possible finer grained locks 
SATD	 todo this check somewhat bogus the maxjvmmemory xmx parameters see annotation llapservicedriver 
SATD	 todo check maximum size compatible with 
SATD	 deprecated favour link hcattableescapechar 
SATD	 the sorted columns superset bucketed columns store this fact can later used optimize some groupby queries note that the order does not matter long the first 
SATD	 fixme replace with hive copy once that copied 
SATD	 todo revisit the fence 
SATD	 todo how handle collisions should cloning columninfo not 
SATD	 optimize the scenario when there are grouping keys and distinct mapreduce jobs are not needed 
SATD	 join key expression likely some expression involving functionsoperators there actual table column for this but the reducesink operator should still have output column corresponding this expression using the columninternalname todo does tablealias matter for this kind expression 
SATD	 todo why does tez api use object for this 
SATD	 todo this log statement looks wrong 
SATD	 relying task succeeding reset the exponent theres notifications whether task gets accepted not that would ideal reset this 
SATD	 need override these methods due difference nullability between hive and calcite for the return types the aggregation particular for count and sum todo should close the semantics gaps between hive and calcite for nullability aggregation calls return types this might useful trigger some additional rewriting rules that would remove unnecessary predicates etc 
SATD	 cancel job the monitor found job submission timeout todo the timeout because lack resources the cluster should ideally also cancel the app request here but facilities from spark yarn its difficult hive side alone see hive 
SATD	 todo should checked server side embedded metastore throws remote metastore throws ttransportexception 
SATD	 todo ever need the port could just away with nodeid altogether 
SATD	 metastore related options that the initialized against when conf var this list changed the metastore instance for the cli will recreated that the change will take effect todo suspect the vast majority these dont need here but requires testing before just pulling them out 
SATD	 originals split wont work due mapreduce issue fileinputformat 
SATD	 todo doesnt support map array now the value should updated after support these data types 
SATD	 todo there are more fields perhaps there should array class 
SATD	 some columns select are pruned this may happen those are constants todo the best solution hook the operator before with the select operator see smbmapjoinq for more details 
SATD	 todo 
SATD	 todo this seems wrong following what hive regular does 
SATD	 todo were asking the metastore what its configuration for this var may want revisit pull from client side instead the reason have this way because the metastore more likely have reasonable config for this than arbitrary client 
SATD	 todo can this result crossthread reuse session state 
SATD	 this conditioncheck could have been avoided but honour the old default not calling wasnt set retain that behaviour todocleanup after verification that the outer isnt really needed here 
SATD	 todo support only non nested case 
SATD	 deprecated favour link removed hive 
SATD	 todo get rid this 
SATD	 todo this not remotely accurate you have many relevant original files 
SATD	 todo could remove extra copy for isuncompressed case copying directly cache 
SATD	 todo hive its possible for bunch tasks come around the same time without the actual executor threads picking any work this will lead unnecessary rejection tasks 
SATD	 conflict when loaded some issue with framework which needs relook into later 
SATD	 deprecated favour link hcattablefileformat removed hive 
SATD	 dangerous lets explicitly add incomplete 
SATD	 this not complete list barely make information schema work 
SATD	 todo perhaps should also summarize the triggers pointing invalid pools 
SATD	 todo could instead get from path here and add normal files for every ugi 
SATD	 cannot get root tablescan operator likely because there join groupby between topop and root tablescan operator dont handle that case and simply return 
SATD	 review are supposed applying the getreadcolumnids 
SATD	 todo this should accept file table names exclude from nonacid acid conversion todo change script comments preamble instead footer how does rename script work hadoop oldname newname and what what about how does this actually get executed all other actions are done via embedded jdbc 
SATD	 todo the global lock might coarse here 
SATD	 todo enums that have both field name and value list 
SATD	 the result not boolean and not all partition agree the result dont remove the condition potentially can miss the case like where todo handle this case making result vector handle all constant values 
SATD	 need remove this static hack but this the way currently get session 
SATD	 todo code section copied over from serdeutils because nonstandard json production there should use quotes for all field names should fix this there and then remove this copy see for details trying enable jackson ignore that doesnt seem workcompilation failure when attempting use that feature having change the production itself 
SATD	 assume should have the exact same object todo could also compare the schema and serde and pass only those the call instead most the time these would the same and llap can handle that 
SATD	 todo hive sortpartitionedge 
SATD	 todo allow the branch specified parameter ptest rather than requiring separate property file 
SATD	 note that the tableexists flag used auth kinda hack and assumes only table will ever imported this assumption broken repl load however weve not chosen expand this map tablesetc since have expanded how auth works with repl dump repl load simply require admin privileges rather than checking each object which quickly becomes untenable and even more costly memory 
SATD	 todo change after hive for now theres rack matching 
SATD	 only small set operations allowed inside explicit transactions dml acid tables ops persistent side effects like use database show tables etc that rollback meaningful todo mark all operations appropriately 
SATD	 todo once multistatement txns are supported add test run next statements single txn 
SATD	 todo hivequeryid extraction parsing the processor payload ugly this can improved once tez fixed 
SATD	 undone why dont these methods take decimalplaces 
SATD	 serializes decimal the maximum bit precision decimal digits note major assumption the fast decimal has already been bounds checked and least has precision not bounds check here for better performance 
SATD	 todo refactor this and more object oriented manner 
SATD	 some data missing from the stream for ppd uncompressed read because index offset relative the entire stream and only read part stream rgs are filtered unlike with compressed data where ppd only filters cbs always get full and index offset relative take care the case when uncompressedstream goes seeking around its incorrect relative partial stream index offset will increase the length our and also account for buffers see creatediskrangeinfo index offset now works long noone seeks into this data before the why would they everything works this hacky stream shouldnt depend having all the data 
SATD	 todo hive ideally sort these completion time once that available 
SATD	 generate temporary path for dynamic partition pruning spark branch todo longer need this use accumulator param basepath param return 
SATD	 todo probably temporary before hive after that may create one per session 
SATD	 fixme managers endofbatch threadlocal can deleted 
SATD	 todo most the time theres inmemory use array 
SATD	 todo could tell the policy that dont care about these and have them evicted could just deallocate them when unlocked and free memory handle that eviction for now just abandon the blocks eventually theyll get evicted 
SATD	 single concurrent request per node currently hardcoded the node includes port number different ams the same host count different nodes only have one request type and not useful send more than one parallel 
SATD	 todo mssplit switch this back once hivemetastoreclient moved 
SATD	 deprecated favour link removed hive 
SATD	 currently map type not supported add back when arrow released 
SATD	 todo why cas the result not checked 
SATD	 todo shouldnt propgate col from tab all 
SATD	 todo readencodedcolumns not supposed throw errors should propagated thru consumer potentially holding locked buffers and must perform its own cleanup also currently readencodedcolumns not stoppable the consumer will discard the data receives for one stripe could probably interrupt checked that 
SATD	 review jvs oct shouldnt also incorporating the flavor attribute into the description 
SATD	 requirements for bucket bucketed their keys both sides and fitting memory obtain number buckets todo incase non bucketed splits would computed based data sizemax part size 
SATD	 note later may able set multiple things together except like 
SATD	 may null tests todo see javadoc 
SATD	 xxx makes sense for possibly not needed anymore 
SATD	 the indices should line fixed 
SATD	 the method for altering table props may set the table nonmm not affect todo all such validation logic should param tbl object image before alter table command null not retrieved yet param props prop values set this alter table command 
SATD	 todo verify skipping charset here okay 
SATD	 todo support dynamic partition for ctas 
SATD	 todo some extra validation can also added this user provided parameter 
SATD	 todo add time abort which not currently tracked 
SATD	 todo dump the end wrapping around 
SATD	 todo currently ignores gby and ptf which may also buffer data memory 
SATD	 the queryid could either picked from the current request being processed generated the current request isnt exactly correct since the query done once return the results generating new one has the added benefit working once this moved out udtf into proper api setting this the generated appid which unique despite the differences taskspec the vertex spec should the same 
SATD	 workaround for bug postgres 
SATD	 necessary divide and multiply get rid fractional digits 
SATD	 todo use common thread pool later 
SATD	 register all permanent functions need improvement 
SATD	 todo perhaps this could use better implementation for now even the hive query result set doesnt support this assume the user knows what hes doing when calling 
SATD	 this our problem means the configuration was wrong 
SATD	 used clients serviceregistry todo this unnecessary 
SATD	 todo should this also handle acid operation etc seems miss lot stuff from hif 
SATD	 todo will these checks work some other user logs isnt doas check required somewhere here well should doas check happen here instead after the user test with hiveserver who the incoming user terms ugi the hive user itself the user who actually submitted the query 
SATD	 todo get rid deepcopy after making sure callers dont use references 
SATD	 todo this fishy init object inspectors based first tag should either init for each tag rowinspector doesnt really matter then can create this ctor and get rid firstrow 
SATD	 snapshot was outdated when locks were acquired hence regenerate context txn list and retry todo lock acquisition should moved before analyze this bit hackish currently acquire snapshot compile the query wrt that snapshot and then acquire locks snapshot still valid continue usual 
SATD	 todo option allow converting orc file insertonly transactional 
SATD	 todo calculate this instead just because were writing the location doesnt mean that itll always wanted the meta store right away 
SATD	 todo propagate this error tezjobmonitor somehow without using killquery 
SATD	 all users belong public role implicitly add that role todo mssplit change this back hivemetastorepublic once hivemetastore has moved standalone metastore mrole publicrole new hivemetastorepublic 
SATD	 create bare needed because writables require defaultconstructed instance hydrate from the datainput todo remove once hbase fixed 
SATD	 todo better with handling types exception here 
SATD	 todo returns json string should recreate object from 
SATD	 todo these methods really need deepcopy 
SATD	 todo numtaskstopreempt currently always 
SATD	 todo optimization add check see theres any capacity available point 
SATD	 wait while for existing tasks terminate xxx this will wait forever 
SATD	 todo replace below with jodatime which supports timezone 
SATD	 todo why this inconsistent with what get names 
SATD	 not implemented 
SATD	 todo this should returning class not just int 
SATD	 there are more distincts distinct not count todo may the same countdistinct key countdistinct key todo deal with duplicate count distinct key 
SATD	 todo use nonzero index check for offset errors 
SATD	 deprecated favour link removed hive 
SATD	 todopc need enhance this with complex fields and gettypeall function 
SATD	 hive adds the same mapping twice wish could fix stuff like that 
SATD	 todo this should moved inner class readerwrite that the only place used 
SATD	 harfilesystem has bug where this method does not work properly the underlying hdfs see mapreduce for more information this method from filesystem 
SATD	 todo clean uprefactor assumptions 
SATD	 todo this executor seems unnecessary here and tezchild 
SATD	 todo verify having not seperate filter shouldnt introduce derived table 
SATD	 this the testperformance cli driver for integrating performance regression tests part the hive unit tests currently this includes support for running explain plans for tpcds workload nonpartitioned dataset scaleset todo support for partitioned data set use hbase metastore instead derbythis suite differs from testclidriver wrt the fact that modify the underlying metastoredatabase reflect the dataset before running the queries 
SATD	 hack note different split strategies return differently typed lists yay java this works purely magic because know which strategy produces which type 
SATD	 todo should called here code too fragile move around 
SATD	 stores binary keyvalue sorted manner get topn keyvalue todo rename topnheap 
SATD	 fixme isnull not updated which might cause problems 
SATD	 todo should use what expr udf 
SATD	 todo should really probably throw keep the existing logic for now 
SATD	 its column level parquet reader which used read batch records for list column todo currently list type only support non nested case 
SATD	 todo not clear why dont the rest the cleanup dagclient not created jobclose will called fail after dagclient creation but before 
SATD	 todo hive implement similar feature like hive spark 
SATD	 todo currently not expose any runtime info for nonstreaming tables future extend this add more information regarding table status total size segments druid loadstatus table historical nodes etc 
SATD	 this rather obscure the end last row cached precisely the split end offset the split the middle the file lrr would read one more row after that therefore unfortunate have onerow read however for that have happened someone should have supplied split that ends inside the last row few bytes earlier than the current split which pretty unlikely what more likely that the split and the last row both end the end file check for this 
SATD	 todo this should also happen any error right now this task will just fail 
SATD	 todo null can also mean that this operation was interrupted should really try recreate the session that case 
SATD	 todo should wait for the entry actually deleted from hdfs would have poll the reader count waiting for reach which point cleanup should occur 
SATD	 fixme for ctas this still needed because location not set sometimes 
SATD	 todo consolidate this code with tezchild 
SATD	 todo this check even needed given what the caller checks 
SATD	 fixme file paths strings should changed either file path anything but string 
SATD	 todo why isacidiudoperation needed here 
SATD	 get all simple fields for partitions and related objects which can map oneonone will this queries use different existing indices for each one not get table and name assuming they are the same are using filter todo might want tune the indexes instead with current ones mysql performs poorly esp with order index large tables even the number actual results small query that returns out partitions can sec sec just adding partid filter that doesnt alter the results probably 
SATD	 tez session relies threadlocal for open are some nonsession thread just use the same sessionstate used for the initial sessions technically given that all pool sessions are initially based this state shoudlnt also set this all times and not rely external session stuff should probably just get rid the thread local usage tezsessionstate 
SATD	 hive input format doesnt handle the special condition paths split correctly 
SATD	 not external itself that the case why 
SATD	 todo this stopgap fix really need change all mappings unique node least this case track the latest unique for llapnode and retry all 
SATD	 fixme druid storage handler relies queryid maintain some staging directories expose queryid session level 
SATD	 todo need speed this for the normal path where all partitions are under the table and dont have stat every partition 
SATD	 this function frequently used need optimize this 
SATD	 this ugly two ways assume that has nullwritable first parameter since are using java and not say programming language theres way check ignore the fact that arg completely incompatible vrb writable because vectorization currently works magic getting vrb from with nonvrb value param just cast blindly and hope for the best which obviously what happens 
SATD	 todo two possible improvements right now kill all the queries here could just kill qpdelta after the queries are killed queued queries would take their place could somehow restart queries could instead put them the front 
SATD	 this method inefficient its only used when something crosses buffer boundaries 
SATD	 todo why doesnt this check class name rather than tostring 
SATD	 todo modify thrift idl generate export stage needed 
SATD	 workaround for testing since tests cant set the env vars 
SATD	 todo the memory release could optimized could release original buffers after are fully done with each original buffer from disk for now release all the end doesnt increase the total amount memory hold just the duration bit this much simpler can just remember original ranges after reading them and release them the end few cases where its easy determine that buffer can freed advance remove from the map 
SATD	 todo perhaps add counters for separate things and multiple buffer cases 
SATD	 todo this might only applicable try moving there 
SATD	 todo cat number these need updated dont bother with deprecated methods this just internal class wait until were ready move all the catalog stuff into 
SATD	 case outer joins need pull records from the sides still need produce output for apart from the big table for full outer join todo this reproduces the logic the loop that was here before assuming 
SATD	 todo not clear why this check and skipseek are needed 
SATD	 todo this safe assumption name collision external names 
SATD	 todo cast function calcite have bug where infer type cast throws 
SATD	 todo fix this actually not need this anymore 
SATD	 deprecated favour link removed hive 
SATD	 based todo use proper method after can depend hadoop 
SATD	 todo isnt there prior impl isdirectory utility pathfilter users dont have write their own 
SATD	 get rid trivial case first that can safely assume nonnull 
SATD	 get the tmp uri path will hdfs path not local mode todo gap this doesnt work however this only the path for writer and reader mismatch dump the sidetable for tag load back hashtable file 
SATD	 todo this duplicates method orc but the method should actually here 
SATD	 todo will this work correctly with acid 
SATD	 filter columns may have index which could partition column sarg todo should this then 
SATD	 todo this interface ugly the two implementations are far apart featurewise 
SATD	 deprecated favour link removed hive 
SATD	 validate false default enable the constraint todo constraint like not null could enabled using alter but validate remains false such cases ideally validate should set true validate existing data 
SATD	 todo this needs enhanced once change management based filesystem implemented 
SATD	 todo would nice check the contents the files could use orcfiledump has methods print supplied stream but those are package private 
SATD	 why null and class checks with the new design windowingspec must contain windowfunctionspec todo cleanup datastructs 
SATD	 lock ensures have consistent view the file data which important given that generate stripe boundaries arbitrarily reading buffer data itself doesnt require that this lock held however everything else stripes list does todo make more granular only care that each one reader sees consistent boundaries could shallowcopy the stripes list then have individual locks inside each 
SATD	 todo wtf 
SATD	 todo avoid reading this from the environment 
SATD	 todo wish could cache the hive object but its not thread safe and each threadlocal cache would need reinitialized for every query this huge pita hive object will cached internally but the compat check will done every time inside get 
SATD	 todo can merge neighboring splits dont init many readers 
SATD	 dont break might find better match later 
SATD	 stats exist for this key add new object the cache todo get rid deepcopy after making sure callers dont use references 
SATD	 todo the current impl triggers are added for tez pool triggers mapping between trigger name and pool name will exist which means all triggers applies tez for llap pool triggers has exist for attaching triggers specific pools for usability provide way for triggers sharinginheritance possibly with following modes only only pool inherit child pools inherit from parent 
SATD	 todo using might wrong might need walk down find the 
SATD	 todo this should come through relbuilder the constructor opposed 
SATD	 todo the contains message check fragile should refactor semanticexception queriable for error code and not simply have message note ifexists might also want invoke this but theres good possibility that ifexists stricter about table existence and applies only the ptn therefore ignoring ifexists here 
SATD	 todo were discussing iter interface and also lazytuple change this when plans for that solidifies 
SATD	 todo add method udfbridge say cast func 
SATD	 todo unregister the task for state updates which could turn unregister the node 
SATD	 note assume length split correct given now lrr interprets offsets reading extra row should instead assume chars and add for isunfortunate 
SATD	 todo maybe add the yarn url for the app 
SATD	 note this particular bit will not work for tables there can multiple directories for different ids could put the path here that would account for the current being written but will not guarantee that other ids have the correct buckets the existing code discards the inferred data when the 
SATD	 todo not having aliases for path usually means some bug should give 
SATD	 todo get rid mapoutputinfo possible 
SATD	 todo this object created once call one method and then immediately destroyed its basically just roundabout way pass arguments static method simplify 
SATD	 todo this should passed the taskattemptcontext instead 
SATD	 fixme use different exception type 
SATD	 this little complicated first look for our own config values this those arent set use the hive ones but hive also has multiple ways this need look both theirs well cant use theirs directly because they wrap the codahale reporters their own and not 
SATD	 todo should replace with once hive removes support for the hadoop series 
SATD	 the import statement specified that were importing external table seem doing the following dont allow replacement unpartitioned preexisting table dont allow replacement partitioned preexisting table where that table external todo does this simply mean dont allow replacement external tables they already exist soie the check superfluous and wrong this can simpler check not then what seem saying that the only case allow allow import into external table the statement destination partitioned table exists long actually 
SATD	 todo handle insert overwrite well hive 
SATD	 todo hive include information about pending requests and last allocation time once yarn service provides this information 
SATD	 todo this point dont know the slot number the requested host cant rollover next available 
SATD	 will this true here dont create new object are already out memory 
SATD	 todo replace this with exceptionhandler shutdownhook 
SATD	 note this can called outside without calling setuppool basically should able handle not being initialized perhaps should get rid the instance and 
SATD	 rowresolver outer query this used resolve correlated columns filter todo this currently will only able resolve reference parent querys column this will not work for references grandparent column 
SATD	 todo use stripe statistics jump over stripes 
SATD	 deprecated favour link removed hive 
SATD	 todo why this needed doesnt represent any cols 
SATD	 todo this invalid for smb keep this for now for legacy reasons see the other overload 
SATD	 todo fix this 
SATD	 this probably should not happen but does least also stop the consumer 
SATD	 this seems like very wrong implementation 
SATD	 todo should make aborttxns write something into txnstxnmetainfo about this 
SATD	 registry again just case todo maybe should enforce that 
SATD	 todo lossy conversion distance considered seconds similar timestamp 
SATD	 todo this fraught with peril 
SATD	 todo not the best way share the address 
SATD	 this maps split path offset index based the number locations provided locations not change across jobs the intention map the same split the same node big problem when nodes change added removed temporarily removed and readded etc that changes the number locations position locations and will cause the cache almost completely invalidated todo support for consistent hashing when combining the split location generator and the serviceregistry 
SATD	 the list empty too many concurrent operations spurious failure list drained and recreated concurrently same for the other list spurious todo the fact that concurrent recreation other list necessitates full stop not ideal the reason that the list not being recreated still uses the list being recreated for boundary check needs the old value the other marker however nodelta means the other marker was already set new value for now assume concurrent recreation rare and the gap before commit tiny 
SATD	 the record count from these counters may not correct the input vertex has edges more than one vertex since this value counts the records going all destination vertices 
SATD	 todo why this copypasted from hiveinputformat 
SATD	 negative length should take precedence over positive value 
SATD	 todo use diskrangelist instead 
SATD	 todo shouldnt ignoreemptyfiles set based executionengine 
SATD	 all errorandsolutions that errorheuristic has generated for the same error they should the same though its possible that different file paths etc 
SATD	 different paths running locally remote filesystem ideally this difference should not exist 
SATD	 todo should this not passed the ctor 
SATD	 currently deserialization complex types not supported 
SATD	 todo what about partitions not the default location 
SATD	 todo currently put task info everywhere before submit and know the real node therefore are going store this separately ideally should roll uniqueness 
SATD	 would nice could return typeinfo 
SATD	 todo replace with storagehandler 
SATD	 deprecated favour link 
SATD	 todo this line can removed once precommit jenkins jobs move java 
SATD	 recovery not implemented yet for ppd path 
SATD	 will only check that hadoopauth not simple does not guarantee kerberos 
SATD	 find the class that has this method note that may not work here because the method 
SATD	 deprecated favour link removed hive 
SATD	 removed hadoop but hive users older hadoop versions may still see this exception have reference name 
SATD	 this dumb hiveoperation not always set see hivehive 
SATD	 todo once hive should able retrieve writeidlist from the conf cachedwriteidlist 
SATD	 this nonpool session get rid 
SATD	 todo remove this 
SATD	 not implemented 
SATD	 todo only ever use one row these time why need cache multiple 
SATD	 getdeserializer get the deserializer for table param conf hadoop config param table the table return returns instantiated deserializer looking class name deserializer stored storage descriptor passed table also initializes the deserializer with schema table exception metaexception any problems instantiating the deserializer todo this should move somewhere into serdejar 
SATD	 not pretty but need way get the size 
SATD	 see ctor comment todo should get rid this 
SATD	 addition that druid allow numeric dimensions now this check not accurate 
SATD	 converted true todo should check convert type string and set true 
SATD	 todo ideally should make special form insert overwrite that could use fast merge path for orc and didnt have create table 
SATD	 todo policy deserialization errors 
SATD	 todo need keep track colnametoposmap for every 
SATD	 creating new querystate unfortunately causes all qout change this separate ticket sharing querystate between generating the plan and executing the query seems bad 
SATD	 todo currently pass null counters because this doesnt use llaprecordreader create counters for nonelevatorusing fragments also 
SATD	 todo not sure that this the correct behavior doesnt make sense create the partition with column with invalid type this should investigated later 
SATD	 todo use currently not available ptftranslator 
SATD	 todo might well kill the this point how that from here 
SATD	 bitsets cant correctly serialized kryos default serializer 
SATD	 todo ideally move some the other cleanup code from resetcurrentdag over here 
SATD	 todopc remove application logic separate interface 
SATD	 todo should this for table 
SATD	 todo this limitation the ast rewriting approach that will not able overcome till proper integration full multiinsert queries with calcite implemented the current rewriting gather references from insert clauses and then updates them with the new subquery references however insert clauses use tab cannot resolve the columns that are referring thus just bail out and those queries will not currently optimized calcite example such query from left join aid bid insert overwrite table joinresult select insert overwrite table joinresult select 
SATD	 todo delete tablesdatabases 
SATD	 todo improve this 
SATD	 todo this only applies current thread its not useful all 
SATD	 tracks tasks which could not allocated immediately tasks are tracked the order requests come different priority levels todo hive for tasks the same priority level may worth attempting schedule tasks with 
SATD	 fixme current objective keep the previous outputsbut this possibly bad 
SATD	 assume the enabled the daemon default cannot reasonably check here 
SATD	 todo since hive not done minor compact compacts insert delta well should not linefile rsgeti 
SATD	 send dropped partition notifications subscribers can receive these notifications for particular table listening topic named dbnametablename with message selector string value value todo datanucleus currently used the hivemetastore for persistence has been found throw npe when serializing objects that contain null for this reason override some fields the storagedescriptor this notification this should fixed after hive upgrade datanucleus from resolved 
SATD	 nice error message should given user 
SATD	 smbjoin possible need correct order 
SATD	 lot these methods could done more efficiently operating the text value directly rather than converting hivechar 
SATD	 its only for gby which should forward all values associated with the key the range limit new value should attatched with the key but current implementation only one values allowed with mapaggreagtion which true default this not common case just forward new keyvalue and forget that todo 
SATD	 review oops somebody left the last command unterminated should fix for them complain for now nice and fix 
SATD	 checks whether given url valid format the current uri format jdbchivehostport jdbchive run embedded mode jdbchivelocalhost connect localhost default port jdbchivelocalhost connect localhost port todo write better regex decide uri format 
SATD	 dont fail would better actually compute range inf 
SATD	 todo this pre post upgrade todo can different tables different filesystems 
SATD	 bloating partinfo with inputjobinfo not good 
SATD	 not need generate the again but rather use directly 
SATD	 todo ideally wed register tezcounters here but seems impossible before registertask 
SATD	 todo have cache for table objects need move that cache elsewhere and use from places like this 
SATD	 todo change the indexcache guava loading cache rather than custom implementation 
SATD	 todo fix comment hive 
SATD	 todo some sessionstate internals are not thread safe the compiletime internals are synced via sessionscope global compile lock the runtime internals work magic they probably work because races are relatively unlikely and few tools run parallel queries from the same session operationstate should refactored out sessionstate and made threadlocal 
SATD	 fixme extract the right info type 
SATD	 todo should this configurable via annotation extending runwith annotation 
SATD	 needs more explanation here xmx not the max heap value jdk you need subtract the survivor fraction from this get actual usable memory before goes into 
SATD	 todo unionq the tab alias not properly propagated down the operator tree this happens when union all used sub query hence even column statistics are available the tab alias will null which will fail get proper column statistics for now assume worst case which denominator 
SATD	 optional feature not implemented 
SATD	 todo dont support this but should since users may create empty partition and then load data into 
SATD	 todo backward compat for hive can removed later 
SATD	 deprecated favour link removed hive 
SATD	 fixme implement consolidateevent similar dumpeventevevroot 
SATD	 todo support checking multiple child operators merge further 
SATD	 according calcite going removed before calcite todo handle correlationid 
SATD	 todo theres versioningetc will come here for now rely external locking ordering calls this should potentially return future for that 
SATD	 todo should this done for use with 
WITHOUT_CLASSIFICATION	 settings borrowed from testjdbcwithminihs 
WITHOUT_CLASSIFICATION	 used for sending notifications about vertex completed for canfinish 
WITHOUT_CLASSIFICATION	 which should interpreted instant semantics 
WITHOUT_CLASSIFICATION	 unwrap the bag 
WITHOUT_CLASSIFICATION	 initialize dfs 
WITHOUT_CLASSIFICATION	 piggybacking import logic for now 
WITHOUT_CLASSIFICATION	 this version hadoop does not support filesystemaccess 
WITHOUT_CLASSIFICATION	 over the associated fields and look the dependencies 
WITHOUT_CLASSIFICATION	 singlecolumn long specific repeated lookup 
WITHOUT_CLASSIFICATION	 all bigtable input columns key expressions are isrepeating then calculate key once lookup once 
WITHOUT_CLASSIFICATION	 jdk 
WITHOUT_CLASSIFICATION	 dont fail the query just because any lineage issue 
WITHOUT_CLASSIFICATION	 this shouldnt really happen byte array 
WITHOUT_CLASSIFICATION	 add new filter 
WITHOUT_CLASSIFICATION	 execute sync mode 
WITHOUT_CLASSIFICATION	 add plugin module jars demand 
WITHOUT_CLASSIFICATION	 cleanup vectorexprargtype 
WITHOUT_CLASSIFICATION	 update the lastinputfile with the currentinputfile 
WITHOUT_CLASSIFICATION	 attach the predicate and group the return clause 
WITHOUT_CLASSIFICATION	 sortcols 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this should just generate one strategy with splits for base and insertdeltas 
WITHOUT_CLASSIFICATION	 the following methods are for java serialization use only 
WITHOUT_CLASSIFICATION	 normal case convert all parameters 
WITHOUT_CLASSIFICATION	 thread generate the separate rows beside the normal thread 
WITHOUT_CLASSIFICATION	 day hrs mins secs day hrs mins days day hrs mins 
WITHOUT_CLASSIFICATION	 only second stripes will satisfy condition and hence single split 
WITHOUT_CLASSIFICATION	 how much can write current write buffer out what need 
WITHOUT_CLASSIFICATION	 comparison methods 
WITHOUT_CLASSIFICATION	 given subquery checks see what the aggegate function 
WITHOUT_CLASSIFICATION	 number rows received 
WITHOUT_CLASSIFICATION	 llap summary 
WITHOUT_CLASSIFICATION	 metadata 
WITHOUT_CLASSIFICATION	 the predicate numeric column and specifies open range key not support conversion negative values are lexicographically stored after positive values and thus they would returned 
WITHOUT_CLASSIFICATION	 submit few queries 
WITHOUT_CLASSIFICATION	 however allow the partition comments change 
WITHOUT_CLASSIFICATION	 add sparkhadoop prefix for yarn properties sparkconf only accept properties started with spark prefix spark would remove sparkhadoop prefix lately and add its hadoop configuration 
WITHOUT_CLASSIFICATION	 only fetch the table actually have listener 
WITHOUT_CLASSIFICATION	 worst case 
WITHOUT_CLASSIFICATION	 technically for ifnotexists case could insert one and discard the other because the first one now exists but seems better report the problem upstream such command doesnt make sense 
WITHOUT_CLASSIFICATION	 key 
WITHOUT_CLASSIFICATION	 buddy block free and the same free list have locked take out for merge 
WITHOUT_CLASSIFICATION	 now that have made sure that the argument primitive type can get the primitive category 
WITHOUT_CLASSIFICATION	 interrupted 
WITHOUT_CLASSIFICATION	 return size 
WITHOUT_CLASSIFICATION	 rewrite the above plan correlaterelleft correlation condition true leftinputrel projecta rexnode aggregate groupby agg agg projectb references covar rightinputrel correlated reference 
WITHOUT_CLASSIFICATION	 number elements average elements average elements times the covariance 
WITHOUT_CLASSIFICATION	 happens when using and the line cmds does not end with 
WITHOUT_CLASSIFICATION	 make sure zero trimming doesnt extend into the integer digits 
WITHOUT_CLASSIFICATION	 traverse all the joins and convert them necessary 
WITHOUT_CLASSIFICATION	 will this point indicating failure 
WITHOUT_CLASSIFICATION	 state should not null future proofing 
WITHOUT_CLASSIFICATION	 link lockid queryid 
WITHOUT_CLASSIFICATION	 replace each the position alias orderby with the actual column 
WITHOUT_CLASSIFICATION	 nothing default 
WITHOUT_CLASSIFICATION	 note this sets loadfiletype incorrectly for acid that relevant for import see setloadfiletype and setisacidiow calls elsewhere for example 
WITHOUT_CLASSIFICATION	 describe how serialize data out user script 
WITHOUT_CLASSIFICATION	 two dots 
WITHOUT_CLASSIFICATION	 make sure map task environment points 
WITHOUT_CLASSIFICATION	 stop reserve order start 
WITHOUT_CLASSIFICATION	 setup resolve make connections 
WITHOUT_CLASSIFICATION	 update job state with the childjob 
WITHOUT_CLASSIFICATION	 exists aleady 
WITHOUT_CLASSIFICATION	 this partition the only partition under partdate 
WITHOUT_CLASSIFICATION	 verify 
WITHOUT_CLASSIFICATION	 can the mapjoin present converted bucketed mapjoin 
WITHOUT_CLASSIFICATION	 skip druid properties which are used druidserde since they are also updated after serdeinfo properties are copied 
WITHOUT_CLASSIFICATION	 note that permanent functions can only properly checked from the session registry permanent functions are read from the metastore during hive initialization the jars are not loaded for the udfs during that time and hive unable instantiate the udf classes add the persistent functions set once permanent udf has been referenced session its functioninfo should registered the session registry and persistent set updated can looked there 
WITHOUT_CLASSIFICATION	 middle item 
WITHOUT_CLASSIFICATION	 there writeentity with for target table replace with writeentity for each partition 
WITHOUT_CLASSIFICATION	 the underlying sslsocket object bound hostport with the given sotimeout 
WITHOUT_CLASSIFICATION	 since want display all the met and not met conditions explain determine all information first 
WITHOUT_CLASSIFICATION	 partition root 
WITHOUT_CLASSIFICATION	 new movework was created then should link all dependent tasks from the movework link 
WITHOUT_CLASSIFICATION	 note that the code updating the state the task does when its out the queue the priorities the queue should correct the top task not killable then task the queue would killable 
WITHOUT_CLASSIFICATION	 create new grouping key for grouping dummy grouping added runtime the group operator creates rows per input row where the number grouping sets 
WITHOUT_CLASSIFICATION	 because local inpath doesnt delete source files 
WITHOUT_CLASSIFICATION	 deal with dynamic partition columns convert exprnodedesc type string 
WITHOUT_CLASSIFICATION	 try arg version 
WITHOUT_CLASSIFICATION	 replacing any previous mapping from old input 
WITHOUT_CLASSIFICATION	 salestxt will need added resource 
WITHOUT_CLASSIFICATION	 this illustration not functioning example 
WITHOUT_CLASSIFICATION	 there should new directory base 
WITHOUT_CLASSIFICATION	 remainder always positive 
WITHOUT_CLASSIFICATION	 when converting from char the value should stripped any trailing spaces 
WITHOUT_CLASSIFICATION	 check may julian day 
WITHOUT_CLASSIFICATION	 this should not start the actual tez 
WITHOUT_CLASSIFICATION	 for each dir get the inputformat and getsplits 
WITHOUT_CLASSIFICATION	 safety valve for extreme cases 
WITHOUT_CLASSIFICATION	 note that have regexes below 
WITHOUT_CLASSIFICATION	 case unions mapjoins possible that the file has already been seen 
WITHOUT_CLASSIFICATION	 theory this should not happen the original copy the udf had this data should able set the udf copy with this same settabledata 
WITHOUT_CLASSIFICATION	 parse validtxnlist 
WITHOUT_CLASSIFICATION	 inputsaddnew readentitypartn this needed all 
WITHOUT_CLASSIFICATION	 test table without portion 
WITHOUT_CLASSIFICATION	 record the final counters 
WITHOUT_CLASSIFICATION	 stop hiveserver 
WITHOUT_CLASSIFICATION	 firstname alan 
WITHOUT_CLASSIFICATION	 skipnulls true and there are rows valuechain all rows partition are null far add null 
WITHOUT_CLASSIFICATION	 expect query successfully completed now 
WITHOUT_CLASSIFICATION	 old view has partitions could not replaced 
WITHOUT_CLASSIFICATION	 oracle specific parser 
WITHOUT_CLASSIFICATION	 base case 
WITHOUT_CLASSIFICATION	 asynch write completed the semaphore 
WITHOUT_CLASSIFICATION	 override this method you want customize how the node dumps out its children 
WITHOUT_CLASSIFICATION	 analyze table pageview 
WITHOUT_CLASSIFICATION	 does same thing with except that this replaces original keyoi with which create hbasekeyfactory provided serde property for hbase 
WITHOUT_CLASSIFICATION	 dont set perms groups for default dir 
WITHOUT_CLASSIFICATION	 setup timer task check for hearbeat timeouts 
WITHOUT_CLASSIFICATION	 start concurrent txn 
WITHOUT_CLASSIFICATION	 also throws ioexception when binary detected 
WITHOUT_CLASSIFICATION	 some nasty examples that show how log format broken and test the regex these are all sourced from genuine logs text sample new imgzemantacom apr aacdfececdccaaec ffcfeadc restgetobject pixygif get http mozilla compatible msie windows net clr net clr text sample new imgzemantacom apr aacdfececdccaaec dbe restgetobject pixygif get pixygifxidbabacdab http zhuaxiacom text sample new staticzemantacom apr aacdfececdccaaec eeeffebfea restheadobject head http accessdenied mozilla compatible msie windows 
WITHOUT_CLASSIFICATION	 call the function 
WITHOUT_CLASSIFICATION	 loginfohar location harlocn 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring 
WITHOUT_CLASSIFICATION	 windowframe may contain just the start boundary the between style expressing windowframe both boundaries are specified 
WITHOUT_CLASSIFICATION	 the partitioning columns the child are more specific than those the parent 
WITHOUT_CLASSIFICATION	 sample the first batch processed for variable sizes 
WITHOUT_CLASSIFICATION	 phase create objects 
WITHOUT_CLASSIFICATION	 update for 
WITHOUT_CLASSIFICATION	 count 
WITHOUT_CLASSIFICATION	 case 
WITHOUT_CLASSIFICATION	 fail transactional set true and the table bucketed but doesnt use orc 
WITHOUT_CLASSIFICATION	 use the expressions from reduce sink 
WITHOUT_CLASSIFICATION	 want the values which are not skewed 
WITHOUT_CLASSIFICATION	 only specified nodes these types will walked empty set means all the nodes will walked 
WITHOUT_CLASSIFICATION	 valid txn list might generated for query compiled using this command thus need reset 
WITHOUT_CLASSIFICATION	 run the vectorized mode 
WITHOUT_CLASSIFICATION	 this point one task and taskp running ask for another task 
WITHOUT_CLASSIFICATION	 write files inside the subdirectory 
WITHOUT_CLASSIFICATION	 subdirectory then recursively list the files 
WITHOUT_CLASSIFICATION	 for thread safe 
WITHOUT_CLASSIFICATION	 otherwise add the join specs 
WITHOUT_CLASSIFICATION	 the target hash table disk spill this row disk well processed later 
WITHOUT_CLASSIFICATION	 this contrived example practice this query would course fail after drop table 
WITHOUT_CLASSIFICATION	 view 
WITHOUT_CLASSIFICATION	 not propagated 
WITHOUT_CLASSIFICATION	 create the final reduce sink operator 
WITHOUT_CLASSIFICATION	 from precision max scale max 
WITHOUT_CLASSIFICATION	 let the processor control start for broadcast inputs 
WITHOUT_CLASSIFICATION	 since are adding the user name the scratch dir not need give more permissions here 
WITHOUT_CLASSIFICATION	 the schema version already checked then ahead and use this metastore 
WITHOUT_CLASSIFICATION	 child can expr alias expr 
WITHOUT_CLASSIFICATION	 leftinputrel contains unique keys each row distinct and can group all the left 
WITHOUT_CLASSIFICATION	 store the inputs hashmap since cant get readentity from inputs since implemented setreadentity used the key that the hashmap has the same behavior equals and hashcode 
WITHOUT_CLASSIFICATION	 this operator has been visited already the rule not need apply the optimization 
WITHOUT_CLASSIFICATION	 the oldsplit may null during the split phase 
WITHOUT_CLASSIFICATION	 first create the archive tmp dir that the job fails the 
WITHOUT_CLASSIFICATION	 close the session before have wait again 
WITHOUT_CLASSIFICATION	 deltes cant raw format 
WITHOUT_CLASSIFICATION	 operationid 
WITHOUT_CLASSIFICATION	 shortcircuited clientside verifying that its empty object not null 
WITHOUT_CLASSIFICATION	 read the newly added via cachedstore 
WITHOUT_CLASSIFICATION	 multikey specific declarations 
WITHOUT_CLASSIFICATION	 the number files for the table should same number buckets 
WITHOUT_CLASSIFICATION	 get the join keys from parent reducesink operators 
WITHOUT_CLASSIFICATION	 whether all files are not sufficient reach sizeleft 
WITHOUT_CLASSIFICATION	 not vrb mode the new cache data ready should use 
WITHOUT_CLASSIFICATION	 directory removal will handled cleanup the sessionstate level 
WITHOUT_CLASSIFICATION	 use only reducer reduce keys 
WITHOUT_CLASSIFICATION	 only set when setting the secure config for 
WITHOUT_CLASSIFICATION	 repl load 
WITHOUT_CLASSIFICATION	 make room for remainder 
WITHOUT_CLASSIFICATION	 resolve storage handler any 
WITHOUT_CLASSIFICATION	 restore default cost model 
WITHOUT_CLASSIFICATION	 test that input name does not change iocontext returned and that each thread gets its own 
WITHOUT_CLASSIFICATION	 ignore exceptions from destroy 
WITHOUT_CLASSIFICATION	 this always nonnull when conversion completed 
WITHOUT_CLASSIFICATION	 true both are null 
WITHOUT_CLASSIFICATION	 utility methods used store pairs ints long 
WITHOUT_CLASSIFICATION	 the nullindicator 
WITHOUT_CLASSIFICATION	 for groupby values with same key topk should forwarded flag used control how topn handled for ptfwindowing partitions 
WITHOUT_CLASSIFICATION	 get default value stored metastore 
WITHOUT_CLASSIFICATION	 add new key add value existing key 
WITHOUT_CLASSIFICATION	 this one can pushed 
WITHOUT_CLASSIFICATION	 there already predicate this src 
WITHOUT_CLASSIFICATION	 expr sanity test strict mode the presence order limit must 
WITHOUT_CLASSIFICATION	 adding columninfo the rowschema signature 
WITHOUT_CLASSIFICATION	 there are two areas exploration for optimization here were serializing the schema with every object assume the schema provided the table always correct dont need this and and can just send the serialized bytes tofrom bytes immediately may save some time but doing this lazily but until theres evidence this useful 
WITHOUT_CLASSIFICATION	 get input data 
WITHOUT_CLASSIFICATION	 master node will serialize readercontext and will make available slaves 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 arrowallocatorlimit ignored allocator was previously created 
WITHOUT_CLASSIFICATION	 the grouping sets data consumed the current job 
WITHOUT_CLASSIFICATION	 completion txnidtxnidselect 
WITHOUT_CLASSIFICATION	 write value the column vector and return back the byte buffer used 
WITHOUT_CLASSIFICATION	 when reading the file for first time get the orc tail from the orc reader and cache the footer cache subsequent requests will get the orc tail from the cache file length and modification time not changed and populate the split info the split info object contains the orc tail from the cache then can skip creating orc reader avoiding filesystem calls 
WITHOUT_CLASSIFICATION	 obviously different expressions 
WITHOUT_CLASSIFICATION	 assert the actual stack traces are exactly equal the written ones and are contained stacktraces list the submission order 
WITHOUT_CLASSIFICATION	 map from newinput 
WITHOUT_CLASSIFICATION	 retain only the largest value for register index 
WITHOUT_CLASSIFICATION	 probe space should least equal the size our designated wbsize 
WITHOUT_CLASSIFICATION	 skewed columns stuff 
WITHOUT_CLASSIFICATION	 key 
WITHOUT_CLASSIFICATION	 deal with nonpartition columns 
WITHOUT_CLASSIFICATION	 dummy instantiation make sure any staticctor code blocks that driver are loaded and ready 
WITHOUT_CLASSIFICATION	 the name the dag what displayed the amjob 
WITHOUT_CLASSIFICATION	 note that this not real double hashing since have consistent hash top 
WITHOUT_CLASSIFICATION	 cant vectorize 
WITHOUT_CLASSIFICATION	 read the value and key length for the first record 
WITHOUT_CLASSIFICATION	 list conf entries not turn into env vars 
WITHOUT_CLASSIFICATION	 limit greater than available rows then not update statistics 
WITHOUT_CLASSIFICATION	 this the bad part the vectorized udf returns the right result 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream 
WITHOUT_CLASSIFICATION	 actual list deser its values 
WITHOUT_CLASSIFICATION	 create new agg function calls and rest project list together 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 partition walker working 
WITHOUT_CLASSIFICATION	 instantiate new factory instance only current one not valid 
WITHOUT_CLASSIFICATION	 extra columns difference between referenced columns needed columns the difference could partition columns 
WITHOUT_CLASSIFICATION	 check this user has grant privileges for this privileges this object 
WITHOUT_CLASSIFICATION	 for each predicate does refer one many aliases one add the filterforpushing list that alias many add filter from merging trees 
WITHOUT_CLASSIFICATION	 cannot entries while currently hold read lock keep track them delete later 
WITHOUT_CLASSIFICATION	 definitely short most shorts fall here 
WITHOUT_CLASSIFICATION	 use the serialization scale and format the string with trailing zeroes round the decimal necessary 
WITHOUT_CLASSIFICATION	 report the row its the first row 
WITHOUT_CLASSIFICATION	 serialize using the serde then below deserialize using deserializeread 
WITHOUT_CLASSIFICATION	 original hashcode with low bits 
WITHOUT_CLASSIFICATION	 return evaluator depending the return type 
WITHOUT_CLASSIFICATION	 assume each partition has unique 
WITHOUT_CLASSIFICATION	 fieldschemas 
WITHOUT_CLASSIFICATION	 check the output fixacidkeyindex should indicate the index was fixed 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 export trivially undoable that nothing needs doing undo 
WITHOUT_CLASSIFICATION	 this returns the row because this formatter only called when the was used serialize the rows thriftable objects 
WITHOUT_CLASSIFICATION	 serialization ctor 
WITHOUT_CLASSIFICATION	 query that should fetch one column 
WITHOUT_CLASSIFICATION	 set the default the created date null there was 
WITHOUT_CLASSIFICATION	 referenced the current expression node 
WITHOUT_CLASSIFICATION	 descriptor not defined because takes variable number arguments with different data types 
WITHOUT_CLASSIFICATION	 note that toolrunner this expected local path see 
WITHOUT_CLASSIFICATION	 generate selection operator for groupby keys only 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqldate 
WITHOUT_CLASSIFICATION	 change the type back int that the same alter can attempted from connection 
WITHOUT_CLASSIFICATION	 test lazybinarymap 
WITHOUT_CLASSIFICATION	 this will turn setugi both client and server processes the test 
WITHOUT_CLASSIFICATION	 leave for clean 
WITHOUT_CLASSIFICATION	 the last block that add should append the current block where the cursor 
WITHOUT_CLASSIFICATION	 regrettable that have wrap the ioexception into runtimeexception but throwing the exception the appropriate result here and hasnext signature will only allow runtimeexceptions iteratorhasnext really should have allowed ioexceptions 
WITHOUT_CLASSIFICATION	 verify that updates the notificationevent with the new event 
WITHOUT_CLASSIFICATION	 spacing 
WITHOUT_CLASSIFICATION	 give 
WITHOUT_CLASSIFICATION	 extract partition spec file name part from path 
WITHOUT_CLASSIFICATION	 retrieve all partitions generated from partition pruner and partition 
WITHOUT_CLASSIFICATION	 test with doastrue 
WITHOUT_CLASSIFICATION	 validate database 
WITHOUT_CLASSIFICATION	 were merging the same location can avoid some metastore calls 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 logdebugrunning hive query sql 
WITHOUT_CLASSIFICATION	 the location input and table output for altertable add partitions 
WITHOUT_CLASSIFICATION	 sort the ngram list frequencies descending order 
WITHOUT_CLASSIFICATION	 this case should expect the test have failed the very last read check 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 only offer these when the input file format not the fast vectorized formats 
WITHOUT_CLASSIFICATION	 that case will select but the rowoi need not amended 
WITHOUT_CLASSIFICATION	 nullsafe issame 
WITHOUT_CLASSIFICATION	 this path intermediate data 
WITHOUT_CLASSIFICATION	 start the server 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautilcalendar 
WITHOUT_CLASSIFICATION	 are caching the include the cache 
WITHOUT_CLASSIFICATION	 use longs because dont have unsigned ints 
WITHOUT_CLASSIFICATION	 the input column can either string list integer values 
WITHOUT_CLASSIFICATION	 integer too large cannot recover trimming fractional digits 
WITHOUT_CLASSIFICATION	 ast has two children the child could partition spec columnname 
WITHOUT_CLASSIFICATION	 typically alter table concatenate run only one partitionone table 
WITHOUT_CLASSIFICATION	 first call fileutilsmkdir make sure that destf directory exists not creates 
WITHOUT_CLASSIFICATION	 add the new the old 
WITHOUT_CLASSIFICATION	 this the case when have mapside smb join one input the join treated dummy vertex 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 need count the current one map table then 
WITHOUT_CLASSIFICATION	 expected result 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 use the short user name for the request 
WITHOUT_CLASSIFICATION	 changes the owner role and verify the change 
WITHOUT_CLASSIFICATION	 partial test init 
WITHOUT_CLASSIFICATION	 optional tblname was specified 
WITHOUT_CLASSIFICATION	 update stats but dont update ndv will not change 
WITHOUT_CLASSIFICATION	 for every sampled alias figure out splits sampled and add them return list 
WITHOUT_CLASSIFICATION	 cleanup the synthetic predicate the tablescan operator and filter replacing with true 
WITHOUT_CLASSIFICATION	 test that exclusive partition locks coalesce one 
WITHOUT_CLASSIFICATION	 hiveserver global init file location 
WITHOUT_CLASSIFICATION	 example code test specific scenarios lowlevelcacheimpl cache new lowlevelcacheimpl new dummycachepolicy new dummyallocator true cleanup thread final int file gaps fbs prioritynormal null null gaps fbs prioritynormal null null gaps fbs prioritynormal null null diskrangelist mutatehelper new mutatehelperdr drinsertafterdr drinsertafterdr booleanref new booleanref mhnext testfactory null 
WITHOUT_CLASSIFICATION	 through the root tasks and verify the input format the map reduce tasks 
WITHOUT_CLASSIFICATION	 bucket the second field the record 
WITHOUT_CLASSIFICATION	 update the info sel operator based the pruned reordered columns 
WITHOUT_CLASSIFICATION	 case cols are null 
WITHOUT_CLASSIFICATION	 get the udtf path 
WITHOUT_CLASSIFICATION	 the table output and location input 
WITHOUT_CLASSIFICATION	 try nonnull path 
WITHOUT_CLASSIFICATION	 get the value length 
WITHOUT_CLASSIFICATION	 makes spilling prediction ismemoryfull too defensive which results unnecessary spilling 
WITHOUT_CLASSIFICATION	 since division has occurred dont format with decimal point 
WITHOUT_CLASSIFICATION	 build the bitset with not null columns 
WITHOUT_CLASSIFICATION	 dont set inputs and outputs the locks have already been taken its pointless 
WITHOUT_CLASSIFICATION	 requests before started 
WITHOUT_CLASSIFICATION	 create the local work for this plan 
WITHOUT_CLASSIFICATION	 needed until there junit release with beforeparam afterparam junit then should remove our own copy 
WITHOUT_CLASSIFICATION	 empty values except first column 
WITHOUT_CLASSIFICATION	 add arraystruct the list columns 
WITHOUT_CLASSIFICATION	 this also resets sessionstateget 
WITHOUT_CLASSIFICATION	 get the create table statement for the table and populate the output 
WITHOUT_CLASSIFICATION	 replace each the position alias orderby with the actual column name 
WITHOUT_CLASSIFICATION	 get columnnames from the first parent 
WITHOUT_CLASSIFICATION	 create the walker the rules dispatcher and the context 
WITHOUT_CLASSIFICATION	 check the objectinspector 
WITHOUT_CLASSIFICATION	 hiveparsekwif hiveparsekwleft hiveparsekwright 
WITHOUT_CLASSIFICATION	 shutdown existing session manager 
WITHOUT_CLASSIFICATION	 hits the index return match range 
WITHOUT_CLASSIFICATION	 this will used hadoop only unsecurenon kerberos mode 
WITHOUT_CLASSIFICATION	 null values 
WITHOUT_CLASSIFICATION	 create the context for the walker 
WITHOUT_CLASSIFICATION	 build the type property string not supplied 
WITHOUT_CLASSIFICATION	 whether contains common join contains return this common join 
WITHOUT_CLASSIFICATION	 struct 
WITHOUT_CLASSIFICATION	 should never happen just case 
WITHOUT_CLASSIFICATION	 add task insert delete materialized view from registry needed 
WITHOUT_CLASSIFICATION	 using async could also reserve one one 
WITHOUT_CLASSIFICATION	 here must deltaxy insert events only must compacting 
WITHOUT_CLASSIFICATION	 connect parent filesinkop 
WITHOUT_CLASSIFICATION	 run sql inserts concurrently 
WITHOUT_CLASSIFICATION	 test binary mode 
WITHOUT_CLASSIFICATION	 getacidstate smart not return any deltas current there base that covers them they were compacted but not yet cleaned this means rechecking compaction needed should cheaper 
WITHOUT_CLASSIFICATION	 its possible that old metadata still refers decimal column type precisionscale this case the default assumed thus nothing here 
WITHOUT_CLASSIFICATION	 the case where agg count countcorvar changed countnullindicator note any nonnullable field from the rhs can used the indicator however true field added the projection list from the rhs for simplicity avoid searching for nonnull fields projecta all gby keys rewritten nullable projexpr aggregate groupbyall left input refs countnullindicator other aggs projectb all left input refs plus the rewritten original projected exprs joinreplace corvar input ref from leftinputrel leftinputrel project everything from rightinputrel plus the nullindicator true rightinputrel 
WITHOUT_CLASSIFICATION	 the new port invalid find one starting with the default client port the default client port not specified starting with random port the random port selected from the range between these ports cannot registered with iana and are intended for dynamic allocation see httpbitlydynports 
WITHOUT_CLASSIFICATION	 and not etc 
WITHOUT_CLASSIFICATION	 this position aggregation store onerow only the aggregate results need adjust the correct position there are keys the gby operator 
WITHOUT_CLASSIFICATION	 subtract from self 
WITHOUT_CLASSIFICATION	 set isnull before calls case tney change their mind 
WITHOUT_CLASSIFICATION	 maxretries code retries until batch decays zero 
WITHOUT_CLASSIFICATION	 should fail try get info out the params 
WITHOUT_CLASSIFICATION	 execution fall back row mode 
WITHOUT_CLASSIFICATION	 optionally read current values big length big value len big value bytes since this the first record the valuerefword directs 
WITHOUT_CLASSIFICATION	 avro requires nullable types defined unions some type and null this annoying and were going hide from the user 
WITHOUT_CLASSIFICATION	 verify the raw object thats been created 
WITHOUT_CLASSIFICATION	 will create subquery expression boolean type 
WITHOUT_CLASSIFICATION	 todo fix throws exception 
WITHOUT_CLASSIFICATION	 this valid yarn service name parse out 
WITHOUT_CLASSIFICATION	 default value empty string which case properties will inherited 
WITHOUT_CLASSIFICATION	 reset the hivesitexml values for following param 
WITHOUT_CLASSIFICATION	 create the ungrouped splits 
WITHOUT_CLASSIFICATION	 not using the gauge avoid races 
WITHOUT_CLASSIFICATION	 all data intitializations 
WITHOUT_CLASSIFICATION	 any table empty inner join involving the tables should yield rows 
WITHOUT_CLASSIFICATION	 keep the arguments for reference want all the nonnumeric arguments the same 
WITHOUT_CLASSIFICATION	 trying using the cardinality from the value range 
WITHOUT_CLASSIFICATION	 through all joins should only contain selects and filters between 
WITHOUT_CLASSIFICATION	 currently getprocedurecolumns always returns empty resultset for hive 
WITHOUT_CLASSIFICATION	 can null incase junit tests skip reset reset thread name release time 
WITHOUT_CLASSIFICATION	 skipped like other ops should all work long the types are right 
WITHOUT_CLASSIFICATION	 arithmetic with type date longcolumnvector storing epoch days and type intervaldaytime storing 
WITHOUT_CLASSIFICATION	 convert the accumulo token hadoop token 
WITHOUT_CLASSIFICATION	 this expr udaf invocation does imply windowing return implies neither implies aggregation implies count implies windowing 
WITHOUT_CLASSIFICATION	 here txn was not found expected state 
WITHOUT_CLASSIFICATION	 change different fields and verify they were altered 
WITHOUT_CLASSIFICATION	 create session domain not present 
WITHOUT_CLASSIFICATION	 put the new mapping 
WITHOUT_CLASSIFICATION	 create fake directory throw exception 
WITHOUT_CLASSIFICATION	 assume and table names are the same for all partition provided arguments 
WITHOUT_CLASSIFICATION	 bgenjjtree extends 
WITHOUT_CLASSIFICATION	 found the union 
WITHOUT_CLASSIFICATION	 build conditions for join and filter and start adding 
WITHOUT_CLASSIFICATION	 bytes key hash set optimized for vector map join this the abstract base for the multikey and string bytes key hash set implementations 
WITHOUT_CLASSIFICATION	 optional byte array 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject 
WITHOUT_CLASSIFICATION	 table itself doesnt exist metastore nothing validate 
WITHOUT_CLASSIFICATION	 msck called add missing paritions into metastore and there are missing partitions 
WITHOUT_CLASSIFICATION	 cache pathexpr 
WITHOUT_CLASSIFICATION	 ignore shutdown errors since there are negative tests 
WITHOUT_CLASSIFICATION	 otherwise enabled deserialize rows using regular serde and add the object inspectable object row vectorizedrowbatch the vectormapoperator 
WITHOUT_CLASSIFICATION	 count number digits the value 
WITHOUT_CLASSIFICATION	 note here create fake directory along with fake files original directoriesfiles 
WITHOUT_CLASSIFICATION	 return for 
WITHOUT_CLASSIFICATION	 skip 
WITHOUT_CLASSIFICATION	 private static final string testtablename autopurgetesttable 
WITHOUT_CLASSIFICATION	 deserialization keys here just get reference bytes 
WITHOUT_CLASSIFICATION	 validate the metadata for the getcolumns result set 
WITHOUT_CLASSIFICATION	 bigger addition 
WITHOUT_CLASSIFICATION	 first set back the backup task with its children task 
WITHOUT_CLASSIFICATION	 authenticate using delegation tokens via the digest mechanism 
WITHOUT_CLASSIFICATION	 write stats objs persistently 
WITHOUT_CLASSIFICATION	 todo plumb progress info thru the reader can get metadata from loader first 
WITHOUT_CLASSIFICATION	 hivemetastore keys reserved for updating listenerevent parameters this key used check listener event run inside current transaction boolean value used for active true active false 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 hadoop vars 
WITHOUT_CLASSIFICATION	 boolean store information about whether valid txn list was generated 
WITHOUT_CLASSIFICATION	 need determine different type needed for dummy partitions 
WITHOUT_CLASSIFICATION	 ignore zerodivide cases 
WITHOUT_CLASSIFICATION	 replication scope allows replacement and does not require empty directories 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this the last time well see the table objects for views add the inputs now isinsideview will tell this view embedded another view 
WITHOUT_CLASSIFICATION	 for bootstrap load the create function should always performed 
WITHOUT_CLASSIFICATION	 determine which rows are non matches determining the delta between inputselected and current batch selected 
WITHOUT_CLASSIFICATION	 create the hive catalog 
WITHOUT_CLASSIFICATION	 use druid default need configured user 
WITHOUT_CLASSIFICATION	 these represent the bucketed columns 
WITHOUT_CLASSIFICATION	 reuse existing connection 
WITHOUT_CLASSIFICATION	 note that will throw anonymous mode not allowed username not query string the request this ensures that the context webhcat allows anonymous even though webhcat itself will throw cant figure out username 
WITHOUT_CLASSIFICATION	 alphabet needed 
WITHOUT_CLASSIFICATION	 write bunch random rows that will used for read benchmark 
WITHOUT_CLASSIFICATION	 end user transaction timeout milliseconds 
WITHOUT_CLASSIFICATION	 first entry existence 
WITHOUT_CLASSIFICATION	 remove old temp table entry and add new entry list temp tables 
WITHOUT_CLASSIFICATION	 frequently occurring error 
WITHOUT_CLASSIFICATION	 normalize significand even 
WITHOUT_CLASSIFICATION	 version with seq version with rcf 
WITHOUT_CLASSIFICATION	 append the path substring since previous match 
WITHOUT_CLASSIFICATION	 dont bother with aggregation this case will probably invalid 
WITHOUT_CLASSIFICATION	 failure help 
WITHOUT_CLASSIFICATION	 optional bool isguaranteed 
WITHOUT_CLASSIFICATION	 second time 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 null for non partitioned table 
WITHOUT_CLASSIFICATION	 after all the rows are processed continue generate results for the rows that results havent generated for the case following and following process first results and then insert nulls for the case preceding and following process results 
WITHOUT_CLASSIFICATION	 setting empty collection ranges will unexpectedly scan all data 
WITHOUT_CLASSIFICATION	 skip the big table pos 
WITHOUT_CLASSIFICATION	 first search from the postovertex 
WITHOUT_CLASSIFICATION	 ppd for multiinsert query not yet implemented assume that nothing can pushed beyond this operator 
WITHOUT_CLASSIFICATION	 havent fixed the tmp path for this mapper yet 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 test that two different tables dont collide their locks 
WITHOUT_CLASSIFICATION	 note this assumes both paths are qualified which they are currently 
WITHOUT_CLASSIFICATION	 varchar 
WITHOUT_CLASSIFICATION	 lazycaching the version 
WITHOUT_CLASSIFICATION	 note that use the same vectors both batches clever very clever 
WITHOUT_CLASSIFICATION	 ast specific data 
WITHOUT_CLASSIFICATION	 build rel for where clause 
WITHOUT_CLASSIFICATION	 delete the temp file the jvm terminate normally through hadoop job kill command caveat wont deleted jvm killed kill 
WITHOUT_CLASSIFICATION	 important signum given the firstbyte not bytekeep 
WITHOUT_CLASSIFICATION	 entry will null due zerodivide 
WITHOUT_CLASSIFICATION	 these have been localized already 
WITHOUT_CLASSIFICATION	 lists recursively compare the list element types 
WITHOUT_CLASSIFICATION	 clear out the union set dont need anymore 
WITHOUT_CLASSIFICATION	 note this called someone who has ensured the buffer not going moved 
WITHOUT_CLASSIFICATION	 get the field objectinspector fieldname and the field object 
WITHOUT_CLASSIFICATION	 get agg ret type calcite 
WITHOUT_CLASSIFICATION	 ptf handling 
WITHOUT_CLASSIFICATION	 update has failed wont try low pri task 
WITHOUT_CLASSIFICATION	 using vint instead bytes 
WITHOUT_CLASSIFICATION	 create case sensitive columns list 
WITHOUT_CLASSIFICATION	 needed kyro 
WITHOUT_CLASSIFICATION	 only set command needs updating the configuration stored client side 
WITHOUT_CLASSIFICATION	 the field that passed not primitive and either the field not declared schema was given initialization the field declared primitive initialization serialize the data json string otherwise serialize the data the 
WITHOUT_CLASSIFICATION	 assert 
WITHOUT_CLASSIFICATION	 bucket count 
WITHOUT_CLASSIFICATION	 the first query happens have full batches 
WITHOUT_CLASSIFICATION	 the primitives 
WITHOUT_CLASSIFICATION	 change the resource plan resize and down and remove remapping users everything will killed and wont change will start one more query from the queue and the query queued will requeued and started the fractions will also all change 
WITHOUT_CLASSIFICATION	 test getlogicallength side file 
WITHOUT_CLASSIFICATION	 its skip failed message the target has changed back the old value 
WITHOUT_CLASSIFICATION	 check that the values the older list are also newer lists should already sorted 
WITHOUT_CLASSIFICATION	 created april change this template choose tools template manager and open the template the editor 
WITHOUT_CLASSIFICATION	 store token the cache 
WITHOUT_CLASSIFICATION	 versions dont match return false 
WITHOUT_CLASSIFICATION	 the list families that have been added the scan 
WITHOUT_CLASSIFICATION	 empty maps 
WITHOUT_CLASSIFICATION	 initialize the constants for the grouping sets that they can reused for 
WITHOUT_CLASSIFICATION	 merge bytes 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 keep backward compatibility 
WITHOUT_CLASSIFICATION	 files the partition 
WITHOUT_CLASSIFICATION	 repeating then expression 
WITHOUT_CLASSIFICATION	 generate map reduce plans 
WITHOUT_CLASSIFICATION	 swsr lock are examining shared read 
WITHOUT_CLASSIFICATION	 trim the bins down the correct number bins 
WITHOUT_CLASSIFICATION	 lookup byte array key the hash map param keybytes byte array containing the key within range param keystart the offset the beginning the key param keylength the length the key param hashmapresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spill the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 found exact match 
WITHOUT_CLASSIFICATION	 are missing section the end the part copy the start noncached 
WITHOUT_CLASSIFICATION	 needs least instances 
WITHOUT_CLASSIFICATION	 validate the effective window frames with the rules link validatewindowframe 
WITHOUT_CLASSIFICATION	 partition level column statistics test 
WITHOUT_CLASSIFICATION	 this lock used make sure removallistener wont close client that being contemplated for returning get 
WITHOUT_CLASSIFICATION	 thread local filesystem stats private and cannot cloned make copy new class 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 fully copy over the contents the new into the old 
WITHOUT_CLASSIFICATION	 view just being created already created view being loaded 
WITHOUT_CLASSIFICATION	 each range checks for overflow first then moves digits 
WITHOUT_CLASSIFICATION	 since this can set make the system generate old style deltaxxxxyyyy file names this primarily needed for testing make sure code can still read files created older code also used comactor 
WITHOUT_CLASSIFICATION	 now release another session the thread that gave reuse can proceed 
WITHOUT_CLASSIFICATION	 see autocolumnstatsq under 
WITHOUT_CLASSIFICATION	 merge cost 
WITHOUT_CLASSIFICATION	 this currently only happens tests see getfiledata comment the interface 
WITHOUT_CLASSIFICATION	 reg info changes add notifications ignore errors and update alloc 
WITHOUT_CLASSIFICATION	 say table has partition are generating select where group rowid 
WITHOUT_CLASSIFICATION	 there avoid clashes 
WITHOUT_CLASSIFICATION	 extract the bits num into bytes from right left 
WITHOUT_CLASSIFICATION	 when are doing row deserialization these are the regular deserializer partition object inspector and vector row assigner 
WITHOUT_CLASSIFICATION	 optional int 
WITHOUT_CLASSIFICATION	 several hive classes depend the metastore apis which not included hiveexecjar these are the relatively safe ones operators classes 
WITHOUT_CLASSIFICATION	 revert the projected columns back because vrg will reused 
WITHOUT_CLASSIFICATION	 sure that null produced 
WITHOUT_CLASSIFICATION	 object map 
WITHOUT_CLASSIFICATION	 make copy the sourcetable would done across classloaders 
WITHOUT_CLASSIFICATION	 child this expression uses materialized view then decrease its cost certain factor this useful for partial rewritings where part plan does not use the materialization but still want decrease its cost chosen instead the original plan 
WITHOUT_CLASSIFICATION	 the bigdecimal class recommends not converting directly from double bigdecimal convert through string 
WITHOUT_CLASSIFICATION	 whatever remains take totake blocks splitting the block offset 
WITHOUT_CLASSIFICATION	 name 
WITHOUT_CLASSIFICATION	 select constant and casts can allowed without threshold check 
WITHOUT_CLASSIFICATION	 there were more batches and this either the first batch weve used the current batch buffer goto return false 
WITHOUT_CLASSIFICATION	 virtual columns needed 
WITHOUT_CLASSIFICATION	 unable find stats for this statstype return null can build stats 
WITHOUT_CLASSIFICATION	 acc short for accumulator its used build the row before forwarding 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 change the location position alias process here 
WITHOUT_CLASSIFICATION	 keys are always primitive 
WITHOUT_CLASSIFICATION	 have evaluate the input format see vectorization enabled not set right here 
WITHOUT_CLASSIFICATION	 update the cache add this new aggregate node 
WITHOUT_CLASSIFICATION	 find most significant bit with starting index 
WITHOUT_CLASSIFICATION	 just run our value expressions over input batch 
WITHOUT_CLASSIFICATION	 test getboolean rules nonboolean columns 
WITHOUT_CLASSIFICATION	 block 
WITHOUT_CLASSIFICATION	 gather input works operators 
WITHOUT_CLASSIFICATION	 keyvalue the index removed retrieve memory usage 
WITHOUT_CLASSIFICATION	 todo convert semantic exception 
WITHOUT_CLASSIFICATION	 first try use the blocks half size every arena 
WITHOUT_CLASSIFICATION	 give the cloned work different name 
WITHOUT_CLASSIFICATION	 originals written before table was converted acid considered written writeid which always committed there need check wrt invalid write ids but originals written load data for example can basex deltaxx must check committed not evn rowid not needed the operator pipeline 
WITHOUT_CLASSIFICATION	 field expression 
WITHOUT_CLASSIFICATION	 unless this overridden does nothing 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 return something 
WITHOUT_CLASSIFICATION	 given ast node this method recursively goes over checkexpr ast finds node type toksubqueryexpr throws error 
WITHOUT_CLASSIFICATION	 under exceptional load hadoop may not able look status finished jobs because has purged them from memory from hives perspective its equivalent the job having failed raise meaningful exception 
WITHOUT_CLASSIFICATION	 perform major compaction 
WITHOUT_CLASSIFICATION	 second pass process operator tree two steps first process the extra trees generated the first pass then process the main tree and the result task will depend 
WITHOUT_CLASSIFICATION	 create scratch dir 
WITHOUT_CLASSIFICATION	 case when mvdeptno null and mvdeptname null then else sources mvs end 
WITHOUT_CLASSIFICATION	 singlecolumn string outer get key 
WITHOUT_CLASSIFICATION	 since left has longer digit tail and doesnt move will shift the right digits our addition into the result 
WITHOUT_CLASSIFICATION	 are secure mode 
WITHOUT_CLASSIFICATION	 return previous block 
WITHOUT_CLASSIFICATION	 the primitive object inspector the source data type for any column being converted otherwise null 
WITHOUT_CLASSIFICATION	 print out all the stages that have childstages 
WITHOUT_CLASSIFICATION	 assume 
WITHOUT_CLASSIFICATION	 make sure the token made into the ugi 
WITHOUT_CLASSIFICATION	 hashstruct 
WITHOUT_CLASSIFICATION	 test with txncommit 
WITHOUT_CLASSIFICATION	 now finish task which will make capacity for task run nothing coming out the delayed queue yet 
WITHOUT_CLASSIFICATION	 reset the result for the evicted index 
WITHOUT_CLASSIFICATION	 genericudfbridge udfbridge genericudfbridge exprgetgenericudf 
WITHOUT_CLASSIFICATION	 drop any functions before dropping 
WITHOUT_CLASSIFICATION	 apply the group and permissions the leaf partition and files need not bother case hdfs permission taken care setting umask 
WITHOUT_CLASSIFICATION	 for cases where the table temporary 
WITHOUT_CLASSIFICATION	 not hbase row key this should either prefix individual qualifier 
WITHOUT_CLASSIFICATION	 tests that different threads get the same object per attempt per input and different between attemptsinputs that attempt inherited between threads and that clearing the attempt produces different result 
WITHOUT_CLASSIFICATION	 get privileges for this user and its role this object 
WITHOUT_CLASSIFICATION	 make certain the target directory exists 
WITHOUT_CLASSIFICATION	 the following test uses query that returns group and user entry the ldap atn should use the groupmembershipkey identify the users for the returned group and the authentication should succeed for the users that group well the lone user this case 
WITHOUT_CLASSIFICATION	 pass the request the responder 
WITHOUT_CLASSIFICATION	 this should never thrown 
WITHOUT_CLASSIFICATION	 columnname 
WITHOUT_CLASSIFICATION	 give our final state uiapi requests any 
WITHOUT_CLASSIFICATION	 well assume that there may nulls the input nonulls true for input vector this more forgiving errors loading the vectors properlywritten vectorized iterator will make sure that isnull set nonulls and isrepeating are true for the vector 
WITHOUT_CLASSIFICATION	 wait for failover close sessions 
WITHOUT_CLASSIFICATION	 for this particular file how many columns will actually read 
WITHOUT_CLASSIFICATION	 foreignkeycols 
WITHOUT_CLASSIFICATION	 second value 
WITHOUT_CLASSIFICATION	 verify serializationutils first 
WITHOUT_CLASSIFICATION	 pushed the whole thing down 
WITHOUT_CLASSIFICATION	 avoid timing issues with notifications and given that hdfs check anyway the authoritative one dont wait infinitely for the notifier just wait little bit 
WITHOUT_CLASSIFICATION	 nonjavadoc see list 
WITHOUT_CLASSIFICATION	 time actually run the dag actual dag runtime 
WITHOUT_CLASSIFICATION	 fktablename 
WITHOUT_CLASSIFICATION	 find functions which name contains tofind hidden the default database 
WITHOUT_CLASSIFICATION	 create argument capturer class variable cast this generic generic class 
WITHOUT_CLASSIFICATION	 only user belonging admin role can drop existing role 
WITHOUT_CLASSIFICATION	 optional int keyid 
WITHOUT_CLASSIFICATION	 expected fail due canceled token 
WITHOUT_CLASSIFICATION	 todo make use this config configure fetch size 
WITHOUT_CLASSIFICATION	 this should use but its protected currently 
WITHOUT_CLASSIFICATION	 need create the correct table descriptor for keyvalue 
WITHOUT_CLASSIFICATION	 exclude trailing comma 
WITHOUT_CLASSIFICATION	 add dist udaf args reduce keys 
WITHOUT_CLASSIFICATION	 evaluate the key expressions just this first batch get the correct key 
WITHOUT_CLASSIFICATION	 need iterate through all children even one found not candidate case the other children could individually pushed 
WITHOUT_CLASSIFICATION	 try more tolerant the input invalid instead incomplete well hit exception here again 
WITHOUT_CLASSIFICATION	 cookies manager used cache cookie returned service the goal avoid doing kdc requests for every request 
WITHOUT_CLASSIFICATION	 find all targets recursively 
WITHOUT_CLASSIFICATION	 leverage the nice batching behaviour async loggersappenders can signal the file manager that needs flush the buffer disk the end batch from users point view this means that all log events are always available the log file without incurring the overhead immediateflushtrue 
WITHOUT_CLASSIFICATION	 exists primarily allow for easier unit tests 
WITHOUT_CLASSIFICATION	 register two tasks same query but different 
WITHOUT_CLASSIFICATION	 for completed instances 
WITHOUT_CLASSIFICATION	 print out the cbo info 
WITHOUT_CLASSIFICATION	 its valid case partition 
WITHOUT_CLASSIFICATION	 finally remove the expression from the tree 
WITHOUT_CLASSIFICATION	 these delims passed serde params 
WITHOUT_CLASSIFICATION	 tracks all instances including ones which have been disabled the past 
WITHOUT_CLASSIFICATION	 login from the keytab 
WITHOUT_CLASSIFICATION	 serialize the struct into mutation 
WITHOUT_CLASSIFICATION	 init input 
WITHOUT_CLASSIFICATION	 includebitset 
WITHOUT_CLASSIFICATION	 prefer date type arguments over other method signatures 
WITHOUT_CLASSIFICATION	 idempotent function add various intermediate files the source for the union the plan has already been created 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 blindly add this union type containing int and double should sufficient for the test case 
WITHOUT_CLASSIFICATION	 the second operator has more than one child stop gathering 
WITHOUT_CLASSIFICATION	 the filter already top tablescan 
WITHOUT_CLASSIFICATION	 find the first ancestor this movetask which some form map reduce task either standard local merge 
WITHOUT_CLASSIFICATION	 construct expressionnodedesc representing join condition 
WITHOUT_CLASSIFICATION	 array for the values pass evaluator 
WITHOUT_CLASSIFICATION	 end master thread state 
WITHOUT_CLASSIFICATION	 implementation kvsource that can handle key and value byteswritable objects 
WITHOUT_CLASSIFICATION	 return single arraylist where the first element the number histogram bins and subsequent elements represent histogram pairs 
WITHOUT_CLASSIFICATION	 used for dynamic partitioning 
WITHOUT_CLASSIFICATION	 with the integer type range checking need know the hive data type 
WITHOUT_CLASSIFICATION	 abort all remaining txns 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 close file streams avoid resource leaking 
WITHOUT_CLASSIFICATION	 for now this can simply fetched from single registry instance 
WITHOUT_CLASSIFICATION	 there clause the output schema will keyvalue 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 note other standard ones include clientuser and clienthostname but dont need them for now 
WITHOUT_CLASSIFICATION	 get keyvaluesreaders from the logicalinput and add them priority queue 
WITHOUT_CLASSIFICATION	 notify listeners the changed value 
WITHOUT_CLASSIFICATION	 nonjavadoc should ideally not modify the tree traverse however since need walk the tree any time when modify the operator might well here 
WITHOUT_CLASSIFICATION	 there should delta dirs plus base dirs the location 
WITHOUT_CLASSIFICATION	 now deprecated 
WITHOUT_CLASSIFICATION	 adjust the memory have account for what have just evicted 
WITHOUT_CLASSIFICATION	 ownertype 
WITHOUT_CLASSIFICATION	 hash table loading happens server side llapdecider could kick out some fragments run outside llap flip the flag runtime case are running outside llap 
WITHOUT_CLASSIFICATION	 how many ways each block splits into target size how many targetsized blocks remain from last split the header index for the beginning the remainder 
WITHOUT_CLASSIFICATION	 very simple counter keep track join entries for key 
WITHOUT_CLASSIFICATION	 this operator check whether unary binary operator 
WITHOUT_CLASSIFICATION	 divide operations are not checked because the output always the type double 
WITHOUT_CLASSIFICATION	 only the pattern valuecol should handled 
WITHOUT_CLASSIFICATION	 construct 
WITHOUT_CLASSIFICATION	 for some strange reason bigdecimal can have scale not support that 
WITHOUT_CLASSIFICATION	 convenience method that makes the intended owner for the delegation token request the current user 
WITHOUT_CLASSIFICATION	 also determine any nulls are present since for join that means match 
WITHOUT_CLASSIFICATION	 remove the context words from the end the list 
WITHOUT_CLASSIFICATION	 stripergs should have been initialized this time with empty array 
WITHOUT_CLASSIFICATION	 create the reloading folder place jar files not exist 
WITHOUT_CLASSIFICATION	 look for passthru case where inputfileformat has and reads vectorizedrowbatch row 
WITHOUT_CLASSIFICATION	 fix the case where parent expressions output data type physical variations decimal whereas least one its children decimal some expressions like for example only accepts decimal for and this time there only both and has decimal 
WITHOUT_CLASSIFICATION	 must get statementid from file name since acid doesnt write into bucketproperty 
WITHOUT_CLASSIFICATION	 nulls right nulls left 
WITHOUT_CLASSIFICATION	 ndvproduct then column stats state must partial and are missing 
WITHOUT_CLASSIFICATION	 validate that the set partition columns found custom path must match 
WITHOUT_CLASSIFICATION	 todo unpause fetching 
WITHOUT_CLASSIFICATION	 copy jar change management fails fail the metastore transaction since the user might delete the jars hdfs externally after dropping the function hence having 
WITHOUT_CLASSIFICATION	 fails the finally clause will remove the lock 
WITHOUT_CLASSIFICATION	 well encode the absolute value sign separate 
WITHOUT_CLASSIFICATION	 java timezone has mention thread safety use thread local instance safe 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 reset the array null values 
WITHOUT_CLASSIFICATION	 this removes orderby only expressions from the projections 
WITHOUT_CLASSIFICATION	 partitions bail early 
WITHOUT_CLASSIFICATION	 make the columns list for the temp table input data file 
WITHOUT_CLASSIFICATION	 inputstream open the given sequence file broken rcfile 
WITHOUT_CLASSIFICATION	 current implementation will never happen but leave here make the logic complete 
WITHOUT_CLASSIFICATION	 try get the session quickly 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 here some query rewrite first get the new fetchrn which sum offset and fetch then push through creating new branchsort with the new fetchrn but offset 
WITHOUT_CLASSIFICATION	 optional string containeridstring 
WITHOUT_CLASSIFICATION	 check the restricted configs that the users cannot set 
WITHOUT_CLASSIFICATION	 insert some data this will again generate only insert deltas and delete deltas delta 
WITHOUT_CLASSIFICATION	 expected exception embedded metastore 
WITHOUT_CLASSIFICATION	 the user has specified location external not check the user 
WITHOUT_CLASSIFICATION	 need copy the data byte byte only case the outputlength length which means there least one escaped 
WITHOUT_CLASSIFICATION	 the view inside another view should have least one parent 
WITHOUT_CLASSIFICATION	 read database via cachedstore 
WITHOUT_CLASSIFICATION	 tez 
WITHOUT_CLASSIFICATION	 generate the partition columns from the parent input 
WITHOUT_CLASSIFICATION	 test incorrect totals dont normalize just make sure dont under overshoot 
WITHOUT_CLASSIFICATION	 the table containing the partition not yet loaded cache 
WITHOUT_CLASSIFICATION	 collect the hiveconflist and hivevarlist separately that they can 
WITHOUT_CLASSIFICATION	 job properties are only relevant for nonnative tables for native tables leave null avoid cluttering 
WITHOUT_CLASSIFICATION	 output final result the aggregation 
WITHOUT_CLASSIFICATION	 username 
WITHOUT_CLASSIFICATION	 create some cover the result 
WITHOUT_CLASSIFICATION	 metastore always support concurrency but certain acid tests depend this being set 
WITHOUT_CLASSIFICATION	 verify droppartition recycle part files 
WITHOUT_CLASSIFICATION	 isrepeating and nulls 
WITHOUT_CLASSIFICATION	 move common logic that can reused 
WITHOUT_CLASSIFICATION	 the expression identify the partition dropped 
WITHOUT_CLASSIFICATION	 this method gets called only the scope that destination table already exists were validating the table appropriate destination import into 
WITHOUT_CLASSIFICATION	 with trimfalse parsing cannot handle spaces 
WITHOUT_CLASSIFICATION	 read all credentials into the credentials instance stored jobconf 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this the case when the big table subquery and probably already bucketed the join column say group operation 
WITHOUT_CLASSIFICATION	 the fractional digits are gone when rounding clear remaining round digits and add 
WITHOUT_CLASSIFICATION	 hive need add project top join since semijoin with join its right input 
WITHOUT_CLASSIFICATION	 varchar columns should have correct display sizeprecision 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 logger jobconf 
WITHOUT_CLASSIFICATION	 plan needs complete before execute and not modify while execution the driver 
WITHOUT_CLASSIFICATION	 well dont recurse but make sure all children are initialized 
WITHOUT_CLASSIFICATION	 overall information this vectorized map operation 
WITHOUT_CLASSIFICATION	 fetch namespace 
WITHOUT_CLASSIFICATION	 topn query results 
WITHOUT_CLASSIFICATION	 read the first batch the first batch itself was null close the reader 
WITHOUT_CLASSIFICATION	 set recursive reads for subdirectories 
WITHOUT_CLASSIFICATION	 returns immutable map with the identity count count 
WITHOUT_CLASSIFICATION	 used for statistics 
WITHOUT_CLASSIFICATION	 add the expression partition specification 
WITHOUT_CLASSIFICATION	 vvavacavbbca 
WITHOUT_CLASSIFICATION	 optionally the next values small length could integer the values information 
WITHOUT_CLASSIFICATION	 the set tablescanoperators for pruning trees 
WITHOUT_CLASSIFICATION	 native vectorization not supported 
WITHOUT_CLASSIFICATION	 establish mapping from the output column the input column 
WITHOUT_CLASSIFICATION	 left full outer join and the left record did not produce results need take that record replace the right side with null values and produce the records 
WITHOUT_CLASSIFICATION	 todo logging currently goes hivelog 
WITHOUT_CLASSIFICATION	 optional string vertexname 
WITHOUT_CLASSIFICATION	 read from arrow stream batchbybatch 
WITHOUT_CLASSIFICATION	 passes null configuration into the serde shouldnt fail immediately this case 
WITHOUT_CLASSIFICATION	 the bottom operator not synthetic and does not contain limit 
WITHOUT_CLASSIFICATION	 skip query hints 
WITHOUT_CLASSIFICATION	 the default value clean 
WITHOUT_CLASSIFICATION	 show column level privileges 
WITHOUT_CLASSIFICATION	 pattern remove the timestamp and other infrastructural info from the out file 
WITHOUT_CLASSIFICATION	 have decimal after enforce precision and scale will become null 
WITHOUT_CLASSIFICATION	 max can even when ndv larger clause than column stats 
WITHOUT_CLASSIFICATION	 authenticate using keytab 
WITHOUT_CLASSIFICATION	 continue read move the secondary 
WITHOUT_CLASSIFICATION	 build reduceside graph 
WITHOUT_CLASSIFICATION	 cant null cant null cant null 
WITHOUT_CLASSIFICATION	 intentional fall through 
WITHOUT_CLASSIFICATION	 normal deduplication 
WITHOUT_CLASSIFICATION	 copy table level hcat keys the partition 
WITHOUT_CLASSIFICATION	 again done want exit because logging issues 
WITHOUT_CLASSIFICATION	 timeout occurs 
WITHOUT_CLASSIFICATION	 replace this with valueof 
WITHOUT_CLASSIFICATION	 first merge all the adjacent bitvectors that could merge and derive new partition names and index 
WITHOUT_CLASSIFICATION	 this qtest lets order the params map lexicographically key this get consistent param ordering between java and java 
WITHOUT_CLASSIFICATION	 clean txntowriteid table for entries under minuncommittedtxn referred any open txns 
WITHOUT_CLASSIFICATION	 worst case when there are column statistics 
WITHOUT_CLASSIFICATION	 set isnull before call case changes mind 
WITHOUT_CLASSIFICATION	 setup for actual notifications not already done for previous task 
WITHOUT_CLASSIFICATION	 obtain second lock this shouldnt block cleaner was acquired after the initial 
WITHOUT_CLASSIFICATION	 expression therefore scan the whole table 
WITHOUT_CLASSIFICATION	 init and run are both potentially long and blocking operations synchronization with the abort operation will not work since they end blocking monitor which does not belong the lock the abort will end getting blocked both these method invocations need handle the abort call their own 
WITHOUT_CLASSIFICATION	 rethrow the exception ioexception 
WITHOUT_CLASSIFICATION	 build hash map from colname object for old columnstats 
WITHOUT_CLASSIFICATION	 first traverse the batch evaluate and prepare the keywrappers 
WITHOUT_CLASSIFICATION	 project 
WITHOUT_CLASSIFICATION	 batch batch 
WITHOUT_CLASSIFICATION	 serializes decimal the maximum bit precision decimal digits 
WITHOUT_CLASSIFICATION	 for the first grandkid replace the original parent 
WITHOUT_CLASSIFICATION	 get one the default separators avoid having set custom separator 
WITHOUT_CLASSIFICATION	 optional bool isguaranteed default false 
WITHOUT_CLASSIFICATION	 otherwise heuristics 
WITHOUT_CLASSIFICATION	 were folding multiple masked lines into one 
WITHOUT_CLASSIFICATION	 leftsemijoin found match and not have any additional predicates skipping the rest the rows the rhs table the semijoin 
WITHOUT_CLASSIFICATION	 zero reducers 
WITHOUT_CLASSIFICATION	 only the minimum cast for decimals other types are assumed safe fix needed also dont anything for nonprimitive children maybe should assert 
WITHOUT_CLASSIFICATION	 since timestamps are averaged with double dont need partial class and since timestamps are output double for avg dont need final class either vectorudafavgmerge partial vectorudafavgmerge final 
WITHOUT_CLASSIFICATION	 startedtime 
WITHOUT_CLASSIFICATION	 the branch represented the list 
WITHOUT_CLASSIFICATION	 case 
WITHOUT_CLASSIFICATION	 this time completes adding just foreign key constraints for table 
WITHOUT_CLASSIFICATION	 perform repldumpload 
WITHOUT_CLASSIFICATION	 seen the users 
WITHOUT_CLASSIFICATION	 open client session 
WITHOUT_CLASSIFICATION	 checking for null the forloop condition prevents nullptr exception and allows fail more gracefully with parsing error 
WITHOUT_CLASSIFICATION	 also the location field partition 
WITHOUT_CLASSIFICATION	 check that find all expected columns 
WITHOUT_CLASSIFICATION	 used based stats collector 
WITHOUT_CLASSIFICATION	 use full partition path for error case 
WITHOUT_CLASSIFICATION	 hiveexception expected 
WITHOUT_CLASSIFICATION	 create paritioned table 
WITHOUT_CLASSIFICATION	 default this will same that super class but need obtain again 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 finally try reuse with something the queue due fairness this wont work 
WITHOUT_CLASSIFICATION	 the following repeatedx values will set any the columns are repeating 
WITHOUT_CLASSIFICATION	 testlazyhbaseobject test for the lazyhbasexxx classes 
WITHOUT_CLASSIFICATION	 check the partitions exist the desttable 
WITHOUT_CLASSIFICATION	 extra value that can return while reading ahead 
WITHOUT_CLASSIFICATION	 this string constant will persisted metastore indicate whether corresponding 
WITHOUT_CLASSIFICATION	 tblpatterns 
WITHOUT_CLASSIFICATION	 mark the start the sync write sync update lastsyncpos 
WITHOUT_CLASSIFICATION	 perform major compaction there should extra base dir now 
WITHOUT_CLASSIFICATION	 defining partition names unsorted order 
WITHOUT_CLASSIFICATION	 find any referenced resources 
WITHOUT_CLASSIFICATION	 abstract method overridden for task execution 
WITHOUT_CLASSIFICATION	 contain the 
WITHOUT_CLASSIFICATION	 add keys this grouping set 
WITHOUT_CLASSIFICATION	 this map which vectorized row batch columns are the value columns 
WITHOUT_CLASSIFICATION	 simulate insert into partitions 
WITHOUT_CLASSIFICATION	 for last batch row group adjust the batch size 
WITHOUT_CLASSIFICATION	 yarntez job dont have the kerberos credentials anymore use the delegation token 
WITHOUT_CLASSIFICATION	 need loop here handle the case where consumer goes away 
WITHOUT_CLASSIFICATION	 that test doesnt block 
WITHOUT_CLASSIFICATION	 have deleterecordid currrecordidinbatch must now move find next the larger deleterecordid that can possibly match anything the batch 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 total blocks total elements the rowcontainer temporary file holding the spilled blocks 
WITHOUT_CLASSIFICATION	 value the configuration object 
WITHOUT_CLASSIFICATION	 the below group fields pools etc can only modified the master thread 
WITHOUT_CLASSIFICATION	 serdetype 
WITHOUT_CLASSIFICATION	 methods should really protected but some places have use this field 
WITHOUT_CLASSIFICATION	 the column map can not generated 
WITHOUT_CLASSIFICATION	 find the value matched column 
WITHOUT_CLASSIFICATION	 this used communicate over the not related tokens used talk llap daemons itself via the securit work 
WITHOUT_CLASSIFICATION	 note that recent metastore stores decimal string 
WITHOUT_CLASSIFICATION	 write out header for the payload 
WITHOUT_CLASSIFICATION	 timeseries query 
WITHOUT_CLASSIFICATION	 value columns 
WITHOUT_CLASSIFICATION	 ptf input that represents source the overall query this could table subquery ptf chain requires execution multiple ptf operators then the original invocation object decomposed into set component invocations every component invocation but the first one ends ptfqueryinputspec instance during the construction the operator plan ptfqueryinputspec object the chain implies connect the ptf operator the input has been generated far 
WITHOUT_CLASSIFICATION	 start hive server 
WITHOUT_CLASSIFICATION	 create mapworks and add them the sparkwork 
WITHOUT_CLASSIFICATION	 optional bool isguaranteed 
WITHOUT_CLASSIFICATION	 check noscan command 
WITHOUT_CLASSIFICATION	 not available nothing 
WITHOUT_CLASSIFICATION	 and use set remember which virtual columns were actually referenced 
WITHOUT_CLASSIFICATION	 schema evolution will insert the acid columns row schema for acid read 
WITHOUT_CLASSIFICATION	 verify that when have kerberos credentials pull the serialized token 
WITHOUT_CLASSIFICATION	 set output isrepeating true make sure gets overwritten similarly with nonulls 
WITHOUT_CLASSIFICATION	 foreigndbname 
WITHOUT_CLASSIFICATION	 same primitive category 
WITHOUT_CLASSIFICATION	 then need create metastore client that proxies that user 
WITHOUT_CLASSIFICATION	 user takes precendence over groups unless ordered explicitly 
WITHOUT_CLASSIFICATION	 optional optional optional 
WITHOUT_CLASSIFICATION	 equalscheck true and the inputoi the same the outputoi 
WITHOUT_CLASSIFICATION	 space usually 
WITHOUT_CLASSIFICATION	 since the udtf operator feeds into lvj operator that will rename all the internal names can just use field name from the udtfs the internal name 
WITHOUT_CLASSIFICATION	 validatecstr 
WITHOUT_CLASSIFICATION	 another quick path 
WITHOUT_CLASSIFICATION	 this pair defined the index build new mutation keyvalue cqrowkey cvcolumnvisibility value 
WITHOUT_CLASSIFICATION	 digit int all lowest decimal digit longword 
WITHOUT_CLASSIFICATION	 standard case 
WITHOUT_CLASSIFICATION	 pull the table schema out the split info 
WITHOUT_CLASSIFICATION	 despite string being primitive cant serialized binary 
WITHOUT_CLASSIFICATION	 reserve spaces for the byte size the list which integer and takes four bytes 
WITHOUT_CLASSIFICATION	 testing negative substring index start index should yield the last characters the string 
WITHOUT_CLASSIFICATION	 backwardforward compatible 
WITHOUT_CLASSIFICATION	 creating dummy table control the event truncate not 
WITHOUT_CLASSIFICATION	 ignore the preupgrade script errors 
WITHOUT_CLASSIFICATION	 over here should have some checks the deserialized object against the orginal object 
WITHOUT_CLASSIFICATION	 remove from src pool 
WITHOUT_CLASSIFICATION	 should error since exists 
WITHOUT_CLASSIFICATION	 for partitionless table initialize partvalue null 
WITHOUT_CLASSIFICATION	 insert some rows into table 
WITHOUT_CLASSIFICATION	 long they are still the same stream and are not already released 
WITHOUT_CLASSIFICATION	 invoke the outputformat entrypoint 
WITHOUT_CLASSIFICATION	 file name 
WITHOUT_CLASSIFICATION	 add added jars 
WITHOUT_CLASSIFICATION	 ideally want specify the different arguments updatelocation separate argnames however did that helpformatter swallows all but the last argument note that this know issue with the helpformatter class that has not been fixed specify all arguments with single argname workaround this helpformatter bug 
WITHOUT_CLASSIFICATION	 the context along 
WITHOUT_CLASSIFICATION	 case test with just originals single split strategy with two splits 
WITHOUT_CLASSIFICATION	 have found match insert this distinct clause head 
WITHOUT_CLASSIFICATION	 wait exception happens otherwise retry immediately 
WITHOUT_CLASSIFICATION	 for conditional task next task list should return the children tasks each task which contained the conditional task 
WITHOUT_CLASSIFICATION	 note the xff just way convert unsigned bytes signed integer 
WITHOUT_CLASSIFICATION	 read from hive test 
WITHOUT_CLASSIFICATION	 dont sync 
WITHOUT_CLASSIFICATION	 this duplicate invocation function dont add windowingspec 
WITHOUT_CLASSIFICATION	 objs 
WITHOUT_CLASSIFICATION	 used only for debugging testing purposes 
WITHOUT_CLASSIFICATION	 fields belong one the next entries 
WITHOUT_CLASSIFICATION	 repeating null 
WITHOUT_CLASSIFICATION	 the map should now empty 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 for each big tables bucket call the start forward 
WITHOUT_CLASSIFICATION	 normal case active session was removed from the pool session was restarted out bounds any userside handling should ignored 
WITHOUT_CLASSIFICATION	 txns table should have atleast one entry because just inserted the newly opened txns 
WITHOUT_CLASSIFICATION	 need set this only for replication tasks 
WITHOUT_CLASSIFICATION	 read count 
WITHOUT_CLASSIFICATION	 this will only get called once since compactrecordreader only returns one record the input split based the split were passed instantiate the real reader and then iterate until finishes since there way parametrize instance class 
WITHOUT_CLASSIFICATION	 merge join work 
WITHOUT_CLASSIFICATION	 set authorization mode 
WITHOUT_CLASSIFICATION	 fill forwardcache with skipvector 
WITHOUT_CLASSIFICATION	 begin conversion 
WITHOUT_CLASSIFICATION	 first take look any fieldschemas contain comma 
WITHOUT_CLASSIFICATION	 key 
WITHOUT_CLASSIFICATION	 make sure get correct number sessions each queue and that dont crash 
WITHOUT_CLASSIFICATION	 this path only potentially encountered during setup otherwise specific partxxxx file name generated and passed 
WITHOUT_CLASSIFICATION	 should not filter any object 
WITHOUT_CLASSIFICATION	 now tell launchmapper which files should add hadoopclasspath 
WITHOUT_CLASSIFICATION	 based the errormsg set hiveexception 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 data columns partition columns 
WITHOUT_CLASSIFICATION	 acquire txn batch 
WITHOUT_CLASSIFICATION	 all privilege expanded these not needed here 
WITHOUT_CLASSIFICATION	 hasnulls true then this array contains true the value null otherwise false the array always allocated batch can reused later and nulls added 
WITHOUT_CLASSIFICATION	 cache has found old buffer for the key and put into array instead our new one 
WITHOUT_CLASSIFICATION	 extract the record type 
WITHOUT_CLASSIFICATION	 initialize all children first 
WITHOUT_CLASSIFICATION	 this either the first batch weve used the current batch buffer 
WITHOUT_CLASSIFICATION	 register running task into the runningtasks structure 
WITHOUT_CLASSIFICATION	 normal close 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 decided treat this collection regular object 
WITHOUT_CLASSIFICATION	 clear the other ones 
WITHOUT_CLASSIFICATION	 named url the 
WITHOUT_CLASSIFICATION	 cannot abandon the attempt here the concurrent operations might have released all the buffer comprising our buddy block necessitating merge into higher list that may deadlock with another thread locking its own victims one can only take list locks separately moving down the alternative would release the free list lock before reserving however iterating the list that way difficult wed have keep track things the main path avoid retrying the same headers repeatedly wed rather keep track extra things failure 
WITHOUT_CLASSIFICATION	 metastore schema version different than hive distribution needs 
WITHOUT_CLASSIFICATION	 copied from errormsgjava 
WITHOUT_CLASSIFICATION	 mappings 
WITHOUT_CLASSIFICATION	 check stats are same need update 
WITHOUT_CLASSIFICATION	 task data structures have been initialized 
WITHOUT_CLASSIFICATION	 read via object store 
WITHOUT_CLASSIFICATION	 list cvalue list rowvalues assertequals cvaluesize 
WITHOUT_CLASSIFICATION	 invalid expression throw some exception but not incompatible metastore 
WITHOUT_CLASSIFICATION	 different sign just add the absolute values 
WITHOUT_CLASSIFICATION	 this set 
WITHOUT_CLASSIFICATION	 all good combine the baseoriginal only etl strategies 
WITHOUT_CLASSIFICATION	 partitions not match currently not merge 
WITHOUT_CLASSIFICATION	 split into digit middle and lowest longwords hand 
WITHOUT_CLASSIFICATION	 optional string appid 
WITHOUT_CLASSIFICATION	 perform minor compaction again this time will remove the subdir for aborted transaction 
WITHOUT_CLASSIFICATION	 need this for jackson work 
WITHOUT_CLASSIFICATION	 used for readfields 
WITHOUT_CLASSIFICATION	 create the mapping for this column with configured encoding 
WITHOUT_CLASSIFICATION	 optimize local fetch does not work with llap due different local directories used containers and llap 
WITHOUT_CLASSIFICATION	 drop table 
WITHOUT_CLASSIFICATION	 initiate cancel request cancel the thread execution and interrupt the thread thread interruption not handled jobexecutecallable then thread may continue running completion the cancel call may fail for some scenarios that case retry the cancel call until returns true max retry count reached param future future object which has handle cancel the thread 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 test that returns emptyset the filter selects partitions 
WITHOUT_CLASSIFICATION	 move clock backwards that allocation after allocation 
WITHOUT_CLASSIFICATION	 the partitions are unknown the call says due the expression evaluator returning null for partition sent partial expression 
WITHOUT_CLASSIFICATION	 check any operator had fatal error early exit during execution 
WITHOUT_CLASSIFICATION	 not cache this its child rdd intend cached 
WITHOUT_CLASSIFICATION	 get the aggregate function matching the name the query 
WITHOUT_CLASSIFICATION	 register that have visited this operator this rule 
WITHOUT_CLASSIFICATION	 only the hive catalog should cached 
WITHOUT_CLASSIFICATION	 store the config system properties 
WITHOUT_CLASSIFICATION	 call getvarcharmaxlength every deserialize call 
WITHOUT_CLASSIFICATION	 create the default white list from list safe config params and regex list 
WITHOUT_CLASSIFICATION	 stats values for col 
WITHOUT_CLASSIFICATION	 nested complex types trigger kryo issue plan deserialization 
WITHOUT_CLASSIFICATION	 the offset the first input does not need change 
WITHOUT_CLASSIFICATION	 partial spec 
WITHOUT_CLASSIFICATION	 number aliases 
WITHOUT_CLASSIFICATION	 this avoids extra serialization deserialization these objects 
WITHOUT_CLASSIFICATION	 poolpath 
WITHOUT_CLASSIFICATION	 the key portion the entry will the internal column name for the join key expression 
WITHOUT_CLASSIFICATION	 kill server 
WITHOUT_CLASSIFICATION	 its leaf add the move task child 
WITHOUT_CLASSIFICATION	 undone 
WITHOUT_CLASSIFICATION	 invoked for test methods 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 schema 
WITHOUT_CLASSIFICATION	 encountered partitioning column this will better handled metadataonly optimizer 
WITHOUT_CLASSIFICATION	 table may not found when materialization cte 
WITHOUT_CLASSIFICATION	 then lets check the one know about 
WITHOUT_CLASSIFICATION	 column type not specified use string 
WITHOUT_CLASSIFICATION	 returns the node currently the top the stack 
WITHOUT_CLASSIFICATION	 job request executor list job status requests 
WITHOUT_CLASSIFICATION	 stats values for col 
WITHOUT_CLASSIFICATION	 test mixed case 
WITHOUT_CLASSIFICATION	 set hashtable memory usage 
WITHOUT_CLASSIFICATION	 remove the locks didnt see dont look for them again next time 
WITHOUT_CLASSIFICATION	 now get all the onetomany things start with partitions 
WITHOUT_CLASSIFICATION	 are good since subquery top level expression 
WITHOUT_CLASSIFICATION	 get the following out the way when you start the session these take 
WITHOUT_CLASSIFICATION	 try read the dropped tbl via cachedstore should throw exception 
WITHOUT_CLASSIFICATION	 its not some operator pass back 
WITHOUT_CLASSIFICATION	 has been removed does not have child for instance semijoin can quickly skip this one 
WITHOUT_CLASSIFICATION	 wont metastoreside ppd for the things have locally 
WITHOUT_CLASSIFICATION	 sanity check that for map got encodings 
WITHOUT_CLASSIFICATION	 null means cannot wrap the cause logged inside 
WITHOUT_CLASSIFICATION	 see method comment 
WITHOUT_CLASSIFICATION	 invalid merge smaller register merge bigger 
WITHOUT_CLASSIFICATION	 alpha order please 
WITHOUT_CLASSIFICATION	 everything comes from cache 
WITHOUT_CLASSIFICATION	 this mapper class used for merge file work 
WITHOUT_CLASSIFICATION	 blocks until the async query complete 
WITHOUT_CLASSIFICATION	 see also the usage for binary for now only want text 
WITHOUT_CLASSIFICATION	 disabling rewriting removing from cache 
WITHOUT_CLASSIFICATION	 the column must aggregate column inserted gby dont have account for this column when computing product ndvs 
WITHOUT_CLASSIFICATION	 choose cumulative 
WITHOUT_CLASSIFICATION	 export works file level you have copyn the table dir youll have those output 
WITHOUT_CLASSIFICATION	 change the filter condition into join condition 
WITHOUT_CLASSIFICATION	 can only happen theres evictor thread interrupted 
WITHOUT_CLASSIFICATION	 set null information the small table results area 
WITHOUT_CLASSIFICATION	 multiply make room for sign bit 
WITHOUT_CLASSIFICATION	 there maybe more than splits the group however they all have unique path assert that 
WITHOUT_CLASSIFICATION	 check individual elements subrecord 
WITHOUT_CLASSIFICATION	 process the batch 
WITHOUT_CLASSIFICATION	 left repeats 
WITHOUT_CLASSIFICATION	 check the contents the file 
WITHOUT_CLASSIFICATION	 now hivedecimal 
WITHOUT_CLASSIFICATION	 start offset each field 
WITHOUT_CLASSIFICATION	 string object 
WITHOUT_CLASSIFICATION	 inject behavior where repl load failed when try load table and partition 
WITHOUT_CLASSIFICATION	 grammar prohibits more than column are guaranteed have only element this lists 
WITHOUT_CLASSIFICATION	 the node cannot accept task the moment 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 most likely this means its temp table 
WITHOUT_CLASSIFICATION	 set double data vector array entries for null elements the correct value unlike other colscalar operations this one doesnt benefit from carrying over nan values from the input array 
WITHOUT_CLASSIFICATION	 field 
WITHOUT_CLASSIFICATION	 theres lock manager essentially means didnt acquire locks the first place thus need release locks 
WITHOUT_CLASSIFICATION	 for now not limit this one per split 
WITHOUT_CLASSIFICATION	 detect there are multiple attributes join key 
WITHOUT_CLASSIFICATION	 not call mqgetrowcountjoin will trigger 
WITHOUT_CLASSIFICATION	 used kyro 
WITHOUT_CLASSIFICATION	 there was preexisting work generated for the bigtable mapjoin side need hook the work generated for the associated with the rsmj pattern with the preexisting work otherwise need associate that the mapjoin linked the work associated with the rsmj pattern 
WITHOUT_CLASSIFICATION	 recursively remove any its parent who only have this child 
WITHOUT_CLASSIFICATION	 first remove all the grants 
WITHOUT_CLASSIFICATION	 lets make sure only read the relevant part the writable case reuse 
WITHOUT_CLASSIFICATION	 load them into setlong 
WITHOUT_CLASSIFICATION	 todo cache the information from the metastore 
WITHOUT_CLASSIFICATION	 pass removing invalid candidates misses far exceed max tolerable misses 
WITHOUT_CLASSIFICATION	 note the null array indexed keyindex which not available internally the mapping from long double etc index key index kept once the separate vectorcolumnsetinfo object 
WITHOUT_CLASSIFICATION	 mrinput not interest since itll always ready 
WITHOUT_CLASSIFICATION	 out the request provided that its signed 
WITHOUT_CLASSIFICATION	 rule searching for dynamic pruning expr theres least expression wrapping 
WITHOUT_CLASSIFICATION	 expecting not change the size internal structures 
WITHOUT_CLASSIFICATION	 adds the taskid the fspkey 
WITHOUT_CLASSIFICATION	 check bucketing both was done the same way 
WITHOUT_CLASSIFICATION	 bare 
WITHOUT_CLASSIFICATION	 the right input correlator should produce correlated variables 
WITHOUT_CLASSIFICATION	 letter byte latin capital with grave bytes 
WITHOUT_CLASSIFICATION	 throw new null fschema fschema pigschema tableschema pigschema tableschema 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 ensure counters are set when data has actually been read 
WITHOUT_CLASSIFICATION	 this here normally communicator does this 
WITHOUT_CLASSIFICATION	 follow the reducesink operator upstream which small table side 
WITHOUT_CLASSIFICATION	 try the extremes precision and scale 
WITHOUT_CLASSIFICATION	 parse and initialize the hbase columns mapping 
WITHOUT_CLASSIFICATION	 filters for pushing 
WITHOUT_CLASSIFICATION	 needed columns 
WITHOUT_CLASSIFICATION	 the name the functiontable 
WITHOUT_CLASSIFICATION	 set register value and compute inverse pow for register value 
WITHOUT_CLASSIFICATION	 bgenjjtree senum 
WITHOUT_CLASSIFICATION	 note setting these separately very hairy issue certain combinations since cannot decide what type table this becomes without taking both into account and many cases the conversion might illegal the only thing allow true txprops for backward compat 
WITHOUT_CLASSIFICATION	 sum small tables size this join exceeds configured limit hence cannot convert 
WITHOUT_CLASSIFICATION	 partnames 
WITHOUT_CLASSIFICATION	 the object does not exist want add 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 lookup the specified type and set this nodes type precludes forward and self references for now 
WITHOUT_CLASSIFICATION	 retain the original join desc the map join 
WITHOUT_CLASSIFICATION	 currently not support ptf operator 
WITHOUT_CLASSIFICATION	 get list selected column ids 
WITHOUT_CLASSIFICATION	 not supported 
WITHOUT_CLASSIFICATION	 this method assumes that the list has null entries that enforced elsewhere the vectorizer class null passed list entry behavior not defined the future null values are allowed the list sure handle valued logic correctly not col null should considered unknown that would become false the where clause and cause the row question filtered out see the discussion jira hive 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 round even 
WITHOUT_CLASSIFICATION	 parse the first byte vintvlong determine the number bytes 
WITHOUT_CLASSIFICATION	 sql comment prefix beeline also supports shellstyle prefix 
WITHOUT_CLASSIFICATION	 write datum out stream 
WITHOUT_CLASSIFICATION	 this optimize queries the form select countdistinct key from where sorted and bucketized key partial aggregation performed the mapper and the reducer gets row partial result per mapper 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 what position the mapjoin the different parent work items will have 
WITHOUT_CLASSIFICATION	 hopefully this will helpful case npes 
WITHOUT_CLASSIFICATION	 partialsum 
WITHOUT_CLASSIFICATION	 start concurrent testing 
WITHOUT_CLASSIFICATION	 bgenjjtree definitiontype 
WITHOUT_CLASSIFICATION	 merge nomatchs and match selected 
WITHOUT_CLASSIFICATION	 not 
WITHOUT_CLASSIFICATION	 expecting only single instance task running 
WITHOUT_CLASSIFICATION	 both categories are primitive return the comparison type names 
WITHOUT_CLASSIFICATION	 null out some row column entries undone 
WITHOUT_CLASSIFICATION	 should this group converted mapside group because the grouping keys for 
WITHOUT_CLASSIFICATION	 the objectinspector for array and map expects extra layer 
WITHOUT_CLASSIFICATION	 cancolumnstatsmerge guarantees that accurate before merge 
WITHOUT_CLASSIFICATION	 make comparisons work properly the factor gets the decimals sign too 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 shortminvalue 
WITHOUT_CLASSIFICATION	 now have base file 
WITHOUT_CLASSIFICATION	 key databasetablespdplb hive store lowercase table name metastore and counters character case sensitive 
WITHOUT_CLASSIFICATION	 make copy currentmetavars there race condition that currentmetavars might changed during the execution the method 
WITHOUT_CLASSIFICATION	 write second nonnull element 
WITHOUT_CLASSIFICATION	 stats from reader 
WITHOUT_CLASSIFICATION	 catalog 
WITHOUT_CLASSIFICATION	 parentdbname 
WITHOUT_CLASSIFICATION	 ssl enabled override the given value hadooprpcprotection and set authentication this disables any encryption provided sasl since ssl already provides 
WITHOUT_CLASSIFICATION	 can immediately reuse session theres nothing wait for just return 
WITHOUT_CLASSIFICATION	 get column statistics for all output columns 
WITHOUT_CLASSIFICATION	 test drop view 
WITHOUT_CLASSIFICATION	 grouping sets need transform them into immutablebitset objects for calcite 
WITHOUT_CLASSIFICATION	 try temporarily adding the parent 
WITHOUT_CLASSIFICATION	 will call abort 
WITHOUT_CLASSIFICATION	 create the directory 
WITHOUT_CLASSIFICATION	 field 
WITHOUT_CLASSIFICATION	 parent keys are null empty bail out 
WITHOUT_CLASSIFICATION	 grantortype 
WITHOUT_CLASSIFICATION	 add row chain except case unb preceding only firstval needs tracked 
WITHOUT_CLASSIFICATION	 forward arg forward arg workdir llapdaemonsite llapdaemonsite forward via configjson forward arg used localize jars used localize jars used localize jars forward via configjson llapdaemonsite relevant parameter forward arg forward arg forward via configjson llapdaemonsite 
WITHOUT_CLASSIFICATION	 type name should already set subclass 
WITHOUT_CLASSIFICATION	 mysql can use intn 
WITHOUT_CLASSIFICATION	 nway first small table 
WITHOUT_CLASSIFICATION	 field 
WITHOUT_CLASSIFICATION	 only returns subset per call 
WITHOUT_CLASSIFICATION	 for computing the autogenerated field ids thrift 
WITHOUT_CLASSIFICATION	 todo later may have map 
WITHOUT_CLASSIFICATION	 containers are not being tracked for reuse this safe ignore since deallocate task will come 
WITHOUT_CLASSIFICATION	 field 
WITHOUT_CLASSIFICATION	 hive move this fallback logic cliconfigs 
WITHOUT_CLASSIFICATION	 enable cache and use default strategy 
WITHOUT_CLASSIFICATION	 calcite literal millis convert seconds 
WITHOUT_CLASSIFICATION	 the order the fields the lazybinary small table value must used 
WITHOUT_CLASSIFICATION	 data structures coming from qbjointree 
WITHOUT_CLASSIFICATION	 get appropriate object from the string representation the value 
WITHOUT_CLASSIFICATION	 did not see skew key this table continue next table are trying avoid extra call filesystemexists 
WITHOUT_CLASSIFICATION	 single long value hash map based the serialize the long key into binarysortable format into output buffer accepted 
WITHOUT_CLASSIFICATION	 insert reduceside 
WITHOUT_CLASSIFICATION	 dimension 
WITHOUT_CLASSIFICATION	 the schemaevolution class has added the acid metadata columns lets update our readertypes ppd code will work correctly 
WITHOUT_CLASSIFICATION	 for there rankdenserank function there are unpushedpred the form rnkvalue constant then use the smallest constant val the ranklimit the windowingtablfn there are wdw fns with end boundary past the current row the condition can pushed down limit nonjavadoc see javautilstack javalangobject 
WITHOUT_CLASSIFICATION	 optional int 
WITHOUT_CLASSIFICATION	 prepends partition spec input path candidate file name 
WITHOUT_CLASSIFICATION	 setup tablescan 
WITHOUT_CLASSIFICATION	 add the current constant struct the right hand side the clause 
WITHOUT_CLASSIFICATION	 define the expected schema 
WITHOUT_CLASSIFICATION	 count number true values seen far 
WITHOUT_CLASSIFICATION	 construct case expression handle the null indicator this also covers the case where left correlated subquery projects fields from outer relation since loj cannot produce nulls the lhs the projection now need make nullable lhs reference using nullability indicator this this indicator null means the subquery does not produce any value result any rhs ref this usbquery needs produce null value 
WITHOUT_CLASSIFICATION	 there need add colname again otherwise will get duplicate colnames 
WITHOUT_CLASSIFICATION	 this has called before initializing the instance hmshandler using the hook startup ensures that the hook always has priority over settings xml the thread local conf needs used because this point 
WITHOUT_CLASSIFICATION	 required input format 
WITHOUT_CLASSIFICATION	 corresponding struct fields the same time 
WITHOUT_CLASSIFICATION	 end udfrowsequencejava 
WITHOUT_CLASSIFICATION	 map priv being granted required privileges 
WITHOUT_CLASSIFICATION	 this filter has correlated reference create value generator 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 add whether the row filtered not 
WITHOUT_CLASSIFICATION	 bail there are any changes note that dont care about aba here all the stuff the left has been taken out already noone can touch and all the stuff the right yet seen dont care they changed with this its the same free list the processing sequence will remain the same going right 
WITHOUT_CLASSIFICATION	 there are more active client sessions stop the server 
WITHOUT_CLASSIFICATION	 destination memory this the only partition memory proceed without check destination partition being empty indicates write buffer will allocated thus need check memory full check periodically 
WITHOUT_CLASSIFICATION	 the url does not have database name add the trailing 
WITHOUT_CLASSIFICATION	 testspecific delay just before the check happens 
WITHOUT_CLASSIFICATION	 tabletypes 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring 
WITHOUT_CLASSIFICATION	 fetch all the hints 
WITHOUT_CLASSIFICATION	 set alias fetch work 
WITHOUT_CLASSIFICATION	 dynamic partition replace input path root paths with dynamic partition 
WITHOUT_CLASSIFICATION	 dont bother validating 
WITHOUT_CLASSIFICATION	 create list top nodes 
WITHOUT_CLASSIFICATION	 check the context matches 
WITHOUT_CLASSIFICATION	 user privileges testdb testtable testtable testtable testtable testdb 
WITHOUT_CLASSIFICATION	 make sure minihs closes all its connections 
WITHOUT_CLASSIFICATION	 already copied successfully ignore 
WITHOUT_CLASSIFICATION	 string allaliases 
WITHOUT_CLASSIFICATION	 all stats variables are visible for testing 
WITHOUT_CLASSIFICATION	 return the errors that occur the most frequently 
WITHOUT_CLASSIFICATION	 target compression block the middle the range slice the range two 
WITHOUT_CLASSIFICATION	 limit length chars 
WITHOUT_CLASSIFICATION	 need wait the last iteration 
WITHOUT_CLASSIFICATION	 map keep track which child generated with work 
WITHOUT_CLASSIFICATION	 anythingelse foo 
WITHOUT_CLASSIFICATION	 show tables should faster than that 
WITHOUT_CLASSIFICATION	 update with new values 
WITHOUT_CLASSIFICATION	 this task was added preemption list remove 
WITHOUT_CLASSIFICATION	 varchar not between 
WITHOUT_CLASSIFICATION	 return single arraylist where the first element the number bins bins and subsequent elements represent bins pairs 
WITHOUT_CLASSIFICATION	 set the results 
WITHOUT_CLASSIFICATION	 are not allowed lose digits multiply compatible with oldhivedecimal behavior overflow consider does make sense restrictive just did repeated addition would succeed 
WITHOUT_CLASSIFICATION	 forwarded the batch this method 
WITHOUT_CLASSIFICATION	 test serialization 
WITHOUT_CLASSIFICATION	 handle both file and jarurlentry the case shaded hive libs 
WITHOUT_CLASSIFICATION	 and the operator dot then its table column reference 
WITHOUT_CLASSIFICATION	 max disabled can safely return true 
WITHOUT_CLASSIFICATION	 append new config params whitelist 
WITHOUT_CLASSIFICATION	 happens when unexpected failure occurs getattribute for example 
WITHOUT_CLASSIFICATION	 cvalue list rowvalues assertequals cvaluesize listval list cvalueget assertequals listvalsize mapval map listvalget assertequals mapvalsize listvalget listval list cvalueget mapval map listvalget assertequals mapvalsize assertequalsb mapvalgeta assertequalsd mapvalgetc listvalget 
WITHOUT_CLASSIFICATION	 does not need actual time just nonzero distinct value test against 
WITHOUT_CLASSIFICATION	 both blockedby are either there not 
WITHOUT_CLASSIFICATION	 dynamic part vals specified 
WITHOUT_CLASSIFICATION	 get the app report from yarn 
WITHOUT_CLASSIFICATION	 the case where agg countcorvar changed countnullindicator note any nonnullable field from the rhs can used the indicator however true field added the projection list from the rhs for simplicity avoid searching for nonnull fields projecta all gby keys rewritten nullable projexpr aggregate groupbyall left input refs countnullindicator other aggs projectb all left input refs plus the rewritten original projected exprs joinreplace corvar input ref from leftinputrel leftinputrel project everything from rightinputrel plus the nullindicator true rightinputrel 
WITHOUT_CLASSIFICATION	 lastheartbeattime 
WITHOUT_CLASSIFICATION	 must produce the same result murmurhashhash with seed 
WITHOUT_CLASSIFICATION	 the first group 
WITHOUT_CLASSIFICATION	 insert creates copy 
WITHOUT_CLASSIFICATION	 instantiate the metastore server handler directly instead connecting through the network 
WITHOUT_CLASSIFICATION	 recursive 
WITHOUT_CLASSIFICATION	 have finished tree walking correlation detection will first see need abort the operator tree has not been changed 
WITHOUT_CLASSIFICATION	 the caller probably created the new session with the old config but update the 
WITHOUT_CLASSIFICATION	 authorize against the table operation that location permissions can checked any 
WITHOUT_CLASSIFICATION	 path path throws ioexception 
WITHOUT_CLASSIFICATION	 this needed prevent the hikaridatasource from trying connect the 
WITHOUT_CLASSIFICATION	 set data location and input format must text 
WITHOUT_CLASSIFICATION	 pending change boolean 
WITHOUT_CLASSIFICATION	 convert the work containing sortmerge join into work had regular join note that the operator tree not changed still contains the smb join but the plan changed aliastowork etc contain all the paths was regular join 
WITHOUT_CLASSIFICATION	 bucket 
WITHOUT_CLASSIFICATION	 need discard the buffer cannot lock eviction takes care that 
WITHOUT_CLASSIFICATION	 increment the min txn that heartbeat thread will heartbeat only from the next open transaction the current transaction going committed fail dont need heartbeat for current transaction 
WITHOUT_CLASSIFICATION	 data left current page load data from new page 
WITHOUT_CLASSIFICATION	 object equality issame means that the objects are semantically equal 
WITHOUT_CLASSIFICATION	 currentinputfile will updated only inputfilechanged inputfilechanged not called throughout the operator tree currentinputpath wont used anyways 
WITHOUT_CLASSIFICATION	 destination partition any the intermediate destination directory the final destination directory 
WITHOUT_CLASSIFICATION	 the left child not join multijoin operator 
WITHOUT_CLASSIFICATION	 must read through fields not want 
WITHOUT_CLASSIFICATION	 ensure partinfos tableinfo initialized 
WITHOUT_CLASSIFICATION	 create subquery 
WITHOUT_CLASSIFICATION	 returns true statement represented line not complete and needs additional reading from console used handlemultilinecmd method 
WITHOUT_CLASSIFICATION	 can only start once fragment has completed the map should clear this point 
WITHOUT_CLASSIFICATION	 todo use for now till dynamicserde ready 
WITHOUT_CLASSIFICATION	 same responsibility initializeoi but for the rawinput 
WITHOUT_CLASSIFICATION	 hive depends filesplits 
WITHOUT_CLASSIFICATION	 important note for multiand the class will catch cases with more parameters 
WITHOUT_CLASSIFICATION	 initialization isnt finished until all parents all operators are initialized for broadcast joins that means initializing the dummy parent operators well 
WITHOUT_CLASSIFICATION	 mapjoin later 
WITHOUT_CLASSIFICATION	 newstate columnstatsstate complete partial none complete complete partial partial partial partial partial partial none complete partial none 
WITHOUT_CLASSIFICATION	 case theres delay for the heartbeat but the delay within the reapers tolerance then txt should able commit 
WITHOUT_CLASSIFICATION	 the number data columns that the current reader will return only applicable for vectorrow deserialization 
WITHOUT_CLASSIFICATION	 initialize with data type conversion parameters 
WITHOUT_CLASSIFICATION	 validation required 
WITHOUT_CLASSIFICATION	 trigger scheduling since new node became available 
WITHOUT_CLASSIFICATION	 there nothing change 
WITHOUT_CLASSIFICATION	 invalid values 
WITHOUT_CLASSIFICATION	 devenagari sign virama bytes 
WITHOUT_CLASSIFICATION	 compare the mapper get offset method the list mappings 
WITHOUT_CLASSIFICATION	 store the previous value for the path specification 
WITHOUT_CLASSIFICATION	 all good such partition exists move 
WITHOUT_CLASSIFICATION	 posmap unfortunate consequence batchingiterating thru results 
WITHOUT_CLASSIFICATION	 case viewfs need lookup where the actual file know the filesystem use resolvepath sure shot way knowing which file system the file 
WITHOUT_CLASSIFICATION	 construct setop rel 
WITHOUT_CLASSIFICATION	 localtmppath the root all the stats under there will selstatsfiles selstatsfiles etc where sel and sel are the ids 
WITHOUT_CLASSIFICATION	 apply schema evolution adding some columns 
WITHOUT_CLASSIFICATION	 more data 
WITHOUT_CLASSIFICATION	 roleprivileges 
WITHOUT_CLASSIFICATION	 format clustered statement 
WITHOUT_CLASSIFICATION	 drop one function see what remains 
WITHOUT_CLASSIFICATION	 compute seconds first subtract the milliseconds stored the nanos field the timestamp from the result gettime 
WITHOUT_CLASSIFICATION	 avoid allocating temporary variables for special cases signum scale zero 
WITHOUT_CLASSIFICATION	 dont expect missing buckets from mere actually there should buckets just pass null bucketing context union suffix should also accounted for 
WITHOUT_CLASSIFICATION	 optional queryidentifier 
WITHOUT_CLASSIFICATION	 tablesused 
WITHOUT_CLASSIFICATION	 there should one argument that array struct 
WITHOUT_CLASSIFICATION	 the join filters out the nulls its there are 
WITHOUT_CLASSIFICATION	 convert seconds milliseconds 
WITHOUT_CLASSIFICATION	 native vectorization not supported 
WITHOUT_CLASSIFICATION	 loop through all the inputs determine the appropriate return typelength return type all char inputs return char all varchar inputs return varchar all charvarchar inputs return varchar all binary inputs return binary otherwise return string 
WITHOUT_CLASSIFICATION	 error checking later and detect just dot 
WITHOUT_CLASSIFICATION	 explicitly close zkdatabase since zookeeperserver does not close them 
WITHOUT_CLASSIFICATION	 required for insertion into treemap 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 there can never more concurrent takers than uncommitted ones 
WITHOUT_CLASSIFICATION	 also add settings clusterspecificconf make sure these get picked whoever started this 
WITHOUT_CLASSIFICATION	 any preexisting datanucleus property should passed along 
WITHOUT_CLASSIFICATION	 struct value simply list values the schema can used map the field name the position the list 
WITHOUT_CLASSIFICATION	 intwritable can just sum the reduce 
WITHOUT_CLASSIFICATION	 walk over the input row resolver and copy the output 
WITHOUT_CLASSIFICATION	 tokresourceuri restype respath 
WITHOUT_CLASSIFICATION	 hive command operations ends here 
WITHOUT_CLASSIFICATION	 can pretty sure that entire line can processed single command since always add line separator the end while calling 
WITHOUT_CLASSIFICATION	 prevent construction outside the get method 
WITHOUT_CLASSIFICATION	 note the sync marker seen the header 
WITHOUT_CLASSIFICATION	 data was moved from original location cache directory need move back 
WITHOUT_CLASSIFICATION	 now add the tables and columns from the current connection 
WITHOUT_CLASSIFICATION	 not block each other since they are part the same txn 
WITHOUT_CLASSIFICATION	 there current unread chunk read from that else get the next chunk 
WITHOUT_CLASSIFICATION	 deserialize split 
WITHOUT_CLASSIFICATION	 multiple means lookup 
WITHOUT_CLASSIFICATION	 make sure there subquery top level expression 
WITHOUT_CLASSIFICATION	 use existing location 
WITHOUT_CLASSIFICATION	 this where set the sort columns that will using for keyvalueinputmerge 
WITHOUT_CLASSIFICATION	 cache helper data for deserialization could avoid having 
WITHOUT_CLASSIFICATION	 whatever have 
WITHOUT_CLASSIFICATION	 final long roundmultiplyfactor absroundpower 
WITHOUT_CLASSIFICATION	 reset data container prevent being added again 
WITHOUT_CLASSIFICATION	 not lot you can here 
WITHOUT_CLASSIFICATION	 are dealing with bag tuple column need worry about subschema 
WITHOUT_CLASSIFICATION	 are forcing the usage vectorudfadaptor for test purposes 
WITHOUT_CLASSIFICATION	 update field collations 
WITHOUT_CLASSIFICATION	 regex the form column name following characters are not allowed column name 
WITHOUT_CLASSIFICATION	 for nonvectorized operator case wrap the reader possible 
WITHOUT_CLASSIFICATION	 string storage type overrides table level default binary storage 
WITHOUT_CLASSIFICATION	 alas crossed some dst boundary the time day doesnt matter the caller well 
WITHOUT_CLASSIFICATION	 nulls 
WITHOUT_CLASSIFICATION	 clear jointree and parse context 
WITHOUT_CLASSIFICATION	 might generate select operator top the join operator for semijoin 
WITHOUT_CLASSIFICATION	 serialize path offset length using filesplit 
WITHOUT_CLASSIFICATION	 big case write the length vint and then the value bytes 
WITHOUT_CLASSIFICATION	 lets validate that the serde exists 
WITHOUT_CLASSIFICATION	 find the immediate parent possible for for query like select from where implies depends parent would not check last alias the array for parent can not itself 
WITHOUT_CLASSIFICATION	 note tablealias must valid nonambiguous table alias because weve checked that toktableorcols process method 
WITHOUT_CLASSIFICATION	 compute the keys 
WITHOUT_CLASSIFICATION	 cant use toarray here because java dumb when comes generics arrays 
WITHOUT_CLASSIFICATION	 initialize using data type names projection the column range typessize 
WITHOUT_CLASSIFICATION	 user specified row limit set the query 
WITHOUT_CLASSIFICATION	 whether this vertex dummy which does not really exists but created 
WITHOUT_CLASSIFICATION	 the set join operators which can converted bucketed map join 
WITHOUT_CLASSIFICATION	 simulate update partitions depending causeconflict choose one the partitions 
WITHOUT_CLASSIFICATION	 replace column references checkexprast with corresponding columns input 
WITHOUT_CLASSIFICATION	 utc has such adjustment 
WITHOUT_CLASSIFICATION	 the first input correlator always the rel defining 
WITHOUT_CLASSIFICATION	 tests not setting maxrows return all tests setting maxrows return all 
WITHOUT_CLASSIFICATION	 nothing bail out 
WITHOUT_CLASSIFICATION	 nothing there operator tree associated with sourcealias source there not operator tree associated with targetalias target 
WITHOUT_CLASSIFICATION	 events for the source colums tasks 
WITHOUT_CLASSIFICATION	 one the params null then expected null 
WITHOUT_CLASSIFICATION	 can have multiple branches due dpp semijoin opt 
WITHOUT_CLASSIFICATION	 throw exception the user trying truncate column which doesnt exist 
WITHOUT_CLASSIFICATION	 use the object already have 
WITHOUT_CLASSIFICATION	 this new key keep writing the first record 
WITHOUT_CLASSIFICATION	 since this inside delta dir created hive earlier can only contain bucketx bucketxflushlength 
WITHOUT_CLASSIFICATION	 add the archive file distributed cache 
WITHOUT_CLASSIFICATION	 new table 
WITHOUT_CLASSIFICATION	 intermediate key anything between key and the following 
WITHOUT_CLASSIFICATION	 user specified fraction always takes precedence 
WITHOUT_CLASSIFICATION	 clone the column stats and return 
WITHOUT_CLASSIFICATION	 replacement allowed the existing table older than event 
WITHOUT_CLASSIFICATION	 now add the projuniquekeyset the child keys that are fully projected 
WITHOUT_CLASSIFICATION	 the gby key constant 
WITHOUT_CLASSIFICATION	 limit the number threads that can launched 
WITHOUT_CLASSIFICATION	 all bigtable input columns key expressions are isrepeating then 
WITHOUT_CLASSIFICATION	 substitution option define 
WITHOUT_CLASSIFICATION	 make sure metastore doesnt mess with our bogus stats updates 
WITHOUT_CLASSIFICATION	 enforce minimum precision factor 
WITHOUT_CLASSIFICATION	 just examine the lower word 
WITHOUT_CLASSIFICATION	 position the single native vector map join small table 
WITHOUT_CLASSIFICATION	 test that database and table dont coalesce 
WITHOUT_CLASSIFICATION	 not generated cookie continue 
WITHOUT_CLASSIFICATION	 serialize the value 
WITHOUT_CLASSIFICATION	 urldecoder misnamed class since actually decodes xwwwformurlencoded mime type rather than actual url encoding which the file path has therefore would decode which incorrect spaces are actually either unencoded encoded replace first that they are kept sacred during the decoding process 
WITHOUT_CLASSIFICATION	 granularity partition column 
WITHOUT_CLASSIFICATION	 bucket should small and bucket should large make sure thats the case 
WITHOUT_CLASSIFICATION	 list leaves that werent under and expressions 
WITHOUT_CLASSIFICATION	 note due tez the session may actually invalid case some errors currently reopen attempted reuse will take care that cannot tell the session usable until try return this the pool even its unusable reopen supposed handle this 
WITHOUT_CLASSIFICATION	 udf which sleeps for simulate long running query 
WITHOUT_CLASSIFICATION	 pass 
WITHOUT_CLASSIFICATION	 just return 
WITHOUT_CLASSIFICATION	 update the state removedfromlist that parallel notifyunlock doesnt modify 
WITHOUT_CLASSIFICATION	 need input object inspector that for the row will extract out the vectorized row batch not for example original inspector for orc table etc 
WITHOUT_CLASSIFICATION	 since subtraction not commutative can must subtract the order passed 
WITHOUT_CLASSIFICATION	 set tez execution summary false 
WITHOUT_CLASSIFICATION	 length file count directory count 
WITHOUT_CLASSIFICATION	 the column partition column skip the optimization 
WITHOUT_CLASSIFICATION	 rights signum wins notice the negation because are subtracting right 
WITHOUT_CLASSIFICATION	 failover workeridentity 
WITHOUT_CLASSIFICATION	 create tables 
WITHOUT_CLASSIFICATION	 best effort 
WITHOUT_CLASSIFICATION	 skip the counting the values are the same for windowing countdistinct case 
WITHOUT_CLASSIFICATION	 there only one blank utf 
WITHOUT_CLASSIFICATION	 get the tables for the desired pattern populate the output stream 
WITHOUT_CLASSIFICATION	 this may change after every setmapjoinkey call 
WITHOUT_CLASSIFICATION	 return static variable with results set some set values 
WITHOUT_CLASSIFICATION	 optional int physicaledgecount 
WITHOUT_CLASSIFICATION	 verify mergeandmovetask not optimized 
WITHOUT_CLASSIFICATION	 this was added during plan generation 
WITHOUT_CLASSIFICATION	 group distribute sort cluster 
WITHOUT_CLASSIFICATION	 race protection 
WITHOUT_CLASSIFICATION	 ready for cleaning state this case 
WITHOUT_CLASSIFICATION	 simple trims 
WITHOUT_CLASSIFICATION	 not split 
WITHOUT_CLASSIFICATION	 the last key column the dummy grouping set figure out which scratch column was used can overwrite the dummy 
WITHOUT_CLASSIFICATION	 nonencrypted path equals strength 
WITHOUT_CLASSIFICATION	 script file 
WITHOUT_CLASSIFICATION	 ignore any predicates partition columns because have already accounted for these the table row count 
WITHOUT_CLASSIFICATION	 thread name for reporter thread 
WITHOUT_CLASSIFICATION	 not overwrite there are duplicate keys 
WITHOUT_CLASSIFICATION	 partition keys can not set but added one remove for 
WITHOUT_CLASSIFICATION	 note this may just block wait for session based parallelism 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 because append the prefix when serializing the column 
WITHOUT_CLASSIFICATION	 stats values for col 
WITHOUT_CLASSIFICATION	 set the client 
WITHOUT_CLASSIFICATION	 stats the big input 
WITHOUT_CLASSIFICATION	 noop for default output file 
WITHOUT_CLASSIFICATION	 generate reducesink operator for the join 
WITHOUT_CLASSIFICATION	 enables orc ppd create delta should push predicate here 
WITHOUT_CLASSIFICATION	 string chars long byte and byte char 
WITHOUT_CLASSIFICATION	 merge stats from cache with metastore cache 
WITHOUT_CLASSIFICATION	 unique keys for this test 
WITHOUT_CLASSIFICATION	 protected boolean useminmax 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 remove the candidate filter ops 
WITHOUT_CLASSIFICATION	 dictates which operator allowed 
WITHOUT_CLASSIFICATION	 buffer already allocated keep using dont reallocate 
WITHOUT_CLASSIFICATION	 finally put the ranges list for future use shared between rgs 
WITHOUT_CLASSIFICATION	 isatty system call will return the file descriptor terminal else 
WITHOUT_CLASSIFICATION	 add shutdown hook flush the history history file and also close all open connections 
WITHOUT_CLASSIFICATION	 get the actual converted schema 
WITHOUT_CLASSIFICATION	 test failed 
WITHOUT_CLASSIFICATION	 fastisshort returns false 
WITHOUT_CLASSIFICATION	 dynamic value which will determined during query runtime 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this should not vectorize all 
WITHOUT_CLASSIFICATION	 nothing localize 
WITHOUT_CLASSIFICATION	 restore the hashmap from disk deserializing currently kryo used for this purpose 
WITHOUT_CLASSIFICATION	 has nulls repeating 
WITHOUT_CLASSIFICATION	 ignore the keys which are local source warehouse 
WITHOUT_CLASSIFICATION	 check grpset require additional job 
WITHOUT_CLASSIFICATION	 char 
WITHOUT_CLASSIFICATION	 rows 
WITHOUT_CLASSIFICATION	 the configured owner does not own the file throw 
WITHOUT_CLASSIFICATION	 optional int vertexindex 
WITHOUT_CLASSIFICATION	 called set the appropriate input format for tasks 
WITHOUT_CLASSIFICATION	 grouping sets are not allowed this restriction can lifted future 
WITHOUT_CLASSIFICATION	 index fetchoperator which providing smallest one 
WITHOUT_CLASSIFICATION	 for longs and java overheads semiarbitrary 
WITHOUT_CLASSIFICATION	 the child and operator extract its children 
WITHOUT_CLASSIFICATION	 multikey value hash map optimized for vector map join the key uninterpreted bytes 
WITHOUT_CLASSIFICATION	 modify sourcetable 
WITHOUT_CLASSIFICATION	 partition droppped after repl dump 
WITHOUT_CLASSIFICATION	 conversion needed and not variablelength argument just return what passed 
WITHOUT_CLASSIFICATION	 ctimestamp cdecimal cbinary cdate cvarchar cchar cbinary 
WITHOUT_CLASSIFICATION	 setup web 
WITHOUT_CLASSIFICATION	 decimal string formatting 
WITHOUT_CLASSIFICATION	 case max list members max query string length 
WITHOUT_CLASSIFICATION	 notify tests and global async ops 
WITHOUT_CLASSIFICATION	 only allow integer index for now 
WITHOUT_CLASSIFICATION	 test that values that know are missing are shown absent 
WITHOUT_CLASSIFICATION	 reversedmemorymb set make memory allocation fraction adjustment needed 
WITHOUT_CLASSIFICATION	 change query fetchtask use new location specified results cache 
WITHOUT_CLASSIFICATION	 ast expression not valid column name for table 
WITHOUT_CLASSIFICATION	 only need run the logic for tables missed 
WITHOUT_CLASSIFICATION	 initialize configuration 
WITHOUT_CLASSIFICATION	 nothing check 
WITHOUT_CLASSIFICATION	 register the cacheaware path that parquet reader would thru 
WITHOUT_CLASSIFICATION	 the following tests use serialized asts that generated using hive from branch 
WITHOUT_CLASSIFICATION	 test that exclusive locks coalesce one 
WITHOUT_CLASSIFICATION	 reset the bean 
WITHOUT_CLASSIFICATION	 ends character this means they appended time indicator 
WITHOUT_CLASSIFICATION	 running normal async query with exceptionsthen need close ophandle 
WITHOUT_CLASSIFICATION	 roottasks the entry point for all generated tasks 
WITHOUT_CLASSIFICATION	 called map operator propagated recursively single parented descendants 
WITHOUT_CLASSIFICATION	 someone already allocated this arena just the usual thing 
WITHOUT_CLASSIFICATION	 value 
WITHOUT_CLASSIFICATION	 otherwise take the child itself 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlrowid 
WITHOUT_CLASSIFICATION	 vectorized expression that dont expect will called due shortcircuit evaluation 
WITHOUT_CLASSIFICATION	 another query referring that property with the conf overlay should fail 
WITHOUT_CLASSIFICATION	 fallthrough 
WITHOUT_CLASSIFICATION	 this check necessary because for spark branch the result array from getinputpaths above could empty and therefore numthreads could 
WITHOUT_CLASSIFICATION	 delete the parent node all the children have been deleted 
WITHOUT_CLASSIFICATION	 need for grouping and the target tasks this code path should never triggered the moment grouping disabled dagutils uses 
WITHOUT_CLASSIFICATION	 the list has invalid port update with valid port 
WITHOUT_CLASSIFICATION	 the handling results 
WITHOUT_CLASSIFICATION	 get splits 
WITHOUT_CLASSIFICATION	 int 
WITHOUT_CLASSIFICATION	 bad value type 
WITHOUT_CLASSIFICATION	 since hashcode not used just put arbitrary number 
WITHOUT_CLASSIFICATION	 udfoppositive noop however still create and then remove here make sure only allow 
WITHOUT_CLASSIFICATION	 check that constraints have catalog name properly set first 
WITHOUT_CLASSIFICATION	 evaluate else expression only and copy all its results second input parameter but column 
WITHOUT_CLASSIFICATION	 confvar overridden hivesitexml 
WITHOUT_CLASSIFICATION	 oldinput has the original group keys the front 
WITHOUT_CLASSIFICATION	 dont increment the reader count for explain queries 
WITHOUT_CLASSIFICATION	 position the biggest small table 
WITHOUT_CLASSIFICATION	 not invert between result add column expression here 
WITHOUT_CLASSIFICATION	 compute required mapping 
WITHOUT_CLASSIFICATION	 the above example and are simple trees 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 unlock database operation release the lock explicitly the operation itself dont need locked set the writeentity writetype ddlnolock here otherwise will conflict with hives transaction 
WITHOUT_CLASSIFICATION	 cannot restart place because the user might receive failure and return the session the master thread without the irrelevant flag set fact the query might have succeeded the gap and the session might already returned queue restart thru 
WITHOUT_CLASSIFICATION	 note the following section metadataonly import handling logic interleaved with regular replimport logic the rule thumb being followed here that mdonly imports are essentially alters they not load data and should not creating any metadata they should replacing instead the only place makes sense for mdonly import create the case table thats been dropped and recreated the case unpartitioned table all other cases should behave like noop pure alter 
WITHOUT_CLASSIFICATION	 blockingqueue methods 
WITHOUT_CLASSIFICATION	 char 
WITHOUT_CLASSIFICATION	 filtermode condition always true always false 
WITHOUT_CLASSIFICATION	 return random number with length digits string results may negative positive 
WITHOUT_CLASSIFICATION	 nonnative vector map differences for left outer join and filtered 
WITHOUT_CLASSIFICATION	 vectorized class available problem try use the vectorudfadaptor when configured note assume has not been set because are executing test that didnt create hiveconf etc usage vectorudfadaptor that case 
WITHOUT_CLASSIFICATION	 create server that doesnt interpret any kerberos stuff 
WITHOUT_CLASSIFICATION	 maybe will kill the condition 
WITHOUT_CLASSIFICATION	 lock 
WITHOUT_CLASSIFICATION	 find available privileges 
WITHOUT_CLASSIFICATION	 rootoperators are all the table scan operators sequence traversal 
WITHOUT_CLASSIFICATION	 the move hasnt been made already 
WITHOUT_CLASSIFICATION	 debugtest related methods 
WITHOUT_CLASSIFICATION	 create more staging data with copyn files and ldoverwrite 
WITHOUT_CLASSIFICATION	 dont wait for the cluster not started this besteffort 
WITHOUT_CLASSIFICATION	 end multijoinjava 
WITHOUT_CLASSIFICATION	 test empty database 
WITHOUT_CLASSIFICATION	 case was done and noone looked 
WITHOUT_CLASSIFICATION	 order columns are used key columns for constructing the reducesinkoperator since not explicitly add these outputcolumnnames need set includekeycols false while creating the reducesinkdesc 
WITHOUT_CLASSIFICATION	 this cover the case where hive table may have mapkey value but the data file type arraystructvalue value using index place type name 
WITHOUT_CLASSIFICATION	 must some unix variant 
WITHOUT_CLASSIFICATION	 set the offset and length for the two elements 
WITHOUT_CLASSIFICATION	 when deleterecordid currrecordidinbatch this record the batch has been deleted 
WITHOUT_CLASSIFICATION	 test parent references from statement 
WITHOUT_CLASSIFICATION	 remember all threads that were running the time started line processing hook the custom ctrlc handler while processing this line 
WITHOUT_CLASSIFICATION	 this will populated case dynamic partitioning and list bucketing 
WITHOUT_CLASSIFICATION	 dummy mapping used for all and table name mappings 
WITHOUT_CLASSIFICATION	 create syntax tree for function call testudfcol col col 
WITHOUT_CLASSIFICATION	 determine recursively the ptf lead lag function being used expression 
WITHOUT_CLASSIFICATION	 have come this far either the previous commands are all successful this command line either case this counts successful run 
WITHOUT_CLASSIFICATION	 untyped nulls 
WITHOUT_CLASSIFICATION	 add partition cols necessary see for details 
WITHOUT_CLASSIFICATION	 combine 
WITHOUT_CLASSIFICATION	 check roundups before settings values result 
WITHOUT_CLASSIFICATION	 hive pending refactor push file forward 
WITHOUT_CLASSIFICATION	 get the rootlogger which you dont have logjtestproperties defined will only log errors 
WITHOUT_CLASSIFICATION	 initialize one columns target related arrays 
WITHOUT_CLASSIFICATION	 create the queryid appender for the queryid route 
WITHOUT_CLASSIFICATION	 map 
WITHOUT_CLASSIFICATION	 since same thread creates metastore client for streaming connection thread and heartbeat thread explicitly disable metastore client cache 
WITHOUT_CLASSIFICATION	 should fail because the 
WITHOUT_CLASSIFICATION	 set during the init phase hiveserver auth mode kerberos 
WITHOUT_CLASSIFICATION	 since this terminal operator update counters explicitly forward not called 
WITHOUT_CLASSIFICATION	 max fraction errors allowed throw error only after this many errors 
WITHOUT_CLASSIFICATION	 tez processor needs configure object registry first 
WITHOUT_CLASSIFICATION	 capture arguments static 
WITHOUT_CLASSIFICATION	 return value constant case arg constant 
WITHOUT_CLASSIFICATION	 beeline mode need hook use connect case the showdbinprompt set the database name needed 
WITHOUT_CLASSIFICATION	 serialize using another serde and read out that object repr 
WITHOUT_CLASSIFICATION	 there must least one column vector 
WITHOUT_CLASSIFICATION	 trim the size needed 
WITHOUT_CLASSIFICATION	 create all children 
WITHOUT_CLASSIFICATION	 order and null order 
WITHOUT_CLASSIFICATION	 maxcreatetime 
WITHOUT_CLASSIFICATION	 verify result rounded digits 
WITHOUT_CLASSIFICATION	 puts int little endian order 
WITHOUT_CLASSIFICATION	 add new rel its the maps 
WITHOUT_CLASSIFICATION	 query acquires the lock and takes secs compile 
WITHOUT_CLASSIFICATION	 the operator tree till the sink operator needs processed while fetching the next row fetch from the priority queue possibly containing multiple files the small table given file the big table the remaining tree will processed while processing the join 
WITHOUT_CLASSIFICATION	 for now limit the data types support for vectorized struct 
WITHOUT_CLASSIFICATION	 that 
WITHOUT_CLASSIFICATION	 only create the movework for nonmm table action needed for table 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 probably view 
WITHOUT_CLASSIFICATION	 make sure create table fails 
WITHOUT_CLASSIFICATION	 initializes current key 
WITHOUT_CLASSIFICATION	 well pass threadlocals the background thread from the foreground handler thread threadlocal hive object needs set background thread the metastore client hive associated with right user current ugi will get used metastore when metastore embedded mode 
WITHOUT_CLASSIFICATION	 testparam yellow 
WITHOUT_CLASSIFICATION	 the bucket task map should have been setup the big table 
WITHOUT_CLASSIFICATION	 add value chain not null skipnulls false 
WITHOUT_CLASSIFICATION	 known not have any nulls 
WITHOUT_CLASSIFICATION	 assertequalso struct cannot this because types null lists are wrong 
WITHOUT_CLASSIFICATION	 add all unique positions referenced 
WITHOUT_CLASSIFICATION	 inspect the output type each key expression 
WITHOUT_CLASSIFICATION	 insert two rows table 
WITHOUT_CLASSIFICATION	 should here 
WITHOUT_CLASSIFICATION	 todo handle exclusion list figures out which tables make acid and optionally performs the operation 
WITHOUT_CLASSIFICATION	 longest run trailing zeroes 
WITHOUT_CLASSIFICATION	 update included with the submit request callback via notifystarted 
WITHOUT_CLASSIFICATION	 fields 
WITHOUT_CLASSIFICATION	 only run for milliseconds 
WITHOUT_CLASSIFICATION	 first argument charcount which consumed this method below 
WITHOUT_CLASSIFICATION	 invalidate all cache entries using this table 
WITHOUT_CLASSIFICATION	 verify record was written correctly parquet 
WITHOUT_CLASSIFICATION	 check metric value 
WITHOUT_CLASSIFICATION	 expect have happened now since hivetxntimeoutsec 
WITHOUT_CLASSIFICATION	 before anyone else accesses would have been allocated and decompressed locally 
WITHOUT_CLASSIFICATION	 todo this depends tez creating separate threads does now that changes some other way propagatefind out attempt would needed see tez 
WITHOUT_CLASSIFICATION	 cant use the cached table because has spilled 
WITHOUT_CLASSIFICATION	 add tables outputs 
WITHOUT_CLASSIFICATION	 this operator then need call the plan generation the 
WITHOUT_CLASSIFICATION	 runtimegetmax gives very different number from the actual xmx sizing you can iterate through the from javalangmanagement figure this out but the hardcoded params the llap runsh result usable heap xxnewratio survivor region which technically not the usable space 
WITHOUT_CLASSIFICATION	 custom pattern set case dynamic partitioning configure custom path 
WITHOUT_CLASSIFICATION	 resultcast cleanup vectorexprargtype 
WITHOUT_CLASSIFICATION	 wait for minute and check again 
WITHOUT_CLASSIFICATION	 metastorehome set use else use hivehome for backwards compatibility 
WITHOUT_CLASSIFICATION	 relativepath 
WITHOUT_CLASSIFICATION	 this method generates the map bucket file splits 
WITHOUT_CLASSIFICATION	 read the newly added partition via cachedstore 
WITHOUT_CLASSIFICATION	 loop get all task completion events because 
WITHOUT_CLASSIFICATION	 from the hive logshivelog can also check for the info statement fgrep total tasks location hivelog each line indicates one run loadtask 
WITHOUT_CLASSIFICATION	 dshr 
WITHOUT_CLASSIFICATION	 the connection was closed create new one 
WITHOUT_CLASSIFICATION	 handle table properties 
WITHOUT_CLASSIFICATION	 this method takes something like string only accepts something like string 
WITHOUT_CLASSIFICATION	 the separators array whether need escape the data when writing out which char use the escape char which chars need escaped 
WITHOUT_CLASSIFICATION	 keep track view alias view name and read entity for for query like select from where keeps track full view name and read entity corresponding alias vvv 
WITHOUT_CLASSIFICATION	 are not always create new client for now 
WITHOUT_CLASSIFICATION	 this for artificially added tokens 
WITHOUT_CLASSIFICATION	 production field 
WITHOUT_CLASSIFICATION	 create output directory not already created 
WITHOUT_CLASSIFICATION	 might under the hive name 
WITHOUT_CLASSIFICATION	 hadoop acls not work with localfilesystem set minidfs 
WITHOUT_CLASSIFICATION	 isnt error the following returns rows the local workers could have died with nothing assigned them 
WITHOUT_CLASSIFICATION	 for numeric types 
WITHOUT_CLASSIFICATION	 get value element information 
WITHOUT_CLASSIFICATION	 stop hiveserver increase header size 
WITHOUT_CLASSIFICATION	 notice the default value for llapioenabled overridden whether are executing under llap 
WITHOUT_CLASSIFICATION	 create fetchwork for non partitioned table 
WITHOUT_CLASSIFICATION	 reduce sink operator the defacto operator for determining keycols emit keys map phase 
WITHOUT_CLASSIFICATION	 spot check decimal columncolumn multiply 
WITHOUT_CLASSIFICATION	 all columns cluster and sort are valid columns 
WITHOUT_CLASSIFICATION	 issuing query for all partitions verify that need update the same columns 
WITHOUT_CLASSIFICATION	 remember the mapping case scan another branch the 
WITHOUT_CLASSIFICATION	 start with size and double when needed 
WITHOUT_CLASSIFICATION	 test downcasting when greater than 
WITHOUT_CLASSIFICATION	 the function name 
WITHOUT_CLASSIFICATION	 write bytes bos 
WITHOUT_CLASSIFICATION	 operand 
WITHOUT_CLASSIFICATION	 format always madvise never 
WITHOUT_CLASSIFICATION	 set true only when deregistration happens web 
WITHOUT_CLASSIFICATION	 may the getting created this load 
WITHOUT_CLASSIFICATION	 threads while the constructor running 
WITHOUT_CLASSIFICATION	 parenthesis the end 
WITHOUT_CLASSIFICATION	 drop table tbl via objectstore 
WITHOUT_CLASSIFICATION	 blank byte 
WITHOUT_CLASSIFICATION	 the grouping set has not yet been processed create new grouping key consider the query select count from group with cube where being executed mapreduce jobs the plan for tablescan groupby reducesink groupby filesink groupbyreducesink worked grouping sets were not present this function called for groupby create new rows for grouping sets for each input row rows are created for the example above anull null null null 
WITHOUT_CLASSIFICATION	 partnames tabparttabpart 
WITHOUT_CLASSIFICATION	 extract configs for processing the python fragments yarn service 
WITHOUT_CLASSIFICATION	 work should the smallest unit for memory allocation 
WITHOUT_CLASSIFICATION	 that know which version wrote the file 
WITHOUT_CLASSIFICATION	 add hive keywords including lowercased versions 
WITHOUT_CLASSIFICATION	 have found the colname need search more exprnodes 
WITHOUT_CLASSIFICATION	 assumed islazy flag set only for repl load flow import always deep copy distcpdoasuser will null default replcopywork 
WITHOUT_CLASSIFICATION	 set the appropriate key the map and test that are able read back correctly 
WITHOUT_CLASSIFICATION	 recompose filter possibly pulling out common elements from dnf expressions 
WITHOUT_CLASSIFICATION	 inline merge join operator selfjoin 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 skewed column names 
WITHOUT_CLASSIFICATION	 this point havent found screw dont know where 
WITHOUT_CLASSIFICATION	 alias key mapping 
WITHOUT_CLASSIFICATION	 unsigned max 
WITHOUT_CLASSIFICATION	 job state job request changes the state are synchronized using setstateandresult this required due two different threads main thread and job execute thread tries change state and organize clean tasks 
WITHOUT_CLASSIFICATION	 decimal point 
WITHOUT_CLASSIFICATION	 perform some updatedelete 
WITHOUT_CLASSIFICATION	 finally put uncompressed data cache 
WITHOUT_CLASSIFICATION	 object inspector hasnt been cached for this typeparams yet create now 
WITHOUT_CLASSIFICATION	 print dependent vertexs 
WITHOUT_CLASSIFICATION	 insiderview will tell this tablescan inside view not 
WITHOUT_CLASSIFICATION	 preallocated members for storing information equal key series for smalltable matches index into the hashmapresults array for the match allmatchindices logical indices into allmatchs the first row match possible series duplicate keys issinglevalue whether there multiple small table values duplicatecounts the duplicate count for each matched key 
WITHOUT_CLASSIFICATION	 checksum does not match likely the file changedremoved retry from path 
WITHOUT_CLASSIFICATION	 safety check for postconditions 
WITHOUT_CLASSIFICATION	 required required optional optional required required optional required optional required required optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 this will throw nosuchlockexception even though its the transaction weve closed because that will have deleted the lock 
WITHOUT_CLASSIFICATION	 doesnt clear underlying hashtable 
WITHOUT_CLASSIFICATION	 simulate emitting all records 
WITHOUT_CLASSIFICATION	 initialize the integer values 
WITHOUT_CLASSIFICATION	 parenttablename 
WITHOUT_CLASSIFICATION	 check whether are replicating 
WITHOUT_CLASSIFICATION	 this checked expression use different template for checked expressions 
WITHOUT_CLASSIFICATION	 combinedsplit 
WITHOUT_CLASSIFICATION	 there are new segments can just bail out 
WITHOUT_CLASSIFICATION	 suppress headers and all objects below 
WITHOUT_CLASSIFICATION	 process join filters 
WITHOUT_CLASSIFICATION	 the lock for updates 
WITHOUT_CLASSIFICATION	 this solely for testing checks the test has set the looped value false and remembers that and then sets true the end have check here 
WITHOUT_CLASSIFICATION	 replace original varsampx with sumx sumx sumx countx case countx when then null else countx end 
WITHOUT_CLASSIFICATION	 delete from tab txn 
WITHOUT_CLASSIFICATION	 this the last for which this buffer will used remove the initial refcount 
WITHOUT_CLASSIFICATION	 one the tables that not memory 
WITHOUT_CLASSIFICATION	 should not happened ignore remaining 
WITHOUT_CLASSIFICATION	 dont want attempt grab more memory than have available percentage bit arbitrary 
WITHOUT_CLASSIFICATION	 could the minuncommittedtxnid lesser than nexttxnidntxnnext 
WITHOUT_CLASSIFICATION	 then throw exception 
WITHOUT_CLASSIFICATION	 this not bulletproof but should allow close session most cases 
WITHOUT_CLASSIFICATION	 create random test string 
WITHOUT_CLASSIFICATION	 arithmetic operations reset the results 
WITHOUT_CLASSIFICATION	 set the session for driver 
WITHOUT_CLASSIFICATION	 create the corresponding hive expression filter partition columns 
WITHOUT_CLASSIFICATION	 bit packing disabled 
WITHOUT_CLASSIFICATION	 single quote seen and the index not inside double quoted string and the previous character was not escape then update the flag 
WITHOUT_CLASSIFICATION	 compatible mapwork 
WITHOUT_CLASSIFICATION	 top will add 
WITHOUT_CLASSIFICATION	 use deep hashcode for arrays 
WITHOUT_CLASSIFICATION	 nothing are not running local mode only submitting the job via child process this case its appropriate that the child jvm use the same memory the parent jvm 
WITHOUT_CLASSIFICATION	 introducing explicit aliases for tbl 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check all parent statistics are available 
WITHOUT_CLASSIFICATION	 environment variables short values 
WITHOUT_CLASSIFICATION	 mergeable next loop iteration 
WITHOUT_CLASSIFICATION	 mark fragment completing but dont actually complete yet the wait queue should now have capacity accept one more fragment 
WITHOUT_CLASSIFICATION	 add the support for read variations vectorized text 
WITHOUT_CLASSIFICATION	 second batch last but one batch will actualbatchsize actualbatchsize same batchsize when exceptions are expected 
WITHOUT_CLASSIFICATION	 big tables that should streamed 
WITHOUT_CLASSIFICATION	 get alias from topops 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 required required required required optional 
WITHOUT_CLASSIFICATION	 get all the dependencies delete 
WITHOUT_CLASSIFICATION	 this the constructor use for smb 
WITHOUT_CLASSIFICATION	 mgby already contains group key need remove distinct column 
WITHOUT_CLASSIFICATION	 check the join operator encountered candidate for being converted 
WITHOUT_CLASSIFICATION	 all children are done this operator also done 
WITHOUT_CLASSIFICATION	 test with null empty randomly 
WITHOUT_CLASSIFICATION	 produces sparse vrb when includes are used need write the dense vrb orc ideally wed use projection columns but orc writer doesnt use them any case would also need build new for orcwriter config this why orcwriter created after this writer the way 
WITHOUT_CLASSIFICATION	 used store each values length 
WITHOUT_CLASSIFICATION	 assumes ranges passed cache read have data 
WITHOUT_CLASSIFICATION	 will split the block headerix splitways ways and take totake blocks which will leave free blocks target size 
WITHOUT_CLASSIFICATION	 create new inputformat instance this the first time see 
WITHOUT_CLASSIFICATION	 new connections goes minihs now 
WITHOUT_CLASSIFICATION	 convert integer related types because converters are not sufficient 
WITHOUT_CLASSIFICATION	 the session hook should set the property 
WITHOUT_CLASSIFICATION	 stringstats 
WITHOUT_CLASSIFICATION	 end month behavior 
WITHOUT_CLASSIFICATION	 fullresourceplan 
WITHOUT_CLASSIFICATION	 waits for other threads join and returns with its 
WITHOUT_CLASSIFICATION	 refer paper 
WITHOUT_CLASSIFICATION	 for should deterministic and stateless 
WITHOUT_CLASSIFICATION	 setting non important configuration should return the same client only 
WITHOUT_CLASSIFICATION	 load empty database 
WITHOUT_CLASSIFICATION	 null server url means local mode 
WITHOUT_CLASSIFICATION	 setup the overflow batch 
WITHOUT_CLASSIFICATION	 get list dynamic partitions 
WITHOUT_CLASSIFICATION	 get the 
WITHOUT_CLASSIFICATION	 only two elements expected partexprparts partition column name and partition value 
WITHOUT_CLASSIFICATION	 optimize revokegrant list remove the overlapping 
WITHOUT_CLASSIFICATION	 can have multiple branches due dpp semijoin opt use dfs traverse all the branches until hit 
WITHOUT_CLASSIFICATION	 collect child projection indexes used 
WITHOUT_CLASSIFICATION	 batchsize when isrepeating 
WITHOUT_CLASSIFICATION	 col names parent 
WITHOUT_CLASSIFICATION	 uri the from path 
WITHOUT_CLASSIFICATION	 dont want check types already checked 
WITHOUT_CLASSIFICATION	 optional iodescriptor 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 service not started yet 
WITHOUT_CLASSIFICATION	 set host 
WITHOUT_CLASSIFICATION	 the first argument just set the return the standard writable version this 
WITHOUT_CLASSIFICATION	 handle hint based semijoin 
WITHOUT_CLASSIFICATION	 evict all blocks 
WITHOUT_CLASSIFICATION	 copy and fixup the parent list the original child instead just assuming 
WITHOUT_CLASSIFICATION	 minimum required 
WITHOUT_CLASSIFICATION	 not used value 
WITHOUT_CLASSIFICATION	 this will also take care the queries query parallelism changed 
WITHOUT_CLASSIFICATION	 the current kinds column vectors 
WITHOUT_CLASSIFICATION	 one row per value 
WITHOUT_CLASSIFICATION	 check this project only projects one expression scalar 
WITHOUT_CLASSIFICATION	 null returned then help message was displayed parsecommandline method 
WITHOUT_CLASSIFICATION	 same also emit extra records from separate thread 
WITHOUT_CLASSIFICATION	 create environment variable that uniquely identifies this script 
WITHOUT_CLASSIFICATION	 print the vertex that has more before the vertex that has fewer 
WITHOUT_CLASSIFICATION	 creates empty union object 
WITHOUT_CLASSIFICATION	 for nongeneric udf type info isnt available this poses problem for hive decimal 
WITHOUT_CLASSIFICATION	 constant map projection known length 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 batchindex classname match issinglevalue currentkey currentkey 
WITHOUT_CLASSIFICATION	 converts partnamespartvals into partnameval partnameval 
WITHOUT_CLASSIFICATION	 marker track there starting single quote without ending double quote 
WITHOUT_CLASSIFICATION	 register plain sasl server provider 
WITHOUT_CLASSIFICATION	 currently known tasks 
WITHOUT_CLASSIFICATION	 this happens when the reducesinks edge has been removed cycle detection logic nothing here 
WITHOUT_CLASSIFICATION	 the mapjoins parents may have been replaced dummy operator 
WITHOUT_CLASSIFICATION	 blindly add this non settable list list integers should sufficient for the test case use the standard list object inspector 
WITHOUT_CLASSIFICATION	 roundpower negative scale means start rounding integer digits the result will integer result will have least absroundpower trailing digits examples where the show the rounding digits round rrr ceiling rrrr notice that any fractional digits will gone the result 
WITHOUT_CLASSIFICATION	 fake dead session 
WITHOUT_CLASSIFICATION	 emptybuckets false 
WITHOUT_CLASSIFICATION	 have records left the group push one those 
WITHOUT_CLASSIFICATION	 the location string will the form database nametable name parse and communicate the information hcatinputformat 
WITHOUT_CLASSIFICATION	 cannot merge 
WITHOUT_CLASSIFICATION	 this not actual list see intermlist 
WITHOUT_CLASSIFICATION	 serialize again 
WITHOUT_CLASSIFICATION	 only publish stats this operators flag was set gather stats 
WITHOUT_CLASSIFICATION	 prepare for next iteration any 
WITHOUT_CLASSIFICATION	 dates are stored long convert and compare 
WITHOUT_CLASSIFICATION	 broken configuration from mapreddefaultxml 
WITHOUT_CLASSIFICATION	 else fall through and add this condition nonequi condition 
WITHOUT_CLASSIFICATION	 reset exec context that initialization the map operator happens properly 
WITHOUT_CLASSIFICATION	 determine distkeylength distincts and then add the first present 
WITHOUT_CLASSIFICATION	 truncate ascii same maximum length 
WITHOUT_CLASSIFICATION	 create new value 
WITHOUT_CLASSIFICATION	 let create partitions without any triggers 
WITHOUT_CLASSIFICATION	 turns out partition columns get marked virtual columninfo need 
WITHOUT_CLASSIFICATION	 create the metastore client the clientugi doing this way will give the client access the token that was added earlier 
WITHOUT_CLASSIFICATION	 remove the directories for aborted transactions only 
WITHOUT_CLASSIFICATION	 test will local mode 
WITHOUT_CLASSIFICATION	 all the insane dst variations where actually end anyones guess 
WITHOUT_CLASSIFICATION	 using reflection and update the mdc 
WITHOUT_CLASSIFICATION	 random column name reduce the chance conflict 
WITHOUT_CLASSIFICATION	 determine the partition columns using the first partition descriptor 
WITHOUT_CLASSIFICATION	 partitions not exist for this table 
WITHOUT_CLASSIFICATION	 pass all the checks can rewrite 
WITHOUT_CLASSIFICATION	 table valid 
WITHOUT_CLASSIFICATION	 not null constraint could enforcedenabled 
WITHOUT_CLASSIFICATION	 extract the information that need 
WITHOUT_CLASSIFICATION	 new instance session 
WITHOUT_CLASSIFICATION	 replacement the existing database state newer than our update 
WITHOUT_CLASSIFICATION	 trigger kill threads and verify get and expected message 
WITHOUT_CLASSIFICATION	 validate and vectorize the map operator tree 
WITHOUT_CLASSIFICATION	 dont wait empty take above that will wait for 
WITHOUT_CLASSIFICATION	 operator with single child 
WITHOUT_CLASSIFICATION	 skip header lines 
WITHOUT_CLASSIFICATION	 there should still now directories the location 
WITHOUT_CLASSIFICATION	 cbo enabled orderby position will processed genplan 
WITHOUT_CLASSIFICATION	 should never happen ctor just assignments 
WITHOUT_CLASSIFICATION	 already consistent can happen wnull lsg 
WITHOUT_CLASSIFICATION	 for null 
WITHOUT_CLASSIFICATION	 groupby generates new vectorized row batch 
WITHOUT_CLASSIFICATION	 made sure the references are for different join inputs 
WITHOUT_CLASSIFICATION	 helper function create vertex for given reducework 
WITHOUT_CLASSIFICATION	 counters with vertex name suffix desiredcounter inputfiles counters inputfilesmap inputfilesmap outcome inputfile 
WITHOUT_CLASSIFICATION	 add file paths the files that will moved the destination the caller needs 
WITHOUT_CLASSIFICATION	 need preserve enabled flag 
WITHOUT_CLASSIFICATION	 get service host 
WITHOUT_CLASSIFICATION	 this simulates the completion update tab txn 
WITHOUT_CLASSIFICATION	 get the cookie name 
WITHOUT_CLASSIFICATION	 register information about pushed predicates 
WITHOUT_CLASSIFICATION	 the pruning needs preserve the order columns the input schema 
WITHOUT_CLASSIFICATION	 push filters only for this qbjointree child qbjointrees have already been handled 
WITHOUT_CLASSIFICATION	 the user has specified queue name themselves create new session also new session created the user tries submit queue using their own credentials expect that with the new security model things will run user hive most cases 
WITHOUT_CLASSIFICATION	 hadoop configuration properties properties with null values are ignored and exist only for the purpose giving symbolic name reference the hive source code properties with nonnull 
WITHOUT_CLASSIFICATION	 not terrible thing even the data not deleted 
WITHOUT_CLASSIFICATION	 length sync entries number bytes hash escape hash 
WITHOUT_CLASSIFICATION	 dont set lineage delete dont have all the columns 
WITHOUT_CLASSIFICATION	 now take the serialized keys just wrote into our scratch column and look them the list 
WITHOUT_CLASSIFICATION	 substitute outputformat name based 
WITHOUT_CLASSIFICATION	 string 
WITHOUT_CLASSIFICATION	 sized block 
WITHOUT_CLASSIFICATION	 tests for the listpartition string partitionspecs string sourcedb string sourcetable string destdb string desttablename method 
WITHOUT_CLASSIFICATION	 that recognizes parenthesis delimiter 
WITHOUT_CLASSIFICATION	 ttl check 
WITHOUT_CLASSIFICATION	 attempt find maxappendattempts possible alternatives filename appending and seeing that destination also clashes were still clashing after that give 
WITHOUT_CLASSIFICATION	 now have delta with inserts only push predicate 
WITHOUT_CLASSIFICATION	 ignore error just return the valid tables that are found 
WITHOUT_CLASSIFICATION	 wait queue could have been reordered the mean time because concurrent task submission remove the specific task instead the head task 
WITHOUT_CLASSIFICATION	 first handle the actual thing found 
WITHOUT_CLASSIFICATION	 look for matches file system counters 
WITHOUT_CLASSIFICATION	 check fast 
WITHOUT_CLASSIFICATION	 verify both groupset and aggrfunction are empty 
WITHOUT_CLASSIFICATION	 reset temp list index 
WITHOUT_CLASSIFICATION	 the todigitsonlybytes stores digits the end the scratch buffer 
WITHOUT_CLASSIFICATION	 give the caller context for future errors 
WITHOUT_CLASSIFICATION	 value not constant bail out 
WITHOUT_CLASSIFICATION	 display all nonvectorized leaf objects unless only 
WITHOUT_CLASSIFICATION	 separate client create the catalog 
WITHOUT_CLASSIFICATION	 commysqljdbcdriver 
WITHOUT_CLASSIFICATION	 there are txns which are open for the given validtxnlist snapshot then just return 
WITHOUT_CLASSIFICATION	 all rows should the inmemory hashmap 
WITHOUT_CLASSIFICATION	 determine the binary words like what does 
WITHOUT_CLASSIFICATION	 test that fetching nonexistent partition yields objectnotfound 
WITHOUT_CLASSIFICATION	 for caching column stats for unpartitioned table 
WITHOUT_CLASSIFICATION	 test repeating right 
WITHOUT_CLASSIFICATION	 grouping should pruned which the last key columns 
WITHOUT_CLASSIFICATION	 use subdir inputpath 
WITHOUT_CLASSIFICATION	 should expect semantic exception being throw this partition should not present 
WITHOUT_CLASSIFICATION	 dfs autocloseable 
WITHOUT_CLASSIFICATION	 leftright skip filtered valid skip filtered valid left alias has any pair for right alias continue 
WITHOUT_CLASSIFICATION	 note pig sets client system properties actually getting the client system properties starting pig you must pass the properties when updating our pig dependency this will need updated 
WITHOUT_CLASSIFICATION	 run the script using beeline 
WITHOUT_CLASSIFICATION	 the ptned table should there both source and target rename was not successful 
WITHOUT_CLASSIFICATION	 this indicates corr var left operand rex call not this used decorrelatelogical correlate appropriately create rex node expression 
WITHOUT_CLASSIFICATION	 this the data copy 
WITHOUT_CLASSIFICATION	 picks topn pairs from input 
WITHOUT_CLASSIFICATION	 this the old logic which assumes that the filenames are sorted alphanumeric order and mapped appropriate bucket number 
WITHOUT_CLASSIFICATION	 job status request executor get status job 
WITHOUT_CLASSIFICATION	 list dynamic partitions 
WITHOUT_CLASSIFICATION	 add partitions with new schema 
WITHOUT_CLASSIFICATION	 set the buffer that will receive the serialized data the output buffer will not reset 
WITHOUT_CLASSIFICATION	 introduce top project operator remove additional columns that have been introduced 
WITHOUT_CLASSIFICATION	 make the new aggrel 
WITHOUT_CLASSIFICATION	 could have multiple sources restrict the same column need take the union the values that case 
WITHOUT_CLASSIFICATION	 such abc 
WITHOUT_CLASSIFICATION	 prepare plan for submission building dag adding resources creating scratch dirs etc 
WITHOUT_CLASSIFICATION	 check that the table acid 
WITHOUT_CLASSIFICATION	 dont load defaults note hivesitexml only available client not 
WITHOUT_CLASSIFICATION	 its new column 
WITHOUT_CLASSIFICATION	 calculate the number bytes the split that are local each 
WITHOUT_CLASSIFICATION	 neginfinity start exclusive 
WITHOUT_CLASSIFICATION	 initial write for ctas and import 
WITHOUT_CLASSIFICATION	 udfs 
WITHOUT_CLASSIFICATION	 max number threads can use check noncombinable paths 
WITHOUT_CLASSIFICATION	 note that originalfiles are all original files recursively not dirs 
WITHOUT_CLASSIFICATION	 note can actually higher than real value 
WITHOUT_CLASSIFICATION	 nonjavadoc order update decimal fast allocation need expose access the internal storage bytes and scale 
WITHOUT_CLASSIFICATION	 netty netty arrowvector arrowmemory arrowformat flatbuffers hppc 
WITHOUT_CLASSIFICATION	 another special case because timestamp not implicitly convertible numeric types 
WITHOUT_CLASSIFICATION	 perform simple checksum here make sure nothing got turned null 
WITHOUT_CLASSIFICATION	 and must have locations outside the table directory 
WITHOUT_CLASSIFICATION	 add the grouping set key the group operator this not the first group operator but subsequent group operator which forwarding the grouping keys introduced the grouping sets for consider select key value count from group key value with rollup assuming mapside aggregation and skew the plan would look like tablescan select groupby reducesink groupby select filesink this function called for groupby pass the additional grouping keys introduced 
WITHOUT_CLASSIFICATION	 hold lock file hdfs session dir indicate the use 
WITHOUT_CLASSIFICATION	 semijoin optimization branch which should look like parentselgbrsgbrs 
WITHOUT_CLASSIFICATION	 run reverse dns lookup the url 
WITHOUT_CLASSIFICATION	 can not kerberos auth will the input split generation the 
WITHOUT_CLASSIFICATION	 first drop all the dependencies 
WITHOUT_CLASSIFICATION	 just use view name location 
WITHOUT_CLASSIFICATION	 only apply this rule unionall true and sortfetch not null and more than 
WITHOUT_CLASSIFICATION	 make sure that the session returned the pool doesnt live the global 
WITHOUT_CLASSIFICATION	 handle cancel loop 
WITHOUT_CLASSIFICATION	 regardless the following exception 
WITHOUT_CLASSIFICATION	 there may speculative tasks waiting 
WITHOUT_CLASSIFICATION	 need the work detangle this 
WITHOUT_CLASSIFICATION	 the task hasnt started and killed report back the that the task has been killed 
WITHOUT_CLASSIFICATION	 mnot test this 
WITHOUT_CLASSIFICATION	 possible since either container task can unregistered 
WITHOUT_CLASSIFICATION	 value for indicates that the mapper processed data that does not meet filter criteria merge should noop 
WITHOUT_CLASSIFICATION	 spend most wait for executors come 
WITHOUT_CLASSIFICATION	 tries get lock and 
WITHOUT_CLASSIFICATION	 currently returned bootdumpbeginreplid dont consolidate the events after bootstrap 
WITHOUT_CLASSIFICATION	 tezjsonparser 
WITHOUT_CLASSIFICATION	 want query level fairness and dont want the get queue hold session 
WITHOUT_CLASSIFICATION	 for partial and final 
WITHOUT_CLASSIFICATION	 committed open ids 
WITHOUT_CLASSIFICATION	 and load data into the same table which should now land deltaxx 
WITHOUT_CLASSIFICATION	 try merge the join with the right child 
WITHOUT_CLASSIFICATION	 read via objectstore 
WITHOUT_CLASSIFICATION	 create the object inspector for the input columns and initialize the udtf 
WITHOUT_CLASSIFICATION	 the hmsc not shared across threads the only way could get closed while are doing healthcheck removallistener closes the synchronization takes care that removallistener wont 
WITHOUT_CLASSIFICATION	 create table with bad avro uri 
WITHOUT_CLASSIFICATION	 password 
WITHOUT_CLASSIFICATION	 transaction for the compaction for now 
WITHOUT_CLASSIFICATION	 existing table valid but the partition spec different then ignore partition validation and create new partition 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 convert equivalent time utc then get day offset 
WITHOUT_CLASSIFICATION	 input record iterator not used 
WITHOUT_CLASSIFICATION	 setup the bloom filter once 
WITHOUT_CLASSIFICATION	 store big keys one table into one dir and same keys other tables into corresponding different dirs one dir per table this map stores mapping from big key dir its corresponding mapjoin task 
WITHOUT_CLASSIFICATION	 process hiveconf get hiveconf param values and set the system property values 
WITHOUT_CLASSIFICATION	 combo url set literal set none 
WITHOUT_CLASSIFICATION	 the interface for single long key hash multiset contains method 
WITHOUT_CLASSIFICATION	 filter disabled injection disabled exception not expected 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 set sasl qop 
WITHOUT_CLASSIFICATION	 print the results 
WITHOUT_CLASSIFICATION	 the then expression either identityexpression column 
WITHOUT_CLASSIFICATION	 test that timestamp arithmetic done utc and then converted back local timezone matching oracle behavior 
WITHOUT_CLASSIFICATION	 additional data type specific setting 
WITHOUT_CLASSIFICATION	 leave some other host found delay else ends allocating random host immediately 
WITHOUT_CLASSIFICATION	 constant operator deterministic and all operands are constant 
WITHOUT_CLASSIFICATION	 the metastore shouldnt care what txn manager hive running but various tests 
WITHOUT_CLASSIFICATION	 point separating ioexception yarnexception others 
WITHOUT_CLASSIFICATION	 bypass the clause and select the first disjunct 
WITHOUT_CLASSIFICATION	 removed and the size before and after the genroottablescan will different 
WITHOUT_CLASSIFICATION	 process the last byte necessary 
WITHOUT_CLASSIFICATION	 dummy parent operators well 
WITHOUT_CLASSIFICATION	 deleterecord firstrecordinbatch until exhaust all the delete records 
WITHOUT_CLASSIFICATION	 dont prevent using nonacid resources txn but lock them 
WITHOUT_CLASSIFICATION	 compare class namemethod name using avoid any object conversion which may cause object creation most cases when the class 
WITHOUT_CLASSIFICATION	 since tab empty update stmt has pblah thus nothing actually update and generate empty dyn part list 
WITHOUT_CLASSIFICATION	 replacement the existing table state newer than our update 
WITHOUT_CLASSIFICATION	 build the status message for the status call 
WITHOUT_CLASSIFICATION	 test need partitionglobal order shufflesort should only used for global order 
WITHOUT_CLASSIFICATION	 this should not happen but ignore for safety 
WITHOUT_CLASSIFICATION	 check partitionvals are legitimate 
WITHOUT_CLASSIFICATION	 the function has explicit name like funcargs then call constructor that explicitly provides the function name the functext argument 
WITHOUT_CLASSIFICATION	 nothing special needs done for grouping sets this the final group operator and multiple rows corresponding the grouping sets have been generated upstream however addition job has been created handle grouping sets 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this the last element 
WITHOUT_CLASSIFICATION	 now copy the data into cache buffers 
WITHOUT_CLASSIFICATION	 per jdbc spec the connection closed sqlexception should thrown 
WITHOUT_CLASSIFICATION	 adding query specs used 
WITHOUT_CLASSIFICATION	 equals and compareto are not compatible with hivedecimals want compareto which returns true iff the numbers are equal the same equals returns true iff equal and the same scale set the decimals not the same 
WITHOUT_CLASSIFICATION	 are translating calcite operators into hive operators 
WITHOUT_CLASSIFICATION	 will not try partial rewriting for rebuild incremental rebuild disabled 
WITHOUT_CLASSIFICATION	 update fetchsize modified server 
WITHOUT_CLASSIFICATION	 dont create sessions for empty entries 
WITHOUT_CLASSIFICATION	 estimate that there will bytes per entry 
WITHOUT_CLASSIFICATION	 run using environment context with cascade 
WITHOUT_CLASSIFICATION	 get the key positions 
WITHOUT_CLASSIFICATION	 test for invalid group name 
WITHOUT_CLASSIFICATION	 estimate size key from column statistics 
WITHOUT_CLASSIFICATION	 this the time zone for test 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 get the top nodes for this task 
WITHOUT_CLASSIFICATION	 validation the same for map join since the small tables are not vectorized 
WITHOUT_CLASSIFICATION	 check for completed transactions 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 outputs are ready 
WITHOUT_CLASSIFICATION	 enable escaping 
WITHOUT_CLASSIFICATION	 accept start dag schedule wait time resource wait time etc 
WITHOUT_CLASSIFICATION	 mapping reducesink mapjoin operators 
WITHOUT_CLASSIFICATION	 make sure the correlated reference forms unique key check that the columns referenced these comparisons form 
WITHOUT_CLASSIFICATION	 add list bucketing location mappings 
WITHOUT_CLASSIFICATION	 inner bigtable only join specific members 
WITHOUT_CLASSIFICATION	 there will ddl task created case its create table not exists 
WITHOUT_CLASSIFICATION	 optional bytes token 
WITHOUT_CLASSIFICATION	 hostname 
WITHOUT_CLASSIFICATION	 left border the max 
WITHOUT_CLASSIFICATION	 close commit 
WITHOUT_CLASSIFICATION	 check the value bloom filter 
WITHOUT_CLASSIFICATION	 large 
WITHOUT_CLASSIFICATION	 test reordering 
WITHOUT_CLASSIFICATION	 schematext 
WITHOUT_CLASSIFICATION	 entitytype 
WITHOUT_CLASSIFICATION	 the output final and complete full aggregation which list doublewritable structs that represent the final histogram pairs bin centers and heights 
WITHOUT_CLASSIFICATION	 this corresponds mapstring 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 initialize the forward operator 
WITHOUT_CLASSIFICATION	 now cancel the delegation token 
WITHOUT_CLASSIFICATION	 this offer will accpeted and evicted 
WITHOUT_CLASSIFICATION	 run without roundoff 
WITHOUT_CLASSIFICATION	 this subquery and must have alias 
WITHOUT_CLASSIFICATION	 values array 
WITHOUT_CLASSIFICATION	 set udf classes for type casting data types rowmode 
WITHOUT_CLASSIFICATION	 root object array map collection add estimators for fields 
WITHOUT_CLASSIFICATION	 register only the attempt known case unregister call came before the register call 
WITHOUT_CLASSIFICATION	 define the serde parameters 
WITHOUT_CLASSIFICATION	 check the config used very often 
WITHOUT_CLASSIFICATION	 perform the same uri normalization createdatabasecore 
WITHOUT_CLASSIFICATION	 create new external table 
WITHOUT_CLASSIFICATION	 and continue processing the children 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 spark 
WITHOUT_CLASSIFICATION	 remove 
WITHOUT_CLASSIFICATION	 its easier then because simply division and then scale down 
WITHOUT_CLASSIFICATION	 these aggregations should updated only once 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 delegate back the local serializerowid method 
WITHOUT_CLASSIFICATION	 bound skip beginning and end the source 
WITHOUT_CLASSIFICATION	 not public since must have the field count and other information 
WITHOUT_CLASSIFICATION	 help option requested then display help and exit 
WITHOUT_CLASSIFICATION	 not native vectorized 
WITHOUT_CLASSIFICATION	 this was acid 
WITHOUT_CLASSIFICATION	 way 
WITHOUT_CLASSIFICATION	 need clone the plan 
WITHOUT_CLASSIFICATION	 should have null for nonspecified comments 
WITHOUT_CLASSIFICATION	 not native vectorized 
WITHOUT_CLASSIFICATION	 insert merge lands part updates land there 
WITHOUT_CLASSIFICATION	 when have multiple values save the next value records offset here 
WITHOUT_CLASSIFICATION	 removed 
WITHOUT_CLASSIFICATION	 output columns the following order the columns representing the output from window fns the input rows columns 
WITHOUT_CLASSIFICATION	 how get config paths and aminfo 
WITHOUT_CLASSIFICATION	 for orc there tez job for table stats 
WITHOUT_CLASSIFICATION	 this happens register calls getmetrics 
WITHOUT_CLASSIFICATION	 obtain input and all related data structures 
WITHOUT_CLASSIFICATION	 text file comparison 
WITHOUT_CLASSIFICATION	 initializeop can overridden initializing data structures for vectorforward 
WITHOUT_CLASSIFICATION	 master node will serialize writercontext and will make available slaves 
WITHOUT_CLASSIFICATION	 transactions 
WITHOUT_CLASSIFICATION	 warnings 
WITHOUT_CLASSIFICATION	 list last keys for each stripe 
WITHOUT_CLASSIFICATION	 the function deterministic and the children are constants 
WITHOUT_CLASSIFICATION	 string select columnname from cipartname null tabcolstats partcolstats where dbname cidbname and tablename citablename cipartname null and partitionname cipartname 
WITHOUT_CLASSIFICATION	 default double but one the sides already decimal complete the operation that type 
WITHOUT_CLASSIFICATION	 calcite yearmonth literal value months bigdecimal 
WITHOUT_CLASSIFICATION	 verify proper null output data value 
WITHOUT_CLASSIFICATION	 assert classinvariant 
WITHOUT_CLASSIFICATION	 can tolerate this this the previous behavior 
WITHOUT_CLASSIFICATION	 older version hadoop should have had this field 
WITHOUT_CLASSIFICATION	 eliminate plans with more than one tablescanoperator 
WITHOUT_CLASSIFICATION	 convert hive projections calcite 
WITHOUT_CLASSIFICATION	 create callables with different queries 
WITHOUT_CLASSIFICATION	 this constructor appeared and specifies that not want linewrap use any newline separator 
WITHOUT_CLASSIFICATION	 subdirectories 
WITHOUT_CLASSIFICATION	 get the serialized value for the column 
WITHOUT_CLASSIFICATION	 isvectormapjoin 
WITHOUT_CLASSIFICATION	 make sure the ugi contains the token too for good measure 
WITHOUT_CLASSIFICATION	 update max counter new value greater than max seen far 
WITHOUT_CLASSIFICATION	 not allow view defined temp table other materialized view 
WITHOUT_CLASSIFICATION	 evaluate the expression tree 
WITHOUT_CLASSIFICATION	 bgenjjtree throws 
WITHOUT_CLASSIFICATION	 possible that some operators add records after closing the processor make sure 
WITHOUT_CLASSIFICATION	 query being killed until both the kill and the user return 
WITHOUT_CLASSIFICATION	 the uri requested 
WITHOUT_CLASSIFICATION	 check its simple cast expression 
WITHOUT_CLASSIFICATION	 last item end 
WITHOUT_CLASSIFICATION	 nulls etc vice versa 
WITHOUT_CLASSIFICATION	 default using long types 
WITHOUT_CLASSIFICATION	 top null then there are multiple parents well hence lets use parent statistics get data size also maxsplitsize should 
WITHOUT_CLASSIFICATION	 need consistent with that 
WITHOUT_CLASSIFICATION	 localdate must present 
WITHOUT_CLASSIFICATION	 now run its minor compaction dont collapse events 
WITHOUT_CLASSIFICATION	 the default number threads will that means thread pool not used and operation executed with the current thread 
WITHOUT_CLASSIFICATION	 reset the perflogger 
WITHOUT_CLASSIFICATION	 now make copy 
WITHOUT_CLASSIFICATION	 check result now 
WITHOUT_CLASSIFICATION	 the schedule loop will triggered again when the deallocatetask request comes for the preempted task 
WITHOUT_CLASSIFICATION	 step parse the query set dynamic partitioning nonstrict that queries not need any partition 
WITHOUT_CLASSIFICATION	 test when second argument has nulls 
WITHOUT_CLASSIFICATION	 base configuration active configuration 
WITHOUT_CLASSIFICATION	 create table 
WITHOUT_CLASSIFICATION	 property names needed keep internal structure serde 
WITHOUT_CLASSIFICATION	 allow cross join from teecurmatch 
WITHOUT_CLASSIFICATION	 get failed attempts 
WITHOUT_CLASSIFICATION	 local resources are session based 
WITHOUT_CLASSIFICATION	 populate the driver context with the scratch dir info from the repl context that the temp dirs will cleaned later 
WITHOUT_CLASSIFICATION	 the first entry with accumulated count lower corresponds the lower position 
WITHOUT_CLASSIFICATION	 its difficult impossible pass global things compilation have static cache 
WITHOUT_CLASSIFICATION	 queries like select from where foo calcites rule chokes arguably can insert cast boolean such cases but since postgres oracle and sql server fail compile time for such queries its arcane corner case not worth adding that complexity 
WITHOUT_CLASSIFICATION	 optional buffer use when actually copying data next free position buffer 
WITHOUT_CLASSIFICATION	 table name has present min child and max child 
WITHOUT_CLASSIFICATION	 cache use during optimization 
WITHOUT_CLASSIFICATION	 convert partition partition spec string 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 timeout and unspecified error 
WITHOUT_CLASSIFICATION	 file 
WITHOUT_CLASSIFICATION	 find our bearings the stream normally iter will already point either where want just before however rgs can overlap due encoding may have 
WITHOUT_CLASSIFICATION	 this will only set the metastore being accessed from metastore thrift server not from the cli also only the ttransport being used connect 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 test the idempotent behavior abort txn 
WITHOUT_CLASSIFICATION	 void boolean byte short int float long double 
WITHOUT_CLASSIFICATION	 bgenjjtree typebool 
WITHOUT_CLASSIFICATION	 forward any remaining selected rows 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this logic assumes one dag time was not the case itd keep rewriting 
WITHOUT_CLASSIFICATION	 required required required required required required required required required required optional optional 
WITHOUT_CLASSIFICATION	 timestamp timestamp intervaldaytime intervaldaytime 
WITHOUT_CLASSIFICATION	 keys are always primitive respect the binary 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 todo add section like the restricted configs for overrides when theres more than one 
WITHOUT_CLASSIFICATION	 dryrun true immutable true 
WITHOUT_CLASSIFICATION	 each columns length the value 
WITHOUT_CLASSIFICATION	 query the hbase table and check the key valid and only are present 
WITHOUT_CLASSIFICATION	 invalid filesystem schemes 
WITHOUT_CLASSIFICATION	 insert some data this will again generate only insert deltas and delete deltas delta 
WITHOUT_CLASSIFICATION	 have some sort expression tree try jdoql filter pushdown 
WITHOUT_CLASSIFICATION	 regression test for defect reported hive 
WITHOUT_CLASSIFICATION	 many old tests depend this 
WITHOUT_CLASSIFICATION	 list nondistinct aggrs 
WITHOUT_CLASSIFICATION	 dataschema can obtained from 
WITHOUT_CLASSIFICATION	 gather information about the dpp table scans and store the cache 
WITHOUT_CLASSIFICATION	 context for reading using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow 
WITHOUT_CLASSIFICATION	 nonjavadoc see byte 
WITHOUT_CLASSIFICATION	 default run 
WITHOUT_CLASSIFICATION	 this doesnt throw metaexception when setting high max part count 
WITHOUT_CLASSIFICATION	 project operator can continue 
WITHOUT_CLASSIFICATION	 results 
WITHOUT_CLASSIFICATION	 insert entries txntowriteid for aborted write ids 
WITHOUT_CLASSIFICATION	 store the user name the open request case nonsasl authentication 
WITHOUT_CLASSIFICATION	 the partition outside the tabledir 
WITHOUT_CLASSIFICATION	 can inherited from base class 
WITHOUT_CLASSIFICATION	 load them into set 
WITHOUT_CLASSIFICATION	 clip off seconds portion bring nanoseconds into integer portion 
WITHOUT_CLASSIFICATION	 alter table scenarios 
WITHOUT_CLASSIFICATION	 even without type params return default for varchar 
WITHOUT_CLASSIFICATION	 remember the mapping case scan another branch the mapjoin later 
WITHOUT_CLASSIFICATION	 for dense encoding use bias table lookup for hllnobias algorithm 
WITHOUT_CLASSIFICATION	 void can converted any type 
WITHOUT_CLASSIFICATION	 verify that the node blacklisted 
WITHOUT_CLASSIFICATION	 write the suffix the 
WITHOUT_CLASSIFICATION	 unequal maps 
WITHOUT_CLASSIFICATION	 were told use some port but its occupied fail 
WITHOUT_CLASSIFICATION	 remember the branching ops have visited 
WITHOUT_CLASSIFICATION	 remove the join operator from the query join context data structures coming from qbjointree 
WITHOUT_CLASSIFICATION	 special handling for decimal because decimal types need scale and precision parameter this special handling should avoided using returntype uniformly for all cases 
WITHOUT_CLASSIFICATION	 lookup list bucketing pruner 
WITHOUT_CLASSIFICATION	 after some other submission has evicted 
WITHOUT_CLASSIFICATION	 once move hadoop dependency the following paramteer can used 
WITHOUT_CLASSIFICATION	 buffers offset exist exists and stale doesnt 
WITHOUT_CLASSIFICATION	 through the set key columns and find their representatives the values 
WITHOUT_CLASSIFICATION	 need work little harder for our comparison note round down for integer conversion anything below the next minmax will work 
WITHOUT_CLASSIFICATION	 vectorized implementation hexlong that returns string 
WITHOUT_CLASSIFICATION	 enable trash can tested 
WITHOUT_CLASSIFICATION	 nontest mode emit log file which can different from the normal hivelog for example using log some file with different rolling policy 
WITHOUT_CLASSIFICATION	 doublecompare treats and different 
WITHOUT_CLASSIFICATION	 temptable only set when load rewritten 
WITHOUT_CLASSIFICATION	 iterate through all the operators that have candidate fks and choose the that has the minimum selectivity assume that and this have the pkfk relationship this heuristic and can 
WITHOUT_CLASSIFICATION	 remove the locks waiting state 
WITHOUT_CLASSIFICATION	 stat 
WITHOUT_CLASSIFICATION	 workcheckfileformat set true only for load task assumption here dynamic partition context null 
WITHOUT_CLASSIFICATION	 need check catalog for null parsedbname will never return null for the catalog 
WITHOUT_CLASSIFICATION	 they are not the same constant for example union all and 
WITHOUT_CLASSIFICATION	 there should new base directory base original bucket files delta directories deletedelta directories and the 
WITHOUT_CLASSIFICATION	 not supported for temp tables 
WITHOUT_CLASSIFICATION	 performance problem objectstore does its own new hiveconf 
WITHOUT_CLASSIFICATION	 required optional required required optional 
WITHOUT_CLASSIFICATION	 keep track corresponding col partcols 
WITHOUT_CLASSIFICATION	 ctas should not create void type 
WITHOUT_CLASSIFICATION	 compute the number tasks 
WITHOUT_CLASSIFICATION	 put all the mutation 
WITHOUT_CLASSIFICATION	 test double 
WITHOUT_CLASSIFICATION	 initialize one columns source conversion related arrays assumes inittargetentry has already been called 
WITHOUT_CLASSIFICATION	 passes the test valid 
WITHOUT_CLASSIFICATION	 privilege matrix user user groupa groupb public testdb testtable testtable testtable testtable testdb 
WITHOUT_CLASSIFICATION	 register the new dag identifier thats not the one currently registered 
WITHOUT_CLASSIFICATION	 accepted 
WITHOUT_CLASSIFICATION	 capture system out and err 
WITHOUT_CLASSIFICATION	 allow implicit string varchar conversion and vice versa 
WITHOUT_CLASSIFICATION	 set the run frequency low this test doesnt take long 
WITHOUT_CLASSIFICATION	 call open read data split mockmocktable 
WITHOUT_CLASSIFICATION	 executes job request operation thread pool not created then job request executed current thread itself param jobexecutecallable callable object run the job request task 
WITHOUT_CLASSIFICATION	 keep track view alias read entity corresponding the view for for query like select from where keeps track aliases for vvv this used when added input for the query the parents 
WITHOUT_CLASSIFICATION	 handle kill query results part just put them place will resolve what 
WITHOUT_CLASSIFICATION	 worth 
WITHOUT_CLASSIFICATION	 overflow means this smaller 
WITHOUT_CLASSIFICATION	 all zeroes should have handled this earlier 
WITHOUT_CLASSIFICATION	 before any the other core hive classes are loaded 
WITHOUT_CLASSIFICATION	 date 
WITHOUT_CLASSIFICATION	 need extend the tenancy saw newer file with the same content 
WITHOUT_CLASSIFICATION	 count all rows 
WITHOUT_CLASSIFICATION	 fix temp path for alter table concatenate 
WITHOUT_CLASSIFICATION	 conf and then the children 
WITHOUT_CLASSIFICATION	 but hive supports assigning bucket number for each partition this can vary 
WITHOUT_CLASSIFICATION	 new filter currently support comparison functions and between 
WITHOUT_CLASSIFICATION	 both are last day the month then time part should ignored 
WITHOUT_CLASSIFICATION	 sort cost 
WITHOUT_CLASSIFICATION	 begin abort 
WITHOUT_CLASSIFICATION	 legacy file see its bucket file 
WITHOUT_CLASSIFICATION	 call the real method instead the mock 
WITHOUT_CLASSIFICATION	 replication done now the following verifications 
WITHOUT_CLASSIFICATION	 todo these would need propagated from via progress metricnumber allocated guaranteed executors use metricnumber speculative executors use 
WITHOUT_CLASSIFICATION	 convert udaf params exprnodedesc 
WITHOUT_CLASSIFICATION	 generate absolute path relative home directory 
WITHOUT_CLASSIFICATION	 string encrypt 
WITHOUT_CLASSIFICATION	 create unpartitioned table event 
WITHOUT_CLASSIFICATION	 write addition payload required for orc 
WITHOUT_CLASSIFICATION	 such such abc 
WITHOUT_CLASSIFICATION	 mystructlist 
WITHOUT_CLASSIFICATION	 owner can user role 
WITHOUT_CLASSIFICATION	 whole batch spilled 
WITHOUT_CLASSIFICATION	 tblprops will null user didnt use tblprops his create table cmd 
WITHOUT_CLASSIFICATION	 read that many chars 
WITHOUT_CLASSIFICATION	 count zeros until first nonzero digit encountered 
WITHOUT_CLASSIFICATION	 this table has associated index table then attempt build index mutations 
WITHOUT_CLASSIFICATION	 update needs select all the columns rewrite the entire row also need figure out which columns are going replace 
WITHOUT_CLASSIFICATION	 holds restored from disk big table rows 
WITHOUT_CLASSIFICATION	 for the big table only need promote the next group the current group 
WITHOUT_CLASSIFICATION	 now succeeds 
WITHOUT_CLASSIFICATION	 convert the fields 
WITHOUT_CLASSIFICATION	 try using jdbc metadata api get column list user should fail 
WITHOUT_CLASSIFICATION	 data structure control whether certain reference present every 
WITHOUT_CLASSIFICATION	 find the value object update the timestamp the keyvalue value matches the criteria 
WITHOUT_CLASSIFICATION	 replace insert overwrite insert into ast tree will have this shape tokquery tokfrom tokinsert tokdestination this token replaced tokinsertinto toktab toktabname defaultcmvmatview tokselect 
WITHOUT_CLASSIFICATION	 are waiting for connection for long time something really wrong better raise error than hang forever see 
WITHOUT_CLASSIFICATION	 join from multiple relations 
WITHOUT_CLASSIFICATION	 map cvalue map rowvalues assertequals cvaluesize 
WITHOUT_CLASSIFICATION	 keycolumn output name valuetag 
WITHOUT_CLASSIFICATION	 get the sign the decimal 
WITHOUT_CLASSIFICATION	 lazy mode only list files and expect that the eventual copy will pull data default that the import mode insert overwrite writeids snapshot for replicating acidmm tables default means replload bootstrapdump export 
WITHOUT_CLASSIFICATION	 see the filename matches and can read 
WITHOUT_CLASSIFICATION	 create dbtdtpart dtpart dtpart test recycle single file dtpart recycle single partition recycle table 
WITHOUT_CLASSIFICATION	 when union followed multitable insert 
WITHOUT_CLASSIFICATION	 getallfunctions 
WITHOUT_CLASSIFICATION	 this operator has been visited already the rule 
WITHOUT_CLASSIFICATION	 when were inserting the key would have inserted here theres key 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring 
WITHOUT_CLASSIFICATION	 dont recheck with next only lists each collisions 
WITHOUT_CLASSIFICATION	 instances likely incorrect 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 try marking the query complete this external submission 
WITHOUT_CLASSIFICATION	 todo unique join 
WITHOUT_CLASSIFICATION	 then its dynamic partitioning case and shouldnt check the table itself 
WITHOUT_CLASSIFICATION	 simple task registration and unregistration 
WITHOUT_CLASSIFICATION	 getcolumn should return the instance passed 
WITHOUT_CLASSIFICATION	 aws settings 
WITHOUT_CLASSIFICATION	 walk through all the source vertices 
WITHOUT_CLASSIFICATION	 can happen retrying deleting the zlock after exceptions like race condition where parent has already been deleted other process when deleted both cases should not raise error 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 for now throw away file 
WITHOUT_CLASSIFICATION	 override nothing the this test not related with vectorization the parent class creates temporary table this test and alters its properties not override this test that temporary table needs renamed however mentioned this does not serve any purpose this test does not relate vectorization 
WITHOUT_CLASSIFICATION	 should only get here retrying this 
WITHOUT_CLASSIFICATION	 any filters are present the join tree push them top the table 
WITHOUT_CLASSIFICATION	 rfile new 
WITHOUT_CLASSIFICATION	 json key always string 
WITHOUT_CLASSIFICATION	 all the objects must different 
WITHOUT_CLASSIFICATION	 delta 
WITHOUT_CLASSIFICATION	 generate select node for windowing 
WITHOUT_CLASSIFICATION	 empty queue but capacity available due waitqueuesize and return the element 
WITHOUT_CLASSIFICATION	 startdate wed full timestamp full day name 
WITHOUT_CLASSIFICATION	 the name this column the hive schema 
WITHOUT_CLASSIFICATION	 highword digits 
WITHOUT_CLASSIFICATION	 movedthis may change 
WITHOUT_CLASSIFICATION	 get the index separating the user name from domain name the users name the first param username full user name return index domain match not found 
WITHOUT_CLASSIFICATION	 uses pattern and specifies 
WITHOUT_CLASSIFICATION	 build reduce sink details keycols valuecols outcolnames etc for this ptfdesc 
WITHOUT_CLASSIFICATION	 change local 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 different number field names 
WITHOUT_CLASSIFICATION	 process the second childif exists node get partition specs 
WITHOUT_CLASSIFICATION	 initialize using one target data type info 
WITHOUT_CLASSIFICATION	 there are multiple stats for the same scheme from different namenode this method will squash them together 
WITHOUT_CLASSIFICATION	 cant use cacheloader because searcharguments may built either from kryo strings 
WITHOUT_CLASSIFICATION	 since existsnot exists are not affected presence null keys not need generate count countc 
WITHOUT_CLASSIFICATION	 move file would require session details needcopy invokes sessionstateget 
WITHOUT_CLASSIFICATION	 check fast 
WITHOUT_CLASSIFICATION	 hold one refcount 
WITHOUT_CLASSIFICATION	 not 
WITHOUT_CLASSIFICATION	 disallow changing temp table location 
WITHOUT_CLASSIFICATION	 nulls are handled each individual base writer setter could handle nulls centrally here but that would result spurious allocs 
WITHOUT_CLASSIFICATION	 simple storagebased auth have information about columns living different files simple partitionauth and ignore the columns parameter 
WITHOUT_CLASSIFICATION	 prepare the tmp output directory the output tmp directory should exist before jobclose before renaming after job completion 
WITHOUT_CLASSIFICATION	 puts gets hits unused unused 
WITHOUT_CLASSIFICATION	 union type not supported calcite 
WITHOUT_CLASSIFICATION	 todo see can get rid this used one place distinguish archived parts 
WITHOUT_CLASSIFICATION	 failed update this task instead retrying for this task find another change isguaranteed and modify maps wed need the epic lock will not 
WITHOUT_CLASSIFICATION	 modify tablescanoperator inplace knows operate vectorized 
WITHOUT_CLASSIFICATION	 strip off the delimiter 
WITHOUT_CLASSIFICATION	 new row also inserted the usual delta file for update event 
WITHOUT_CLASSIFICATION	 finish the scheduled compaction for ttp 
WITHOUT_CLASSIFICATION	 current installed configuration 
WITHOUT_CLASSIFICATION	 config set table not temporary and partition being inserted exists capture the list files added for not yet existing partitions insert overwrite new partition dynamic partition inserts the add partition event will capture the list files added generate insert event only inserting into existing partition 
WITHOUT_CLASSIFICATION	 this used during translation decide the internalname alias mapping from the input the ptf carried forward when building the output for this ptf this used internal ptfs noop make names its input available the output general this should false and the names used for the output columns must provided the ptf writer the function getoutputnames 
WITHOUT_CLASSIFICATION	 wait for all writes finish before actually close 
WITHOUT_CLASSIFICATION	 type target column 
WITHOUT_CLASSIFICATION	 partitioned table 
WITHOUT_CLASSIFICATION	 the dispatcher fires the processor corresponding the closest matching rule 
WITHOUT_CLASSIFICATION	 host 
WITHOUT_CLASSIFICATION	 floor date special handling since function hive does include timeunit observe that timeunit information implicit the function name thus translation will proceed correctly just ignore the timeunit 
WITHOUT_CLASSIFICATION	 fktabledb 
WITHOUT_CLASSIFICATION	 prefer right most alias 
WITHOUT_CLASSIFICATION	 lrfuthreshold inf this case 
WITHOUT_CLASSIFICATION	 reached end split 
WITHOUT_CLASSIFICATION	 there any confict then not generate the new select otherwise add into the calcitecollst and generate the new select 
WITHOUT_CLASSIFICATION	 ignore nullscanoptimized paths 
WITHOUT_CLASSIFICATION	 setup our inner big table only join specific members 
WITHOUT_CLASSIFICATION	 always want recreate dont know were created the 
WITHOUT_CLASSIFICATION	 blocking operator groupbyoperator and joinoperator can 
WITHOUT_CLASSIFICATION	 foreigncatalogname 
WITHOUT_CLASSIFICATION	 iterate over the symbol functions the chain are not the end the iterator row null match the current componentfn returns false then return false otherwise set row the next row from the iterator are the end the iterator skip any optional symbol fns star patterns the end but come non optional symbol return false match all fns the chain return true 
WITHOUT_CLASSIFICATION	 xmx not specified 
WITHOUT_CLASSIFICATION	 set longpollingtimeout custom value for different test cases 
WITHOUT_CLASSIFICATION	 note that reset also resets the data buffer for bytes column vectors 
WITHOUT_CLASSIFICATION	 were here well proceed down the next while loop iteration 
WITHOUT_CLASSIFICATION	 create data buffers for value bytes column vectors 
WITHOUT_CLASSIFICATION	 compare every rows 
WITHOUT_CLASSIFICATION	 create object inspector 
WITHOUT_CLASSIFICATION	 lock types 
WITHOUT_CLASSIFICATION	 find the old database 
WITHOUT_CLASSIFICATION	 the sorting order the parent more specific they are equal will copy the order from the child and then fill the order the rest columns with the one taken from parent 
WITHOUT_CLASSIFICATION	 this method mainly intended for debug display purposes 
WITHOUT_CLASSIFICATION	 define summary metrics for each column 
WITHOUT_CLASSIFICATION	 this case have scale before division otherwise might lose precision this costly but inevitable 
WITHOUT_CLASSIFICATION	 but want just reset its null 
WITHOUT_CLASSIFICATION	 optimize performance only looking the first key series equal keys 
WITHOUT_CLASSIFICATION	 generic options parser doesnt seem work 
WITHOUT_CLASSIFICATION	 allocate higher priority should not allocated 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 this done preread stage where theres nothing special wrefcounts just release 
WITHOUT_CLASSIFICATION	 set the comparison the iocontext and the type the udf 
WITHOUT_CLASSIFICATION	 create the hfile writer 
WITHOUT_CLASSIFICATION	 try extended deduplication 
WITHOUT_CLASSIFICATION	 the name the field not optional 
WITHOUT_CLASSIFICATION	 need preserve authorizer flag 
WITHOUT_CLASSIFICATION	 builtin functions shouldnt the session registry and temp functions shouldnt the system registry persistent functions can either registry 
WITHOUT_CLASSIFICATION	 required required required required optional optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 force avoid reading footers 
WITHOUT_CLASSIFICATION	 the result already present the cache return 
WITHOUT_CLASSIFICATION	 setup for different kinds vectorized reading supported read the vectorized input file format which returns vectorizedrowbatch the row read using deserialize each row into the vectorizedrowbatch and read using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow 
WITHOUT_CLASSIFICATION	 meaningful path for nonacidacid 
WITHOUT_CLASSIFICATION	 records will emited from hive 
WITHOUT_CLASSIFICATION	 filterg cannot parse quoted date try parse date here too 
WITHOUT_CLASSIFICATION	 verify whether the sql operation log generated and fetch correctly async mode 
WITHOUT_CLASSIFICATION	 get access the queryinfo instance 
WITHOUT_CLASSIFICATION	 set the row values 
WITHOUT_CLASSIFICATION	 handle mergejoin specially and check for all its children 
WITHOUT_CLASSIFICATION	 push filter top children for discardable 
WITHOUT_CLASSIFICATION	 leftfast 
WITHOUT_CLASSIFICATION	 run initiator clean the row fro the aborted transaction from txns 
WITHOUT_CLASSIFICATION	 there single column return the number distinct values 
WITHOUT_CLASSIFICATION	 test serialization and deserialization with different schemas 
WITHOUT_CLASSIFICATION	 use the rewritten ast 
WITHOUT_CLASSIFICATION	 for writing out single byte 
WITHOUT_CLASSIFICATION	 then search from parent 
WITHOUT_CLASSIFICATION	 the toplevel object inspector nonsettable return false 
WITHOUT_CLASSIFICATION	 the keyevaluate reuses storage that doesnt work with smb mapjoin because holds references keys merging 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 give hms time handle close request 
WITHOUT_CLASSIFICATION	 used for 
WITHOUT_CLASSIFICATION	 ckey not present parent 
WITHOUT_CLASSIFICATION	 replacement exists for this code point emit out the replacement and append the 
WITHOUT_CLASSIFICATION	 try scheduling again 
WITHOUT_CLASSIFICATION	 optional string mystring 
WITHOUT_CLASSIFICATION	 set bit field not null 
WITHOUT_CLASSIFICATION	 retried means retries each with retry with random sleep 
WITHOUT_CLASSIFICATION	 prune partitions 
WITHOUT_CLASSIFICATION	 test that existing exclusive with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 mapjoin should not affected join reordering 
WITHOUT_CLASSIFICATION	 hadoop and earlier way finding the sasl property settings initialize the saslrpcserver ensure qop parameters are read from conf 
WITHOUT_CLASSIFICATION	 drop table with partitions 
WITHOUT_CLASSIFICATION	 thus run the field trimmer again push them back down 
WITHOUT_CLASSIFICATION	 datanucleus propagates some pointless exceptions and rolls back the finally 
WITHOUT_CLASSIFICATION	 get nonexistent key should terminate 
WITHOUT_CLASSIFICATION	 double scalarlong column 
WITHOUT_CLASSIFICATION	 ifelse chain arranged likely order frequency for performance 
WITHOUT_CLASSIFICATION	 check required privileges subset available privileges 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 print operators 
WITHOUT_CLASSIFICATION	 weve already dropped testdbname constructor also drop teardownafterclass 
WITHOUT_CLASSIFICATION	 multiply inverse the division 
WITHOUT_CLASSIFICATION	 data for bucket expect length bucket file 
WITHOUT_CLASSIFICATION	 similarly this map which vectorized row batch columns are the big table value columns since may have value expressions that produce new scratch columns need mapping 
WITHOUT_CLASSIFICATION	 detect incorrect lists 
WITHOUT_CLASSIFICATION	 stripes satisfies the condition 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 user could encounter this stored view definition contains old sql construct which has been eliminated later hive version need provide full debugging info help with fixing the view definition 
WITHOUT_CLASSIFICATION	 restore the old value 
WITHOUT_CLASSIFICATION	 now that are using left outer join join inner count count with outer table wouldnt able tell count zero for inner table since inner join with correlated values will get rid all values where join cond not true where actual inner table will produce zero result handle this case need check both count zero count null 
WITHOUT_CLASSIFICATION	 default means leave tez decide 
WITHOUT_CLASSIFICATION	 restrict the set columns that want read from the accumulo table 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 previous insertunion creates data files 
WITHOUT_CLASSIFICATION	 rowcnt than its either empty table table which stats are not computed assume the worse and dont attempt optimize 
WITHOUT_CLASSIFICATION	 for calciteplanner store qualified name too 
WITHOUT_CLASSIFICATION	 list need refactored out done only once 
WITHOUT_CLASSIFICATION	 try the base config 
WITHOUT_CLASSIFICATION	 affects some less obscure scenario 
WITHOUT_CLASSIFICATION	 for primary keys retrieve the column descriptors retrievecd true which means alter table statement create table statement but are 
WITHOUT_CLASSIFICATION	 hive jars the accumulo classpath which dont want 
WITHOUT_CLASSIFICATION	 the dpp operatorbranch are equal 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 add the input with project top 
WITHOUT_CLASSIFICATION	 make sure handle unary and correctly 
WITHOUT_CLASSIFICATION	 make sure that the unwanted key not present the map 
WITHOUT_CLASSIFICATION	 setup our hash table specialization will the first time the process method called after hybrid grace reload 
WITHOUT_CLASSIFICATION	 create lock and trigger heartbeat with heartbeat the lock wont expire 
WITHOUT_CLASSIFICATION	 for the fetchtask the limit optimization requires fetch all the rows memory and count how many rows get its not practical the 
WITHOUT_CLASSIFICATION	 create the mapping corresponding the grouping set 
WITHOUT_CLASSIFICATION	 out the escaped byte the block above already 
WITHOUT_CLASSIFICATION	 otherwise only order expressions 
WITHOUT_CLASSIFICATION	 gosh really both scaling and down unscaledvalue significand scale twoscaledownscale check overflow while preserving precision need real multiplication 
WITHOUT_CLASSIFICATION	 may parsing delta for insertonly table which may not even orc file cannot have rowids 
WITHOUT_CLASSIFICATION	 get exception finding partition could describe table key return null continue processing for describe table key 
WITHOUT_CLASSIFICATION	 test the mapping empty string all columns 
WITHOUT_CLASSIFICATION	 updated bytes per reducer default 
WITHOUT_CLASSIFICATION	 two the optimization rules convertjoinmapjoin and are put into stats dependent optimizations and run together tezcompiler theres guarantee which one runs first but either case the prior one may have removed chain which the latter one not aware need remember the leaf nodes that chain can skipped for example convertjoinmapjoin removing the reduce sink may also have removed dynamic partition pruning operator chain however doesnt know this and still tries traverse that removed chain which will cause npe this may also happen when happens first 
WITHOUT_CLASSIFICATION	 name for materialization rebuild name for materialization rebuild 
WITHOUT_CLASSIFICATION	 convert expr expr usually input ref except for top 
WITHOUT_CLASSIFICATION	 create semantic analyzer for the query 
WITHOUT_CLASSIFICATION	 which was modified the update stmt choose nonconflicting one 
WITHOUT_CLASSIFICATION	 not release beyond current stream dont know which rgs that buffer for 
WITHOUT_CLASSIFICATION	 move offset point start next input 
WITHOUT_CLASSIFICATION	 all are null all must selected 
WITHOUT_CLASSIFICATION	 the last char escape char read the actual char the serialization format escape and make sure the string nullterminated 
WITHOUT_CLASSIFICATION	 assumes bucketnnnnn format file name 
WITHOUT_CLASSIFICATION	 get column name 
WITHOUT_CLASSIFICATION	 this what the vectorizer class does 
WITHOUT_CLASSIFICATION	 transform the between clause and clause with not top invert true this more straightforward the evaluateexpression method will deal with 
WITHOUT_CLASSIFICATION	 convert lazy object and return 
WITHOUT_CLASSIFICATION	 used determined whether the merge can happen 
WITHOUT_CLASSIFICATION	 succeed trying set transactional true and satisfies bucketing and inputoutputformat requirement 
WITHOUT_CLASSIFICATION	 note should gather stats rather than merge job since dont 
WITHOUT_CLASSIFICATION	 initial state one connection 
WITHOUT_CLASSIFICATION	 inner classes 
WITHOUT_CLASSIFICATION	 should through single process call 
WITHOUT_CLASSIFICATION	 either schema literal schema url serialization class must provided 
WITHOUT_CLASSIFICATION	 seconds wait for app visible 
WITHOUT_CLASSIFICATION	 given the current input row the mapping for input col info columns and cols return the relative path corresponding the row 
WITHOUT_CLASSIFICATION	 sort contains limit operation bail out 
WITHOUT_CLASSIFICATION	 none the operators changing the positions 
WITHOUT_CLASSIFICATION	 first operator the reduce task not the reducesinkoperator but the 
WITHOUT_CLASSIFICATION	 this just simple way generate test data 
WITHOUT_CLASSIFICATION	 create enough note have have separate keyvalueconverter for each keyvalue because the keyvalueconverters can reuse the internal object its not safe use the same keyvalueconverter convert multiple keyvalues 
WITHOUT_CLASSIFICATION	 use the trick mentioned less hashing same performance building better bloom filter kirsch etal from abstract only two hash functions are necessary effectively implement bloom filter without any loss the asymptotic false positive probability 
WITHOUT_CLASSIFICATION	 validate the metastore client call ensure throws exception partition fields contain unicode characters commas 
WITHOUT_CLASSIFICATION	 first item 
WITHOUT_CLASSIFICATION	 check for dpp and semijoin dpp 
WITHOUT_CLASSIFICATION	 any value will become zero even possibility rounding 
WITHOUT_CLASSIFICATION	 will cause overflow for result position must yield null 
WITHOUT_CLASSIFICATION	 this error code should really produced hive 
WITHOUT_CLASSIFICATION	 metastore calls timing information 
WITHOUT_CLASSIFICATION	 when truncated included used its length must least the number source type infos when longer assume the caller will default with nulls etc 
WITHOUT_CLASSIFICATION	 are close phase have definitely exhausted the big table input 
WITHOUT_CLASSIFICATION	 check array null empty value null 
WITHOUT_CLASSIFICATION	 pick the formatter use display the results either the normal human readable output json object 
WITHOUT_CLASSIFICATION	 the decimalcolumnvector set method will quickly copy the deserialized decimal writable fields 
WITHOUT_CLASSIFICATION	 find the file the include path 
WITHOUT_CLASSIFICATION	 primarily avoid multiple shutdowns 
WITHOUT_CLASSIFICATION	 temporary tables created during the execution are not the input sources 
WITHOUT_CLASSIFICATION	 null principal 
WITHOUT_CLASSIFICATION	 replace original varpopx with sumx sumx sumx countx countx 
WITHOUT_CLASSIFICATION	 use stringbuilder and then call printerror only once printerror will write both stderr and the error log file situations where both the stderr and the log file output simultaneously output single stream this will look cleaner 
WITHOUT_CLASSIFICATION	 private boolean isouterjoin 
WITHOUT_CLASSIFICATION	 dont send messages pending tasks with the flags they should killed elsewhere 
WITHOUT_CLASSIFICATION	 recursive flush flush all the tree operators 
WITHOUT_CLASSIFICATION	 test does not corrupt existing values mapred env configs 
WITHOUT_CLASSIFICATION	 sort the works that get consistent query plan for multi executionsfor test verification 
WITHOUT_CLASSIFICATION	 process the report 
WITHOUT_CLASSIFICATION	 for wordshift 
WITHOUT_CLASSIFICATION	 thursday august 
WITHOUT_CLASSIFICATION	 neither dir should get created 
WITHOUT_CLASSIFICATION	 statistics annotation fetches column statistics for all required columns which can 
WITHOUT_CLASSIFICATION	 just had leading zeroes value 
WITHOUT_CLASSIFICATION	 find the ancestor which exists check its permissions 
WITHOUT_CLASSIFICATION	 load properties from sparkdefaultsconf 
WITHOUT_CLASSIFICATION	 prepare aggs for updating 
WITHOUT_CLASSIFICATION	 this method updates the input expr changing all the exprnodecolumndesc refer columns given the colexprmap for instance col would become valuecol the execution engine expects filters the join operators expressed that way 
WITHOUT_CLASSIFICATION	 new ngram 
WITHOUT_CLASSIFICATION	 the input node not operator bail out 
WITHOUT_CLASSIFICATION	 should find alias this insert and alias this however wont fix positional order alias case cause wed still have star the top level bail 
WITHOUT_CLASSIFICATION	 set insert doesnt set off but update does 
WITHOUT_CLASSIFICATION	 first generate the expression for the partition and sort keys the cluster clause distribute clause has the aliases for partition function 
WITHOUT_CLASSIFICATION	 because there relationship between cds and sds must set the sds null first before dropping the storage descriptor satisfy foreign key constraints 
WITHOUT_CLASSIFICATION	 see bucketnumreducersq bucketnumreducersq 
WITHOUT_CLASSIFICATION	 are ctas know there are partitions 
WITHOUT_CLASSIFICATION	 the character set for decoding constant can optimize that 
WITHOUT_CLASSIFICATION	 retry the next port 
WITHOUT_CLASSIFICATION	 deltas and base and leave them the cleaner clean 
WITHOUT_CLASSIFICATION	 have already locked the table dont lock the partitions 
WITHOUT_CLASSIFICATION	 root tran must mapinput 
WITHOUT_CLASSIFICATION	 for dpp case dpp sink will appear task and the target work dpp sink will appear task task the child task task task will traversed before task because taskgraphwalker will first put children task the front task queue spark work which equal other found and removed task the dpp sink can removed when task traversedmore detailed see hive 
WITHOUT_CLASSIFICATION	 appended once all the session list are added the url 
WITHOUT_CLASSIFICATION	 insert two rows acid table 
WITHOUT_CLASSIFICATION	 left and right aliases are all valid two values will inner joined 
WITHOUT_CLASSIFICATION	 doesnt vectorize uses neither the vectorzied acid readers 
WITHOUT_CLASSIFICATION	 quotedquerystring 
WITHOUT_CLASSIFICATION	 tracks the last nonoob heartbeat number which counters were sent the 
WITHOUT_CLASSIFICATION	 aggkey statswork used for stats aggregation while statsaggprefix filesinkdesc used for stats publishing they should consistent 
WITHOUT_CLASSIFICATION	 llap object cache unlike others does not use globals thus get the existing one 
WITHOUT_CLASSIFICATION	 start with ssl 
WITHOUT_CLASSIFICATION	 boolean value match for other lengths 
WITHOUT_CLASSIFICATION	 cannot acid 
WITHOUT_CLASSIFICATION	 writing partitioned table then pigschema will have more columns than tableschema partition columns are not part tableschema nothing matching fschemaalias found target table schema log 
WITHOUT_CLASSIFICATION	 see 
WITHOUT_CLASSIFICATION	 collect bucket andor partition information for object hashing 
WITHOUT_CLASSIFICATION	 every slot long 
WITHOUT_CLASSIFICATION	 will try combine multiple clauses into smaller number with compatible keys 
WITHOUT_CLASSIFICATION	 flag for each byte indicate escape needed 
WITHOUT_CLASSIFICATION	 check all the bit vectors can merge 
WITHOUT_CLASSIFICATION	 invalid number 
WITHOUT_CLASSIFICATION	 initialize buffer read the entire stripe 
WITHOUT_CLASSIFICATION	 addpartitions err duplicate keyvals mpart 
WITHOUT_CLASSIFICATION	 dispatch current node 
WITHOUT_CLASSIFICATION	 queries rejected from being cached due nondeterministic functions temp tables other conditions 
WITHOUT_CLASSIFICATION	 called during translation invokes createevaluator which must implemented subclass sets the evaluator with references the tabledef partitionclass partitionmemsize and the transformsrawinput boolean 
WITHOUT_CLASSIFICATION	 handle rename and other changes 
WITHOUT_CLASSIFICATION	 rule cannot applied there are groupingid because will change the value the position will changed 
WITHOUT_CLASSIFICATION	 the length the long array that needs passed 
WITHOUT_CLASSIFICATION	 allow accessing field list element structs directly from list 
WITHOUT_CLASSIFICATION	 print the key 
WITHOUT_CLASSIFICATION	 kill query null then session might have been released pool closed already 
WITHOUT_CLASSIFICATION	 this can easily merged into union 
WITHOUT_CLASSIFICATION	 have kerberos credentials should obtain the delegation token 
WITHOUT_CLASSIFICATION	 first check can run llap need use user code scriptudf dont 
WITHOUT_CLASSIFICATION	 should always get different object and cluster fraction should propagated 
WITHOUT_CLASSIFICATION	 set ismanaged false this not load data operation for which needed 
WITHOUT_CLASSIFICATION	 check status compaction job 
WITHOUT_CLASSIFICATION	 while there are still nodes dispatch 
WITHOUT_CLASSIFICATION	 cachechunks the list just cachechunks from that point 
WITHOUT_CLASSIFICATION	 have more control 
WITHOUT_CLASSIFICATION	 for rest the join type will take min the reduction 
WITHOUT_CLASSIFICATION	 verify that ptned table rename succeded 
WITHOUT_CLASSIFICATION	 bucket map join the big tables bucketing version considered 
WITHOUT_CLASSIFICATION	 stuff 
WITHOUT_CLASSIFICATION	 dump and load insert after truncate record 
WITHOUT_CLASSIFICATION	 branches 
WITHOUT_CLASSIFICATION	 copy the digits the right side the array 
WITHOUT_CLASSIFICATION	 are replacing the current big table with new one thus 
WITHOUT_CLASSIFICATION	 set the actual events for the tasks 
WITHOUT_CLASSIFICATION	 required required required required required required required required required required required required optional optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 expected invoked once 
WITHOUT_CLASSIFICATION	 register taskattempt unregister container taskattempt should also unregistered 
WITHOUT_CLASSIFICATION	 neither cred provider conf have entry return null 
WITHOUT_CLASSIFICATION	 and skewed columns skewed keys selectopclone 
WITHOUT_CLASSIFICATION	 ditto 
WITHOUT_CLASSIFICATION	 free the memory for the column vectors 
WITHOUT_CLASSIFICATION	 the cache and date 
WITHOUT_CLASSIFICATION	 take integer part 
WITHOUT_CLASSIFICATION	 this happens when the code inside the jmx bean setter from the java docs threw exception log and skip outputting the attribute 
WITHOUT_CLASSIFICATION	 used make sure that waiting getsessions dont block update 
WITHOUT_CLASSIFICATION	 example for using cluster configuration xmls 
WITHOUT_CLASSIFICATION	 schemas not match currently not merge 
WITHOUT_CLASSIFICATION	 fail the query the stats are supposed reliable 
WITHOUT_CLASSIFICATION	 create list topop nodes and walk 
WITHOUT_CLASSIFICATION	 simulate delay before finishing the task 
WITHOUT_CLASSIFICATION	 operator since want individually track the number rows from different inputs 
WITHOUT_CLASSIFICATION	 get actual number rows from metastore 
WITHOUT_CLASSIFICATION	 update skew task 
WITHOUT_CLASSIFICATION	 make sure the compactor has chance run once 
WITHOUT_CLASSIFICATION	 which are going return values from the input batch vector expressions 
WITHOUT_CLASSIFICATION	 test repeating null selection 
WITHOUT_CLASSIFICATION	 now delete node for this keys list 
WITHOUT_CLASSIFICATION	 hash code logic from original 
WITHOUT_CLASSIFICATION	 gss credentials for server 
WITHOUT_CLASSIFICATION	 for hybrid grace hash join during the round processing only keep the left side the row not spilled 
WITHOUT_CLASSIFICATION	 dump the drop events and check tables are getting dropped target well 
WITHOUT_CLASSIFICATION	 walk through all found table locations get the most encrypted table 
WITHOUT_CLASSIFICATION	 this the adjusted index after nested column pruning for instance given the struct type sstructaint bboolean only used the pruned type sstructbboolean here the index field changed from when look the data from parquet index needs adjusted accordingly note currently this only used the read path 
WITHOUT_CLASSIFICATION	 matching methods found 
WITHOUT_CLASSIFICATION	 optimize for common case just one row for key container acts iterator 
WITHOUT_CLASSIFICATION	 since map key for pig has strings 
WITHOUT_CLASSIFICATION	 all data and partition columns 
WITHOUT_CLASSIFICATION	 runs instance returns whether not succeeded 
WITHOUT_CLASSIFICATION	 ctrld 
WITHOUT_CLASSIFICATION	 this method must only process nondecimal column vectors convert decimal columns regular decimal 
WITHOUT_CLASSIFICATION	 construct mapping partitionbucket file names and partition bucket number 
WITHOUT_CLASSIFICATION	 generate tags 
WITHOUT_CLASSIFICATION	 mapping from directory which filesinkoperator writes into the columns which that 
WITHOUT_CLASSIFICATION	 create curatorframework instance used the zookeeper client use the create appropriate acls 
WITHOUT_CLASSIFICATION	 should share cte contexts 
WITHOUT_CLASSIFICATION	 assumes that the catalog has already been set 
WITHOUT_CLASSIFICATION	 restart theres internal error 
WITHOUT_CLASSIFICATION	 ndvexpr maxndv expr args 
WITHOUT_CLASSIFICATION	 error creation and want delete anyway 
WITHOUT_CLASSIFICATION	 passwordfile file 
WITHOUT_CLASSIFICATION	 open accumulo connection 
WITHOUT_CLASSIFICATION	 greg firstname 
WITHOUT_CLASSIFICATION	 required optional optional optional 
WITHOUT_CLASSIFICATION	 after this the keywrappers are properly set and hash code computed 
WITHOUT_CLASSIFICATION	 sum input cardinalities 
WITHOUT_CLASSIFICATION	 the user has specified ignore mapjoin hint 
WITHOUT_CLASSIFICATION	 new tai lue letter high bytes 
WITHOUT_CLASSIFICATION	 todo using multiple places serde cache pass this 
WITHOUT_CLASSIFICATION	 empty array 
WITHOUT_CLASSIFICATION	 its not obvious that this the right logic dont record the callback for example and never notify the client job completion 
WITHOUT_CLASSIFICATION	 issue warning for missing file and throw exception 
WITHOUT_CLASSIFICATION	 the absence sorted clause the sorted dynamic partition insert 
WITHOUT_CLASSIFICATION	 timer null start new one timer has completed during previous invocation start new one timer already started and not completed leaving running without resetting 
WITHOUT_CLASSIFICATION	 test that clidriver does not strip comments starting with 
WITHOUT_CLASSIFICATION	 create dest table partitions with custom locations 
WITHOUT_CLASSIFICATION	 substring index refers the index last char the array 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 add mapside ptf operator needed 
WITHOUT_CLASSIFICATION	 clear integer portion keep fraction 
WITHOUT_CLASSIFICATION	 xor yields xor yields 
WITHOUT_CLASSIFICATION	 temp functions are not allowed have qualified names 
WITHOUT_CLASSIFICATION	 sanity check 
WITHOUT_CLASSIFICATION	 alter existing partition aaa via objectstore 
WITHOUT_CLASSIFICATION	 typequalifiers 
WITHOUT_CLASSIFICATION	 abortedbits should all true everything exceptions are aborted txns 
WITHOUT_CLASSIFICATION	 merge work else create merge work add above work the merge work 
WITHOUT_CLASSIFICATION	 find the writeid high water mark based upon txnid high water mark found then need traverse through all write ids less than writeid hwm make exceptions list 
WITHOUT_CLASSIFICATION	 stuff 
WITHOUT_CLASSIFICATION	 weve already gone beyond the specified range 
WITHOUT_CLASSIFICATION	 state that the driver enters after destroy called and the end driver life cycle 
WITHOUT_CLASSIFICATION	 for example partitions sequencefile and rcfile will have different splits 
WITHOUT_CLASSIFICATION	 check all the contain stats and all the ndv are bitvectors 
WITHOUT_CLASSIFICATION	 find target for fetch task conversion optimizer not allows subqueries 
WITHOUT_CLASSIFICATION	 handle datestring common category and numericstring common category 
WITHOUT_CLASSIFICATION	 range starts here 
WITHOUT_CLASSIFICATION	 equalkey match bytes 
WITHOUT_CLASSIFICATION	 are not throwing exception since might transient issue that blocking loading 
WITHOUT_CLASSIFICATION	 consumes whole key 
WITHOUT_CLASSIFICATION	 store table descriptor maptargetwork 
WITHOUT_CLASSIFICATION	 this dbtable 
WITHOUT_CLASSIFICATION	 check whether substitution allowed 
WITHOUT_CLASSIFICATION	 have failed reserve single header not undo the previous ones here the caller has handle this avoid races 
WITHOUT_CLASSIFICATION	 future thought checkforcompaction will check lot file metadata and may expensive long term should consider having thread pool here and running checkforcompactions parallel 
WITHOUT_CLASSIFICATION	 hardcoded from private field need check the path under what sets for namespace since the namespace created with world acls 
WITHOUT_CLASSIFICATION	 project the big table key into the small table result area 
WITHOUT_CLASSIFICATION	 produces fileglobal row number even with ppd 
WITHOUT_CLASSIFICATION	 subtract the spills get all match and nonmatch rows 
WITHOUT_CLASSIFICATION	 stats bookkeeping 
WITHOUT_CLASSIFICATION	 use the specified database specified 
WITHOUT_CLASSIFICATION	 increments one hms connection 
WITHOUT_CLASSIFICATION	 renamea has interesting behavior and are directories doesnt exist does the expected operation and everything that was now exists will make child thus make sure the rename done before creating the meta files which will create basex 
WITHOUT_CLASSIFICATION	 middle items order 
WITHOUT_CLASSIFICATION	 sanity check 
WITHOUT_CLASSIFICATION	 certain queries like select count from table not have any projected columns and still have isreadallcolumns false such cases columnreaders are not needed however colstoinclude not empty should initialize each columnreader 
WITHOUT_CLASSIFICATION	 note incomplete cbs are always exact match 
WITHOUT_CLASSIFICATION	 helper function create edge property from edge type 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 insert timezone for timestamp type 
WITHOUT_CLASSIFICATION	 handle different types create table command note each branch must call after finalizing table properties 
WITHOUT_CLASSIFICATION	 latin small letter gamma bytes 
WITHOUT_CLASSIFICATION	 this expression that should perform comparison for sorted searches 
WITHOUT_CLASSIFICATION	 skipping because hint mark this info 
WITHOUT_CLASSIFICATION	 divide down just before scaledown get round digit 
WITHOUT_CLASSIFICATION	 invalid time part 
WITHOUT_CLASSIFICATION	 project projects the original expressions 
WITHOUT_CLASSIFICATION	 default serde for rcfile 
WITHOUT_CLASSIFICATION	 javameta javaref javaarraymeta javaref 
WITHOUT_CLASSIFICATION	 test 
WITHOUT_CLASSIFICATION	 rewrite 
WITHOUT_CLASSIFICATION	 the count the elements the above that are set 
WITHOUT_CLASSIFICATION	 create the vertex 
WITHOUT_CLASSIFICATION	 init the rng for breaking ties histogram merging fixed seed specified here aid testing but can eliminated use timebased seed which would make the algorithm nondeterministic 
WITHOUT_CLASSIFICATION	 newdirtrue stats not updated 
WITHOUT_CLASSIFICATION	 the fields older and newer match 
WITHOUT_CLASSIFICATION	 method signature changed hadoop cast provider keyprovider 
WITHOUT_CLASSIFICATION	 decimal 
WITHOUT_CLASSIFICATION	 transaction for which the list tables valid write ids are populated 
WITHOUT_CLASSIFICATION	 set hive provider path hiveconf sethiveproviderpath true simulates property set 
WITHOUT_CLASSIFICATION	 case 
WITHOUT_CLASSIFICATION	 set false block the next loop this must called before draining the lists otherwise addcompletion after draining the lists but before setting false will not trigger run may cause one unnecessary run add comes before drain drain list add request settrue setfalse needs avoided 
WITHOUT_CLASSIFICATION	 extract all the inputformatclass names for each chunk the 
WITHOUT_CLASSIFICATION	 remove semijoin optimization smb join created 
WITHOUT_CLASSIFICATION	 tinyint 
WITHOUT_CLASSIFICATION	 use separate metastore client for heartbeat calls ensure heartbeat rpc calls are 
WITHOUT_CLASSIFICATION	 encoded values can push directly row 
WITHOUT_CLASSIFICATION	 construct keys exprnode 
WITHOUT_CLASSIFICATION	 abc abc abc abc all other cases such abcde 
WITHOUT_CLASSIFICATION	 key only string 
WITHOUT_CLASSIFICATION	 drop existing partition bbb via objectstore 
WITHOUT_CLASSIFICATION	 the getter should remove the escape character for 
WITHOUT_CLASSIFICATION	 byte 
WITHOUT_CLASSIFICATION	 txn 
WITHOUT_CLASSIFICATION	 populate the filters structure 
WITHOUT_CLASSIFICATION	 multibyte trims 
WITHOUT_CLASSIFICATION	 option bypass job setup and cleanup was introduced hadoop mapreduce 
WITHOUT_CLASSIFICATION	 case 
WITHOUT_CLASSIFICATION	 lsb bits 
WITHOUT_CLASSIFICATION	 return value modulo but always the positive range and with the mask zero the sign bit make the input mod positive the output will definitely positive 
WITHOUT_CLASSIFICATION	 the database directory 
WITHOUT_CLASSIFICATION	 that geti returns null rather than arrayoutofbounds 
WITHOUT_CLASSIFICATION	 need not any instanceof checks for following 
WITHOUT_CLASSIFICATION	 filtering handled the input batch processing 
WITHOUT_CLASSIFICATION	 this class provides support collect mapreduce stderrstdoutsyslogs from jobtracker and stored into hdfs location the log directory layout compact lilogsjobid directory for jobid directory for attemptid since there api retrieve mapreduce log from jobtracker the code retrieve from jobtracker and parse the html file the current parser only works with hadoop for hadoop would need different parser 
WITHOUT_CLASSIFICATION	 take care 
WITHOUT_CLASSIFICATION	 also null down undone 
WITHOUT_CLASSIFICATION	 this file system counter valid and create counter 
WITHOUT_CLASSIFICATION	 were passing client credentials null since want them read from the subject 
WITHOUT_CLASSIFICATION	 this the case where distinct cols are part keys which case still need add out put col names 
WITHOUT_CLASSIFICATION	 important get exception name collision 
WITHOUT_CLASSIFICATION	 validate true default enable the constraint 
WITHOUT_CLASSIFICATION	 dummy insert into command mark proper last repl after dump 
WITHOUT_CLASSIFICATION	 need check whether this transaction valid and open 
WITHOUT_CLASSIFICATION	 case 
WITHOUT_CLASSIFICATION	 make random array longs 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 return result 
WITHOUT_CLASSIFICATION	 the destf this case the replaced destf still preserves the original destfs permission 
WITHOUT_CLASSIFICATION	 the regexes look for the log files 
WITHOUT_CLASSIFICATION	 that long see abandoned session 
WITHOUT_CLASSIFICATION	 this will null slaves 
WITHOUT_CLASSIFICATION	 invalid zone 
WITHOUT_CLASSIFICATION	 package permission that can construct but others cannot 
WITHOUT_CLASSIFICATION	 get databases for schema pattern 
WITHOUT_CLASSIFICATION	 its populated from lock info 
WITHOUT_CLASSIFICATION	 the retry logic reached after copy error then include the copied file well this needed cannot figure out which file incorrectly copied expecting distcp skip the properly copied file based crc check copy crc mismatch 
WITHOUT_CLASSIFICATION	 change the table partition for collecting stats 
WITHOUT_CLASSIFICATION	 case 
WITHOUT_CLASSIFICATION	 call describe 
WITHOUT_CLASSIFICATION	 the primary isnt done push back into the readers 
WITHOUT_CLASSIFICATION	 run cleaner 
WITHOUT_CLASSIFICATION	 the writable return serialize 
WITHOUT_CLASSIFICATION	 connect using the delegation token passed via configuration object 
WITHOUT_CLASSIFICATION	 got exception that not ioexception typically oom indexoutofbounds internalerror this most likely corruption 
WITHOUT_CLASSIFICATION	 all the branches need bucketleaves 
WITHOUT_CLASSIFICATION	 flatten keyvalue pairs into row object for use serde 
WITHOUT_CLASSIFICATION	 todo type conversion 
WITHOUT_CLASSIFICATION	 query either ctas need create druid meta data query 
WITHOUT_CLASSIFICATION	 obtain additional information should try incremental rewriting rebuild will not try partial rewriting there were updatedelete operations source tables 
WITHOUT_CLASSIFICATION	 set the range the integermax because 
WITHOUT_CLASSIFICATION	 all values should pass test 
WITHOUT_CLASSIFICATION	 enabledisable bias correction using table lookup 
WITHOUT_CLASSIFICATION	 first stripes will satisfy the predicate and merged single split last two stripe will 
WITHOUT_CLASSIFICATION	 finally create ptf 
WITHOUT_CLASSIFICATION	 lock manager 
WITHOUT_CLASSIFICATION	 expected number partitions dropped each those calls 
WITHOUT_CLASSIFICATION	 try find the method 
WITHOUT_CLASSIFICATION	 create query 
WITHOUT_CLASSIFICATION	 this demuxoperator directly connects muxoperator that muxoperator must the parent joinoperator this case that muxoperator should initialized multiple parents that muxoperator 
WITHOUT_CLASSIFICATION	 loop over all the tasks recursively 
WITHOUT_CLASSIFICATION	 tests with queries which cannot executed with directsql because type mismatch the type the num column string but the parameters used the where clause are numbers after falling back orm the number partitions can fetched the method 
WITHOUT_CLASSIFICATION	 this means hive type doesnt refer this field that comes from file schema the field not required for hive table can occur due schema evolution where some field deleted 
WITHOUT_CLASSIFICATION	 code copied over from udfweekofyear implementation 
WITHOUT_CLASSIFICATION	 must parse get the escape count 
WITHOUT_CLASSIFICATION	 need make sure names are set for tez connect things right 
WITHOUT_CLASSIFICATION	 set data column count 
WITHOUT_CLASSIFICATION	 none the cases above matched and everything selected hence will use the same values for the selected and selectedinuse 
WITHOUT_CLASSIFICATION	 write ids 
WITHOUT_CLASSIFICATION	 unfortunately the metastore api revokes all privileges that match principal privilege object type does not filter the grator username this will revoke privileges that are granted other usersthis not sql compliant behavior need changeadd metastore api that has desired behavior 
WITHOUT_CLASSIFICATION	 methods 
WITHOUT_CLASSIFICATION	 now make sure that can reuse the reencoder against completely different record save resources 
WITHOUT_CLASSIFICATION	 all others 
WITHOUT_CLASSIFICATION	 target exists 
WITHOUT_CLASSIFICATION	 assign values vector 
WITHOUT_CLASSIFICATION	 session creation should fail since the schema didnt get created 
WITHOUT_CLASSIFICATION	 likely malformed query select hashdistinct from 
WITHOUT_CLASSIFICATION	 reject default partitions couldnt determine whether should include not note that predicate would only contains partition column parts original predicate 
WITHOUT_CLASSIFICATION	 always should this order see 
WITHOUT_CLASSIFICATION	 ensure destination not empty only for regular import 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 any operator present which prevents the conversion 
WITHOUT_CLASSIFICATION	 exists 
WITHOUT_CLASSIFICATION	 logger the lineage info 
WITHOUT_CLASSIFICATION	 two known tasks left and complete evicted rejected 
WITHOUT_CLASSIFICATION	 final vertices need have least one output 
WITHOUT_CLASSIFICATION	 the qualified table aliases etc 
WITHOUT_CLASSIFICATION	 add implicit type conversion necessary 
WITHOUT_CLASSIFICATION	 remove the entries dont get confused later and think should 
WITHOUT_CLASSIFICATION	 todo buckets not same splits 
WITHOUT_CLASSIFICATION	 save logging message for logj output latter after logj initialize properly 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 format 
WITHOUT_CLASSIFICATION	 started 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlclob 
WITHOUT_CLASSIFICATION	 drop table exists tablename 
WITHOUT_CLASSIFICATION	 for grouping sets add dummy grouping key this dummy key needs added reduce key for consider select key value count from group key value with rollup assuming mapside aggregation and skew the plan would look like tablescan select groupby reducesink groupby select filesink this function called for groupby create additional grouping key 
WITHOUT_CLASSIFICATION	 note the first and last element the byte are not used 
WITHOUT_CLASSIFICATION	 now actually write table generate some partitions 
WITHOUT_CLASSIFICATION	 compare data header with signature 
WITHOUT_CLASSIFICATION	 joinleft joingetright 
WITHOUT_CLASSIFICATION	 might from before the new resource plan 
WITHOUT_CLASSIFICATION	 optional bytes vertexbinary 
WITHOUT_CLASSIFICATION	 stagetype 
WITHOUT_CLASSIFICATION	 either nonmm query load into table from external source 
WITHOUT_CLASSIFICATION	 retrieve job conf into logdir 
WITHOUT_CLASSIFICATION	 map tables fullname its ast 
WITHOUT_CLASSIFICATION	 synthetic conditions there 
WITHOUT_CLASSIFICATION	 types 
WITHOUT_CLASSIFICATION	 tracetrace 
WITHOUT_CLASSIFICATION	 swap and thx 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 next byte null byte there are more bytes 
WITHOUT_CLASSIFICATION	 when there explicit foreign key name associated with the constraint and the key composite expect the foreign keys send order the input list otherwise the below code will break this the first column the constraint generate the foreign key name the below code can result race condition where duplicate names can generated theory however this scenario can ignored for practical purposes because the uniqueness the generated constraint name 
WITHOUT_CLASSIFICATION	 skew key now 
WITHOUT_CLASSIFICATION	 last row starting the end the split would read 
WITHOUT_CLASSIFICATION	 disable memory checking 
WITHOUT_CLASSIFICATION	 check the rest command specified explicitly use hcatalog 
WITHOUT_CLASSIFICATION	 these must after nonpartition cols 
WITHOUT_CLASSIFICATION	 connects using the command line arguments there are two possible ways connect here using the cmd line arguments like using properties propertyfile 
WITHOUT_CLASSIFICATION	 restore everything default setup avoid discrepancy between junit test runs 
WITHOUT_CLASSIFICATION	 numattr this means join one single key column 
WITHOUT_CLASSIFICATION	 allocate memory for the histogram bins 
WITHOUT_CLASSIFICATION	 add empty checksum string for filesystems that dont generate one 
WITHOUT_CLASSIFICATION	 remove cyclic dependencies for dpp 
WITHOUT_CLASSIFICATION	 there distinct aggregation update all aggregations 
WITHOUT_CLASSIFICATION	 have created hbase table delete roll back 
WITHOUT_CLASSIFICATION	 locked for defrag 
WITHOUT_CLASSIFICATION	 put each check separate trycatch that particular cycle fails itll try again the next cycle 
WITHOUT_CLASSIFICATION	 create two input paths that two map tasks get triggered there could other ways trigger two map tasks 
WITHOUT_CLASSIFICATION	 well count misses iterate 
WITHOUT_CLASSIFICATION	 istransactional 
WITHOUT_CLASSIFICATION	 swsrwait lock are examining waiting this case keep looking its possible that something front blocking 
WITHOUT_CLASSIFICATION	 cant remainder null 
WITHOUT_CLASSIFICATION	 build the routing table 
WITHOUT_CLASSIFICATION	 therefore the maximum total size serialized timestamp 
WITHOUT_CLASSIFICATION	 avoid intial spike when using multiple 
WITHOUT_CLASSIFICATION	 only need calculate once itll the same for other partitions this job 
WITHOUT_CLASSIFICATION	 checkif current row belongs the current accumulated partition not process the current partition reset input partition set currentkey the newkey null has changed 
WITHOUT_CLASSIFICATION	 dont expect any nesting most cases lot present union and are some examples where would have few levels respectively 
WITHOUT_CLASSIFICATION	 the value before the list record offset make byte segment reference absolute 
WITHOUT_CLASSIFICATION	 strategy requested through config 
WITHOUT_CLASSIFICATION	 the generated unique then this could affect file golden files for tests that run explain queries 
WITHOUT_CLASSIFICATION	 here then there are open txns and must resolved committed aborted either way there are open txns with the there because delete below has which correct for the case when there open txn concurrency even new txn starts starts commits still true that there are currently open txns that overlap with any committed txn with commitid commithighwatermark set next line plain readcommitted enough 
WITHOUT_CLASSIFICATION	 optimizer 
WITHOUT_CLASSIFICATION	 sources represent vertex names 
WITHOUT_CLASSIFICATION	 hadoop group mapping that maps user same group 
WITHOUT_CLASSIFICATION	 corresponding with bucket number and hence their ois 
WITHOUT_CLASSIFICATION	 for cbo provided orderings dont attempt reorder joins only convert consecutive joins into nway joins 
WITHOUT_CLASSIFICATION	 inbuilt assumption that the testdir has only one output file 
WITHOUT_CLASSIFICATION	 incrementset input counters 
WITHOUT_CLASSIFICATION	 create placeholder entry with pending state 
WITHOUT_CLASSIFICATION	 used ptfs 
WITHOUT_CLASSIFICATION	 with the next argument being for beelineopts 
WITHOUT_CLASSIFICATION	 max threshold for cnf conversion having elements andlist will converted maybe 
WITHOUT_CLASSIFICATION	 call 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 rule and passes the context along 
WITHOUT_CLASSIFICATION	 the mapping from newtag the index the corresponding child 
WITHOUT_CLASSIFICATION	 first actually give duck 
WITHOUT_CLASSIFICATION	 print name 
WITHOUT_CLASSIFICATION	 checking for deletedelta only that this functionality can exercised code which cannot produce any deltas with mix updateinsert events 
WITHOUT_CLASSIFICATION	 generate the result for the windowing ending the current row 
WITHOUT_CLASSIFICATION	 pick trust store config from the given path 
WITHOUT_CLASSIFICATION	 for updates first column rowid 
WITHOUT_CLASSIFICATION	 for and listssingleaddcall 
WITHOUT_CLASSIFICATION	 add not null constraint check 
WITHOUT_CLASSIFICATION	 trim off ending any 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl call check existence side file for mockmocktbl 
WITHOUT_CLASSIFICATION	 there could more scheme after execution execution might accessing different filesystem dont find matching scheme before execution just use the after execution values directly without computing delta difference 
WITHOUT_CLASSIFICATION	 http mode 
WITHOUT_CLASSIFICATION	 currently the algorithm flushes the entries this can changed the future 
WITHOUT_CLASSIFICATION	 avro only allows maps with string keys 
WITHOUT_CLASSIFICATION	 support both listobject and object have differently 
WITHOUT_CLASSIFICATION	 need map from the reducer the corresponding reducework 
WITHOUT_CLASSIFICATION	 data prematurely ended return start dont move our field position 
WITHOUT_CLASSIFICATION	 looks like operation 
WITHOUT_CLASSIFICATION	 after decoding can push value 
WITHOUT_CLASSIFICATION	 the vectorrow deserialization the one row into the vectorizedrowbatch 
WITHOUT_CLASSIFICATION	 add dummy instances all slots where llaps are mia can haz insertiterator 
WITHOUT_CLASSIFICATION	 update aggregations the aggregation for distinct case hash aggregation the client tells whether new entry for sortbased aggregations the last row compared with the current one figure out whether has changed cleanup the lastinvoke logic can pushed the caller and this function can independent that the client should always notify whether different row not param aggs the aggregations evaluated param row the row being processed param rowinspector the inspector for the row param hashaggr whether hash aggregation being performed not param newentryforhashaggr only valid hash aggregation whether new entry not 
WITHOUT_CLASSIFICATION	 convert the first entries 
WITHOUT_CLASSIFICATION	 some rows have already been assigned values assign the remaining cannot use copyselected method here 
WITHOUT_CLASSIFICATION	 schematype 
WITHOUT_CLASSIFICATION	 setup appropriate ugi for the session 
WITHOUT_CLASSIFICATION	 delegate the groups converters 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this thismag thisscale right rightmag rightscale this right thismag rightmag thisscale rightscale 
WITHOUT_CLASSIFICATION	 all other ops using txns row 
WITHOUT_CLASSIFICATION	 the following data should changed other data should the same 
WITHOUT_CLASSIFICATION	 numeric primitive type 
WITHOUT_CLASSIFICATION	 thisisouterjoin isouterjoin primitivetypeinfo primitivetypeinfos new readstringresults byteswritable new byteswritable 
WITHOUT_CLASSIFICATION	 the number writers seems based number jobs for the src query todo check number filesinks length length 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 last group batch take the nonstreaming group aggregation values and write output columns for all rows every batch the group each group batch finished being written they are forwarded the next operator 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 implementation listobject and assorted methods 
WITHOUT_CLASSIFICATION	 falling back 
WITHOUT_CLASSIFICATION	 otherwise sleep and try again 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 second three was selected 
WITHOUT_CLASSIFICATION	 not using method because partitioning columns are also treated virtual columns columninfo 
WITHOUT_CLASSIFICATION	 show throw 
WITHOUT_CLASSIFICATION	 here the slow path that can return info txns which were not expected state 
WITHOUT_CLASSIFICATION	 print out the sizes pretty set print out human friendly format otherwise print out were row 
WITHOUT_CLASSIFICATION	 set col expressions for the dynamic values using this input 
WITHOUT_CLASSIFICATION	 cached values save round trips database 
WITHOUT_CLASSIFICATION	 ranges implies that there are possible ranges lookup 
WITHOUT_CLASSIFICATION	 decimal comparison 
WITHOUT_CLASSIFICATION	 append additional virtual columns for storing statistics 
WITHOUT_CLASSIFICATION	 when and has children some conjunction over field that isnt the column mapped the accumulo rowid and when conjunction generates ranges which are empty the children the conjunction are disjoint these two cases need kept separate null andranges implies that ranges couldnt computed while empty list 
WITHOUT_CLASSIFICATION	 make the following condition all the values match for all the columns 
WITHOUT_CLASSIFICATION	 only initialize once the tasks that need run periodically 
WITHOUT_CLASSIFICATION	 testing setting length larger than array length which should cap the length itself 
WITHOUT_CLASSIFICATION	 newbucketcollist had null value means that least one the input bucket columns did not have representative found the output columns assume the data longer bucketed 
WITHOUT_CLASSIFICATION	 tablenamepattern string columnnamepattern 
WITHOUT_CLASSIFICATION	 the original lists not contain collisions only one old 
WITHOUT_CLASSIFICATION	 prepare data good for any implementation variation 
WITHOUT_CLASSIFICATION	 remove the partition paths know about 
WITHOUT_CLASSIFICATION	 this method gets called multiple times hive some invocations the properties will empty need detect when the properties are not empty initialise the class variables see javautilproperties 
WITHOUT_CLASSIFICATION	 empty strings are marked invalid utf single byte sequence valid utf stream cannot 
WITHOUT_CLASSIFICATION	 this skew key need handle separate map reduce job 
WITHOUT_CLASSIFICATION	 add the merge job 
WITHOUT_CLASSIFICATION	 should not happen here this for replication 
WITHOUT_CLASSIFICATION	 are going something useful now 
WITHOUT_CLASSIFICATION	 create one dummy lock can through the loop below though only really need txnid 
WITHOUT_CLASSIFICATION	 test string column varchar literal comparison 
WITHOUT_CLASSIFICATION	 used log memory usage periodically 
WITHOUT_CLASSIFICATION	 get the size cache before 
WITHOUT_CLASSIFICATION	 rebuild the tree since original empty 
WITHOUT_CLASSIFICATION	 get the groupby aliases these are aliased the entries the select list 
WITHOUT_CLASSIFICATION	 assume that since are joining the same key all tables would have either optimized nonoptimized key hence can pass any key any table reference that mjkb could determine whether can use optimized keys 
WITHOUT_CLASSIFICATION	 small table indices has more information keys than retain use exists 
WITHOUT_CLASSIFICATION	 set test data 
WITHOUT_CLASSIFICATION	 compute deltas and write the values varints 
WITHOUT_CLASSIFICATION	 change future 
WITHOUT_CLASSIFICATION	 invalid load path 
WITHOUT_CLASSIFICATION	 used queue requests while the sparkcontext being created 
WITHOUT_CLASSIFICATION	 task requested host got host host full 
WITHOUT_CLASSIFICATION	 multikey get key 
WITHOUT_CLASSIFICATION	 dont this optimization with updates deletes 
WITHOUT_CLASSIFICATION	 optional required optional 
WITHOUT_CLASSIFICATION	 sessionstatedriver needs restarted with the tez conf settings 
WITHOUT_CLASSIFICATION	 set temp file containing results sent hiveclient 
WITHOUT_CLASSIFICATION	 ignore updates that occured before this cached query was created 
WITHOUT_CLASSIFICATION	 used determine whether child tasks can run 
WITHOUT_CLASSIFICATION	 making sure treat dynamic partitioning jobs they were immutable 
WITHOUT_CLASSIFICATION	 mystring 
WITHOUT_CLASSIFICATION	 each element the array startpositioni startpositioni 
WITHOUT_CLASSIFICATION	 remove the reducesinkoperator 
WITHOUT_CLASSIFICATION	 ival 
WITHOUT_CLASSIFICATION	 java cruft pair long 
WITHOUT_CLASSIFICATION	 for alter table exchange partition need select delete input insert output 
WITHOUT_CLASSIFICATION	 traverse through the from string one code point time 
WITHOUT_CLASSIFICATION	 this record produced result this time remove from the storage will not need produce result with null values anymore 
WITHOUT_CLASSIFICATION	 reduce sink contains the columns needed need aggregate from children 
WITHOUT_CLASSIFICATION	 setup the new map work 
WITHOUT_CLASSIFICATION	 start generate multiple map join tasks 
WITHOUT_CLASSIFICATION	 connect parentchild work with brodacast edge 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create and attach the filesink for the merge 
WITHOUT_CLASSIFICATION	 correlated vars across subqueries within same query needs have different 
WITHOUT_CLASSIFICATION	 columns which are bucketedsorted 
WITHOUT_CLASSIFICATION	 are left with single child return the child 
WITHOUT_CLASSIFICATION	 the array index cant reserved 
WITHOUT_CLASSIFICATION	 sending kill message the right here dont need wait for the task complete 
WITHOUT_CLASSIFICATION	 kill always takes priority over move 
WITHOUT_CLASSIFICATION	 remove cast boolean not null boolean vice versa filter accepts nullable and notnullable conditions but cast might get the way other rewrites 
WITHOUT_CLASSIFICATION	 above get doesnt set 
WITHOUT_CLASSIFICATION	 perform spnego login using the hadoop shim api the configuration available 
WITHOUT_CLASSIFICATION	 uninitialized vertices will report count 
WITHOUT_CLASSIFICATION	 not need anything the expression probably pushed previously 
WITHOUT_CLASSIFICATION	 nway join all later small tables for all later small tables follow the same pattern the previously loaded tables 
WITHOUT_CLASSIFICATION	 replica ssd 
WITHOUT_CLASSIFICATION	 validate noscan 
WITHOUT_CLASSIFICATION	 setting the comparison equal the search should use the block 
WITHOUT_CLASSIFICATION	 grab round digit from lowest word 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 adding constant memory for stringcommon the rabit hole deep implement memoryestimate interface also constant overhead 
WITHOUT_CLASSIFICATION	 enable retries work around bonecp bug 
WITHOUT_CLASSIFICATION	 this record larger than maxkey need stop 
WITHOUT_CLASSIFICATION	 eventconsumer invalidate cache entries based metastore notification events alter table add partition etc 
WITHOUT_CLASSIFICATION	 cause was useless intermediate cause and was replace with its own cause 
WITHOUT_CLASSIFICATION	 convert lazy object 
WITHOUT_CLASSIFICATION	 the table missing either due droprename which follows the operation 
WITHOUT_CLASSIFICATION	 compare strings char comparison semantics may different ifwhen implemented 
WITHOUT_CLASSIFICATION	 simple thread wait until the server has started and then signal the other threads begin 
WITHOUT_CLASSIFICATION	 last resort 
WITHOUT_CLASSIFICATION	 bail out can not infer unit 
WITHOUT_CLASSIFICATION	 the table partitioned need select the partition columns well 
WITHOUT_CLASSIFICATION	 get the first sel after 
WITHOUT_CLASSIFICATION	 mark the write ids state per the txn state 
WITHOUT_CLASSIFICATION	 value null not found exception would get thrown 
WITHOUT_CLASSIFICATION	 audience cant exist its own 
WITHOUT_CLASSIFICATION	 reopen happens even when close fails theres not much here 
WITHOUT_CLASSIFICATION	 add any required resources 
WITHOUT_CLASSIFICATION	 dont think this can have any filesinkoperators more future proofing 
WITHOUT_CLASSIFICATION	 remember the connections between and event 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 draw and replace 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 need preserve currenttimestamp 
WITHOUT_CLASSIFICATION	 for kafka streaming tables mandatory set kafka topic 
WITHOUT_CLASSIFICATION	 setstring can override this null propagation 
WITHOUT_CLASSIFICATION	 check the default values are set for all unfilled attributes 
WITHOUT_CLASSIFICATION	 for each struct subfield create equivalent resourcefieldschema 
WITHOUT_CLASSIFICATION	 for tables directory structure 
WITHOUT_CLASSIFICATION	 get sum for all columns reduce the number queries 
WITHOUT_CLASSIFICATION	 have conflict the number reducers will not optimize this plan from here 
WITHOUT_CLASSIFICATION	 checking for status 
WITHOUT_CLASSIFICATION	 prepare the constant for use when the function called used during initialization 
WITHOUT_CLASSIFICATION	 there overlap between columns and partitioning columns 
WITHOUT_CLASSIFICATION	 ensure pig can read data correctly 
WITHOUT_CLASSIFICATION	 make acid table and make sure assign rowids correctly 
WITHOUT_CLASSIFICATION	 convert all the children cnf 
WITHOUT_CLASSIFICATION	 mapjoin mergejoin make sure that they are the reduce side otherwise bail out 
WITHOUT_CLASSIFICATION	 there should already instance the session pool manager not ignoring fine while stopping hiveserver 
WITHOUT_CLASSIFICATION	 this operator cannot converted mapjoin cause output expected sorted join key 
WITHOUT_CLASSIFICATION	 there are nulls our column 
WITHOUT_CLASSIFICATION	 the vectorizer class enforces that there only one tablescanoperator dont need the more complicated multiple root operator mapping that mapoperator has 
WITHOUT_CLASSIFICATION	 this code pretty much completely based hadoops the only reason could not use that hadoop class asis was because needs serverconnection object which relevant hadoop rpc but not here the metastore the 
WITHOUT_CLASSIFICATION	 after this the noninitial refcounts are the responsibility the consumer 
WITHOUT_CLASSIFICATION	 unpack the output 
WITHOUT_CLASSIFICATION	 setting empty list results reading none 
WITHOUT_CLASSIFICATION	 intvalue 
WITHOUT_CLASSIFICATION	 compress the owids into compressedowid data structure that records 
WITHOUT_CLASSIFICATION	 here someone must have added new work object should walked find filesinks 
WITHOUT_CLASSIFICATION	 let the schema and version auto created 
WITHOUT_CLASSIFICATION	 all column types 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check the list where expressions already added they arent duplicated 
WITHOUT_CLASSIFICATION	 test serialization and deserialization with different schemas 
WITHOUT_CLASSIFICATION	 only preempt the task being preempted below the dag 
WITHOUT_CLASSIFICATION	 require view ownership for alterdrop view 
WITHOUT_CLASSIFICATION	 create map join task for the given big table position 
WITHOUT_CLASSIFICATION	 this case the result will surely more than bit even after division 
WITHOUT_CLASSIFICATION	 end loop over pending tasks 
WITHOUT_CLASSIFICATION	 get the output schema 
WITHOUT_CLASSIFICATION	 have check receive prefix partition keys table scheme like tabledshr archive partition will work and archive partitionhr wont 
WITHOUT_CLASSIFICATION	 parameters null means the input tablesplit empty 
WITHOUT_CLASSIFICATION	 wait for seconds before checking any init error init should fast error need make this configurable 
WITHOUT_CLASSIFICATION	 extend any repeating values and nonulls indicator the input 
WITHOUT_CLASSIFICATION	 matching result for attemptid input 
WITHOUT_CLASSIFICATION	 uri 
WITHOUT_CLASSIFICATION	 test drop database 
WITHOUT_CLASSIFICATION	 serde the throwable string parse for the root cause 
WITHOUT_CLASSIFICATION	 nope look see our conf dir has been explicitly set 
WITHOUT_CLASSIFICATION	 invalid case 
WITHOUT_CLASSIFICATION	 serialize rest the field the aggbuffer 
WITHOUT_CLASSIFICATION	 total number input rows needed for hash aggregation only 
WITHOUT_CLASSIFICATION	 this probably doesnt need sync but nobody calls this doesnt matter 
WITHOUT_CLASSIFICATION	 clone joincond 
WITHOUT_CLASSIFICATION	 note assume production llap always runs under yarn 
WITHOUT_CLASSIFICATION	 the aggregation type avg use the average the existing ones 
WITHOUT_CLASSIFICATION	 get rid sqcountcheck group key constant hive 
WITHOUT_CLASSIFICATION	 false positives probability are ready tolerate for the underlying bloom filter 
WITHOUT_CLASSIFICATION	 replaces the join operator with new commonjoinoperator removes the 
WITHOUT_CLASSIFICATION	 ignore for now will probably try send the count already have again are assuming here that cant talk will eventually fail 
WITHOUT_CLASSIFICATION	 get the field out struct 
WITHOUT_CLASSIFICATION	 skip incompatible file files that are missing stripe statistics are set incompatible 
WITHOUT_CLASSIFICATION	 current cookie name current cookie value 
WITHOUT_CLASSIFICATION	 ignore agg calls which are not distinct have the wrong set arguments were rewriting aggs whose args are sal will rewrite countdistinct sal and sumdistinct sal but ignore countdistinct gender sumsal 
WITHOUT_CLASSIFICATION	 skip the big tables 
WITHOUT_CLASSIFICATION	 create partitions 
WITHOUT_CLASSIFICATION	 insert overwrite 
WITHOUT_CLASSIFICATION	 map zexpr for 
WITHOUT_CLASSIFICATION	 allocate next null byte 
WITHOUT_CLASSIFICATION	 common inner bigonly join result processing 
WITHOUT_CLASSIFICATION	 read sync bytes 
WITHOUT_CLASSIFICATION	 batch full using too much space 
WITHOUT_CLASSIFICATION	 test will local mode 
WITHOUT_CLASSIFICATION	 check for overflow 
WITHOUT_CLASSIFICATION	 hash table memory usage allowed used case nonstaged mapjoin 
WITHOUT_CLASSIFICATION	 set the foreign key constraints properly the tabcolstats data 
WITHOUT_CLASSIFICATION	 current state final state notify spark job ids before notifying about the state transition 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 set the backup task from curr task 
WITHOUT_CLASSIFICATION	 get all the ops 
WITHOUT_CLASSIFICATION	 truncate slice byte array maximum number characters and return the new byte length 
WITHOUT_CLASSIFICATION	 convert join gby semijoin 
WITHOUT_CLASSIFICATION	 because llap arrow output depends the code path this required for row outputs need write size batch signal eos the consumer 
WITHOUT_CLASSIFICATION	 close closing open scope should 
WITHOUT_CLASSIFICATION	 blocks particular size well try split yet larger blocks until run out 
WITHOUT_CLASSIFICATION	 parse out the context and havent already done and while were also parse out the precision factor the user has supplied one 
WITHOUT_CLASSIFICATION	 fill host with tasks leave host empty try running task host should preempt await preemption request try running another task host should preempt await preemption request 
WITHOUT_CLASSIFICATION	 for development can add and and state nvaestatenumber 
WITHOUT_CLASSIFICATION	 not power two add one more 
WITHOUT_CLASSIFICATION	 the set object containing the list this optimized for lookup the data type the column 
WITHOUT_CLASSIFICATION	 the number entries added the hash table since the last time checked the average variable size 
WITHOUT_CLASSIFICATION	 this makes the jars available sqoop client 
WITHOUT_CLASSIFICATION	 checks the caller must ensure the offsets are correct 
WITHOUT_CLASSIFICATION	 the class name the generic udf being used the filter 
WITHOUT_CLASSIFICATION	 one child 
WITHOUT_CLASSIFICATION	 cleaner would remove the obsolete files 
WITHOUT_CLASSIFICATION	 commenttest columns name col type string format storedas rcfile 
WITHOUT_CLASSIFICATION	 bgenjjtree header 
WITHOUT_CLASSIFICATION	 given that not delete empty slot means match 
WITHOUT_CLASSIFICATION	 stats are already present and forcerecompute isnt set nothing 
WITHOUT_CLASSIFICATION	 throws hcatexception see 
WITHOUT_CLASSIFICATION	 either argument string convert double decimal because number string form should always convertible into either those 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 level create col col count for each branch 
WITHOUT_CLASSIFICATION	 check for max input size 
WITHOUT_CLASSIFICATION	 insert overwrite path 
WITHOUT_CLASSIFICATION	 hive remove parallelism check the beeline test flakyness gets fixed int numthreads 
WITHOUT_CLASSIFICATION	 enable extended nesting levels 
WITHOUT_CLASSIFICATION	 bigint 
WITHOUT_CLASSIFICATION	 unknown 
WITHOUT_CLASSIFICATION	 there sortmerge join followed regular join the smbjoinoperator may not get initialized all consider the following query smb join 
WITHOUT_CLASSIFICATION	 this means the name empty 
WITHOUT_CLASSIFICATION	 ensure child size 
WITHOUT_CLASSIFICATION	 this the constructor use for the bucket map join case 
WITHOUT_CLASSIFICATION	 text string 
WITHOUT_CLASSIFICATION	 need use the current cluster for the scan operator views 
WITHOUT_CLASSIFICATION	 tagtoinput for reduce work 
WITHOUT_CLASSIFICATION	 merge the files the destination tablepartitions creating maponly merge job underlying data rcfile orcfile rcfileblockmerge task orcfilestripemerge task would created 
WITHOUT_CLASSIFICATION	 transform first insert branch into update 
WITHOUT_CLASSIFICATION	 nothing soft references will clean themselves 
WITHOUT_CLASSIFICATION	 set the output record fiddling with the pointers that can 
WITHOUT_CLASSIFICATION	 and finally were ready create and start the session 
WITHOUT_CLASSIFICATION	 check the partitions dont exist the sourcetable 
WITHOUT_CLASSIFICATION	 verify fields were altered during the altertable operation 
WITHOUT_CLASSIFICATION	 after super analyze readentity defaultacidtblpart writeentity defaultacidtblpart tableinsert after udsa read defaultacidtblpart write partitionupdate partitionupdate todo why acquire per partition locks you have many partitions thats hugely inefficient 
WITHOUT_CLASSIFICATION	 serialize keyrow into key bytes 
WITHOUT_CLASSIFICATION	 read the fields 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 need enforce nullability applying additional cast operator over the transformed expression 
WITHOUT_CLASSIFICATION	 cursor for the inlist array cursor for element list per innot inclause cursor for inclause lists per query 
WITHOUT_CLASSIFICATION	 maps recursively compare the key and value types 
WITHOUT_CLASSIFICATION	 decimaltxt 
WITHOUT_CLASSIFICATION	 something preventing metastore from starting 
WITHOUT_CLASSIFICATION	 test select namedstruct from 
WITHOUT_CLASSIFICATION	 create partition key schema 
WITHOUT_CLASSIFICATION	 list map join 
WITHOUT_CLASSIFICATION	 for while 
WITHOUT_CLASSIFICATION	 authorize drops there was drop privilege requirement 
WITHOUT_CLASSIFICATION	 set this multiple times 
WITHOUT_CLASSIFICATION	 call the corresponding handler evaluate this row and forward the result 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring int 
WITHOUT_CLASSIFICATION	 current code version datas version datas version exceptions expected 
WITHOUT_CLASSIFICATION	 the big table 
WITHOUT_CLASSIFICATION	 map keep track what reduce sinks have hooked 
WITHOUT_CLASSIFICATION	 all child expressions deterministic function are constants evaluate such udf immediately 
WITHOUT_CLASSIFICATION	 must deterministic order map see hive 
WITHOUT_CLASSIFICATION	 some part the requested range not cached the cached offset past the requested 
WITHOUT_CLASSIFICATION	 decide whether rewrite subquery 
WITHOUT_CLASSIFICATION	 get own kerberos credentials for accepting connection 
WITHOUT_CLASSIFICATION	 true when the random access readfield method deserializeread are being used 
WITHOUT_CLASSIFICATION	 next this for tables and partitions 
WITHOUT_CLASSIFICATION	 create view has limit operator this can happen fetch parent operator 
WITHOUT_CLASSIFICATION	 throws the new column types are not compatible with the current column types 
WITHOUT_CLASSIFICATION	 this case have find out which columns can merged 
WITHOUT_CLASSIFICATION	 mock inputs 
WITHOUT_CLASSIFICATION	 update the partitions for table cache 
WITHOUT_CLASSIFICATION	 partial null then there was overflow and myaggsum will marked not set 
WITHOUT_CLASSIFICATION	 session already has violation 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 might have been deleted already 
WITHOUT_CLASSIFICATION	 this only mode just initialize the dfs root directory 
WITHOUT_CLASSIFICATION	 replicate all the events happened far 
WITHOUT_CLASSIFICATION	 default case 
WITHOUT_CLASSIFICATION	 couldnt find the parent insert replace with allcolref 
WITHOUT_CLASSIFICATION	 add path set 
WITHOUT_CLASSIFICATION	 test that underflow produces null 
WITHOUT_CLASSIFICATION	 execution mode not set null returned 
WITHOUT_CLASSIFICATION	 timestamp columnscalar where scalar really cast constant timestamp 
WITHOUT_CLASSIFICATION	 begin write commit 
WITHOUT_CLASSIFICATION	 encountered numeric value extract out the entire number 
WITHOUT_CLASSIFICATION	 recreate cluster 
WITHOUT_CLASSIFICATION	 only predicate supported right now this has extended support expression tree when multiple conditions are required hive 
WITHOUT_CLASSIFICATION	 the mapping from newtag its corresponding oldtag oldtag the tag assigned reducesinkoperators before correlation optimizer optimizes the operator tree newtag the tag assigned reducesinkoperators after correlation optimizer optimizes the operator tree example have operator tree shown below join gby join gby join and join are executed the same reducer optimized correlation optimizer will have oldtag newtag need know the mapping from the newtag oldtag and revert the newtag oldtag make operators the operator tree 
WITHOUT_CLASSIFICATION	 allow multiple foreign keys snowflake schema csfkssize parentssize means have single and all 
WITHOUT_CLASSIFICATION	 the absence column statistics the estimated number rowsdata size that will 
WITHOUT_CLASSIFICATION	 populate pathtopartitioninfo and pathtoaliases paths 
WITHOUT_CLASSIFICATION	 disruptor logjapi logjcore logjslfj logjapi needed for ndc 
WITHOUT_CLASSIFICATION	 ensure theres more invocations 
WITHOUT_CLASSIFICATION	 some version upgrades often dont change schema they are equivalent version that has corresponding schema equivalent 
WITHOUT_CLASSIFICATION	 privilege 
WITHOUT_CLASSIFICATION	 the set routine enforces precision and scale 
WITHOUT_CLASSIFICATION	 test that setting read all resets column ids 
WITHOUT_CLASSIFICATION	 add the expr reducekeys not present 
WITHOUT_CLASSIFICATION	 must inside together with queries 
WITHOUT_CLASSIFICATION	 still exist 
WITHOUT_CLASSIFICATION	 set admin role user belongs there 
WITHOUT_CLASSIFICATION	 forwarding 
WITHOUT_CLASSIFICATION	 the response will have one entry per table and hence get only one openwriteids 
WITHOUT_CLASSIFICATION	 none the expressions are constant nothing 
WITHOUT_CLASSIFICATION	 create scratch dir without lock files 
WITHOUT_CLASSIFICATION	 falling off and the executor service being ready schedule new task 
WITHOUT_CLASSIFICATION	 removing headerlogix from getsecondintlogix head logix 
WITHOUT_CLASSIFICATION	 change body implemented methods use file settings file templates 
WITHOUT_CLASSIFICATION	 already contains stats stats not updated when forcerecompute isnt set 
WITHOUT_CLASSIFICATION	 just check nametype for equality dont compare comment 
WITHOUT_CLASSIFICATION	 toktablepartition 
WITHOUT_CLASSIFICATION	 just loop through all values not need store anything though this just for test purposes 
WITHOUT_CLASSIFICATION	 stats from writer 
WITHOUT_CLASSIFICATION	 configuration for async thread pool sessionmanager 
WITHOUT_CLASSIFICATION	 successfully converted agg calls into projects 
WITHOUT_CLASSIFICATION	 and not column because that how data processed 
WITHOUT_CLASSIFICATION	 filesystemcachemap 
WITHOUT_CLASSIFICATION	 the same size sort file name followed startposition 
WITHOUT_CLASSIFICATION	 older committed state equivalent newer state then there should committed ids between oldhwm and newhwm and newinvalidids should have exactly newhwm oldhwm 
WITHOUT_CLASSIFICATION	 nonhash aggregation 
WITHOUT_CLASSIFICATION	 then return immediately 
WITHOUT_CLASSIFICATION	 should not happen since the input were verified before passed 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 created using hint skip 
WITHOUT_CLASSIFICATION	 not all clear how flatten these last two out useful way and one uses 
WITHOUT_CLASSIFICATION	 allocate the various arrays 
WITHOUT_CLASSIFICATION	 update max register value 
WITHOUT_CLASSIFICATION	 the user has already been notified completion sessioninitcontext 
WITHOUT_CLASSIFICATION	 drop view ignore error 
WITHOUT_CLASSIFICATION	 need also push projections calling setoutputschema hcatinputformat have get the requiredfields information from the udfcontext translate schema and then pass the reason this here because setlocation called pig runtime and time are not sure when hcatinputformat needs know about pruned projections doing here will ensure communicate hcatinputformat about pruned projections getsplits and createrecordreader time 
WITHOUT_CLASSIFICATION	 add metric with nonnull value 
WITHOUT_CLASSIFICATION	 get all the failure execution hooks and execute them 
WITHOUT_CLASSIFICATION	 note this real query that depends one the metastore tables 
WITHOUT_CLASSIFICATION	 statistics not implemented currently 
WITHOUT_CLASSIFICATION	 binary mode 
WITHOUT_CLASSIFICATION	 first entry 
WITHOUT_CLASSIFICATION	 this point have delta files for insert for update should push predicate into one but not the following select were push into the update delta wed filter out before doing merge and thus produce the value for row the right result rows 
WITHOUT_CLASSIFICATION	 remaining column expressions would candidate for value 
WITHOUT_CLASSIFICATION	 sort columns 
WITHOUT_CLASSIFICATION	 only one the tasks should ever added restsks 
WITHOUT_CLASSIFICATION	 completion notifier vars 
WITHOUT_CLASSIFICATION	 now set the response headers 
WITHOUT_CLASSIFICATION	 make sure nonnullvalued confvar properties override the hadoop configuration 
WITHOUT_CLASSIFICATION	 used kryo 
WITHOUT_CLASSIFICATION	 mark this entry being use caller will need release later 
WITHOUT_CLASSIFICATION	 requires conditional evaluation for good performance 
WITHOUT_CLASSIFICATION	 case might get called repeatedly 
WITHOUT_CLASSIFICATION	 validate the second parameter which the number histogram bins 
WITHOUT_CLASSIFICATION	 test that write blocks two writes 
WITHOUT_CLASSIFICATION	 note this assumes that the pattern where the same session object reset with different tez client not used was used lot the past but appears gone from most session pool paths and this patch removes the last one reopen 
WITHOUT_CLASSIFICATION	 assert class invariant 
WITHOUT_CLASSIFICATION	 disable dictionary encoding for the writer 
WITHOUT_CLASSIFICATION	 verify dirs 
WITHOUT_CLASSIFICATION	 why are still using writables 
WITHOUT_CLASSIFICATION	 some query attempted lock thus lockwaiting state but giving due timeout for example 
WITHOUT_CLASSIFICATION	 current getinputsummary returns for each file found current getinputsummary returns for each file found 
WITHOUT_CLASSIFICATION	 for testing only 
WITHOUT_CLASSIFICATION	 some the conversion methods throw this exception numeric parsing errors 
WITHOUT_CLASSIFICATION	 currently only optimized the query the content the from clause 
WITHOUT_CLASSIFICATION	 after compactioncleanup all entries from txntowriteid should cleaned all txns are committed 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject int 
WITHOUT_CLASSIFICATION	 parse string query hint 
WITHOUT_CLASSIFICATION	 false and true the input vector simple dictionary used with two entries references false and references true the dictionary 
WITHOUT_CLASSIFICATION	 subtract out the bits just added 
WITHOUT_CLASSIFICATION	 the expected number distinct values when choosing values with replacement from integers have several uniformly distributed attributes with distinct values they behave one uniformly 
WITHOUT_CLASSIFICATION	 are committing this vertex vectorized 
WITHOUT_CLASSIFICATION	 runas 
WITHOUT_CLASSIFICATION	 setup scratch batch that will used play back big table rows that were spilled 
WITHOUT_CLASSIFICATION	 have correlated column build data type from outer 
WITHOUT_CLASSIFICATION	 loginforeading offset prevheadoffset lrptroffset 
WITHOUT_CLASSIFICATION	 killed something 
WITHOUT_CLASSIFICATION	 need deserialize and serialize query intervals are written the json druid query with user timezone this default hive time semantics 
WITHOUT_CLASSIFICATION	 spot check only null repeating behavior are checked elsewhere for the same template 
WITHOUT_CLASSIFICATION	 have register this front right now otherwise its possible for the task start 
WITHOUT_CLASSIFICATION	 numhashfunctions byte numbits bytes 
WITHOUT_CLASSIFICATION	 hdfs warehouse 
WITHOUT_CLASSIFICATION	 only get here could map all join keys source table columns 
WITHOUT_CLASSIFICATION	 process singlecolumn long inner join vectorized row batch 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 required for deserialization 
WITHOUT_CLASSIFICATION	 since allow write operations cache while prewarm happening dont add databases that were deleted while were preparing list for prewarm skip overwriting exisiting object which present because was added after prewarm started 
WITHOUT_CLASSIFICATION	 remove ending and starting 
WITHOUT_CLASSIFICATION	 change the query use partvals instead the name which 
WITHOUT_CLASSIFICATION	 the dateformat not provided the user invalid use the default format yyyymmdd 
WITHOUT_CLASSIFICATION	 ignore the interrupt status while returning the session but set back the thread case anything else needs deal with 
WITHOUT_CLASSIFICATION	 once have decided the map join the tree would transform from join mapjoin big table small table for tez 
WITHOUT_CLASSIFICATION	 check that delta dir has version file with expected value 
WITHOUT_CLASSIFICATION	 defer 
WITHOUT_CLASSIFICATION	 pretend its vectorized the nonvector wrapped enabled 
WITHOUT_CLASSIFICATION	 thanks hbase storage handler 
WITHOUT_CLASSIFICATION	 type 
WITHOUT_CLASSIFICATION	 for tables other than the big table need fetch more data until reach new group done 
WITHOUT_CLASSIFICATION	 start the heartbeat after delay which exceeds the hivetxntimeout 
WITHOUT_CLASSIFICATION	 bgenjjtree typedefinition 
WITHOUT_CLASSIFICATION	 this hook used for verifying the table access key information that generated and maintained the queryplan object the tableaccessanalyer all the hook does print out the tablekeys per operator recorded the tableaccessinfo the queryplan 
WITHOUT_CLASSIFICATION	 launch hadoop command file windows 
WITHOUT_CLASSIFICATION	 updates deletes inserts 
WITHOUT_CLASSIFICATION	 startrow inclusive while stoprow exclusive this util method returns very next bytearray which will occur after the current one padding current one with trailing byte 
WITHOUT_CLASSIFICATION	 the hash map for this specialized class 
WITHOUT_CLASSIFICATION	 insert overwrite table temp select from temp such case select will only have one instance and would have two locating bucketcol such cases will generate error bail out 
WITHOUT_CLASSIFICATION	 whether this deduplicated 
WITHOUT_CLASSIFICATION	 try infer possible sort columns the query the sequence must prsselfsparent 
WITHOUT_CLASSIFICATION	 also need the expr for the partitioned table 
WITHOUT_CLASSIFICATION	 with the high message size limit this connection should work 
WITHOUT_CLASSIFICATION	 the sel operator the semijoin branch there should only one column the operator 
WITHOUT_CLASSIFICATION	 set the operator plan after generating splits that changes configs 
WITHOUT_CLASSIFICATION	 set can add them the list input cols check 
WITHOUT_CLASSIFICATION	 this will happen only when loading tables and reach the limit number tasks can create hence know here that the table should exist and there should lastpartitionname 
WITHOUT_CLASSIFICATION	 this does not work hence opening and closing file for every event writerhflush 
WITHOUT_CLASSIFICATION	 order overlapping keys should exactly the same 
WITHOUT_CLASSIFICATION	 this entry point are going assume that these are logical table columns perhaps should thru the code and clean this more explicit for now will start with this single assumption and maintain clear semantics from here 
WITHOUT_CLASSIFICATION	 are dropping from unmanaged unset the flag and vice versa 
WITHOUT_CLASSIFICATION	 register information about created predicates 
WITHOUT_CLASSIFICATION	 partitionspecs 
WITHOUT_CLASSIFICATION	 set backup task 
WITHOUT_CLASSIFICATION	 need verify that when reading datum with updated reader schema that the datum then returns the reader schema its own since depend this behavior order avoid reencoding the datum 
WITHOUT_CLASSIFICATION	 set the first one active others are backups 
WITHOUT_CLASSIFICATION	 exceptions including interruptexception and other keeperexception 
WITHOUT_CLASSIFICATION	 required optional 
WITHOUT_CLASSIFICATION	 normal case the last parameter normal parameter conversionhelper can called without method parameter length checkings for terminatepartial and merge calls 
WITHOUT_CLASSIFICATION	 mocked session starts with default queue 
WITHOUT_CLASSIFICATION	 bucketjoin possible need correct bucketing 
WITHOUT_CLASSIFICATION	 trigger the creation llap registry client use clients may using different 
WITHOUT_CLASSIFICATION	 meta store check command equivalent add partition command input objects are passed currently but keeping admin priv requirement inputs just case some input object like file 
WITHOUT_CLASSIFICATION	 create default socket factory based standard jsse trust material 
WITHOUT_CLASSIFICATION	 database not the one currently using 
WITHOUT_CLASSIFICATION	 verify create tablefunction calls only add partitions 
WITHOUT_CLASSIFICATION	 max disabled can safely return false 
WITHOUT_CLASSIFICATION	 implemented navigable set protected single lock and using conditions manage blocking 
WITHOUT_CLASSIFICATION	 ignored 
WITHOUT_CLASSIFICATION	 create file sink operator for this file name 
WITHOUT_CLASSIFICATION	 will required 
WITHOUT_CLASSIFICATION	 add partition metastore for dynamic partition make metastore call for every new partition value that encounter even partition already exists exists check require metastore call anyways 
WITHOUT_CLASSIFICATION	 directly serialize fieldbyfield the lazybinary format this alternative way serialize than what provided lazybinaryserde 
WITHOUT_CLASSIFICATION	 should not remove the dynamic partition pruner generated synthetic predicates 
WITHOUT_CLASSIFICATION	 created tables under 
WITHOUT_CLASSIFICATION	 create new columninfo replacing structcolumn with structcolumn 
WITHOUT_CLASSIFICATION	 the order the join condition expressions dont matter merge can happen every target condition present some position the node condition list there node condition which not equal any target condition 
WITHOUT_CLASSIFICATION	 always show array 
WITHOUT_CLASSIFICATION	 need localize the additional jars and files 
WITHOUT_CLASSIFICATION	 alternate unused 
WITHOUT_CLASSIFICATION	 type 
WITHOUT_CLASSIFICATION	 int 
WITHOUT_CLASSIFICATION	 address the colexp collist etc for the sel 
WITHOUT_CLASSIFICATION	 assign all remaining rows 
WITHOUT_CLASSIFICATION	 add new request executed 
WITHOUT_CLASSIFICATION	 the ptned table should miss target the table was marked virtually dropped 
WITHOUT_CLASSIFICATION	 this may happen for schemaless tables where columns are dynamically supplied serdes 
WITHOUT_CLASSIFICATION	 cache disabled dont use 
WITHOUT_CLASSIFICATION	 stageattributes 
WITHOUT_CLASSIFICATION	 metastore stuff sure update hiveconfmetavars when you add something here 
WITHOUT_CLASSIFICATION	 simulates the set command 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 these methods are required serialization 
WITHOUT_CLASSIFICATION	 use both args ease development delete this one may 
WITHOUT_CLASSIFICATION	 password 
WITHOUT_CLASSIFICATION	 check entries beyond first 
WITHOUT_CLASSIFICATION	 successfully perform compaction tablepartition that have successful records 
WITHOUT_CLASSIFICATION	 not change the initial bytes which contain 
WITHOUT_CLASSIFICATION	 merge the two partials 
WITHOUT_CLASSIFICATION	 threshold switch from sparse dense encoding 
WITHOUT_CLASSIFICATION	 thread local conf from hms 
WITHOUT_CLASSIFICATION	 trivially retryable 
WITHOUT_CLASSIFICATION	 casts exact types including long double etc are needed some special cases 
WITHOUT_CLASSIFICATION	 this test assumes the hivecontrib jar has been built part the hive build also dependent the udfexampleadd class within that jar 
WITHOUT_CLASSIFICATION	 this what allows the crossdomain reads the contents only apply idempotent get ops all others need crumbs 
WITHOUT_CLASSIFICATION	 note the default value for null fields vectorization for int types nan for 
WITHOUT_CLASSIFICATION	 the pruning needs preserve the order columns the input schema 
WITHOUT_CLASSIFICATION	 make path ensure slash the end 
WITHOUT_CLASSIFICATION	 left key only needs adjusted there are system 
WITHOUT_CLASSIFICATION	 allocated caller 
WITHOUT_CLASSIFICATION	 add shutdown hook for catching sigterm sigint 
WITHOUT_CLASSIFICATION	 object overhead bytes for bitcount bytes for bitlength bytes for firstnonzerobytenum bytes for firstnonzerointnum bytes for lowestsetbit bytes for size magnitude since max precision only for hivedecimal bytes padding since java memory allocations are byte aligned 
WITHOUT_CLASSIFICATION	 get the column names and their corresponding types 
WITHOUT_CLASSIFICATION	 various utility method 
WITHOUT_CLASSIFICATION	 walk through udaf and add them 
WITHOUT_CLASSIFICATION	 different tasks 
WITHOUT_CLASSIFICATION	 optional int guaranteedtaskcount 
WITHOUT_CLASSIFICATION	 verify that returns events after specified event 
WITHOUT_CLASSIFICATION	 this proves data written acid layout was made acid 
WITHOUT_CLASSIFICATION	 handle map can read list struct data liststructkey value mapkey 
WITHOUT_CLASSIFICATION	 failed set job status completed which mean the main thread would have exited and not waiting for the result call cleanup execute any cleanup 
WITHOUT_CLASSIFICATION	 map since user group checks and config files are terms short name 
WITHOUT_CLASSIFICATION	 multikey hash map based the 
WITHOUT_CLASSIFICATION	 create the route objects based the nodes 
WITHOUT_CLASSIFICATION	 backtrack partition columns crs prs 
WITHOUT_CLASSIFICATION	 stored directories 
WITHOUT_CLASSIFICATION	 construct column name list for reference filter push down 
WITHOUT_CLASSIFICATION	 split since only have bucket file base delta not flushed committed yet empty 
WITHOUT_CLASSIFICATION	 test get stats column for which stats doesnt exist 
WITHOUT_CLASSIFICATION	 use the original bytes case decoding should fail 
WITHOUT_CLASSIFICATION	 gmt zone gmt pst 
WITHOUT_CLASSIFICATION	 bootstrap repl and then export table 
WITHOUT_CLASSIFICATION	 exact numbers power can the same 
WITHOUT_CLASSIFICATION	 join 
WITHOUT_CLASSIFICATION	 this must final map reduce task the task containing the file sink operator that writes the final output 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 note sessions torestart are always use they cannot expire parallel 
WITHOUT_CLASSIFICATION	 count number null values seen far 
WITHOUT_CLASSIFICATION	 defaultvalue 
WITHOUT_CLASSIFICATION	 vars that are not used the join key 
WITHOUT_CLASSIFICATION	 explicit avro serialization not supported yet revert default 
WITHOUT_CLASSIFICATION	 todo split count not the same buckets 
WITHOUT_CLASSIFICATION	 write the base 
WITHOUT_CLASSIFICATION	 the first child directory then rest would directory too according hcatalog dir structure recurse that case 
WITHOUT_CLASSIFICATION	 there path element other than report but not fail 
WITHOUT_CLASSIFICATION	 since maxdepth not yet reached are missing partition columns currentpath 
WITHOUT_CLASSIFICATION	 update has writer but which creates buckets where the new rows land 
WITHOUT_CLASSIFICATION	 user from 
WITHOUT_CLASSIFICATION	 equalkey match big length 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this function determines whether sparkpruningsink with mapjoin this will called check whether the tree should split for dpp for mapjoin wont also called determine whether dpp should enabled for anything other than mapjoin 
WITHOUT_CLASSIFICATION	 testsputnew undercol 
WITHOUT_CLASSIFICATION	 happen since layer either knows how produce rowid not but safe 
WITHOUT_CLASSIFICATION	 have unset the env workarounds they dont confuse each other between tests 
WITHOUT_CLASSIFICATION	 setup some helper config variables 
WITHOUT_CLASSIFICATION	 create payload 
WITHOUT_CLASSIFICATION	 operationstarted 
WITHOUT_CLASSIFICATION	 write the number elements sparse map required for reconstruction 
WITHOUT_CLASSIFICATION	 for deserialization 
WITHOUT_CLASSIFICATION	 verify that there data the resultset 
WITHOUT_CLASSIFICATION	 perform casting using hive rules 
WITHOUT_CLASSIFICATION	 found all the operators that are supposed process 
WITHOUT_CLASSIFICATION	 response written response headers mapoutput 
WITHOUT_CLASSIFICATION	 hand reset the big table columns 
WITHOUT_CLASSIFICATION	 day granularity 
WITHOUT_CLASSIFICATION	 process global init file hiverc 
WITHOUT_CLASSIFICATION	 drop all tables 
WITHOUT_CLASSIFICATION	 creates the static cache 
WITHOUT_CLASSIFICATION	 cost transferring map outputs gby operator 
WITHOUT_CLASSIFICATION	 mysql returns the string not wellformed numeric value return bytevalueof but decided return null instead which more conservative 
WITHOUT_CLASSIFICATION	 make sure negative numbers comes before positive numbers 
WITHOUT_CLASSIFICATION	 figure out what have read 
WITHOUT_CLASSIFICATION	 partial aggregation not done for distincts the mapper however the data bucketedsorted the distinct key partial aggregation can performed the mapper 
WITHOUT_CLASSIFICATION	 compare must compare with scaling updown 
WITHOUT_CLASSIFICATION	 the write count does not matter the map will fail its first 
WITHOUT_CLASSIFICATION	 version guava the classpath depending the deploy mode 
WITHOUT_CLASSIFICATION	 end relbuilderjava 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 sum input and output are decimal any mode partial partial final complete 
WITHOUT_CLASSIFICATION	 add our base the list directories search for files 
WITHOUT_CLASSIFICATION	 the output partial aggregation struct containing 
WITHOUT_CLASSIFICATION	 session being killed need coordinate between that and the user these two cases dont need distinguished for now 
WITHOUT_CLASSIFICATION	 will update current number open txns 
WITHOUT_CLASSIFICATION	 shouldnt cast strings other types because that can break original data cases leading zeros zeros trailing after decimal point 
WITHOUT_CLASSIFICATION	 source table spec for tablescanoperator same for filesinkoperator same for filesinkoperator aggregation key prefix are stats completely reliable 
WITHOUT_CLASSIFICATION	 first row determines isgroupresultnull and double firstvalue stream fill result repeated 
WITHOUT_CLASSIFICATION	 check result now 
WITHOUT_CLASSIFICATION	 open new client session 
WITHOUT_CLASSIFICATION	 now weve got table check that works 
WITHOUT_CLASSIFICATION	 previous base directory should stay until cleaner kicks 
WITHOUT_CLASSIFICATION	 allow set and dfs commands used during testing 
WITHOUT_CLASSIFICATION	 when weve buffered the max allowed spill the oldest one make space 
WITHOUT_CLASSIFICATION	 let function decide can handle this special case 
WITHOUT_CLASSIFICATION	 remove backlink 
WITHOUT_CLASSIFICATION	 for complex object serialize json format 
WITHOUT_CLASSIFICATION	 coldouble 
WITHOUT_CLASSIFICATION	 only expect here because well get whichever the partitions published its stats last 
WITHOUT_CLASSIFICATION	 long columnscalar 
WITHOUT_CLASSIFICATION	 check result 
WITHOUT_CLASSIFICATION	 convert object types used the authorization plugin interface 
WITHOUT_CLASSIFICATION	 handle overflow precision issue 
WITHOUT_CLASSIFICATION	 collect the dynamic pruning conditions 
WITHOUT_CLASSIFICATION	 requested logger not found add the new logger with the requested level 
WITHOUT_CLASSIFICATION	 todo the way add hash fns does exhibit some irregularities seems like the iter has better distribution many cases even better that the original hash that trips the above criteria even the rest flat 
WITHOUT_CLASSIFICATION	 todo nested 
WITHOUT_CLASSIFICATION	 not specified 
WITHOUT_CLASSIFICATION	 skip the child unique key part not projected 
WITHOUT_CLASSIFICATION	 this point one will take the write lock and update can the last check 
WITHOUT_CLASSIFICATION	 initialize record writer with connection and write info 
WITHOUT_CLASSIFICATION	 boolean keysareequal currentkeys null newkeys null newkeys false 
WITHOUT_CLASSIFICATION	 memory for compaction map job minor compaction more than delta dirs major compaction more than 
WITHOUT_CLASSIFICATION	 lsb bits used locate offset within the block 
WITHOUT_CLASSIFICATION	 same reason above this the case when have the main work item after the merge work has been created for the small table side 
WITHOUT_CLASSIFICATION	 add all input columns 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 preserved table 
WITHOUT_CLASSIFICATION	 event truncate last repl repldumpidxy 
WITHOUT_CLASSIFICATION	 satisfying precondition means column statistics available 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 see comment dumping rows via sql for why this doesnt work pig hive tget lget 
WITHOUT_CLASSIFICATION	 query qualify for the optimization 
WITHOUT_CLASSIFICATION	 open files 
WITHOUT_CLASSIFICATION	 add hive operator level statistics recordsin recordsout 
WITHOUT_CLASSIFICATION	 check that all the jars are added the classpath 
WITHOUT_CLASSIFICATION	 schema should same 
WITHOUT_CLASSIFICATION	 this indicates there second vint containing the additional bits the seconds field 
WITHOUT_CLASSIFICATION	 union keys 
WITHOUT_CLASSIFICATION	 translate the grouping set bit field into boolean arrays 
WITHOUT_CLASSIFICATION	 get valid window function spec 
WITHOUT_CLASSIFICATION	 for the big table only need promote the next group the current group 
WITHOUT_CLASSIFICATION	 lock the buffer validate and add results 
WITHOUT_CLASSIFICATION	 this not join condition 
WITHOUT_CLASSIFICATION	 not want modify the writable provided the object since not copy 
WITHOUT_CLASSIFICATION	 this where cut the tree described above also remember that 
WITHOUT_CLASSIFICATION	 partitions updated 
WITHOUT_CLASSIFICATION	 querys session has compile lock timeout sec should 
WITHOUT_CLASSIFICATION	 constraintname 
WITHOUT_CLASSIFICATION	 insert overwrite one partition with multiple files 
WITHOUT_CLASSIFICATION	 lockidinternal 
WITHOUT_CLASSIFICATION	 allow tcp keep alive socket option for for hiveserver maximum timeout for the socket 
WITHOUT_CLASSIFICATION	 undone convert byte character 
WITHOUT_CLASSIFICATION	 now have the roottasks set for insert select 
WITHOUT_CLASSIFICATION	 try with parameterized varchar types 
WITHOUT_CLASSIFICATION	 taken care higher level 
WITHOUT_CLASSIFICATION	 are seeing this mapjoin for the first time initialize the plan are seeing this mapjoin for the second later time then atleast one the branches for this mapjoin have been encounered join the plan with the plan created 
WITHOUT_CLASSIFICATION	 but the writeentity complete ddl operations instead ddl sets the writetype use determine its lockmode and first check the writetype was set 
WITHOUT_CLASSIFICATION	 there should calls create partitions with batch sizes 
WITHOUT_CLASSIFICATION	 single node may fail 
WITHOUT_CLASSIFICATION	 not authorized this implementation operation allowed 
WITHOUT_CLASSIFICATION	 from javautilcalendar 
WITHOUT_CLASSIFICATION	 check config properties expected with embedded metastore client 
WITHOUT_CLASSIFICATION	 this could replaced bucketing bucketed fetcher for next 
WITHOUT_CLASSIFICATION	 todo extract interface when needed 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream long 
WITHOUT_CLASSIFICATION	 find the common class for type conversion 
WITHOUT_CLASSIFICATION	 rename unpartitioned table 
WITHOUT_CLASSIFICATION	 initialize 
WITHOUT_CLASSIFICATION	 steptracker should now which indicates has started 
WITHOUT_CLASSIFICATION	 make the partition target not empty 
WITHOUT_CLASSIFICATION	 drop cascade functions dropped cascade 
WITHOUT_CLASSIFICATION	 these will become 
WITHOUT_CLASSIFICATION	 destination file present and checksum source mismatch then retry copy 
WITHOUT_CLASSIFICATION	 were cloning the operator plan but were retaining the original work that means that root operators have replaced with the cloned ops the replacement map 
WITHOUT_CLASSIFICATION	 column doesnt appear partition column for the table 
WITHOUT_CLASSIFICATION	 this not mistake catname the where clause twice 
WITHOUT_CLASSIFICATION	 found udf metastore now add the function registry 
WITHOUT_CLASSIFICATION	 rewrite case into nvl 
WITHOUT_CLASSIFICATION	 get the first record 
WITHOUT_CLASSIFICATION	 return the compressioncodec used for this file 
WITHOUT_CLASSIFICATION	 user might have only specified partial list partition keys which case add other partition keys partspec 
WITHOUT_CLASSIFICATION	 the entire storage xffs means xffs means 
WITHOUT_CLASSIFICATION	 add new table via cachedstore 
WITHOUT_CLASSIFICATION	 copy with buffer not close 
WITHOUT_CLASSIFICATION	 test february leap year viewed due days diff from 
WITHOUT_CLASSIFICATION	 restore original values 
WITHOUT_CLASSIFICATION	 release this shared timer resource 
WITHOUT_CLASSIFICATION	 this mapjoin but not suited for sort merge bucket map join check outer joins 
WITHOUT_CLASSIFICATION	 remove old parents 
WITHOUT_CLASSIFICATION	 all partitions are altered atomically all prehooks are fired together followed all post hooks 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create table must fail 
WITHOUT_CLASSIFICATION	 was never called want start with first txn 
WITHOUT_CLASSIFICATION	 create the column stats table 
WITHOUT_CLASSIFICATION	 nothing here silently return 
WITHOUT_CLASSIFICATION	 the copytobuffer will reposition and reread the input buffer 
WITHOUT_CLASSIFICATION	 case test just close the log files not remove them 
WITHOUT_CLASSIFICATION	 our dbname equal but tablename blank were interested this dblevel event 
WITHOUT_CLASSIFICATION	 doas not enabled pass the principalkeypad sparksubmit order support the possible delegation token renewal spark 
WITHOUT_CLASSIFICATION	 max allow tez pick 
WITHOUT_CLASSIFICATION	 mystringenummap 
WITHOUT_CLASSIFICATION	 this handle synpos where pos headerend 
WITHOUT_CLASSIFICATION	 cut prefix from hives map key 
WITHOUT_CLASSIFICATION	 tarjans algo 
WITHOUT_CLASSIFICATION	 reset just the value columns and value buffer 
WITHOUT_CLASSIFICATION	 retrieve settings hiveconf that arent also set the jobconf 
WITHOUT_CLASSIFICATION	 setup 
WITHOUT_CLASSIFICATION	 must spark branch 
WITHOUT_CLASSIFICATION	 share the same write buffers with our value store 
WITHOUT_CLASSIFICATION	 set inner schema for dtype 
WITHOUT_CLASSIFICATION	 either initialcapacity too large overflows 
WITHOUT_CLASSIFICATION	 replace the current task with the new generated conditional task 
WITHOUT_CLASSIFICATION	 security property names 
WITHOUT_CLASSIFICATION	 case max list members max query string length and exact members single clause 
WITHOUT_CLASSIFICATION	 nonempty java opts with bad xmx specification 
WITHOUT_CLASSIFICATION	 count 
WITHOUT_CLASSIFICATION	 partition columns occur data want remove them find out positions partition columns schema provided user also need update the output schema with these deletions 
WITHOUT_CLASSIFICATION	 merge work only needs input and output 
WITHOUT_CLASSIFICATION	 replace the task with the new task copy the children and parents the old 
WITHOUT_CLASSIFICATION	 are running this constructor has the same behavior but the default was changed add wrapping and newlines 
WITHOUT_CLASSIFICATION	 this point the task has been added into the queue may have caused eviction for some other task 
WITHOUT_CLASSIFICATION	 were visiting terminal weve created ourselves just skip and keep going 
WITHOUT_CLASSIFICATION	 there could interval where desired counter value not populated the time make this check 
WITHOUT_CLASSIFICATION	 test mode print the logs the output 
WITHOUT_CLASSIFICATION	 after that heuristic used decide 
WITHOUT_CLASSIFICATION	 max tolerable variance for matches 
WITHOUT_CLASSIFICATION	 used for sending information for scheduling priority 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 already encoded thriftable object thriftformatter 
WITHOUT_CLASSIFICATION	 javacc not edit this line 
WITHOUT_CLASSIFICATION	 actionexpression 
WITHOUT_CLASSIFICATION	 sorting the keys from the properties helps create deterministic url which tested for various configuration 
WITHOUT_CLASSIFICATION	 the list being drained cannot increase the delta anymore 
WITHOUT_CLASSIFICATION	 serializationlib 
WITHOUT_CLASSIFICATION	 mapside work 
WITHOUT_CLASSIFICATION	 the alias may not present case subquery 
WITHOUT_CLASSIFICATION	 this for runtime minmax pushdown dont need not between 
WITHOUT_CLASSIFICATION	 sigh 
WITHOUT_CLASSIFICATION	 appropriate locations 
WITHOUT_CLASSIFICATION	 see the comment the other issamekey 
WITHOUT_CLASSIFICATION	 second group 
WITHOUT_CLASSIFICATION	 verify that when stats are already present and forcerecompute specified they are recomputed 
WITHOUT_CLASSIFICATION	 close the inner output stream before closing the outer output stream for chunked output this means dont write endofdata indicator 
WITHOUT_CLASSIFICATION	 include type name for precisionscale 
WITHOUT_CLASSIFICATION	 this inspector initialized still need 
WITHOUT_CLASSIFICATION	 this sql standard average zero items should null 
WITHOUT_CLASSIFICATION	 key index can nullnull there only single stripe just start fresh 
WITHOUT_CLASSIFICATION	 set vertexmanagerplugin whether its cross product destination vertex 
WITHOUT_CLASSIFICATION	 this not transactional 
WITHOUT_CLASSIFICATION	 negative number 
WITHOUT_CLASSIFICATION	 cannot slice compressed files 
WITHOUT_CLASSIFICATION	 notifies when memory usage after 
WITHOUT_CLASSIFICATION	 accumulate the counts 
WITHOUT_CLASSIFICATION	 set primary key name null before sending listener 
WITHOUT_CLASSIFICATION	 unlike may call this method multiple times for each 
WITHOUT_CLASSIFICATION	 our input repeating inputcolnumber 
WITHOUT_CLASSIFICATION	 column authorization checked through table scan operators 
WITHOUT_CLASSIFICATION	 nothing needs done 
WITHOUT_CLASSIFICATION	 run cleaner should remove the delta dirs and old base dir 
WITHOUT_CLASSIFICATION	 the local batch has been consumed entirely reset 
WITHOUT_CLASSIFICATION	 try eat trailing blank padding 
WITHOUT_CLASSIFICATION	 set common hash for this job that when create any temporary directory later guaranteed unique 
WITHOUT_CLASSIFICATION	 optional int signaturekeyid 
WITHOUT_CLASSIFICATION	 construct the astnode for the column that will join with the outerquery expression for select from where select from this will build toktableorcol identifiersq identifierb where the alias generated for the subquery 
WITHOUT_CLASSIFICATION	 make sure isnt 
WITHOUT_CLASSIFICATION	 int hadoopmem 
WITHOUT_CLASSIFICATION	 delete data ignore unknowndb cascade 
WITHOUT_CLASSIFICATION	 these two are used indicate that are running tests 
WITHOUT_CLASSIFICATION	 here each batch has written data and committed bucket since table only has bucket each deltas has bucket and bucketflushlength furthermore each bucket has now received more datalogically its buffered but not yet committed 
WITHOUT_CLASSIFICATION	 last batch successful remove from partsnotinms 
WITHOUT_CLASSIFICATION	 undone copied from 
WITHOUT_CLASSIFICATION	 validate and vectorize the reduce operator tree 
WITHOUT_CLASSIFICATION	 any log specific settings via hiveconf will ignored 
WITHOUT_CLASSIFICATION	 buffer now the heap buffer the list use buffer again 
WITHOUT_CLASSIFICATION	 must old client talking dont know its conservative 
WITHOUT_CLASSIFICATION	 forward reset key and value columns 
WITHOUT_CLASSIFICATION	 existing shard present the database use the current version 
WITHOUT_CLASSIFICATION	 this method reached when error occurs while sending msg the session must bad 
WITHOUT_CLASSIFICATION	 partitions updated 
WITHOUT_CLASSIFICATION	 need release memory cache eviction 
WITHOUT_CLASSIFICATION	 move the files back original data location 
WITHOUT_CLASSIFICATION	 throw hivesqlexception when async calls 
WITHOUT_CLASSIFICATION	 need patch the dest back original into new query this makes assumptions about the structure the ast 
WITHOUT_CLASSIFICATION	 round with default decimal places 
WITHOUT_CLASSIFICATION	 map keep track which root generated which work 
WITHOUT_CLASSIFICATION	 may need merge with list temp tables 
WITHOUT_CLASSIFICATION	 the user didnt specify serde use the default 
WITHOUT_CLASSIFICATION	 may some retry logic here 
WITHOUT_CLASSIFICATION	 loginfoegetkey egetvalue 
WITHOUT_CLASSIFICATION	 skip this validatecolumnname always returns true 
WITHOUT_CLASSIFICATION	 the settings conf overlay should not part session config 
WITHOUT_CLASSIFICATION	 select true fields child none child and none 
WITHOUT_CLASSIFICATION	 nothing default 
WITHOUT_CLASSIFICATION	 checks the status the rpc call throws exception case error 
WITHOUT_CLASSIFICATION	 set checkpoint task dependant add partition tasks same dump retried for bootstrap skip current partition update 
WITHOUT_CLASSIFICATION	 return false for null 
WITHOUT_CLASSIFICATION	 error should not timeout 
WITHOUT_CLASSIFICATION	 from metastore for security 
WITHOUT_CLASSIFICATION	 hive doesnt support primary keys using local schema with empty resultset 
WITHOUT_CLASSIFICATION	 compaction doesnt work under transaction and hence pass null for validtxnlist 
WITHOUT_CLASSIFICATION	 warning 
WITHOUT_CLASSIFICATION	 doubleval 
WITHOUT_CLASSIFICATION	 note that pathtoalias will behave the original plan was join plan 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautillist 
WITHOUT_CLASSIFICATION	 reduction but lets still test the original predicate see was already constant which case dont need any runtime decision about filtering 
WITHOUT_CLASSIFICATION	 must deterministic order maps see hive 
WITHOUT_CLASSIFICATION	 vectorize this parents children plug them into vectorparents children list add those children vector children nextparentlist 
WITHOUT_CLASSIFICATION	 put parameters aggregations reducevalues 
WITHOUT_CLASSIFICATION	 the overflow batch 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 timestamp patterns should default normal timestamp format 
WITHOUT_CLASSIFICATION	 required required required required required required 
WITHOUT_CLASSIFICATION	 extra bytes the end 
WITHOUT_CLASSIFICATION	 note assume that batchsize will consistent with vectors passed this rather brittle same other readers 
WITHOUT_CLASSIFICATION	 default 
WITHOUT_CLASSIFICATION	 allocate free allocate 
WITHOUT_CLASSIFICATION	 list the new files destination path which gets copied from source 
WITHOUT_CLASSIFICATION	 argument handling 
WITHOUT_CLASSIFICATION	 all the elements are representing null then return true 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 reset the log buffer verify new dump without any api call does not contain func 
WITHOUT_CLASSIFICATION	 all are filtered 
WITHOUT_CLASSIFICATION	 put session into the pool 
WITHOUT_CLASSIFICATION	 first authorize the call 
WITHOUT_CLASSIFICATION	 child isnt flattened because parent repeating null 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 column statistics index contains only the number rows 
WITHOUT_CLASSIFICATION	 from 
WITHOUT_CLASSIFICATION	 everyone has permission write but with sticky set that delete restricted this required since the path same for all users and everyone writes into 
WITHOUT_CLASSIFICATION	 delimiters for sql statements are any nonletterornumber characters except underscore and characters that are specified the database valid name identifiers 
WITHOUT_CLASSIFICATION	 favor broad plans over deep plans 
WITHOUT_CLASSIFICATION	 need keep the original list operators the map join know 
WITHOUT_CLASSIFICATION	 map from primitivetypeinfo 
WITHOUT_CLASSIFICATION	 irrelevant from eventids this can tracked the itself instead polluting the task also since have all the mrinput events here theyll all sent together 
WITHOUT_CLASSIFICATION	 input has not been rewritten not rewrite this rel 
WITHOUT_CLASSIFICATION	 rights signum wins 
WITHOUT_CLASSIFICATION	 read all the world 
WITHOUT_CLASSIFICATION	 this means correated value generator wasnt generated 
WITHOUT_CLASSIFICATION	 are done since there are keys check for 
WITHOUT_CLASSIFICATION	 the start the split points into the middle the cached slice cannot use the cached block its encoded and columnar cannot map the file 
WITHOUT_CLASSIFICATION	 this keep track subquery correlated and contains aggregate this computed calciteplanner while planning and later required subuery remove rule hence this passed using hiveplannercontext 
WITHOUT_CLASSIFICATION	 finish for execute 
WITHOUT_CLASSIFICATION	 convert agg args calcite 
WITHOUT_CLASSIFICATION	 entire response written out safe enable timeout handling 
WITHOUT_CLASSIFICATION	 see comment 
WITHOUT_CLASSIFICATION	 lookup the delegation token first the connection url then configuration 
WITHOUT_CLASSIFICATION	 ensures all params are indented 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 return the number columns recorded this files header 
WITHOUT_CLASSIFICATION	 substitution only supported nonbeeline mode 
WITHOUT_CLASSIFICATION	 the output columns does not contains the input table 
WITHOUT_CLASSIFICATION	 need disable join emit interval since for outer joins with post conditions need have the full view the right matching rows know whether need produce row with null values not 
WITHOUT_CLASSIFICATION	 hosts per host requests the same priority first host next host last with host third should allocate host host will wait 
WITHOUT_CLASSIFICATION	 mocks publishing logic 
WITHOUT_CLASSIFICATION	 multikey outer get key 
WITHOUT_CLASSIFICATION	 sqlstate errorcode should set 
WITHOUT_CLASSIFICATION	 first need find the minuncommittedtxnid which currently seen any open transactions there are txns which are currently open aborted the system then current value 
WITHOUT_CLASSIFICATION	 acid doesnt maintain this just makes logic more explicit 
WITHOUT_CLASSIFICATION	 dont use fieldschemaequals since also compares comments which unnecessary for this method 
WITHOUT_CLASSIFICATION	 create the required command line options 
WITHOUT_CLASSIFICATION	 parse out the individual parts 
WITHOUT_CLASSIFICATION	 are aborting all txns the current batch need heartbeat 
WITHOUT_CLASSIFICATION	 remove the existing partition columns from the field schema 
WITHOUT_CLASSIFICATION	 test major compaction 
WITHOUT_CLASSIFICATION	 partitionview does not have not need update its column stats 
WITHOUT_CLASSIFICATION	 hll algorithm shows stronger bias for values range compensate for this short range bias linear counting used for values before this short range the original paper also says similar bias seen for long range values due hash collisions range for the default case not have worry about this long range bias the paper used bit hashing and use bit hashing default values are too high observe long range bias hash collisions 
WITHOUT_CLASSIFICATION	 let the dummy the parent mapjoin 
WITHOUT_CLASSIFICATION	 have extracted the count from the hash multiset result dont keep 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 all columns select for example 
WITHOUT_CLASSIFICATION	 assuming the used memory equally divided among all executors 
WITHOUT_CLASSIFICATION	 turn bytes back into identifier for cache lookup 
WITHOUT_CLASSIFICATION	 high word multiplier multiply digits decimal words digit commad 
WITHOUT_CLASSIFICATION	 create expressions for project operators before and after the union 
WITHOUT_CLASSIFICATION	 make sure semijoin not enabled then not remove the dynamic partition pruning predicates 
WITHOUT_CLASSIFICATION	 find databases which name contains tofind hidden 
WITHOUT_CLASSIFICATION	 retain output column number from parent 
WITHOUT_CLASSIFICATION	 result grantor principal 
WITHOUT_CLASSIFICATION	 create database with view 
WITHOUT_CLASSIFICATION	 ensure partition present 
WITHOUT_CLASSIFICATION	 check already have initiated are working compaction for this partition table skip are just waiting cleaning can still check may time compact again even though havent cleaned todo this not robust you can easily run alter table start compaction between 
WITHOUT_CLASSIFICATION	 set rreturntype 
WITHOUT_CLASSIFICATION	 bgenjjtree typebyte 
WITHOUT_CLASSIFICATION	 interval year month comparisons 
WITHOUT_CLASSIFICATION	 semantic error not possible must bug convert internal error 
WITHOUT_CLASSIFICATION	 value too large should also null 
WITHOUT_CLASSIFICATION	 create valid table 
WITHOUT_CLASSIFICATION	 for use from construct from userinput 
WITHOUT_CLASSIFICATION	 record info metrics 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the last line didnt match the patterns either the stack trace definitely over 
WITHOUT_CLASSIFICATION	 for partial and final objectinspectors for partial aggregations list 
WITHOUT_CLASSIFICATION	 buffers test are fakes not linked cache notify cache policy explicitly 
WITHOUT_CLASSIFICATION	 did not change the location there need move the table directories 
WITHOUT_CLASSIFICATION	 check exprnodecolumndesc wrapped expr 
WITHOUT_CLASSIFICATION	 specific order 
WITHOUT_CLASSIFICATION	 get table details from tabnametotabobject cache 
WITHOUT_CLASSIFICATION	 this should fail because txn aborted due timeout 
WITHOUT_CLASSIFICATION	 this will abort the txn 
WITHOUT_CLASSIFICATION	 for each task set the key descriptor for the reducer 
WITHOUT_CLASSIFICATION	 dynamic partitions dynamic partitions are generated dppartspecs may not initialized 
WITHOUT_CLASSIFICATION	 alternate between returning deleted and not this easier than actually tracking operations test that this getting properly called checking that only half the records show base files after major compactions 
WITHOUT_CLASSIFICATION	 above decimal 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite partition with new exclusive coalesces 
WITHOUT_CLASSIFICATION	 initialize constant arguments 
WITHOUT_CLASSIFICATION	 todo determine the progress 
WITHOUT_CLASSIFICATION	 multiply 
WITHOUT_CLASSIFICATION	 num rows whose output evaluated 
WITHOUT_CLASSIFICATION	 dot operator remember the field name the rhs the left semijoin 
WITHOUT_CLASSIFICATION	 use udf query 
WITHOUT_CLASSIFICATION	 can now create multijoin operator 
WITHOUT_CLASSIFICATION	 this nonlocal warehouse then adding resources from the local filesystem may mean that other clients will not able access the resources disallow resources from local filesystem this case 
WITHOUT_CLASSIFICATION	 insert overwrite not supported for acid tables 
WITHOUT_CLASSIFICATION	 perform alters for incremental replication 
WITHOUT_CLASSIFICATION	 this now lifo operation 
WITHOUT_CLASSIFICATION	 get the current batch size 
WITHOUT_CLASSIFICATION	 class static variables 
WITHOUT_CLASSIFICATION	 the batchindex for the rows that are for the thenelse rows respectively 
WITHOUT_CLASSIFICATION	 columns 
WITHOUT_CLASSIFICATION	 these are only used for tests 
WITHOUT_CLASSIFICATION	 now have archive with partitions 
WITHOUT_CLASSIFICATION	 get the partition specs 
WITHOUT_CLASSIFICATION	 validate the function name 
WITHOUT_CLASSIFICATION	 prepare buffers 
WITHOUT_CLASSIFICATION	 firstname owen foobar substrlastname and firstname between david and greg 
WITHOUT_CLASSIFICATION	 the lock then there are locks this heartbeat 
WITHOUT_CLASSIFICATION	 xmx specified bytes 
WITHOUT_CLASSIFICATION	 the input the select does not matter over the expressions 
WITHOUT_CLASSIFICATION	 initialize the object inspectors 
WITHOUT_CLASSIFICATION	 production async functiontype name fieldlist throws commaorsemicolon 
WITHOUT_CLASSIFICATION	 wrap the client with threadsafe proxy serialize the rpc calls 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 colexprmapsize size cols from sel branch 
WITHOUT_CLASSIFICATION	 whether root tasks after materialized cte linkage have been resolved 
WITHOUT_CLASSIFICATION	 inside our own that can also store requested quantile values between calls 
WITHOUT_CLASSIFICATION	 loop for middles 
WITHOUT_CLASSIFICATION	 through each event and dump out each event eventlevel dump dir inside dumproot 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 debugstacktrace egetstacktrace 
WITHOUT_CLASSIFICATION	 simple container registration and unregistration without any task attempt being involved 
WITHOUT_CLASSIFICATION	 test with null args 
WITHOUT_CLASSIFICATION	 the column number for this one column join specialization 
WITHOUT_CLASSIFICATION	 check any left pair exists for right objects 
WITHOUT_CLASSIFICATION	 newinput 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 hivecommand nonsql statement such setting property adding resource 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 for all the source tables that have lateral view attach the 
WITHOUT_CLASSIFICATION	 default partition and bucket columns are sorted ascending order 
WITHOUT_CLASSIFICATION	 when creating the reader below there are read ops per bucket file liststatus and open 
WITHOUT_CLASSIFICATION	 create new projectunionproject sequences 
WITHOUT_CLASSIFICATION	 find operators which are the children specified filterop and there are these 
WITHOUT_CLASSIFICATION	 and one partition 
WITHOUT_CLASSIFICATION	 now allow the users specify any pools 
WITHOUT_CLASSIFICATION	 update the object 
WITHOUT_CLASSIFICATION	 rounds 
WITHOUT_CLASSIFICATION	 now vary isrepeating nulls possible left right 
WITHOUT_CLASSIFICATION	 anything else including boolean and string null 
WITHOUT_CLASSIFICATION	 the task has been terminated and the duck accounted for based local state whatever were doing irrelevant the metrics have also been updated 
WITHOUT_CLASSIFICATION	 this test case currently fails since add partitions dont inherit anything from tables 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 get the lowvalue 
WITHOUT_CLASSIFICATION	 should cancel hcat token was acquired hcat and not was supplied oozie the latter 
WITHOUT_CLASSIFICATION	 longs produces intervaldaytime 
WITHOUT_CLASSIFICATION	 end additional steps 
WITHOUT_CLASSIFICATION	 the fix delayed that the parent operators arent modified until the entire operator tree has been vectorized 
WITHOUT_CLASSIFICATION	 represents the schema exposed queryblock 
WITHOUT_CLASSIFICATION	 block 
WITHOUT_CLASSIFICATION	 its move task get the path the files were moved from this what any preceding map reduce task inferred information about and moving does not invalidate those assumptions this can happen when conditional merge added before the final movetask but the 
WITHOUT_CLASSIFICATION	 but for native tables need prefix match for subdirectories unlike nonnative tables prefix mixups dont seem potential problem here since are always dealing with the path something deeper than the table location 
WITHOUT_CLASSIFICATION	 data type conversion needed data type conversion check assume alter table prevented conversions that cannot handle vectordeserialize lazysimple capable converting its own lazybinary partition schema assumed match file contents conversion necessary from partition field values vector columns rowdeserialize partition schema assumed match file contents conversion necessary from partition field values vector columns 
WITHOUT_CLASSIFICATION	 use nonsettable struct object inspector 
WITHOUT_CLASSIFICATION	 can handle only case trunc date type 
WITHOUT_CLASSIFICATION	 raw splits 
WITHOUT_CLASSIFICATION	 aggregate expression from the rest nodes 
WITHOUT_CLASSIFICATION	 are also not supposed call setdone since are only part the operation 
WITHOUT_CLASSIFICATION	 pktabledb 
WITHOUT_CLASSIFICATION	 this should never happen with linked list queue 
WITHOUT_CLASSIFICATION	 need create the table manually rather than creating task since has exist compile the insert into 
WITHOUT_CLASSIFICATION	 repeated string tablesread 
WITHOUT_CLASSIFICATION	 jdoexception wrapped metaexception wrapped invocationexception 
WITHOUT_CLASSIFICATION	 get tmp file uri 
WITHOUT_CLASSIFICATION	 only one instance available cannot failover 
WITHOUT_CLASSIFICATION	 trigger movetrigger new moveexpression new etl trigger killtrigger new moveexpression new string query select tundercol tvalue from tablename join tablename tundercoltundercol order tundercol tvalue liststring setcmds new arraylist setcmdsaddset setcmdsaddset setcmdsaddset liststring errcaptureexpect new arraylist manager events summary get move kill return movebigread bigwritekill violation queue trigger movetrigger violated violation etl queue trigger killtrigger violated hdfsbytesread hdfsbyteswritten get pool cluster move pool etl cluster kill pool null cluster return pool null cluster setcmds killtrigger violated errcaptureexpect 
WITHOUT_CLASSIFICATION	 recent hadoop versions use deleteonexit clean tmp files 
WITHOUT_CLASSIFICATION	 the remainder 
WITHOUT_CLASSIFICATION	 generate all expressions from lateral view 
WITHOUT_CLASSIFICATION	 status 
WITHOUT_CLASSIFICATION	 should either select select distinct 
WITHOUT_CLASSIFICATION	 effectively final but has volatile since its accessed different 
WITHOUT_CLASSIFICATION	 concatenation dbname tablename and partition keyvalues 
WITHOUT_CLASSIFICATION	 production struct thisname fieldlist 
WITHOUT_CLASSIFICATION	 ensure task preempted based time match its allocated containerid 
WITHOUT_CLASSIFICATION	 using init instead this because the operation that needs run before delegating the other ctor and this messes chaining ctors 
WITHOUT_CLASSIFICATION	 udafs are present new columns needs added 
WITHOUT_CLASSIFICATION	 signature restriction nsoe and nsoe being flat exception prevents from setting the cause the nsoe the metaexception should not lose the info got here but its very likely that the metaexception irrelevant and actually nsoe message should log and throw nsoe with the msg 
WITHOUT_CLASSIFICATION	 use the ugi object that got added 
WITHOUT_CLASSIFICATION	 numnulls 
WITHOUT_CLASSIFICATION	 lets try store original column name this column got folded this useful for optimizations like groupbyoptimizer 
WITHOUT_CLASSIFICATION	 set the new hivesitexml 
WITHOUT_CLASSIFICATION	 insert mapside 
WITHOUT_CLASSIFICATION	 drain any calls which may have come during the last execution the loop 
WITHOUT_CLASSIFICATION	 handling subquery expressions where clause contains subquery expressions then true else false extract subquery expressionsn from where clause this nested subquery nthere are more than subquery expressions then yes throw unsupported error else rewrite search condition nremove subquery predicate build qbsubquery extract correlated predicates nfrom where clause add correlated items nselect list and group construct join predicate nfrom correlation predicates generate plan forn modified subquery build the join conditionn for parent query subquery join build the qbjointree from the join condition update parent query filtern with any post join conditions endif endif support for sub queries having clause and large this works the same way subqueries the where clause the one addum the handling aggregation expressions from the outer query appearing correlation clauses such correlating predicates are allowed minouterquertx subqueryy this requires special handling when converting joins see qbsubqueryrewrite method method for detailed comments 
WITHOUT_CLASSIFICATION	 operator with children 
WITHOUT_CLASSIFICATION	 crlf substitution return original line 
WITHOUT_CLASSIFICATION	 task processed 
WITHOUT_CLASSIFICATION	 copied from acidutils dont have put the code using this into 
WITHOUT_CLASSIFICATION	 tracks existing requests which are cycled through 
WITHOUT_CLASSIFICATION	 partition has been spilled disk its size will wont picked 
WITHOUT_CLASSIFICATION	 evaluate the function result for each row the partition 
WITHOUT_CLASSIFICATION	 texception 
WITHOUT_CLASSIFICATION	 create lfd even for ctas its noop move but still seems used for stats 
WITHOUT_CLASSIFICATION	 remove ending 
WITHOUT_CLASSIFICATION	 create the reduce sink operator 
WITHOUT_CLASSIFICATION	 column index stream type buffers 
WITHOUT_CLASSIFICATION	 column interpreted the row key 
WITHOUT_CLASSIFICATION	 sortby asc 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl call check existence side file for mockmocktbl call open mockmocktbl call check existence side file for mockmocktbl 
WITHOUT_CLASSIFICATION	 optional int appattemptnumber 
WITHOUT_CLASSIFICATION	 make clone existing hive conf 
WITHOUT_CLASSIFICATION	 supportskeytypes 
WITHOUT_CLASSIFICATION	 necessary clean the transaction the 
WITHOUT_CLASSIFICATION	 use the unparsetranslator resolve unqualified table names 
WITHOUT_CLASSIFICATION	 shouldnt actually called the test will fail 
WITHOUT_CLASSIFICATION	 ssl support 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this the initial state for lock 
WITHOUT_CLASSIFICATION	 see the class comment hif handles for all input formats try handle again particular for the nonrecursive originalsonly getsplits call will just get confused this bypass was not necessary when tables didnt support originals now that they use this path for anything table related although everything except the originals could still handled acidutils like regular nontxn table 
WITHOUT_CLASSIFICATION	 ctas make the movetasks destination directory the tables destination 
WITHOUT_CLASSIFICATION	 check the root expression for final candidates 
WITHOUT_CLASSIFICATION	 now decompress copy the data into cache buffers 
WITHOUT_CLASSIFICATION	 step merge mapjointask into the mapside its child 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 initializing stats publisher 
WITHOUT_CLASSIFICATION	 not come from the local branch 
WITHOUT_CLASSIFICATION	 file for local 
WITHOUT_CLASSIFICATION	 verify the next write 
WITHOUT_CLASSIFICATION	 repeating with other nullable 
WITHOUT_CLASSIFICATION	 the output the commonjoinoperator the input columninfo 
WITHOUT_CLASSIFICATION	 our limit max precision integer digits leading zeros below the dot has zeroes below the dot 
WITHOUT_CLASSIFICATION	 rerun constant propagation fold any new constants introduced the operator optimizers 
WITHOUT_CLASSIFICATION	 request task task already started previously set time 
WITHOUT_CLASSIFICATION	 run the cleaner thread when the cache maxfull full 
WITHOUT_CLASSIFICATION	 trim the trailing 
WITHOUT_CLASSIFICATION	 localityrequested randomly pick node containing free slots 
WITHOUT_CLASSIFICATION	 and udf 
WITHOUT_CLASSIFICATION	 default return the urlparam passed asis 
WITHOUT_CLASSIFICATION	 orc files can converted full acid transactional tables 
WITHOUT_CLASSIFICATION	 tests for droppartitionstring dbname string tblname liststring partvals boolean deletedata method 
WITHOUT_CLASSIFICATION	 latin small letter with hook bytes 
WITHOUT_CLASSIFICATION	 remove the table folder 
WITHOUT_CLASSIFICATION	 point old releases 
WITHOUT_CLASSIFICATION	 add root tasks runnable 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 remains copy from current read buffer less than wbsize def 
WITHOUT_CLASSIFICATION	 location 
WITHOUT_CLASSIFICATION	 make sure hostport pair valid the status the location does not matter 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 here just reuse the threadcount configuration for since this results better performance the number missing partitions discovered are later added metastore using threadpool size have different sized pool here the smaller sized pool the two becomes bottleneck 
WITHOUT_CLASSIFICATION	 you can blame this owen 
WITHOUT_CLASSIFICATION	 the table could not cached due memory limit stop prewarm 
WITHOUT_CLASSIFICATION	 create the batch will use return data for this 
WITHOUT_CLASSIFICATION	 foo 
WITHOUT_CLASSIFICATION	 the master thread and various workers 
WITHOUT_CLASSIFICATION	 long colulmn text column 
WITHOUT_CLASSIFICATION	 has not keyword 
WITHOUT_CLASSIFICATION	 avoid tmp directories closer 
WITHOUT_CLASSIFICATION	 and got duck 
WITHOUT_CLASSIFICATION	 create the work for moving the table 
WITHOUT_CLASSIFICATION	 efficiently add computed values the last batch group key 
WITHOUT_CLASSIFICATION	 how many jobs have been started 
WITHOUT_CLASSIFICATION	 dont proper overlap checking because would cost cycles and think will never happen perform the most basic check here 
WITHOUT_CLASSIFICATION	 default assume filter tag are good 
WITHOUT_CLASSIFICATION	 reset database location 
WITHOUT_CLASSIFICATION	 the partition columns are set once for the partition and are marked repeating 
WITHOUT_CLASSIFICATION	 align the thenelse types 
WITHOUT_CLASSIFICATION	 task and its child task has been converted from join mapjoin see the two tasks can merged 
WITHOUT_CLASSIFICATION	 test doubledouble version 
WITHOUT_CLASSIFICATION	 these values are equal when txn entry made should never equal after heartbeat which 
WITHOUT_CLASSIFICATION	 but that will specific hdfs through storagehandler mechanism 
WITHOUT_CLASSIFICATION	 methods setreset getnextnotification modifier 
WITHOUT_CLASSIFICATION	 construct value table desc 
WITHOUT_CLASSIFICATION	 any exact match 
WITHOUT_CLASSIFICATION	 repl policy should created based the table name context 
WITHOUT_CLASSIFICATION	 this depends 
WITHOUT_CLASSIFICATION	 add schemarr relnodeschema map 
WITHOUT_CLASSIFICATION	 dont check explicit pool match for apps both are specified the jdbc string 
WITHOUT_CLASSIFICATION	 always return something from getaclforpath this should not happen 
WITHOUT_CLASSIFICATION	 boolean value match for char field 
WITHOUT_CLASSIFICATION	 insert reduce side with reduce side input 
WITHOUT_CLASSIFICATION	 dynamic partition throw the exception 
WITHOUT_CLASSIFICATION	 add the move task for those partitions that not need merging 
WITHOUT_CLASSIFICATION	 extract partition desc from sorted map ascending order part dir 
WITHOUT_CLASSIFICATION	 the timestamp column 
WITHOUT_CLASSIFICATION	 split split 
WITHOUT_CLASSIFICATION	 class variables 
WITHOUT_CLASSIFICATION	 container 
WITHOUT_CLASSIFICATION	 use only reducer case cartesian product 
WITHOUT_CLASSIFICATION	 add jars find them map contents jar name that can avoid 
WITHOUT_CLASSIFICATION	 generate ddl task and make dependent task the leaf 
WITHOUT_CLASSIFICATION	 limit milliseconds only 
WITHOUT_CLASSIFICATION	 mapjoinoperator 
WITHOUT_CLASSIFICATION	 make sure doesnt already exist 
WITHOUT_CLASSIFICATION	 convert nonskewed table skewed table 
WITHOUT_CLASSIFICATION	 the last field singlevaluetrue 
WITHOUT_CLASSIFICATION	 remove table entry from sessionstate 
WITHOUT_CLASSIFICATION	 generate groupbyoperator 
WITHOUT_CLASSIFICATION	 test that throws hcatexception the partitionkey incorrect 
WITHOUT_CLASSIFICATION	 complex pattern 
WITHOUT_CLASSIFICATION	 construct agginfo 
WITHOUT_CLASSIFICATION	 use partition schema properties set the partition descriptor properties set true 
WITHOUT_CLASSIFICATION	 get the semijoin rhs table name and field name 
WITHOUT_CLASSIFICATION	 all column names referenced including virtual columns used 
WITHOUT_CLASSIFICATION	 see fasthivedecimalimpl for more details these fields 
WITHOUT_CLASSIFICATION	 make sure nothing escapes this run method and kills the metastore large 
WITHOUT_CLASSIFICATION	 the update fails because the task has terminated the node 
WITHOUT_CLASSIFICATION	 some spark plans cause hash and other modes get this ignore 
WITHOUT_CLASSIFICATION	 inputsplitinfo false 
WITHOUT_CLASSIFICATION	 sending out messages before taimpl knows how handle them 
WITHOUT_CLASSIFICATION	 will need handle alternate workdir well this case derive from branch 
WITHOUT_CLASSIFICATION	 since have open transaction only values above are expected 
WITHOUT_CLASSIFICATION	 just the primitive types 
WITHOUT_CLASSIFICATION	 cannot simplify bail out 
WITHOUT_CLASSIFICATION	 evaluator results are first 
WITHOUT_CLASSIFICATION	 get the column table aliases from the expression start from the tokfunction 
WITHOUT_CLASSIFICATION	 gather expressions ast aggregations 
WITHOUT_CLASSIFICATION	 directly deserialize with the caller reading fieldbyfield the lazybinary serialization format the caller responsible for calling the read method for the right type each field after calling readnextfield reading some fields require results object receive value information separate results object created the caller initialization per different field even for the same type some type values are reference either bytes the deserialization buffer other type specific buffers those references are only valid until the next time set called 
WITHOUT_CLASSIFICATION	 the columns row schema not contained column expression map then those are the aggregate columns that are added gby operator will estimate the column statistics for those newly added columns 
WITHOUT_CLASSIFICATION	 check that the correlated variables referenced these 
WITHOUT_CLASSIFICATION	 this ensures the incremental dump doesnt get all events for replication 
WITHOUT_CLASSIFICATION	 not relevant creating new partition 
WITHOUT_CLASSIFICATION	 for outer join remember our input rows before expression filtering before hash table matching can generate results for all rows matching and non matching 
WITHOUT_CLASSIFICATION	 dummyops reference all the hashtabledummy operators the plan these have separately initialized when setup task their function mainly root ops give the mapjoin the correct 
WITHOUT_CLASSIFICATION	 and necessary load the jars this thread 
WITHOUT_CLASSIFICATION	 new tai lue letter high bytes 
WITHOUT_CLASSIFICATION	 determine which input rel oldordinal references and adjust oldordinal relative that input rel 
WITHOUT_CLASSIFICATION	 not selected 
WITHOUT_CLASSIFICATION	 expected exception due lexer error 
WITHOUT_CLASSIFICATION	 jump the field want and read 
WITHOUT_CLASSIFICATION	 used windowing 
WITHOUT_CLASSIFICATION	 look for hivesitexml the classpath and log its location found 
WITHOUT_CLASSIFICATION	 want start sufficiently high enough the iterator stack 
WITHOUT_CLASSIFICATION	 array partitions holding the triplets total number small table rows memory the max memory limit that can allocated the actual memory used row size the small table whether theres any spilled partition the partition into which spill the big table row 
WITHOUT_CLASSIFICATION	 remove filtermap for outer alias filter not exist that 
WITHOUT_CLASSIFICATION	 check that subquery top level conjuncts remove from the where clause ast 
WITHOUT_CLASSIFICATION	 check the parent coming from table scan what the version 
WITHOUT_CLASSIFICATION	 for each joining table set dir for big key and small keys properly 
WITHOUT_CLASSIFICATION	 figure out who should run the file operations 
WITHOUT_CLASSIFICATION	 calcite literal millis need convert seconds 
WITHOUT_CLASSIFICATION	 create new mapredlocalwork 
WITHOUT_CLASSIFICATION	 indicates malformed version 
WITHOUT_CLASSIFICATION	 support nested column pruning need track the path from the top the nested 
WITHOUT_CLASSIFICATION	 hashbased aggregations 
WITHOUT_CLASSIFICATION	 restrictionh subqueries only supported the sql where clause 
WITHOUT_CLASSIFICATION	 the table different dfs than the partition replace the partitions dfs with the tables dfs 
WITHOUT_CLASSIFICATION	 does not work the underlying inputsplit has package visibility 
WITHOUT_CLASSIFICATION	 common class between charvarchar string 
WITHOUT_CLASSIFICATION	 dont want not exit because issue with logging 
WITHOUT_CLASSIFICATION	 updates deletes from insert and new part 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 http transport mode set the thread local proxy username thrifthttpservlet 
WITHOUT_CLASSIFICATION	 hive native 
WITHOUT_CLASSIFICATION	 infer sort columns from operator tree 
WITHOUT_CLASSIFICATION	 long 
WITHOUT_CLASSIFICATION	 complete txn 
WITHOUT_CLASSIFICATION	 leading spaces 
WITHOUT_CLASSIFICATION	 one vint without nanos 
WITHOUT_CLASSIFICATION	 routines for stubbing into writables 
WITHOUT_CLASSIFICATION	 execdriver has plan path cannot derive vrb stuff for the wrapper 
WITHOUT_CLASSIFICATION	 copy the properties from storagehandler jobproperties 
WITHOUT_CLASSIFICATION	 should never executed 
WITHOUT_CLASSIFICATION	 replace insert overwrite insert 
WITHOUT_CLASSIFICATION	 ivalue 
WITHOUT_CLASSIFICATION	 for example the case select from join would direct dependency 
WITHOUT_CLASSIFICATION	 check compatibility with subsequent files 
WITHOUT_CLASSIFICATION	 return false otherwise 
WITHOUT_CLASSIFICATION	 killed something but still got rejected wait bit give chance our previous victim actually die 
WITHOUT_CLASSIFICATION	 setup the based the input tabledefs columns the window functions 
WITHOUT_CLASSIFICATION	 reached here did not find replication spec the node its immediate children defaults are pretend replication not happening and the statement above running asis 
WITHOUT_CLASSIFICATION	 change new table and append stats for the new table 
WITHOUT_CLASSIFICATION	 kryo will set this hope 
WITHOUT_CLASSIFICATION	 the newposition the same the previousposition weve reached the end the binary search the new position least big the size the split any 
WITHOUT_CLASSIFICATION	 origpathstrhdfshost for example 
WITHOUT_CLASSIFICATION	 select udtf 
WITHOUT_CLASSIFICATION	 delete the parent temp directory all custom dynamic partitions 
WITHOUT_CLASSIFICATION	 this best effort optimization bail out error conditions and try generate and execute slower plan 
WITHOUT_CLASSIFICATION	 column types all partitioned columns used for generating partition specification 
WITHOUT_CLASSIFICATION	 the fifth could combined again 
WITHOUT_CLASSIFICATION	 save state for future iterations 
WITHOUT_CLASSIFICATION	 the buffer can only removed after the removed flag has been set are able lock here noone can set the removed flag and thus remove that would also mean that the header not free and noone will touch the header either 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlblob 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 dont log the exception people just get confused 
WITHOUT_CLASSIFICATION	 its not internal name this what want 
WITHOUT_CLASSIFICATION	 will adjust start and end that could record the metrics save the originals 
WITHOUT_CLASSIFICATION	 prevent excessive logging case deadlocks slowness 
WITHOUT_CLASSIFICATION	 allow implicit string date conversion 
WITHOUT_CLASSIFICATION	 close the underlying connection pool avoid leaks 
WITHOUT_CLASSIFICATION	 since left integer always some products here are not included 
WITHOUT_CLASSIFICATION	 write byte say whether skip pruning not 
WITHOUT_CLASSIFICATION	 that uniont null converted just within map 
WITHOUT_CLASSIFICATION	 tracks hivequeryid queryidentifier this can only set when config parsed tezprocessor all the other existing code passes queryid equal everywhere switch the runtime and move parsing the payload the the actual hive queryid could 
WITHOUT_CLASSIFICATION	 make right child left 
WITHOUT_CLASSIFICATION	 tests for partition tablename string dbname liststring partvals method 
WITHOUT_CLASSIFICATION	 schemagroup 
WITHOUT_CLASSIFICATION	 locality information before those without locality information 
WITHOUT_CLASSIFICATION	 test basic right trim and truncate vector 
WITHOUT_CLASSIFICATION	 check map side aggregation possible not based column stats 
WITHOUT_CLASSIFICATION	 vectorization enabled 
WITHOUT_CLASSIFICATION	 collect operator observe the output the script 
WITHOUT_CLASSIFICATION	 udtf 
WITHOUT_CLASSIFICATION	 weve already set this one need clone for the next work 
WITHOUT_CLASSIFICATION	 zookeeper property name pick the correct jaas conf section 
WITHOUT_CLASSIFICATION	 body 
WITHOUT_CLASSIFICATION	 newoutput the index the cor var the referenced position list plus the offset referenced position list 
WITHOUT_CLASSIFICATION	 convert integer value representing timestamp nanoseconds one that represents timestamp seconds with fraction since the epoch 
WITHOUT_CLASSIFICATION	 position file where the first row this block starts 
WITHOUT_CLASSIFICATION	 there should adjustments for leap seconds 
WITHOUT_CLASSIFICATION	 fifo policy doesnt care 
WITHOUT_CLASSIFICATION	 note this behavior may have change ever implement vectorized merge join 
WITHOUT_CLASSIFICATION	 add key key slot slot pairindex pairindex empty slot 
WITHOUT_CLASSIFICATION	 keep the columns only the columns that are part the final output 
WITHOUT_CLASSIFICATION	 verify mergeonlytask not optimized 
WITHOUT_CLASSIFICATION	 bail out nothing 
WITHOUT_CLASSIFICATION	 not external table then create one 
WITHOUT_CLASSIFICATION	 construct 
WITHOUT_CLASSIFICATION	 first change the filter condition into join condition 
WITHOUT_CLASSIFICATION	 need make metastore call 
WITHOUT_CLASSIFICATION	 dont drop notificationlog sequencetable and its used other table which are not txn related generate primary key these tables are dropped and other tables are not dropped then will create key duplicate error while inserting other table 
WITHOUT_CLASSIFICATION	 division with overflowzerodivide check error produces null output 
WITHOUT_CLASSIFICATION	 create filesinkoperator for the file name tasktmpdir 
WITHOUT_CLASSIFICATION	 vecaggrdesc aggregationbuffer void agg vectorizedrowbatch unit void int aggregateindex vectorizedrowbatch vrg void agg long boolean matchesstring name columnvectortype inputcolvectortype columnvectortype outputcolvectortype mode mode batch int batchindex int columnnum aggregationbuffer agg 
WITHOUT_CLASSIFICATION	 this called either with error that was queued error that was set into the atomic reference this class the latter besteffort and used opportunistically skip processing long queue when the error happens 
WITHOUT_CLASSIFICATION	 check for aborted txns 
WITHOUT_CLASSIFICATION	 make sure they are the same before and after compaction 
WITHOUT_CLASSIFICATION	 note and power multiply 
WITHOUT_CLASSIFICATION	 setting success false make sure that the listener fails rollback happens 
WITHOUT_CLASSIFICATION	 todo factor security manager into pipeline todo factor out encodedecode permit binary shuffle todo factor out decode index permit alt models 
WITHOUT_CLASSIFICATION	 check for nested message found set the schema else return 
WITHOUT_CLASSIFICATION	 analyze table partition compute statistics the plan consists simple mapredtask followed statstask the task just simple tablescanoperator 
WITHOUT_CLASSIFICATION	 refering sqloperationjava there chance that hivesqlexception throws and the async background operation submits thread pool successfully the same time cleanup ophandle directly when got hivesqlexception 
WITHOUT_CLASSIFICATION	 todo define groups regex and use 
WITHOUT_CLASSIFICATION	 propagate 
WITHOUT_CLASSIFICATION	 usually controlled bucketing 
WITHOUT_CLASSIFICATION	 private constructor 
WITHOUT_CLASSIFICATION	 the interface adds the single long key hash multiset contains method 
WITHOUT_CLASSIFICATION	 write the plan out 
WITHOUT_CLASSIFICATION	 test for char type 
WITHOUT_CLASSIFICATION	 request and validate the request cookies 
WITHOUT_CLASSIFICATION	 test that changing column data type fails 
WITHOUT_CLASSIFICATION	 references external fields for async splitinfo generation 
WITHOUT_CLASSIFICATION	 get select expression list 
WITHOUT_CLASSIFICATION	 case when this expression 
WITHOUT_CLASSIFICATION	 outputrel the generated augmented select with extra unselected 
WITHOUT_CLASSIFICATION	 initialize event processor and dispatcher 
WITHOUT_CLASSIFICATION	 get the constant value associated with the current element the struct 
WITHOUT_CLASSIFICATION	 successful 
WITHOUT_CLASSIFICATION	 for hashmap 
WITHOUT_CLASSIFICATION	 some the columns stats are missing this implies partition schema has changed will merge columns present both overwrite stats for columns absent metastore and leave alone columns stats missing from stats task this last case may leave stats stale state this will addressed later 
WITHOUT_CLASSIFICATION	 clone all the operators between union and filescan and push them above the union remove the union the tree below union gets delinked after that 
WITHOUT_CLASSIFICATION	 the wait queue should able fit least waitqueue currentfreeexecutor slots 
WITHOUT_CLASSIFICATION	 point making acid table these other props are not set since will just throw exceptions when someone tries use the table 
WITHOUT_CLASSIFICATION	 mgby follow mgby here start countkey 
WITHOUT_CLASSIFICATION	 not registered for this node register and send state successful 
WITHOUT_CLASSIFICATION	 give more 
WITHOUT_CLASSIFICATION	 create partition 
WITHOUT_CLASSIFICATION	 collect the newly added partitions will report the dynamically added partitions txnhandler 
WITHOUT_CLASSIFICATION	 based the statement generate the selectoperator 
WITHOUT_CLASSIFICATION	 found parent mapjoin operator its size should already reflect any other mapjoins connected 
WITHOUT_CLASSIFICATION	 total rows emit during the whole iteration excluding the rows emitted the separate thread 
WITHOUT_CLASSIFICATION	 deletedata 
WITHOUT_CLASSIFICATION	 this test assumes the hivecontrib jar has been built part the hive build 
WITHOUT_CLASSIFICATION	 collect table access keys information for operators that can benefit from bucketing 
WITHOUT_CLASSIFICATION	 verify 
WITHOUT_CLASSIFICATION	 the predicate matches then return true otherwise visit the next set nodes that havent been seen 
WITHOUT_CLASSIFICATION	 require the use recursive input dirs for union processing 
WITHOUT_CLASSIFICATION	 should have severed the ties 
WITHOUT_CLASSIFICATION	 multiply divide digit commad scale down fraction digits negative exponent number zeros after dot down shift 
WITHOUT_CLASSIFICATION	 log and ignore 
WITHOUT_CLASSIFICATION	 verify that the number events since began least more 
WITHOUT_CLASSIFICATION	 the master thread 
WITHOUT_CLASSIFICATION	 note the current implementation does not allow importing external location this intentional since want the destination tables managed tables this assumption should change some point the future will need some its checks changed allow for replacing external tables 
WITHOUT_CLASSIFICATION	 dont worry about setting raw data size diff have idea how calculate that without finding the row are updating deleting which would mess 
WITHOUT_CLASSIFICATION	 change the newly created mapred plan was join operator 
WITHOUT_CLASSIFICATION	 check physical path 
WITHOUT_CLASSIFICATION	 partition columns virtual columns 
WITHOUT_CLASSIFICATION	 select all with the first expression and expect the other children not invoked 
WITHOUT_CLASSIFICATION	 read the record with the same record reader 
WITHOUT_CLASSIFICATION	 the group expression anything other than list columns 
WITHOUT_CLASSIFICATION	 another thread adds entry before the check this one 
WITHOUT_CLASSIFICATION	 this required for writing null key for file based tables 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 all checks short names 
WITHOUT_CLASSIFICATION	 update sum length with the new length 
WITHOUT_CLASSIFICATION	 this replication spec then replacemode semantics might apply were already asking for table replacement then can skip this check however otherwise replication scope and weve not been explicitly asked replace should check the object were looking exists and trigger replacemode semantics 
WITHOUT_CLASSIFICATION	 note the reason use string name the hive hbase handler here because not want introduce compiledependency the hivehbasehandler module from within hivehcatalog this parameter was added due the requirement hive 
WITHOUT_CLASSIFICATION	 original path and not available well 
WITHOUT_CLASSIFICATION	 the original mapredtask and this new generated mapredlocaltask 
WITHOUT_CLASSIFICATION	 check the ndvrows from the sel the destination tablescan the semijoin opt going 
WITHOUT_CLASSIFICATION	 positive numbers flip just the first bit 
WITHOUT_CLASSIFICATION	 build error message 
WITHOUT_CLASSIFICATION	 input 
WITHOUT_CLASSIFICATION	 schedule tasks give out two ducks two higher pri tasks get them give out more the last task gets and one duck unused give out more goes unused then revoke similarly steps with the opposite effect 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 drop table will clean the table entry from the compaction queue and hence worker have effect 
WITHOUT_CLASSIFICATION	 and finally hook any events that need sent the tez 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 merge 
WITHOUT_CLASSIFICATION	 tracks appmasters which heartbeats are being sent this should not used for any other messages like taskkilled etc 
WITHOUT_CLASSIFICATION	 the jars libjars will localized cwd the launcher task then libjars will 
WITHOUT_CLASSIFICATION	 locks newrequestlist locks completednodes 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 this insert update delete acid table then mark that the 
WITHOUT_CLASSIFICATION	 since there only have locks for current query any 
WITHOUT_CLASSIFICATION	 set the mapjoin hint needs disabled 
WITHOUT_CLASSIFICATION	 ignore not required this will never 
WITHOUT_CLASSIFICATION	 user provided configs 
WITHOUT_CLASSIFICATION	 this test 
WITHOUT_CLASSIFICATION	 ignore closing quote 
WITHOUT_CLASSIFICATION	 create the equality condition 
WITHOUT_CLASSIFICATION	 add spark job metrics metrics collected spark itself jvmgctime 
WITHOUT_CLASSIFICATION	 boring scenario two concurrent revokes same above 
WITHOUT_CLASSIFICATION	 deletedelta and deletedelta which are created result compacting 
WITHOUT_CLASSIFICATION	 todo now assume the key object supports hashcode and equals functions 
WITHOUT_CLASSIFICATION	 for information only 
WITHOUT_CLASSIFICATION	 verify the accumulated logs 
WITHOUT_CLASSIFICATION	 get completed attempts 
WITHOUT_CLASSIFICATION	 update credential provider location the password the credential provider already set the sparkconf 
WITHOUT_CLASSIFICATION	 count the number tasks and kill application goes beyond the limit 
WITHOUT_CLASSIFICATION	 create the context for walking operators 
WITHOUT_CLASSIFICATION	 virtualcolumncount 
WITHOUT_CLASSIFICATION	 dont break old callers that are trying reuse storages 
WITHOUT_CLASSIFICATION	 create rows file copy 
WITHOUT_CLASSIFICATION	 myenum 
WITHOUT_CLASSIFICATION	 there are nulls then the iteration the same all cases 
WITHOUT_CLASSIFICATION	 add embedded rawstore can cleanup later avoid memory leak 
WITHOUT_CLASSIFICATION	 hive introduces update event which will capture changes allocation after get 
WITHOUT_CLASSIFICATION	 any 
WITHOUT_CLASSIFICATION	 add new table via objectstore 
WITHOUT_CLASSIFICATION	 try next available server zookeeper retry all the servers again retry enabled 
WITHOUT_CLASSIFICATION	 double 
WITHOUT_CLASSIFICATION	 drop foreign key 
WITHOUT_CLASSIFICATION	 set partition sorted partition bucket sorted 
WITHOUT_CLASSIFICATION	 verbose logs should contain everything including execution and performance 
WITHOUT_CLASSIFICATION	 dont accidentally cache the value shouldnt 
WITHOUT_CLASSIFICATION	 check most significant part first 
WITHOUT_CLASSIFICATION	 update value 
WITHOUT_CLASSIFICATION	 number elements average elements average elements times the variance elements times the variance elements times the covariance 
WITHOUT_CLASSIFICATION	 internal input format used 
WITHOUT_CLASSIFICATION	 test select powrootcolb rootcol from table testroot 
WITHOUT_CLASSIFICATION	 the code point from from string already has replacement deleted dont need anything just move the next code point 
WITHOUT_CLASSIFICATION	 return true 
WITHOUT_CLASSIFICATION	 boolean signal whether tagging will used join 
WITHOUT_CLASSIFICATION	 right repeats and null 
WITHOUT_CLASSIFICATION	 vrb was created from vrbctx already have preallocated column vectors return old cvs any caller assume these things all have the same schema 
WITHOUT_CLASSIFICATION	 handle the logical operators 
WITHOUT_CLASSIFICATION	 begin commit 
WITHOUT_CLASSIFICATION	 use get the original parents because 
WITHOUT_CLASSIFICATION	 there are some elements that were cached parallel take care them 
WITHOUT_CLASSIFICATION	 tablename 
WITHOUT_CLASSIFICATION	 which case stats need not updated 
WITHOUT_CLASSIFICATION	 not support task level progress nothing here 
WITHOUT_CLASSIFICATION	 hold output needed hold boolean output 
WITHOUT_CLASSIFICATION	 are here either unpartitioned table partitioned table with predicates 
WITHOUT_CLASSIFICATION	 the arg part another list 
WITHOUT_CLASSIFICATION	 test decimal column decimal scalar addition this used cover all the cases used the source code template 
WITHOUT_CLASSIFICATION	 required required required required required required optional optional 
WITHOUT_CLASSIFICATION	 storagedescriptor has empty list fields serde will report them 
WITHOUT_CLASSIFICATION	 update service registry configs caveat this has nothing with the actual settings read the needed use hiveconf dynamically switch between instances 
WITHOUT_CLASSIFICATION	 file dump should write session state consoles error stream 
WITHOUT_CLASSIFICATION	 need obtain intersection all the privileges 
WITHOUT_CLASSIFICATION	 min txn incremented linearly within transaction batch 
WITHOUT_CLASSIFICATION	 major version number should match for backward compatibility 
WITHOUT_CLASSIFICATION	 need check that datasource was not specified user 
WITHOUT_CLASSIFICATION	 add added files 
WITHOUT_CLASSIFICATION	 get calcite agg 
WITHOUT_CLASSIFICATION	 footersummary 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 always serialize the string type using the escaped algorithm for lazystring 
WITHOUT_CLASSIFICATION	 passthru constructors 
WITHOUT_CLASSIFICATION	 should not treated like needs data 
WITHOUT_CLASSIFICATION	 skip one 
WITHOUT_CLASSIFICATION	 the most preemptable task still too important for kill put back 
WITHOUT_CLASSIFICATION	 itself missing then throw error 
WITHOUT_CLASSIFICATION	 duplicate avoid modification 
WITHOUT_CLASSIFICATION	 from 
WITHOUT_CLASSIFICATION	 singlecolumn long check for repeating 
WITHOUT_CLASSIFICATION	 use global when key for reduce 
WITHOUT_CLASSIFICATION	 uses all decimal longs 
WITHOUT_CLASSIFICATION	 handle mapjoin specially and check for all its children 
WITHOUT_CLASSIFICATION	 setentryvalid already increments the reader count set usedcacheentry gets released 
WITHOUT_CLASSIFICATION	 partitioned table 
WITHOUT_CLASSIFICATION	 not present 
WITHOUT_CLASSIFICATION	 partition already exists and arent overwriting then respect its current location info rather than picking from the parent tabledesc 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 must class 
WITHOUT_CLASSIFICATION	 process partition pruning sinks 
WITHOUT_CLASSIFICATION	 get should fail now since ttl and weve snoozed for seconds 
WITHOUT_CLASSIFICATION	 move the original parent directory the intermediate original directory 
WITHOUT_CLASSIFICATION	 this initializes currentfileread 
WITHOUT_CLASSIFICATION	 read single value 
WITHOUT_CLASSIFICATION	 handle repeating 
WITHOUT_CLASSIFICATION	 test mod create test log file which will contain only logs which are supposed written the qtest output 
WITHOUT_CLASSIFICATION	 new vertex 
WITHOUT_CLASSIFICATION	 while threads are blocked should still able get and return session 
WITHOUT_CLASSIFICATION	 implies limit 
WITHOUT_CLASSIFICATION	 choosing the function since the one duplicated the dummy database 
WITHOUT_CLASSIFICATION	 used spark mode decide whether global order needed 
WITHOUT_CLASSIFICATION	 conf for nonllap 
WITHOUT_CLASSIFICATION	 location 
WITHOUT_CLASSIFICATION	 convert constant back rexnode 
WITHOUT_CLASSIFICATION	 acls for znodes nonkerberized cluster the world 
WITHOUT_CLASSIFICATION	 get all parents 
WITHOUT_CLASSIFICATION	 valid filesystem schemes 
WITHOUT_CLASSIFICATION	 now diff the lists 
WITHOUT_CLASSIFICATION	 this union type 
WITHOUT_CLASSIFICATION	 project projects the original expressions 
WITHOUT_CLASSIFICATION	 nonstrict mode and there predicates all get everything 
WITHOUT_CLASSIFICATION	 creates job request object and sets execution environment creates thread pool execute job requests param requesttype job request type param config name used extract number concurrent requests serviced param config name used extract maximum time task can execute request 
WITHOUT_CLASSIFICATION	 throw new not sort order and unique 
WITHOUT_CLASSIFICATION	 dont visit multiple times 
WITHOUT_CLASSIFICATION	 remove old table objects hash 
WITHOUT_CLASSIFICATION	 dont attempt scheduling for additional priorities 
WITHOUT_CLASSIFICATION	 verify entries added compactionqueue for each tablepartition 
WITHOUT_CLASSIFICATION	 specifically necessary for dpp because might have created lots and true and true conditions 
WITHOUT_CLASSIFICATION	 local session path 
WITHOUT_CLASSIFICATION	 the sort and bucket cols have match both sides for this 
WITHOUT_CLASSIFICATION	 with this root operator 
WITHOUT_CLASSIFICATION	 the table bucketed and bucketing enforced the following the number buckets smaller than the number maximum reducers create those many reducers not create multifilesink instead filesink the multifilesink will spray the data into multiple buckets that way can support very large number buckets without needing very large number reducers 
WITHOUT_CLASSIFICATION	 keysaddkey would need list stmtrs pairs for each key 
WITHOUT_CLASSIFICATION	 when the first buffer loaded resultsetnext should called times 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 groups the clause names into lists that any two clauses the same list has the same group and distinct keys and clause appears more than one list returns list the 
WITHOUT_CLASSIFICATION	 should having tree which looks like this join are the join operator now 
WITHOUT_CLASSIFICATION	 the time either success failed are called the task itself knows that has terminated and will ignore subsequent kill requests they out 
WITHOUT_CLASSIFICATION	 close the session which should free the txnhandlerlocks held the session done the finally block make sure free the locks otherwise the cleanup teardown will get stuck waiting the lock held here acidtbl 
WITHOUT_CLASSIFICATION	 spark property for now dont support changing spark app name the fly 
WITHOUT_CLASSIFICATION	 last batch successful remove from partsnotinfs 
WITHOUT_CLASSIFICATION	 update runtime 
WITHOUT_CLASSIFICATION	 nonjavadoc see int int 
WITHOUT_CLASSIFICATION	 one the branches definitely bucketleaf 
WITHOUT_CLASSIFICATION	 open till limit but not exceed 
WITHOUT_CLASSIFICATION	 find the biggest small table also calculate total data size all small tables 
WITHOUT_CLASSIFICATION	 initialize using and column projection list 
WITHOUT_CLASSIFICATION	 avoids creating tez client sessions internally takes much longer currently 
WITHOUT_CLASSIFICATION	 handle the rejection outside the lock 
WITHOUT_CLASSIFICATION	 what need way get buckets not splits 
WITHOUT_CLASSIFICATION	 where clause and pick the second disjunct from the operation 
WITHOUT_CLASSIFICATION	 object overhead bytes for intcompact bytes for precision bytes for scale size biginteger 
WITHOUT_CLASSIFICATION	 should initialize the value for createvalue 
WITHOUT_CLASSIFICATION	 more entries than oldinvalidids 
WITHOUT_CLASSIFICATION	 separator 
WITHOUT_CLASSIFICATION	 there will not any tez job above this task 
WITHOUT_CLASSIFICATION	 forward arg forward arg forward arg 
WITHOUT_CLASSIFICATION	 enable assertion 
WITHOUT_CLASSIFICATION	 big table candidates 
WITHOUT_CLASSIFICATION	 invoke the inputformat entrypoint 
WITHOUT_CLASSIFICATION	 not using expressions 
WITHOUT_CLASSIFICATION	 and transformation creates nodes andor thus triggered 
WITHOUT_CLASSIFICATION	 from the original sparkwork 
WITHOUT_CLASSIFICATION	 this not subquery predicate then join the null check subquery see for details why and how this constructed 
WITHOUT_CLASSIFICATION	 vector reduce key partition columns are repeated test element 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 theres memory available fail 
WITHOUT_CLASSIFICATION	 note calling last flush length below more for futureproofing when have streaming deletes but currently dont support streaming deletes and this can 
WITHOUT_CLASSIFICATION	 use the columnnames initialize the reusable row object and the columnbuffers reason this being done buffer full should reinitialize the 
WITHOUT_CLASSIFICATION	 indexstore trying tell something 
WITHOUT_CLASSIFICATION	 msd and should same objects not sure how make then same right now 
WITHOUT_CLASSIFICATION	 exprfield constant iff expr constant 
WITHOUT_CLASSIFICATION	 todo hive make use splitsizeestimator the actual task computation needs looked well 
WITHOUT_CLASSIFICATION	 failover didnt succeed log error and exit 
WITHOUT_CLASSIFICATION	 execute the setup queries 
WITHOUT_CLASSIFICATION	 try the output primitive object 
WITHOUT_CLASSIFICATION	 verify the buffer was reset real output doesnt happen because was mocked 
WITHOUT_CLASSIFICATION	 nonjavadoc see int int 
WITHOUT_CLASSIFICATION	 double between 
WITHOUT_CLASSIFICATION	 for keeping track the number elements read just for debugging 
WITHOUT_CLASSIFICATION	 special handling needed times for date timestamp string char and varchar they can named specifically argument types longcolumnvector intfamily date intervalfamily doublecolumnvector floatfamily decimalcolumnvector decimal bytescolumnvector string char varchar timestamp intervaldaytime 
WITHOUT_CLASSIFICATION	 created when the task executed dont care about the correct state here 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 addsplitsforgroup collects separate calls setinputpaths into one where possible the reason for this that this faster some inputformats orc will start threadpool the work and calling multiple times unnecessarily will create lot unnecessary thread pools 
WITHOUT_CLASSIFICATION	 check the table should skipped 
WITHOUT_CLASSIFICATION	 this leaf add exporttask follow 
WITHOUT_CLASSIFICATION	 use the same filesystem input file backuppath not explicitly specified 
WITHOUT_CLASSIFICATION	 initialization fails with retry resource plan change 
WITHOUT_CLASSIFICATION	 todo key and values are object which can eagerly deserialized lazily deserialized accurately estimate the entry size every possible objects key value should implement memoryestimate interface which very intrusive assuming default entry size here 
WITHOUT_CLASSIFICATION	 tez use different way transmitting the hash table basically use reducesinkoperators and set the transfer broadcast instead partitioned consequence use different serde than the mapjoin case 
WITHOUT_CLASSIFICATION	 begin pattern 
WITHOUT_CLASSIFICATION	 unique key the filterinputrel 
WITHOUT_CLASSIFICATION	 lookup the field corresponding the given field and return 
WITHOUT_CLASSIFICATION	 the dependencies should include depth and depth inferred 
WITHOUT_CLASSIFICATION	 locked someone move forceevict evicted this cachespecific removed from allocator structures the final state the memory was released memory manager new allocation before the first use cannot forceevict 
WITHOUT_CLASSIFICATION	 only used acid writer 
WITHOUT_CLASSIFICATION	 the current version jetty server doesnt have the status hence passing this constant 
WITHOUT_CLASSIFICATION	 milliseconds 
WITHOUT_CLASSIFICATION	 not allocated yet 
WITHOUT_CLASSIFICATION	 the following data members are only required support the deprecated constructor and builder 
WITHOUT_CLASSIFICATION	 for partial specifications need partitions follow the scheme 
WITHOUT_CLASSIFICATION	 create filter sqcountcheckcount instead project because relfieldtrimmer 
WITHOUT_CLASSIFICATION	 commented out because the name becomes innerfield default call course pig but the metadata itd anonymous this would autogenerated which fine 
WITHOUT_CLASSIFICATION	 case outer joins need push records through even one the sides done sending records for the case full outer join the right side needs send data for the join even after the left side has completed sending all the records its side this can done once initialize time and close these tags will still forward records until they have more send also subsequent joins need fetch their data well since any join following the outer join could produce results with one the outer sides depending the join condition could optimize for the case inner joins the future here 
WITHOUT_CLASSIFICATION	 byte 
WITHOUT_CLASSIFICATION	 required required optional required 
WITHOUT_CLASSIFICATION	 full table name format dbnametablename 
WITHOUT_CLASSIFICATION	 adding same property key twice should throw unique key constraint violation exception 
WITHOUT_CLASSIFICATION	 assume almost always performance win fill all isnull can safely reset nonulls 
WITHOUT_CLASSIFICATION	 subquery filter 
WITHOUT_CLASSIFICATION	 pool didnt exist wouldnt have returned 
WITHOUT_CLASSIFICATION	 and table the table not sorted 
WITHOUT_CLASSIFICATION	 ignore client only queries 
WITHOUT_CLASSIFICATION	 optimization the first child file have reached the leaf directory move the parent directory itself instead moving each file under the directory see hcatalog note for future append implementation this optimization another reason dynamic partitioning currently incompatible with append mutable tables 
WITHOUT_CLASSIFICATION	 for caching partition objects 
WITHOUT_CLASSIFICATION	 change serde lazysimpleserde columnsetserde 
WITHOUT_CLASSIFICATION	 add uncovered acid delta splits 
WITHOUT_CLASSIFICATION	 use the basic string bytes read get access then use our optimal truncatetrim method that does not use java string objects 
WITHOUT_CLASSIFICATION	 should set child class 
WITHOUT_CLASSIFICATION	 restore original state 
WITHOUT_CLASSIFICATION	 could not resolve all the function children fail 
WITHOUT_CLASSIFICATION	 everything try normal shutdown 
WITHOUT_CLASSIFICATION	 discard possibly type related sorting order and replace with alphabetical 
WITHOUT_CLASSIFICATION	 all calls fail 
WITHOUT_CLASSIFICATION	 validate the plan 
WITHOUT_CLASSIFICATION	 nullable remarks columndef sqldatatype sqldatetimesub charoctetlength ordinalposition isnullable scopecatalog scopeschema scopetable sourcedatatype isautoincrement 
WITHOUT_CLASSIFICATION	 handle the isnull array first tight loops 
WITHOUT_CLASSIFICATION	 verify all scopes are closed 
WITHOUT_CLASSIFICATION	 ignore dummy inputs 
WITHOUT_CLASSIFICATION	 test both are not configured 
WITHOUT_CLASSIFICATION	 add new column with cascade option 
WITHOUT_CLASSIFICATION	 create should not return resultset 
WITHOUT_CLASSIFICATION	 dfs stuff 
WITHOUT_CLASSIFICATION	 querytimeout means timeout 
WITHOUT_CLASSIFICATION	 generate full partition specification 
WITHOUT_CLASSIFICATION	 store some hash bits ref for every expansion need add one bit hash have enough bits well that dont well rehash loginfoexpanding the hashtable capacity capacity 
WITHOUT_CLASSIFICATION	 build rel for src subquery join 
WITHOUT_CLASSIFICATION	 expect that theres only one field schema 
WITHOUT_CLASSIFICATION	 lag the whole partition not the iterator range 
WITHOUT_CLASSIFICATION	 replace the buffer our big range list well current results 
WITHOUT_CLASSIFICATION	 categorize the partitions returned and confirm that all partitions are accounted for 
WITHOUT_CLASSIFICATION	 the manner which the values this column are deserialized fromto accumulo 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 allow decimals and will return truncated integer that case therefore wont throw exception here checking the fractional part happens below 
WITHOUT_CLASSIFICATION	 opening allowed after closing 
WITHOUT_CLASSIFICATION	 the encoding method simple replace all the special characters with the corresponding number ascii note that unicode not supported table names and have explicit checks for 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 set columns list for temp table 
WITHOUT_CLASSIFICATION	 try pulling directly from url 
WITHOUT_CLASSIFICATION	 diffaftersleep total sleeptime 
WITHOUT_CLASSIFICATION	 scale down left and compare 
WITHOUT_CLASSIFICATION	 after super analyze read defaultacidtblpart write defaultacidtblpart tableinsert after udsa read defaultacidtblpart write partitionupdate todo this causes read lock the whole table clearly overkill 
WITHOUT_CLASSIFICATION	 table partitioned user did not specify partition 
WITHOUT_CLASSIFICATION	 multiply scale normalize not use negative scale our representation example has negative scale since scale number digits below the dot normalized scale 
WITHOUT_CLASSIFICATION	 all field names are the form key value 
WITHOUT_CLASSIFICATION	 only evaluate veve cast constant recursive casting 
WITHOUT_CLASSIFICATION	 permissions for metric directory 
WITHOUT_CLASSIFICATION	 marshalunmarshal znode data 
WITHOUT_CLASSIFICATION	 replication destination will not external 
WITHOUT_CLASSIFICATION	 multikey hash set optimized for vector map join the key stored the provided bytes uninterpreted 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 find the number reducers such that divisor totalfiles 
WITHOUT_CLASSIFICATION	 node has free capacity but disabled node has capcaity delay reenable tiemout 
WITHOUT_CLASSIFICATION	 assume that for table theres only the base directory are good 
WITHOUT_CLASSIFICATION	 this not called consecutivechunk stuff parquet this were used might make sense make faster 
WITHOUT_CLASSIFICATION	 note that create the cluster name from user conf hence user can target cluster but then create the signer using hiveconf hence control the config and stuff 
WITHOUT_CLASSIFICATION	 validate 
WITHOUT_CLASSIFICATION	 omit nulls 
WITHOUT_CLASSIFICATION	 create encoded data reader 
WITHOUT_CLASSIFICATION	 check whether the part exists not 
WITHOUT_CLASSIFICATION	 interim row count can not less due containment assumption join cardinality computation 
WITHOUT_CLASSIFICATION	 make mockinstance here setting the instance name the same this mock instance 
WITHOUT_CLASSIFICATION	 inner join specific 
WITHOUT_CLASSIFICATION	 creates more files that partition 
WITHOUT_CLASSIFICATION	 match for entire batch 
WITHOUT_CLASSIFICATION	 note originally named this isempty but that name conflicted with another interface 
WITHOUT_CLASSIFICATION	 add all dependencies edges the graph 
WITHOUT_CLASSIFICATION	 todo expose nonprimitive structured object while maintaining jdbc compliance 
WITHOUT_CLASSIFICATION	 environment variables name 
WITHOUT_CLASSIFICATION	 calculate the expected timeout based the elapsed time between waiting start time and polling start time 
WITHOUT_CLASSIFICATION	 add column info corresponding virtual columns 
WITHOUT_CLASSIFICATION	 test that exclusive lock blocks read and write 
WITHOUT_CLASSIFICATION	 null tests 
WITHOUT_CLASSIFICATION	 create initialize and test 
WITHOUT_CLASSIFICATION	 this test will make sure that every entry has test here 
WITHOUT_CLASSIFICATION	 remove the previous renewable jars 
WITHOUT_CLASSIFICATION	 the tpcds scale set this way the optimizer will generate plans for set 
WITHOUT_CLASSIFICATION	 hadoop this timeout spec behaves strnage manner means with retry however does this but does thrice essentially retries the number times the entire config 
WITHOUT_CLASSIFICATION	 test inputformat with column prune 
WITHOUT_CLASSIFICATION	 stub outputformat actions 
WITHOUT_CLASSIFICATION	 for now only alter name owner parameters cols bucketcols are allowed 
WITHOUT_CLASSIFICATION	 fetch the counters 
WITHOUT_CLASSIFICATION	 dont know the acceptable size for java array well use boundary 
WITHOUT_CLASSIFICATION	 check the file format the file matches that the partition 
WITHOUT_CLASSIFICATION	 ensure find the single row which matches our timestamp where field has value 
WITHOUT_CLASSIFICATION	 least partition does not contain row count then mark basic stats state partial 
WITHOUT_CLASSIFICATION	 read should get rows 
WITHOUT_CLASSIFICATION	 this code that creates the result for the granularity functions has been brought from druid 
WITHOUT_CLASSIFICATION	 last row last batch determines isgroupresultnull and double lastvalue 
WITHOUT_CLASSIFICATION	 partition level statistics requested add predicate and group needed rewritten query 
WITHOUT_CLASSIFICATION	 insert overwrite table from source table 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 should send message undo what just did 
WITHOUT_CLASSIFICATION	 not clean the writers the callback should 
WITHOUT_CLASSIFICATION	 only inline followed array supported cbo 
WITHOUT_CLASSIFICATION	 only store longs our longcolumnvector 
WITHOUT_CLASSIFICATION	 captures how input should partitioned this captured list astnodes that are the expressions the distributecluster clause specifying the partitioning applied for ptf invocation 
WITHOUT_CLASSIFICATION	 possible that nullscan can fire skip this rule 
WITHOUT_CLASSIFICATION	 localize llap client jars 
WITHOUT_CLASSIFICATION	 the next byte should the marker 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 nulls not repeating 
WITHOUT_CLASSIFICATION	 partitions dropped 
WITHOUT_CLASSIFICATION	 all tables are cached this not possible future can support this randomly 
WITHOUT_CLASSIFICATION	 help 
WITHOUT_CLASSIFICATION	 todo use hbase fixed 
WITHOUT_CLASSIFICATION	 reconnect was successful 
WITHOUT_CLASSIFICATION	 cut the operator tree not retain connections from the parent downstream 
WITHOUT_CLASSIFICATION	 the second one should combined into the first 
WITHOUT_CLASSIFICATION	 fill high long from lower long 
WITHOUT_CLASSIFICATION	 param tuple return null throws ioexception see 
WITHOUT_CLASSIFICATION	 start here with least one byte 
WITHOUT_CLASSIFICATION	 throw special exception since its usually wellknown misconfiguration 
WITHOUT_CLASSIFICATION	 char text value already stripped trailing space 
WITHOUT_CLASSIFICATION	 constants for bit variant 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 set parameters the ngram estimator object 
WITHOUT_CLASSIFICATION	 compare long value with 
WITHOUT_CLASSIFICATION	 temp tables exempted from checks 
WITHOUT_CLASSIFICATION	 the optimization has been stopped for the reasons like being not qualified lack the stats data not continue this process for example for query select maxvalue from src union all select maxvalue from src has been union remove optimized the ast tree will become tsselgbyrsgbyfs tsselgbyrsgbyfs branch for src not optimized because src does not have column stats 
WITHOUT_CLASSIFICATION	 key value are already read 
WITHOUT_CLASSIFICATION	 clear most members 
WITHOUT_CLASSIFICATION	 all the parent sparktasks that this new task depend they dont already exists 
WITHOUT_CLASSIFICATION	 hhelp 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 delete output file exit 
WITHOUT_CLASSIFICATION	 copy the current object contents into the output only copy selected entries indicated selectedinuse and the sel array 
WITHOUT_CLASSIFICATION	 use different separator values 
WITHOUT_CLASSIFICATION	 but preserve table name sql 
WITHOUT_CLASSIFICATION	 already verified that the join can converted bucket map join 
WITHOUT_CLASSIFICATION	 clear rounding portion lower longword and add right scale roundmultiplyfactor 
WITHOUT_CLASSIFICATION	 disabled service discovery enabled return the already populated params mode params already populated with active server host info 
WITHOUT_CLASSIFICATION	 nonfinalcandidates predicates should empty 
WITHOUT_CLASSIFICATION	 default version 
WITHOUT_CLASSIFICATION	 version schema for this version hive 
WITHOUT_CLASSIFICATION	 create hiveconf once since this expensive 
WITHOUT_CLASSIFICATION	 already called doas need doas here 
WITHOUT_CLASSIFICATION	 there should call create partitions with batch sizes 
WITHOUT_CLASSIFICATION	 have just ensured the item not the list have definite state now 
WITHOUT_CLASSIFICATION	 test class write series values the designated output stream 
WITHOUT_CLASSIFICATION	 move the data files this newly created partition temp location 
WITHOUT_CLASSIFICATION	 note were not creating copy the list for saving memory 
WITHOUT_CLASSIFICATION	 there are original format files 
WITHOUT_CLASSIFICATION	 returns whether not two lists contain the same elements independent order 
WITHOUT_CLASSIFICATION	 hook default 
WITHOUT_CLASSIFICATION	 sort pushed bail out 
WITHOUT_CLASSIFICATION	 todo return 
WITHOUT_CLASSIFICATION	 check that the defaults did not remain 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 any operator the stack does not support autoconversion this join should not converted 
WITHOUT_CLASSIFICATION	 this min number reducer for deduped avoid query executed too small number reducers for example queries groupbyorderby can executed 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 room for optimization since cannot create empty project operator 
WITHOUT_CLASSIFICATION	 need support field names like key value between mapreduce boundary 
WITHOUT_CLASSIFICATION	 run distcp source filedir too big 
WITHOUT_CLASSIFICATION	 table already transactional migration needed 
WITHOUT_CLASSIFICATION	 string char varchar 
WITHOUT_CLASSIFICATION	 configured limit for reducers 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 have space the cache run cleaner thread 
WITHOUT_CLASSIFICATION	 groupingc groupingc groupingc 
WITHOUT_CLASSIFICATION	 write stmt 
WITHOUT_CLASSIFICATION	 check the input operator with struct children 
WITHOUT_CLASSIFICATION	 collect all dpp sinks 
WITHOUT_CLASSIFICATION	 use common binary decimal conversion method share with 
WITHOUT_CLASSIFICATION	 aggregate functions 
WITHOUT_CLASSIFICATION	 transform have created new filter operator 
WITHOUT_CLASSIFICATION	 reached here then were successful finding alternate internal column mapping and were about proceed 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 count 
WITHOUT_CLASSIFICATION	 used external cache used local cache 
WITHOUT_CLASSIFICATION	 return the row only its not corrupted 
WITHOUT_CLASSIFICATION	 todo embedded metastore changes the table object when clientcreatetable called table storage descriptor location should null 
WITHOUT_CLASSIFICATION	 violation queue 
WITHOUT_CLASSIFICATION	 add the support the 
WITHOUT_CLASSIFICATION	 bloom filter any input and output bytes just modes partial complete 
WITHOUT_CLASSIFICATION	 ref 
WITHOUT_CLASSIFICATION	 allows mocking testing 
WITHOUT_CLASSIFICATION	 set all child tasks 
WITHOUT_CLASSIFICATION	 alias cte contains the table name not the translation because 
WITHOUT_CLASSIFICATION	 find all the agg expressions use linkedhashset ensure determinism 
WITHOUT_CLASSIFICATION	 spaces the end the line 
WITHOUT_CLASSIFICATION	 this node will likely activated after the task timeout expires 
WITHOUT_CLASSIFICATION	 concern the way this mapping goes the order needs preserved for and ptngetvalues 
WITHOUT_CLASSIFICATION	 releasing the locks 
WITHOUT_CLASSIFICATION	 must reset the isnull could set from prev batch use 
WITHOUT_CLASSIFICATION	 aggregation columns hive 
WITHOUT_CLASSIFICATION	 these are sessionstate objects that are copied over work allow for parallel execution based the current use case the methods are selectively synchronized which might need taken care when using other methods 
WITHOUT_CLASSIFICATION	 set the size the struct 
WITHOUT_CLASSIFICATION	 delete sample jars 
WITHOUT_CLASSIFICATION	 code sections initialize fastsetfrom take integer fractional portion binary decimal conversion decimal binary conversionr emulate serializationutils deserialization used orc emulate serializationutils serialization used orc emulate biginteger deserialization used lazybinary and others emulate biginteger serialization used lazybinary and others decimal integer conversion decimal noninteger conversion decimal comparison decimal rounding decimal scale updown decimal precision trailing zeroes decimal addition subtraction decimal multiply decimal division remainder decimal string formatting decimal validation decimal debugging 
WITHOUT_CLASSIFICATION	 specified generate alias using func name 
WITHOUT_CLASSIFICATION	 group mapping groupa user user 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 call with new input inspector 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create map 
WITHOUT_CLASSIFICATION	 verify that there are two calls because two instances the authorization provider 
WITHOUT_CLASSIFICATION	 testing using good enough because use create objectinspectors 
WITHOUT_CLASSIFICATION	 filter condition null transform false 
WITHOUT_CLASSIFICATION	 removing job credential entry cannot set the tasks 
WITHOUT_CLASSIFICATION	 for partial and complete 
WITHOUT_CLASSIFICATION	 return the new list 
WITHOUT_CLASSIFICATION	 allow the user set the orc properties without getting error 
WITHOUT_CLASSIFICATION	 high word gets integer rounding 
WITHOUT_CLASSIFICATION	 some changes optional 
WITHOUT_CLASSIFICATION	 use the hive table name ignoring the default database 
WITHOUT_CLASSIFICATION	 returns the bucket number which the record belongs 
WITHOUT_CLASSIFICATION	 the interface for single byte array key hash map lookup method 
WITHOUT_CLASSIFICATION	 update catalogs 
WITHOUT_CLASSIFICATION	 test that exclusive blocks exclusive and read 
WITHOUT_CLASSIFICATION	 add tokwindow child udaf 
WITHOUT_CLASSIFICATION	 get scheme from filesystem 
WITHOUT_CLASSIFICATION	 assert cvbcolslength must constant per split 
WITHOUT_CLASSIFICATION	 this should use 
WITHOUT_CLASSIFICATION	 process the records the input iterator until new output records are available for serving downstream operator input records are exhausted 
WITHOUT_CLASSIFICATION	 return the new expression containing only partition columns 
WITHOUT_CLASSIFICATION	 helper method create yarn local resource 
WITHOUT_CLASSIFICATION	 field present both validate type has not changed 
WITHOUT_CLASSIFICATION	 get one top level directly from the stack 
WITHOUT_CLASSIFICATION	 iterate through the children nodes the clauses starting from index which corresponds the right hand side the list 
WITHOUT_CLASSIFICATION	 table being modified external need make sure existing table doesnt have enabled constraint since constraints are disallowed with such tables 
WITHOUT_CLASSIFICATION	 requested host died unknown host requested fallback random selection 
WITHOUT_CLASSIFICATION	 old logic 
WITHOUT_CLASSIFICATION	 initialize and evaluate 
WITHOUT_CLASSIFICATION	 required required required optional optional optional 
WITHOUT_CLASSIFICATION	 nullsafe issame for lists exprnodedesc 
WITHOUT_CLASSIFICATION	 choose keep the invalid stats and only change the setting 
WITHOUT_CLASSIFICATION	 otherwise convert rawtype will fall into the following 
WITHOUT_CLASSIFICATION	 datetimestamp higher precedence than stringgroup 
WITHOUT_CLASSIFICATION	 partitionid 
WITHOUT_CLASSIFICATION	 make the offset nonzero keep things interesting 
WITHOUT_CLASSIFICATION	 was committed all others open 
WITHOUT_CLASSIFICATION	 negative number 
WITHOUT_CLASSIFICATION	 check for empty partitions 
WITHOUT_CLASSIFICATION	 update only the basic statistics the absence column statistics 
WITHOUT_CLASSIFICATION	 prepare output buffer accept results 
WITHOUT_CLASSIFICATION	 the conf using the connection hook 
WITHOUT_CLASSIFICATION	 restriction subquery cannot use the same table alias one used the outer query 
WITHOUT_CLASSIFICATION	 the lock may have been released ignore and continue 
WITHOUT_CLASSIFICATION	 little strange that forget the dummy row read 
WITHOUT_CLASSIFICATION	 ask default first 
WITHOUT_CLASSIFICATION	 now start concurrent txn 
WITHOUT_CLASSIFICATION	 first look the classpath 
WITHOUT_CLASSIFICATION	 metastore schema only allows maximum for constraint name column 
WITHOUT_CLASSIFICATION	 process records until done 
WITHOUT_CLASSIFICATION	 weve prewarmed this database continue with the next one 
WITHOUT_CLASSIFICATION	 this method will scale down and round fit necessary 
WITHOUT_CLASSIFICATION	 will use decimal all else fails 
WITHOUT_CLASSIFICATION	 maps from work the dpps contains 
WITHOUT_CLASSIFICATION	 create the join operator with its descriptor 
WITHOUT_CLASSIFICATION	 execute child jvm 
WITHOUT_CLASSIFICATION	 invoked during from ast tree processing encountering ptf invocation tree form tokptblfunction name partitioningspec arguments setup ptfinvocationspec for this top level ptf invocation 
WITHOUT_CLASSIFICATION	 add the attemptdir the watch set scan and add the list found files 
WITHOUT_CLASSIFICATION	 now copy missing chunks and parts chunks into cache buffers 
WITHOUT_CLASSIFICATION	 the partitions were not added due memory limit return false 
WITHOUT_CLASSIFICATION	 the input sorted and are executing search based the arguments this filter 
WITHOUT_CLASSIFICATION	 joined with multiple small tables different keys 
WITHOUT_CLASSIFICATION	 add tez counters for task execution and llap 
WITHOUT_CLASSIFICATION	 are revoking from updating task 
WITHOUT_CLASSIFICATION	 project everything from the lhs and then those from the original 
WITHOUT_CLASSIFICATION	 test without nulls 
WITHOUT_CLASSIFICATION	 empty string delim 
WITHOUT_CLASSIFICATION	 getfunction 
WITHOUT_CLASSIFICATION	 reserve blocks this arena that would empty the sections requisite size 
WITHOUT_CLASSIFICATION	 looks like doesnt exist lock that two threads dont create once 
WITHOUT_CLASSIFICATION	 materialized view based rewriting disable for ctas and creation queries trying avoid any problem 
WITHOUT_CLASSIFICATION	 expect well only see notacquired here 
WITHOUT_CLASSIFICATION	 root interface for vector map join hash table which could hash map hash multiset hash set 
WITHOUT_CLASSIFICATION	 empty hschema construct 
WITHOUT_CLASSIFICATION	 through all small tables and get the mapping from bucket file name 
WITHOUT_CLASSIFICATION	 room above for rounding 
WITHOUT_CLASSIFICATION	 get explain plan for the query 
WITHOUT_CLASSIFICATION	 modify table schema the source 
WITHOUT_CLASSIFICATION	 the valuelist will save all data for listcolumnvector temporary 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 supposed get rows maxrows isnt set 
WITHOUT_CLASSIFICATION	 report the row its the first time 
WITHOUT_CLASSIFICATION	 select one child none child and none 
WITHOUT_CLASSIFICATION	 for nonlist single value the offset for the variable length long vlong holding the value length followed the key length 
WITHOUT_CLASSIFICATION	 slice before the start the split 
WITHOUT_CLASSIFICATION	 case overflow return 
WITHOUT_CLASSIFICATION	 both schema information are provided they should the same 
WITHOUT_CLASSIFICATION	 narrow down the possible choices based type affinity 
WITHOUT_CLASSIFICATION	 break loop equal comparator 
WITHOUT_CLASSIFICATION	 sgetlength and will never resize the buffer down 
WITHOUT_CLASSIFICATION	 todo this method ever called more than one jar getting the dir and the 
WITHOUT_CLASSIFICATION	 joinoperator assumes the key backed list consistent the value array also converted 
WITHOUT_CLASSIFICATION	 test with just high water mark 
WITHOUT_CLASSIFICATION	 already registered send updates this node for the specific source nothing for now unless tracking tasks later point 
WITHOUT_CLASSIFICATION	 without the round this conversion fails 
WITHOUT_CLASSIFICATION	 partial aggregation result returned terminatepartial partial result struct containing long field named count 
WITHOUT_CLASSIFICATION	 since now have scalar subqueries can get subquery expression having dont want include aggregate from within subquery 
WITHOUT_CLASSIFICATION	 allow analyze the whole table and dynamic partitions 
WITHOUT_CLASSIFICATION	 this negative because want the positive the default when nothing specified 
WITHOUT_CLASSIFICATION	 should also possible calculate this based tsgettime only 
WITHOUT_CLASSIFICATION	 summary for test result 
WITHOUT_CLASSIFICATION	 base time 
WITHOUT_CLASSIFICATION	 union 
WITHOUT_CLASSIFICATION	 repeat the expression the same batch the result must unchanged 
WITHOUT_CLASSIFICATION	 args child func 
WITHOUT_CLASSIFICATION	 need constant one side 
WITHOUT_CLASSIFICATION	 return either the arguments null 
WITHOUT_CLASSIFICATION	 because hive doesnt support null type appropriately typed boolean 
WITHOUT_CLASSIFICATION	 called generate the taks tree from the parse contextoperator tree 
WITHOUT_CLASSIFICATION	 verify resulting dirs 
WITHOUT_CLASSIFICATION	 create walker which walks the tree dfs manner while maintaining the operator stack 
WITHOUT_CLASSIFICATION	 without any caching 
WITHOUT_CLASSIFICATION	 instead fall back default behavior for determining input records 
WITHOUT_CLASSIFICATION	 very small heap elements 
WITHOUT_CLASSIFICATION	 only tasks that cannot finish immediately are preemptable other words all inputs 
WITHOUT_CLASSIFICATION	 the serde 
WITHOUT_CLASSIFICATION	 this pretty lame qtestutilqtestutil uses hivesiteurl load specific hivesitexml from dataconfsubdir this makes follow the same logic otherwise hiveconf and metastoreconf may load different hivesitexml for example hiveconf uses and metastoreconf dataconfhivesitexml 
WITHOUT_CLASSIFICATION	 make current task depends this new generated localmapjointask 
WITHOUT_CLASSIFICATION	 case the statement create materialized view 
WITHOUT_CLASSIFICATION	 interim row count can not less due containment assumption join cardinality computation interimnumrows represent number matches for join keys two sides represent number nonmatches 
WITHOUT_CLASSIFICATION	 for count 
WITHOUT_CLASSIFICATION	 result principal 
WITHOUT_CLASSIFICATION	 case all files locations not exist 
WITHOUT_CLASSIFICATION	 may the table getting created this load 
WITHOUT_CLASSIFICATION	 seconds for first retry assuming object was closed and open will fix 
WITHOUT_CLASSIFICATION	 read can unlock initial refcounts for the buffers that end before 
WITHOUT_CLASSIFICATION	 test for varchar type 
WITHOUT_CLASSIFICATION	 single value 
WITHOUT_CLASSIFICATION	 serializes decimal the maximum bit precision decimal digits note major assumption the fast decimal has already been bounds checked and least has precision not bounds check here for better performance 
WITHOUT_CLASSIFICATION	 for persistent function 
WITHOUT_CLASSIFICATION	 last chance look the old hive config value still avoiding defaults 
WITHOUT_CLASSIFICATION	 rename the event directories such way that the length varies will encounter createtable truncate followed insert for the insert set the event longer such that old comparator picks insert before truncate event ids createtable truncate insert changed createtable truncate insert but truncate have then having insert wont sufficient test the scenario set any event comes after createtable starts with event ids createtable truncate insert changed 
WITHOUT_CLASSIFICATION	 set the destination for the select query inside the ctas 
WITHOUT_CLASSIFICATION	 cannot use base file its range contains open write param writeid from basexxxx 
WITHOUT_CLASSIFICATION	 includes use the standard batch 
WITHOUT_CLASSIFICATION	 get vertex status can use but will expensive get status from for every refresh the lets infer the state from task counts 
WITHOUT_CLASSIFICATION	 select distinct windowing gby handled 
WITHOUT_CLASSIFICATION	 secondly extract information about the part the tree that can merged well some structural information memory consumption that needs 
WITHOUT_CLASSIFICATION	 this will also handle copyn files any 
WITHOUT_CLASSIFICATION	 just negate get the size 
WITHOUT_CLASSIFICATION	 both commit rollback clear all locks for this 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 only for charvarchar return types 
WITHOUT_CLASSIFICATION	 open txn 
WITHOUT_CLASSIFICATION	 principaltype 
WITHOUT_CLASSIFICATION	 handle the close 
WITHOUT_CLASSIFICATION	 else skip this one 
WITHOUT_CLASSIFICATION	 invalidate 
WITHOUT_CLASSIFICATION	 perform delete 
WITHOUT_CLASSIFICATION	 retrieve log from task tracker 
WITHOUT_CLASSIFICATION	 wait for the events processed 
WITHOUT_CLASSIFICATION	 not the first put blank separator 
WITHOUT_CLASSIFICATION	 get the result 
WITHOUT_CLASSIFICATION	 disable auto parallelism for bucket map joins 
WITHOUT_CLASSIFICATION	 trigger transformation 
WITHOUT_CLASSIFICATION	 first request for host 
WITHOUT_CLASSIFICATION	 round using the halfeven method used hive 
WITHOUT_CLASSIFICATION	 unsupported null 
WITHOUT_CLASSIFICATION	 should generate finf 
WITHOUT_CLASSIFICATION	 only need update the work with the hashtable sink operator with the same mapjoin desc can tell that comparing the bucket file name mapping map instance they should exactly the same one due the way how the bucket mapjoin context constructed 
WITHOUT_CLASSIFICATION	 tasks should have been started yet checked initial state 
WITHOUT_CLASSIFICATION	 test set random adds high precision 
WITHOUT_CLASSIFICATION	 writing both acid and nonacid resources the same txn txnid 
WITHOUT_CLASSIFICATION	 request came from old version the client this matches old behavior 
WITHOUT_CLASSIFICATION	 partition value not there then dynamic partition key 
WITHOUT_CLASSIFICATION	 utility visit all nodes ast tree 
WITHOUT_CLASSIFICATION	 getfunctionsstring catalog string schemapattern string functionnamepattern getschemas gettablesstring catalog string schemapattern string tablenamepattern string types gettabletypes gettypeinfo 
WITHOUT_CLASSIFICATION	 conf for llap 
WITHOUT_CLASSIFICATION	 get detailed read position information help diagnose exceptions 
WITHOUT_CLASSIFICATION	 query for minimum values all the queries and they can only increase any concurrent 
WITHOUT_CLASSIFICATION	 dummy impl 
WITHOUT_CLASSIFICATION	 may not idempotent but safe retry 
WITHOUT_CLASSIFICATION	 trailing spaces are not significant 
WITHOUT_CLASSIFICATION	 results cache directory should cleaned process termination 
WITHOUT_CLASSIFICATION	 sorting the list the descending order that deletes happen backtofront 
WITHOUT_CLASSIFICATION	 create new columnstatistics desc represent partition level column stats 
WITHOUT_CLASSIFICATION	 validate there the new insertions for column 
WITHOUT_CLASSIFICATION	 were combining sses and the time has expired 
WITHOUT_CLASSIFICATION	 compute value and hashcode wed either store forward them 
WITHOUT_CLASSIFICATION	 add udtf aliases 
WITHOUT_CLASSIFICATION	 find out all equivalent works the set 
WITHOUT_CLASSIFICATION	 parentschemaname 
WITHOUT_CLASSIFICATION	 handle case with nulls dont function the value null save time because calling the function can expensive 
WITHOUT_CLASSIFICATION	 translate projection indexes join schema adding offset 
WITHOUT_CLASSIFICATION	 evaluate the result given partition 
WITHOUT_CLASSIFICATION	 denominator zero convert the batch nulls 
WITHOUT_CLASSIFICATION	 disabled hive 
WITHOUT_CLASSIFICATION	 add back the queue for the next heartbeat and schedule the actual heartbeat 
WITHOUT_CLASSIFICATION	 check materialization defined its own invalidation time window 
WITHOUT_CLASSIFICATION	 this ensures that show locks prints the locks the same order they are examined 
WITHOUT_CLASSIFICATION	 test basic right trim bytes slice 
WITHOUT_CLASSIFICATION	 reach beginning the row group this required for ispresent stream 
WITHOUT_CLASSIFICATION	 also check hashcode 
WITHOUT_CLASSIFICATION	 default allow only addprops and dropprops alteroptype null case stats update 
WITHOUT_CLASSIFICATION	 already initialized 
WITHOUT_CLASSIFICATION	 semijoin created using hint marked useful skip 
WITHOUT_CLASSIFICATION	 doing string comps here value objects hive pig are different equals doesnt work 
WITHOUT_CLASSIFICATION	 cannot called during mapreduce tasks cache necessary values during query compilation and rely plan serialization bring this info the object during the mapreduce tasks 
WITHOUT_CLASSIFICATION	 read the stopping point for the first flush and make sure only see 
WITHOUT_CLASSIFICATION	 add the list work decompress 
WITHOUT_CLASSIFICATION	 the next item will new root 
WITHOUT_CLASSIFICATION	 optional string classname 
WITHOUT_CLASSIFICATION	 this contains basexxx deltaxxxyyy 
WITHOUT_CLASSIFICATION	 use the original fsop path here case while the new fsop merges files inside the directory the original movetask still commits based the parent note that this path can only triggered for merge thats part insert for now tables not support 
WITHOUT_CLASSIFICATION	 nonpartitioned 
WITHOUT_CLASSIFICATION	 environmentcontext 
WITHOUT_CLASSIFICATION	 initialize based table properties and they are not available see can find the job configuration have look these two places instead just the conf because streaming ingest uses table properties while normal hive sql will place this 
WITHOUT_CLASSIFICATION	 complete fractional digits shear off zero result 
WITHOUT_CLASSIFICATION	 ndv the join can not exceed the cardinality cross join 
WITHOUT_CLASSIFICATION	 this test done 
WITHOUT_CLASSIFICATION	 select none child one child and none 
WITHOUT_CLASSIFICATION	 multikey specific variables 
WITHOUT_CLASSIFICATION	 change the parent the original smbjoin operator point the map 
WITHOUT_CLASSIFICATION	 need check the other input branches for union following the first branch may need cast the data types for specific columns 
WITHOUT_CLASSIFICATION	 specifying explicitly will override the values from the url make sure dont override the values present the url with empty values 
WITHOUT_CLASSIFICATION	 add part spec range spec child tokwindow 
WITHOUT_CLASSIFICATION	 this test checks that have minor compacted delta for the txn range then will make any delete delta that range obsolete 
WITHOUT_CLASSIFICATION	 note rarely called unless buffers are very large evict lot lfu case 
WITHOUT_CLASSIFICATION	 check that the path between crs and prs there are only select operators 
WITHOUT_CLASSIFICATION	 generate special repeated case 
WITHOUT_CLASSIFICATION	 does this make sense 
WITHOUT_CLASSIFICATION	 but the registry was fully initialized thus need add 
WITHOUT_CLASSIFICATION	 for typeinfofactory use only 
WITHOUT_CLASSIFICATION	 get the key column names and check the keys are all constants 
WITHOUT_CLASSIFICATION	 only explain uses 
WITHOUT_CLASSIFICATION	 the form partition 
WITHOUT_CLASSIFICATION	 get columns for sel from lvj 
WITHOUT_CLASSIFICATION	 test date string 
WITHOUT_CLASSIFICATION	 insert overwrite command 
WITHOUT_CLASSIFICATION	 nonpartition expressions are converted nulls 
WITHOUT_CLASSIFICATION	 verify rename after bootstrap successful 
WITHOUT_CLASSIFICATION	 open record reader read next split 
WITHOUT_CLASSIFICATION	 side files are only created streaming ingest this compaction may have insert delta here with side files there because the original writer died 
WITHOUT_CLASSIFICATION	 the user asked for formatted output dump the json output the output stream 
WITHOUT_CLASSIFICATION	 adds delta and deletedelta 
WITHOUT_CLASSIFICATION	 test nul character 
WITHOUT_CLASSIFICATION	 the cross product the big table equal key rows values against the small table matching key which has value rows into overflow batch 
WITHOUT_CLASSIFICATION	 the encoding namecodes dont contain pound signs 
WITHOUT_CLASSIFICATION	 for cases where different rel nodes are referring same correlation var case not avoid generating another correlation var and record the rel using the same correlation 
WITHOUT_CLASSIFICATION	 means this interior transaction should already have transaction created that active 
WITHOUT_CLASSIFICATION	 the specified path directory iterate through all files 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 although technically its unbounded its unlikely will ever see ndv 
WITHOUT_CLASSIFICATION	 need necessarily override this method since default impl assumes hdfs based location string 
WITHOUT_CLASSIFICATION	 objectstore methods overridden with injected behavior 
WITHOUT_CLASSIFICATION	 tablescan will also followed select operator find the expressions for the 
WITHOUT_CLASSIFICATION	 resolve futures used for testing 
WITHOUT_CLASSIFICATION	 call the regular method since does error checking 
WITHOUT_CLASSIFICATION	 the column number and type information for this one column string reduce key 
WITHOUT_CLASSIFICATION	 the registry dynamic refreshes 
WITHOUT_CLASSIFICATION	 the number times has returned nonnull errors 
WITHOUT_CLASSIFICATION	 only process partition which skewed and list bucketed 
WITHOUT_CLASSIFICATION	 builds partition spec can build suitable where clause 
WITHOUT_CLASSIFICATION	 iterator the reducer operator tree 
WITHOUT_CLASSIFICATION	 initialize satisfy compiler finals 
WITHOUT_CLASSIFICATION	 subset keycols the size should too this condition maybe too strict may extend the future 
WITHOUT_CLASSIFICATION	 all inserts are committed and hence would expect txntowriteid entries for acidtbl and entries for acidtblpart each insert would have allocated writeid 
WITHOUT_CLASSIFICATION	 temporary functions dont have any database namespace associated with 
WITHOUT_CLASSIFICATION	 check equivalent versions should compatible 
WITHOUT_CLASSIFICATION	 the starttime may not set the sparktask finished too fast because sparkjobmonitor will sleep for second then check the state right after sleep the spark job may already completed this case set starttime the same submittime 
WITHOUT_CLASSIFICATION	 exception handling routines 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 more required 
WITHOUT_CLASSIFICATION	 comments are separated see method getschema metastoreutils where this string columnscomments generated 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 initially children inputs set later with setinput methods 
WITHOUT_CLASSIFICATION	 now try evict with locked buffer still the list 
WITHOUT_CLASSIFICATION	 identical strings should equal 
WITHOUT_CLASSIFICATION	 the simd optimized form 
WITHOUT_CLASSIFICATION	 find databases which name contains tofind 
WITHOUT_CLASSIFICATION	 unicode case 
WITHOUT_CLASSIFICATION	 find given owid rowid pair deleted not perform two binary searches most the first binary search the compressed owids match found only then the next binary search the larger rowid vector between the given toindex fromindex 
WITHOUT_CLASSIFICATION	 stolen from hives metricstestutils probably should break out into its own class 
WITHOUT_CLASSIFICATION	 doesnt have notion small and saves the full value int overflow expectednull but was 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 run initialization statements the connection 
WITHOUT_CLASSIFICATION	 avro should always use the table properties for initialization see hive 
WITHOUT_CLASSIFICATION	 there more than one 
WITHOUT_CLASSIFICATION	 mystructset 
WITHOUT_CLASSIFICATION	 bytes per long the refs assume data will empty this just sanity check 
WITHOUT_CLASSIFICATION	 the bucket value should same for all the records 
WITHOUT_CLASSIFICATION	 none pattern 
WITHOUT_CLASSIFICATION	 want have only one auth bridge the past this was handled shimloader but since were longer using that well here 
WITHOUT_CLASSIFICATION	 check that the data removed 
WITHOUT_CLASSIFICATION	 partition 
WITHOUT_CLASSIFICATION	 write any remaining bytes the out stream 
WITHOUT_CLASSIFICATION	 test 
WITHOUT_CLASSIFICATION	 isexternalquery the call from within hte daemon permission check required 
WITHOUT_CLASSIFICATION	 the output partial aggregation struct containing long count two double averages two double variances and double covariance 
WITHOUT_CLASSIFICATION	 will cache have the entire part 
WITHOUT_CLASSIFICATION	 parse until union separator currentlevel 
WITHOUT_CLASSIFICATION	 the query should completed now 
WITHOUT_CLASSIFICATION	 input parameters 
WITHOUT_CLASSIFICATION	 this our way documenting that are mutating the contents this writables internal timestamp 
WITHOUT_CLASSIFICATION	 nonjavadoc see int this processor has pushpull model first call this method push but the rest pulled until run out records 
WITHOUT_CLASSIFICATION	 interfaces for functions without uservisible name 
WITHOUT_CLASSIFICATION	 when last txn finished abortcommit the currenttxnindex pointing that txn need start from next one any also batch was created but was never called want start with first txn 
WITHOUT_CLASSIFICATION	 statistics 
WITHOUT_CLASSIFICATION	 sumc 
WITHOUT_CLASSIFICATION	 need check guaranteed here was false would already the queue 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 for orc and parquet all the following statements are the same analyze table partition compute statistics analyze table partition compute statistics noscan 
WITHOUT_CLASSIFICATION	 create limit desc with limit value 
WITHOUT_CLASSIFICATION	 become available 
WITHOUT_CLASSIFICATION	 call the real precreatetable method 
WITHOUT_CLASSIFICATION	 the general column statistics 
WITHOUT_CLASSIFICATION	 test that schema was loaded correctly 
WITHOUT_CLASSIFICATION	 row 
WITHOUT_CLASSIFICATION	 this should make the search linear sync the beginning the block being searched set the comparison null and the flag reset the range should unset 
WITHOUT_CLASSIFICATION	 all columns the expression must partitioned columns 
WITHOUT_CLASSIFICATION	 iterate over all expression either after select select transform 
WITHOUT_CLASSIFICATION	 the values are equal the queue limit fixed 
WITHOUT_CLASSIFICATION	 only store the latest error there are multiple 
WITHOUT_CLASSIFICATION	 check admin option has been specified 
WITHOUT_CLASSIFICATION	 stack methods 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 verify that udf whitelist can executed 
WITHOUT_CLASSIFICATION	 error with driver 
WITHOUT_CLASSIFICATION	 case lateral views followed join the same tree can traversed more than one 
WITHOUT_CLASSIFICATION	 operator allpath 
WITHOUT_CLASSIFICATION	 store postexec hooks calls can look them later 
WITHOUT_CLASSIFICATION	 only meant for use the querytracker 
WITHOUT_CLASSIFICATION	 have add this bit exception handling here because functionapply does not allow throw the actual exception that might checked exception wind needing throw runtimeexception with the previously thrown exception its cause however since returns throwable instead exception have account for the possibility that the underlying code might have thrown throwable that wrapped instead which case continuing throw the runtimeexception the best thing can 
WITHOUT_CLASSIFICATION	 liststring from tableacidtbl order didnt match autocommittrue stringifyvaluesrows 
WITHOUT_CLASSIFICATION	 add the rest the metadata keys 
WITHOUT_CLASSIFICATION	 path shared 
WITHOUT_CLASSIFICATION	 for 
WITHOUT_CLASSIFICATION	 originally the mvtask and the child move task the mrandmvtask contain the same movework object the blobstore optimizations are and the inputoutput paths are merged the move only movework the mvtask and the child move task the mrandmvtask will contain different movework objects which causes problems not just this case but also general the child move task the mrandmvtask should 
WITHOUT_CLASSIFICATION	 analyze create table command 
WITHOUT_CLASSIFICATION	 now copy over the data when isnullindex false 
WITHOUT_CLASSIFICATION	 tests location returned when the first file found later lookup order 
WITHOUT_CLASSIFICATION	 convert the rest and put into the last entry 
WITHOUT_CLASSIFICATION	 retrieved from object cache 
WITHOUT_CLASSIFICATION	 add virtual columns for analyze table 
WITHOUT_CLASSIFICATION	 the not vectorized information has been stored the mapwork vertex 
WITHOUT_CLASSIFICATION	 count null input which for count and output long just modes partial complete 
WITHOUT_CLASSIFICATION	 then the buffer was the list remove 
WITHOUT_CLASSIFICATION	 cast single column 
WITHOUT_CLASSIFICATION	 remove semijoin optimization creates cycle with mapside joins 
WITHOUT_CLASSIFICATION	 need first join and flush out data left the previous file 
WITHOUT_CLASSIFICATION	 local mode 
WITHOUT_CLASSIFICATION	 repl load not partition level always table level passing null for partition specs also repl load doesnt support external table and hence location set well 
WITHOUT_CLASSIFICATION	 the token file location and mapreduce job tag should right after the tool argument 
WITHOUT_CLASSIFICATION	 fail trying set transactional false not allowed 
WITHOUT_CLASSIFICATION	 global used when setting errors etc 
WITHOUT_CLASSIFICATION	 long between 
WITHOUT_CLASSIFICATION	 not closing this the moment shutdown since this could shared instance 
WITHOUT_CLASSIFICATION	 findroots returns all root operators ops that result operator 
WITHOUT_CLASSIFICATION	 not really much can here 
WITHOUT_CLASSIFICATION	 always use localhost for hostname some tests like ssl validation ones are tied localhost being present the certificate name 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 schedule outside the schedulelock which should only used wait the condition 
WITHOUT_CLASSIFICATION	 since store references hivedecimalwritable instances must use the update method instead plain assignment 
WITHOUT_CLASSIFICATION	 todo invalid valid invalid 
WITHOUT_CLASSIFICATION	 there end node that not the limitwherefalse 
WITHOUT_CLASSIFICATION	 need fetch the table before dropped that can passed postexecution hook 
WITHOUT_CLASSIFICATION	 this can happen all values stream are nulls last row group values are all null 
WITHOUT_CLASSIFICATION	 floatdouble string types have default value for null 
WITHOUT_CLASSIFICATION	 dependency class used for explain 
WITHOUT_CLASSIFICATION	 note that this uses short user name without consideration for kerberos realm this seems the common approach for hdfs permissions but may better consider the realm although not the host not the full name 
WITHOUT_CLASSIFICATION	 first need check valid convert mergeinsert into succeed modify the plan and afterwards the ast should acid table 
WITHOUT_CLASSIFICATION	 hcatpy will become the first argument pass command python 
WITHOUT_CLASSIFICATION	 modify the middle for view rewrite 
WITHOUT_CLASSIFICATION	 check the status all the columns all the partitions exists 
WITHOUT_CLASSIFICATION	 deserialization code 
WITHOUT_CLASSIFICATION	 from the duplicate publish 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 cannot contain nondeterministic function 
WITHOUT_CLASSIFICATION	 inherit java system variables 
WITHOUT_CLASSIFICATION	 note could just what already above from disk data except for the validation that not strictly necessary and knowntornstart which optimization 
WITHOUT_CLASSIFICATION	 null null not allowed this check 
WITHOUT_CLASSIFICATION	 drop all the tables 
WITHOUT_CLASSIFICATION	 not set environment context 
WITHOUT_CLASSIFICATION	 have set ndv 
WITHOUT_CLASSIFICATION	 column exprmap 
WITHOUT_CLASSIFICATION	 granttime 
WITHOUT_CLASSIFICATION	 single long key hash set optimized for vector map join 
WITHOUT_CLASSIFICATION	 literal decimal 
WITHOUT_CLASSIFICATION	 for current schema evolution 
WITHOUT_CLASSIFICATION	 method hivemetastoreclient 
WITHOUT_CLASSIFICATION	 compare timestamp integer seconds double seconds with fractional nanoseonds 
WITHOUT_CLASSIFICATION	 ignore will generated sel 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 generate sortcols and order 
WITHOUT_CLASSIFICATION	 create dummy task move needed 
WITHOUT_CLASSIFICATION	 this correlator was generated previous invocation this rule further work 
WITHOUT_CLASSIFICATION	 volatile because heartbeat may different thread 
WITHOUT_CLASSIFICATION	 lock entire heap heap still full should not able evict insert 
WITHOUT_CLASSIFICATION	 the nullable side the 
WITHOUT_CLASSIFICATION	 check the database location the default location based the old warehouse root then change the database location the default based the current warehouse root 
WITHOUT_CLASSIFICATION	 time zone file was written from metadata 
WITHOUT_CLASSIFICATION	 changes the value variable the corresponding change will made this mapping 
WITHOUT_CLASSIFICATION	 constructor used hiverexexecutorimpl 
WITHOUT_CLASSIFICATION	 this only lives for the duration the service init 
WITHOUT_CLASSIFICATION	 there could races here heartbeat delivered the old value just after have received successful confirmation from the api are about overwrite the latter could solve this adding version smth like that ignoring discrepancies unless have previously received update error for this task however the only effect 
WITHOUT_CLASSIFICATION	 dont clear the attempt the stuff will cleared 
WITHOUT_CLASSIFICATION	 singlecolumn long specific variables 
WITHOUT_CLASSIFICATION	 projections from child 
WITHOUT_CLASSIFICATION	 first group 
WITHOUT_CLASSIFICATION	 get local time tozone 
WITHOUT_CLASSIFICATION	 retrievecd false not need deep retrieval the table column descriptor 
WITHOUT_CLASSIFICATION	 execute cli driver work 
WITHOUT_CLASSIFICATION	 construct column statistics object from the result 
WITHOUT_CLASSIFICATION	 there are more than children any level dont anything 
WITHOUT_CLASSIFICATION	 now general lookup 
WITHOUT_CLASSIFICATION	 nothing when the optimization off 
WITHOUT_CLASSIFICATION	 lastaccesstime 
WITHOUT_CLASSIFICATION	 the underlying sslsocket object bound hostport with the given sotimeout and sslcontext created with the given params 
WITHOUT_CLASSIFICATION	 given jar add stored key and all its transitive dependencies value used for deleting transitive dependencies 
WITHOUT_CLASSIFICATION	 test from server client too 
WITHOUT_CLASSIFICATION	 this can set for old behavior nulls printed empty strings 
WITHOUT_CLASSIFICATION	 sessionhandle 
WITHOUT_CLASSIFICATION	 make nonblocking 
WITHOUT_CLASSIFICATION	 window frame that has only the start boundary then interpreted between start boundary and current row window specification with order specification and window frame interpreted range between unbounded preceding and current row window specification with order and window frame interpreted rows between unbounded preceding and unbounded following 
WITHOUT_CLASSIFICATION	 separate split 
WITHOUT_CLASSIFICATION	 add and scratchdir specify nondefault scratch dir 
WITHOUT_CLASSIFICATION	 handle tablescanoperator here can safely ignore table alias and the current comparator implementation does not 
WITHOUT_CLASSIFICATION	 check the input line multiline command which needs read further 
WITHOUT_CLASSIFICATION	 extract the collation for this operator and the collations 
WITHOUT_CLASSIFICATION	 set output format parameters these are not supported but only 
WITHOUT_CLASSIFICATION	 see custom compositekey class was provided 
WITHOUT_CLASSIFICATION	 doing characters comparison directly instead regular expression matching for simple patterns like abc 
WITHOUT_CLASSIFICATION	 anything but 
WITHOUT_CLASSIFICATION	 swap column vectors but keep selected vector unchanged 
WITHOUT_CLASSIFICATION	 these members are used outofband params for the innerloop supperprocessop callbacks 
WITHOUT_CLASSIFICATION	 there cannot exist any distinct aggregate 
WITHOUT_CLASSIFICATION	 dont actually create the key 
WITHOUT_CLASSIFICATION	 can the join operator converted sortmerge join operator 
WITHOUT_CLASSIFICATION	 can retrieve later 
WITHOUT_CLASSIFICATION	 ignore files eliminated ppd length 
WITHOUT_CLASSIFICATION	 above should have thrown there such catalog 
WITHOUT_CLASSIFICATION	 idempotent case for destdb 
WITHOUT_CLASSIFICATION	 there may race for this slot requery after delay with some probability 
WITHOUT_CLASSIFICATION	 serialize the row byteswritable 
WITHOUT_CLASSIFICATION	 dont propagate errors from close since this will lose the original error above 
WITHOUT_CLASSIFICATION	 there are not enough failed compactions yet should return false 
WITHOUT_CLASSIFICATION	 figure out can lock too 
WITHOUT_CLASSIFICATION	 optional bytes userpayload 
WITHOUT_CLASSIFICATION	 calculate key once lookup once 
WITHOUT_CLASSIFICATION	 invoke the method 
WITHOUT_CLASSIFICATION	 jdbc driver error 
WITHOUT_CLASSIFICATION	 check whether this list map 
WITHOUT_CLASSIFICATION	 augment conf with the settings from the started llap configuration 
WITHOUT_CLASSIFICATION	 map original column index among selected columns 
WITHOUT_CLASSIFICATION	 now generate operator 
WITHOUT_CLASSIFICATION	 update outrwsch 
WITHOUT_CLASSIFICATION	 make new generated task depends all the parent tasks current task 
WITHOUT_CLASSIFICATION	 convert the table acid todo remove transprop after hive 
WITHOUT_CLASSIFICATION	 random 
WITHOUT_CLASSIFICATION	 patterns that are included performance logging level performance mode show execution and performance logger messages 
WITHOUT_CLASSIFICATION	 now determine the small table results 
WITHOUT_CLASSIFICATION	 loop over all the operators recursively 
WITHOUT_CLASSIFICATION	 todo use fileid right from the list after hdfs get dfs client and 
WITHOUT_CLASSIFICATION	 generate the reducesinkoperator for the group query block the new reducesinkoperator will child inputoperatorinfo will put all group keys and the distinct field any the mapreduce sort key and all other fields the mapreduce value param numpartitionfields the number fields for mapreduce partitioning this usually the number fields the group keys return the new reducesinkoperator throws semanticexception 
WITHOUT_CLASSIFICATION	 data should not visible 
WITHOUT_CLASSIFICATION	 drop two files they are moved 
WITHOUT_CLASSIFICATION	 current map join null means has been handled currentmapjoin process 
WITHOUT_CLASSIFICATION	 cartesian product row count easy infer 
WITHOUT_CLASSIFICATION	 max numbitvectors bytes enough 
WITHOUT_CLASSIFICATION	 this will null master 
WITHOUT_CLASSIFICATION	 since using non strict mode get shared lock 
WITHOUT_CLASSIFICATION	 set synthetic flag that would push filter below this one 
WITHOUT_CLASSIFICATION	 check argument string array strings 
WITHOUT_CLASSIFICATION	 remove trailing comma 
WITHOUT_CLASSIFICATION	 workaround for ignore like jdo does for now 
WITHOUT_CLASSIFICATION	 sizes least tables nway join known and their sum smaller than 
WITHOUT_CLASSIFICATION	 get the return objectinspector 
WITHOUT_CLASSIFICATION	 since this conversion from nonacid acid nextwriteid should not have entry for this table also has unique index case should not violated 
WITHOUT_CLASSIFICATION	 recheck make sure someone didnt create while waited 
WITHOUT_CLASSIFICATION	 assert that the table created has hcat instrumentation and that were still able read 
WITHOUT_CLASSIFICATION	 check table transactional 
WITHOUT_CLASSIFICATION	 have manually reset the jobconf make sure gets picked 
WITHOUT_CLASSIFICATION	 property defined hivesitexml only 
WITHOUT_CLASSIFICATION	 for now this should true 
WITHOUT_CLASSIFICATION	 currently multiinsrt doesnt allow same tablepartition output branch 
WITHOUT_CLASSIFICATION	 typeclassname 
WITHOUT_CLASSIFICATION	 timestamp represented long internally need any thing here 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 are now sending message update again return both callbacks 
WITHOUT_CLASSIFICATION	 mark this task final map reduce task ignoring the optional merge task 
WITHOUT_CLASSIFICATION	 intermediate outputs joinsgroupbys 
WITHOUT_CLASSIFICATION	 used all flavors 
WITHOUT_CLASSIFICATION	 construct column name list and types for reference filter push down 
WITHOUT_CLASSIFICATION	 for partitionless table initialize partvalue empty string can have partitionless table even have partition keys when there only only partition selected and the partition key not part the projectioninclude list 
WITHOUT_CLASSIFICATION	 magic value usage invalid with nanotime once years may log extra 
WITHOUT_CLASSIFICATION	 close writer 
WITHOUT_CLASSIFICATION	 call file stat split mockmocktable 
WITHOUT_CLASSIFICATION	 exception happens after docopyonce then need call getfilestoretry with copy error false retry 
WITHOUT_CLASSIFICATION	 these many values reach beginning the row group 
WITHOUT_CLASSIFICATION	 write the blob 
WITHOUT_CLASSIFICATION	 simple tree with single parent 
WITHOUT_CLASSIFICATION	 this session should never default session unless something has messed 
WITHOUT_CLASSIFICATION	 when done handleupdate may break the iterator the order these checks important 
WITHOUT_CLASSIFICATION	 load the hash table 
WITHOUT_CLASSIFICATION	 get rid spills before start modifying the batch 
WITHOUT_CLASSIFICATION	 violating which can cause data loss 
WITHOUT_CLASSIFICATION	 elements key value 
WITHOUT_CLASSIFICATION	 compare the two map objects for equality 
WITHOUT_CLASSIFICATION	 database itself null then can not filter out anything 
WITHOUT_CLASSIFICATION	 parameter value still false connection the alter still goes through 
WITHOUT_CLASSIFICATION	 can null since the task may have completed meanwhile 
WITHOUT_CLASSIFICATION	 the simd optimized form 
WITHOUT_CLASSIFICATION	 test setter for configuration object 
WITHOUT_CLASSIFICATION	 unless least one was not found 
WITHOUT_CLASSIFICATION	 remove dpp based expected size the output data 
WITHOUT_CLASSIFICATION	 extract stage plans 
WITHOUT_CLASSIFICATION	 verify that session wasnt closed transport close 
WITHOUT_CLASSIFICATION	 move data from temp directory the actual table directory metastore operation required 
WITHOUT_CLASSIFICATION	 constants for bit variant 
WITHOUT_CLASSIFICATION	 need check the original schema see this actually fixed 
WITHOUT_CLASSIFICATION	 get all the driver run hooks and preexecute them 
WITHOUT_CLASSIFICATION	 tab tab 
WITHOUT_CLASSIFICATION	 check aggoutputproj projects only one expression 
WITHOUT_CLASSIFICATION	 runcreate materialized view dbname matview select from dbname unptned driver verifysetupselect from dbname matview unptndata driver 
WITHOUT_CLASSIFICATION	 this should never happen least for now throw 
WITHOUT_CLASSIFICATION	 filterinputrel 
WITHOUT_CLASSIFICATION	 subclass must indicate whether will transform the raw input before fed through the partitioning mechanics 
WITHOUT_CLASSIFICATION	 tracks instances known both yarn service and llap 
WITHOUT_CLASSIFICATION	 lets dont fail future timeout since have timeout for prewarm 
WITHOUT_CLASSIFICATION	 the join 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 oninit will called hmshandler initialization and set this true 
WITHOUT_CLASSIFICATION	 this method will return only after the cache has updated once 
WITHOUT_CLASSIFICATION	 test basic truncate bytes slice 
WITHOUT_CLASSIFICATION	 task typically task gets rerun times fails 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 not null constraint name default constraint name 
WITHOUT_CLASSIFICATION	 need extrapolation 
WITHOUT_CLASSIFICATION	 also match for this converted maponly job 
WITHOUT_CLASSIFICATION	 test with open transactions 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 for all other kinds operators assume the output big the 
WITHOUT_CLASSIFICATION	 set min ndv value both columns involved join 
WITHOUT_CLASSIFICATION	 indicating that the previous cookie has expired 
WITHOUT_CLASSIFICATION	 hive disable for explain analyze 
WITHOUT_CLASSIFICATION	 have classlevel annotation that says whether the udfs vectorization expressions 
WITHOUT_CLASSIFICATION	 read database table partition via cachedstore 
WITHOUT_CLASSIFICATION	 support for decimal input must convert 
WITHOUT_CLASSIFICATION	 use the basic the extended version the optimizer 
WITHOUT_CLASSIFICATION	 setup our batch with the same column schema the big table batch that can used build join output results 
WITHOUT_CLASSIFICATION	 make sure initialize necessary 
WITHOUT_CLASSIFICATION	 cases out could pass the path and type directly metastore 
WITHOUT_CLASSIFICATION	 create new conf file using contents from current one 
WITHOUT_CLASSIFICATION	 mapping from tablename table object metastore 
WITHOUT_CLASSIFICATION	 change the table name back 
WITHOUT_CLASSIFICATION	 job callable task for job list operation overrides behavior execute list jobs need override behavior cleanup there nothing done list jobs operation timed out interrupted 
WITHOUT_CLASSIFICATION	 write the orc file the mock file system 
WITHOUT_CLASSIFICATION	 reset 
WITHOUT_CLASSIFICATION	 unlock the previous lock 
WITHOUT_CLASSIFICATION	 test 
WITHOUT_CLASSIFICATION	 create warehouse with that user impersonation has issues 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 touch the next file 
WITHOUT_CLASSIFICATION	 step create the insert query 
WITHOUT_CLASSIFICATION	 now calculate which rows were filtered out they are logically matches 
WITHOUT_CLASSIFICATION	 pattern 
WITHOUT_CLASSIFICATION	 write key buffer compute hashcode and compare its new key will 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this function called the parent should only include constant 
WITHOUT_CLASSIFICATION	 method get the valid write ids list for the given table 
WITHOUT_CLASSIFICATION	 worker threads stuff 
WITHOUT_CLASSIFICATION	 create test table 
WITHOUT_CLASSIFICATION	 noautocompact need check both cases 
WITHOUT_CLASSIFICATION	 will touch all blocks random order 
WITHOUT_CLASSIFICATION	 updates key with sequence number 
WITHOUT_CLASSIFICATION	 this thread should throw exception 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 successfully insert some data into acid tables that have records 
WITHOUT_CLASSIFICATION	 deltaxxxxyyyy format 
WITHOUT_CLASSIFICATION	 real column name which the operation being performed 
WITHOUT_CLASSIFICATION	 ignore 
WITHOUT_CLASSIFICATION	 partition spec not specified but column schema can have partitions specified 
WITHOUT_CLASSIFICATION	 expression for the table 
WITHOUT_CLASSIFICATION	 ignored the mbean itself was not found which should never happen because just accessed perhaps something unregistered inbetween but this happens just dont output the attribute 
WITHOUT_CLASSIFICATION	 for local src file copy hdfs 
WITHOUT_CLASSIFICATION	 groupby into the reduce keys 
WITHOUT_CLASSIFICATION	 encodedcolumnbatch already decompressed dont really need pass codec but need know the original data compressed not this used skip positions row index properly the file originally compressed then position compressed offset row index should skipped get uncompressed offset else position should not skipped 
WITHOUT_CLASSIFICATION	 add all except the right side the bad positions 
WITHOUT_CLASSIFICATION	 the else clause 
WITHOUT_CLASSIFICATION	 perform compaction join result after compaction should still the same 
WITHOUT_CLASSIFICATION	 multikey long check for repeating 
WITHOUT_CLASSIFICATION	 write the escaped byte 
WITHOUT_CLASSIFICATION	 build exprnode corresponding colums 
WITHOUT_CLASSIFICATION	 the new conjuncts are already present the plan bail out 
WITHOUT_CLASSIFICATION	 the value doesnt matter 
WITHOUT_CLASSIFICATION	 regardless whether was removed successfully after failing remove restart since just restart this from under the user mark handle properly when 
WITHOUT_CLASSIFICATION	 mark one the transactions exception test that invalid transactions are being handled properly exclude transaction 
WITHOUT_CLASSIFICATION	 commit the txn under hwm 
WITHOUT_CLASSIFICATION	 spot check decimal column modulo decimal column 
WITHOUT_CLASSIFICATION	 special char 
WITHOUT_CLASSIFICATION	 cannot obtain better estimate without providing somehow which case using statistics would completely unnecessary 
WITHOUT_CLASSIFICATION	 the target data textinputformat 
WITHOUT_CLASSIFICATION	 definitely not byte 
WITHOUT_CLASSIFICATION	 fits two longwords 
WITHOUT_CLASSIFICATION	 hint disable runtime filtering 
WITHOUT_CLASSIFICATION	 need evaluate result for every pruned partition 
WITHOUT_CLASSIFICATION	 not transactional nothing more 
WITHOUT_CLASSIFICATION	 through the argclasses and for any string void date time start looking for doubles 
WITHOUT_CLASSIFICATION	 out range due time 
WITHOUT_CLASSIFICATION	 number bits store the number zero runs 
WITHOUT_CLASSIFICATION	 get the latest timestamp all the cells the row timestamp from hbase 
WITHOUT_CLASSIFICATION	 whether this operator outer join 
WITHOUT_CLASSIFICATION	 see working uts 
WITHOUT_CLASSIFICATION	 all dml should fail with dummytxnmanager acid table 
WITHOUT_CLASSIFICATION	 the table alias should exist 
WITHOUT_CLASSIFICATION	 second insert round with new inserts into previously existing partition yesterday 
WITHOUT_CLASSIFICATION	 override external stuff these could also injected extra classes 
WITHOUT_CLASSIFICATION	 set small time unit cookie max age that the server sends 
WITHOUT_CLASSIFICATION	 need pass virtual columns reader 
WITHOUT_CLASSIFICATION	 this intentionally duplicated because hive 
WITHOUT_CLASSIFICATION	 the future could allow users specify quote character that doesnt need escaping but for now 
WITHOUT_CLASSIFICATION	 did not see skew key this table continue next table 
WITHOUT_CLASSIFICATION	 assumption top portion tree could only limitobproject 
WITHOUT_CLASSIFICATION	 for now expose nonprimitive string 
WITHOUT_CLASSIFICATION	 reallocate only any filters pruned 
WITHOUT_CLASSIFICATION	 get all the stats for colnames partnames 
WITHOUT_CLASSIFICATION	 close 
WITHOUT_CLASSIFICATION	 eventid 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 read enough data for just the first message decoded 
WITHOUT_CLASSIFICATION	 maxdecimal with round longer than digits 
WITHOUT_CLASSIFICATION	 validate the first parameter which the expression compute over this should 
WITHOUT_CLASSIFICATION	 verify that non whitelist params cant set 
WITHOUT_CLASSIFICATION	 hadoop doesnt support credential merging this will fail 
WITHOUT_CLASSIFICATION	 initialize the rowid array when have some delete events 
WITHOUT_CLASSIFICATION	 todo rsjoin 
WITHOUT_CLASSIFICATION	 find the index the least significant bit that 
WITHOUT_CLASSIFICATION	 testing with nulls 
WITHOUT_CLASSIFICATION	 srsw lock are examining shared write 
WITHOUT_CLASSIFICATION	 use the remapped arguments for the nondistinct aggregate calls 
WITHOUT_CLASSIFICATION	 caching instances only case the yarn registry each host based list will get its own copy 
WITHOUT_CLASSIFICATION	 overflow batchs 
WITHOUT_CLASSIFICATION	 write json the temp file 
WITHOUT_CLASSIFICATION	 left border the min 
WITHOUT_CLASSIFICATION	 have more than one group key batch will buffer their contents dont buffer the key columns since they are constant for the group key buffer the nonkey input columns and buffer any streaming columns that will already have their output values 
WITHOUT_CLASSIFICATION	 there are some txns the list which does not have write allocated and hence ahead and get the next write for the given table and update with new next write 
WITHOUT_CLASSIFICATION	 get the databases for the desired pattern populate the output stream 
WITHOUT_CLASSIFICATION	 flag 
WITHOUT_CLASSIFICATION	 were dealing with array strings 
WITHOUT_CLASSIFICATION	 can tablereference subquery another ptf invocation for tableref set the source the alias returned processtable for subquery set the source the alias returned processsubquery for ptf invocation recursively call processptfchain 
WITHOUT_CLASSIFICATION	 implement needed 
WITHOUT_CLASSIFICATION	 walk through existing map truncate path that test wont mask then can verify location right 
WITHOUT_CLASSIFICATION	 should propagate the error message properly 
WITHOUT_CLASSIFICATION	 where the inverse multiplication result find the quotient integer decimal portion please see comments for 
WITHOUT_CLASSIFICATION	 revert back local 
WITHOUT_CLASSIFICATION	 resolve for the method based argument types 
WITHOUT_CLASSIFICATION	 create database specifically not replicated across per design since user drops database and recreates another with the same one want distinguish between the two will replicate the drop across but after that the goal that new created new replication definition should created the replication implementer above this thus extend noopreplicationtask and the only additional thing validate event type 
WITHOUT_CLASSIFICATION	 order key columns partition columns bucket number column 
WITHOUT_CLASSIFICATION	 job request got interrupted job kill should have started return client with with queueexception 
WITHOUT_CLASSIFICATION	 verify that udf default whitelist can executed 
WITHOUT_CLASSIFICATION	 simply create 
WITHOUT_CLASSIFICATION	 restore state repeating and non nulls indicators 
WITHOUT_CLASSIFICATION	 for now decrement the count avoid accounting errors 
WITHOUT_CLASSIFICATION	 the starting position grouping set need known 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 additional conf settings specified the command line 
WITHOUT_CLASSIFICATION	 need translate the exprnodefielddesc too identifiers struct 
WITHOUT_CLASSIFICATION	 let the vectorassignrow class the conversion 
WITHOUT_CLASSIFICATION	 the call succeeded presumably the api there 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 insert some data this will generate only insert deltas and delete deltas delta 
WITHOUT_CLASSIFICATION	 because need revert the tag row its old tag and cannot pass new tag this method which used get the old tag from the mapping newtagtooldtag bypass this method muxoperator and directly call process children process method 
WITHOUT_CLASSIFICATION	 should not getting invoked should invoked instead 
WITHOUT_CLASSIFICATION	 want have project after join since sqcountchecks count expression wouldnt needed further 
WITHOUT_CLASSIFICATION	 called lazymap 
WITHOUT_CLASSIFICATION	 print the per vertex summary 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 has nothing cached 
WITHOUT_CLASSIFICATION	 implementation and assorted methods 
WITHOUT_CLASSIFICATION	 check left valid 
WITHOUT_CLASSIFICATION	 stateful implies nondeterministic regardless whatever the deterministic annotation declares 
WITHOUT_CLASSIFICATION	 convert the elements 
WITHOUT_CLASSIFICATION	 delete and all tables 
WITHOUT_CLASSIFICATION	 thread cancelling the query 
WITHOUT_CLASSIFICATION	 beginning with distcpoptions should honoured 
WITHOUT_CLASSIFICATION	 create the object inspector for the input columns and initialize the 
WITHOUT_CLASSIFICATION	 remember the jobconf cloned for each mapwork wont clone for again 
WITHOUT_CLASSIFICATION	 values accumulators can only read the sparkcontext side this field used when creating snapshot sent the rsc client 
WITHOUT_CLASSIFICATION	 dont create context for the column 
WITHOUT_CLASSIFICATION	 table and view second read entity 
WITHOUT_CLASSIFICATION	 serialize time 
WITHOUT_CLASSIFICATION	 set the server side see 
WITHOUT_CLASSIFICATION	 initially zero 
WITHOUT_CLASSIFICATION	 for every field 
WITHOUT_CLASSIFICATION	 for constructing hcatpartitions afresh argument 
WITHOUT_CLASSIFICATION	 but whether the table itself partitioned not know 
WITHOUT_CLASSIFICATION	 try again with null value 
WITHOUT_CLASSIFICATION	 implicit use batchindex 
WITHOUT_CLASSIFICATION	 check partitioning column order and types 
WITHOUT_CLASSIFICATION	 log exception but ignore inability start 
WITHOUT_CLASSIFICATION	 requires schema change 
WITHOUT_CLASSIFICATION	 small table information 
WITHOUT_CLASSIFICATION	 binary mode for embedded mode the jdbc uri the form and does not contain hostport string result port parsed per the java uri conventions 
WITHOUT_CLASSIFICATION	 carefully handle nulls 
WITHOUT_CLASSIFICATION	 not null constraint should reference single column 
WITHOUT_CLASSIFICATION	 were dealing with array arrays strings 
WITHOUT_CLASSIFICATION	 check the forward and backward compatibility 
WITHOUT_CLASSIFICATION	 not demuxoperator should have single child 
WITHOUT_CLASSIFICATION	 function name 
WITHOUT_CLASSIFICATION	 irrelevant see comment above irrelevant see comment above 
WITHOUT_CLASSIFICATION	 for primitive type add directly 
WITHOUT_CLASSIFICATION	 create new mapping 
WITHOUT_CLASSIFICATION	 try make something reasonable pass the base class 
WITHOUT_CLASSIFICATION	 prepare updated partition columns for small tables get the positions bucketed columns 
WITHOUT_CLASSIFICATION	 loginforeturning final parent ptnrootlocation 
WITHOUT_CLASSIFICATION	 singlecolumn string specific save key 
WITHOUT_CLASSIFICATION	 test dryrun schema initialization 
WITHOUT_CLASSIFICATION	 set them back 
WITHOUT_CLASSIFICATION	 dont save maxwidth automatically set based the terminal configuration 
WITHOUT_CLASSIFICATION	 will not deleted the user will run archive again clear this 
WITHOUT_CLASSIFICATION	 find out the segment with latest version and maximum partition number 
WITHOUT_CLASSIFICATION	 set num threads that singlethreaded checkmetastore called 
WITHOUT_CLASSIFICATION	 bucket bucket bucket 
WITHOUT_CLASSIFICATION	 always call init because the hook name the configuration could have changed 
WITHOUT_CLASSIFICATION	 truncatetable event unpartitioned table 
WITHOUT_CLASSIFICATION	 for the filtered out rows that didnt logically get looked the hash table need generate match results for those too 
WITHOUT_CLASSIFICATION	 have not added this column desc before bail out 
WITHOUT_CLASSIFICATION	 hiveconf well 
WITHOUT_CLASSIFICATION	 the work needs know about the dummy operators they have separately initialized 
WITHOUT_CLASSIFICATION	 find all aggregate calls without distinct 
WITHOUT_CLASSIFICATION	 this not real bloom filter but cheap version the memory access bloom filters several cases well have mapjoin spills because the value columns are few hundred columns text each while there are very few keys total few thousand this cheap exit option prevent spilling the bigtable such scenario 
WITHOUT_CLASSIFICATION	 need add select since order schema may have more columns than result schema 
WITHOUT_CLASSIFICATION	 getname 
WITHOUT_CLASSIFICATION	 todo remove hive 
WITHOUT_CLASSIFICATION	 make sure this isnt one the partitioning columns thats not supported 
WITHOUT_CLASSIFICATION	 output get the evaluate method 
WITHOUT_CLASSIFICATION	 second connection should not able see the table 
WITHOUT_CLASSIFICATION	 result 
WITHOUT_CLASSIFICATION	 get all locks for particular object 
WITHOUT_CLASSIFICATION	 cleanup thread 
WITHOUT_CLASSIFICATION	 the client has wait and retry 
WITHOUT_CLASSIFICATION	 shared all session functions 
WITHOUT_CLASSIFICATION	 after each test 
WITHOUT_CLASSIFICATION	 accurate long value cannot obtained 
WITHOUT_CLASSIFICATION	 this acid format always read recursively regardless what the jobconf says 
WITHOUT_CLASSIFICATION	 table empty can only lock the table 
WITHOUT_CLASSIFICATION	 this basically means stop has been called 
WITHOUT_CLASSIFICATION	 string comparisons 
WITHOUT_CLASSIFICATION	 recurse over all the source tables 
WITHOUT_CLASSIFICATION	 read just the first column 
WITHOUT_CLASSIFICATION	 first row the process should only started necessary may conflict with some 
WITHOUT_CLASSIFICATION	 convert long string the string output into the argument byte array beginning character the length returned 
WITHOUT_CLASSIFICATION	 execute another query 
WITHOUT_CLASSIFICATION	 check the output fixacidkeyindex should indicate nothing required fixing 
WITHOUT_CLASSIFICATION	 the output partial aggregation struct containing long count two double averages and double covariance 
WITHOUT_CLASSIFICATION	 operator stack the dispatcher generates the plan from the operator tree 
WITHOUT_CLASSIFICATION	 replace default keystore with keystore for wwwexamplecom 
WITHOUT_CLASSIFICATION	 get the abortedwriteids which are already sorted ascending order 
WITHOUT_CLASSIFICATION	 validation methods 
WITHOUT_CLASSIFICATION	 not need this format accessor using objectnode this candidate for removal well 
WITHOUT_CLASSIFICATION	 set auth privileges 
WITHOUT_CLASSIFICATION	 update partition schema have fields 
WITHOUT_CLASSIFICATION	 the varchar type info need set prior initialization and must preserved when the plan serialized other processes 
WITHOUT_CLASSIFICATION	 hive vars 
WITHOUT_CLASSIFICATION	 since were reusing the compiled plan need update its start time for current run 
WITHOUT_CLASSIFICATION	 imetastoreclient needed access token store dbtokenstore used will got via hivegetconfgetmsc thread where the called avoid the cyclic reference pass the hive class dbtokenstore where used get threadlocal hive object with synchronized metastoreclient using java reflection note there will two lifelong opened mscs one stored thread local hive object the other daemon thread spawned remove expired tokens 
WITHOUT_CLASSIFICATION	 create some delta directories 
WITHOUT_CLASSIFICATION	 the implementation hcatfieldschema bit messy since with the addition parametrized types char need represent something richer than enum but for backwards compatibility and effort required full refactoring this class has both type and typeinfosimilarly for 
WITHOUT_CLASSIFICATION	 success but with nothing return can return empty list 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 remove and sel introduced enforce bucketingsorting config 
WITHOUT_CLASSIFICATION	 addconstraint event 
WITHOUT_CLASSIFICATION	 our preliminary mapping wont work out well handle that below 
WITHOUT_CLASSIFICATION	 assume that putlock throws the middle its treat buffers not being locked and blindly deallocate them since they are not going used therefore dont remove them from the cleanup list will after sending consumer this relies sequence calls cachefiledata and sendecb 
WITHOUT_CLASSIFICATION	 capacity left node the next task should allocated node after times out 
WITHOUT_CLASSIFICATION	 guava stores the hashcodes little endian order 
WITHOUT_CLASSIFICATION	 resolve column expression input expression using expression mapping current operator 
WITHOUT_CLASSIFICATION	 optimization copied from bigdecimal 
WITHOUT_CLASSIFICATION	 root operator union can happen reducers 
WITHOUT_CLASSIFICATION	 udfyear 
WITHOUT_CLASSIFICATION	 remove trailing empty splits 
WITHOUT_CLASSIFICATION	 right trim and truncate slice byte array maximum number characters and return the new byte length 
WITHOUT_CLASSIFICATION	 create partitioned table 
WITHOUT_CLASSIFICATION	 build the new predicate and return 
WITHOUT_CLASSIFICATION	 the dummy option should not have made either only options 
WITHOUT_CLASSIFICATION	 try transform possible candidates 
WITHOUT_CLASSIFICATION	 for executors 
WITHOUT_CLASSIFICATION	 the error message changed for then need modification getnextnotification 
WITHOUT_CLASSIFICATION	 decimal noninteger conversion 
WITHOUT_CLASSIFICATION	 query hints 
WITHOUT_CLASSIFICATION	 populating the empty string bytes putting static since should immutable and can shared 
WITHOUT_CLASSIFICATION	 next file the path 
WITHOUT_CLASSIFICATION	 byte bit patterns the form xxxxxx are continuation bytes all other bit patterns are the first byte character 
WITHOUT_CLASSIFICATION	 unique rows 
WITHOUT_CLASSIFICATION	 instantiate driver compile the query passed this udf running part existing query which may already using the sessionstate txnmanager this new driver also tries use the same txnmanager then this may mess the existing state the txnmanager initialize the new driver with new txnmanager that does not use the 
WITHOUT_CLASSIFICATION	 shared plan utils for tez 
WITHOUT_CLASSIFICATION	 sometimes rowschema empty fetch stats columns exprmap 
WITHOUT_CLASSIFICATION	 batchindex 
WITHOUT_CLASSIFICATION	 the writehwm under txnhwm 
WITHOUT_CLASSIFICATION	 think this wrong the alter table statement should come the table topic not the 
WITHOUT_CLASSIFICATION	 write partitioninfo into output 
WITHOUT_CLASSIFICATION	 return the mockinstances connector 
WITHOUT_CLASSIFICATION	 make sure works with nothing expire 
WITHOUT_CLASSIFICATION	 reach here succeed 
WITHOUT_CLASSIFICATION	 this tests the case where older data has ambiguous structure but the correct interpretation can determined from the repeated name 
WITHOUT_CLASSIFICATION	 for current query 
WITHOUT_CLASSIFICATION	 the rhs table columns should not output from the join 
WITHOUT_CLASSIFICATION	 copy across file system encryption zones 
WITHOUT_CLASSIFICATION	 write many records because sometimes the recordwriter for the format test behaves different with one record than bunch records 
WITHOUT_CLASSIFICATION	 groupby query 
WITHOUT_CLASSIFICATION	 add log links and other diagnostics from yarn service 
WITHOUT_CLASSIFICATION	 the following two are used for join processing 
WITHOUT_CLASSIFICATION	 also reading beyond our byte range produces null 
WITHOUT_CLASSIFICATION	 for acid nonbucketed case the filenames have the format consistent with insertupdatedelete ops like copy etc the extension only maintained for files which are compressed 
WITHOUT_CLASSIFICATION	 this nested sql script then flatten 
WITHOUT_CLASSIFICATION	 generate groupbyoperator 
WITHOUT_CLASSIFICATION	 indicates request has completed node 
WITHOUT_CLASSIFICATION	 with local spark context all user sessions share the same spark context 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create clone the operator 
WITHOUT_CLASSIFICATION	 input data 
WITHOUT_CLASSIFICATION	 copy the src the destination and create local resource 
WITHOUT_CLASSIFICATION	 the jobtracker setting its initial value 
WITHOUT_CLASSIFICATION	 getrows will call estimaterowcount 
WITHOUT_CLASSIFICATION	 list configurations currently the list consists hadoop version and execution mode only 
WITHOUT_CLASSIFICATION	 the properties does not define any transactional properties return default type 
WITHOUT_CLASSIFICATION	 well wait for for node creation 
WITHOUT_CLASSIFICATION	 least single item project required 
WITHOUT_CLASSIFICATION	 get number partitions doing count partid 
WITHOUT_CLASSIFICATION	 test that existing exclusive table with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 need replace the dummy operators the work with the cloned ones 
WITHOUT_CLASSIFICATION	 assumes the reader count has been incremented automatically the results cache either lookup creating the cache entry 
WITHOUT_CLASSIFICATION	 assumes partitioned table 
WITHOUT_CLASSIFICATION	 just set really small lower bound 
WITHOUT_CLASSIFICATION	 admin check 
WITHOUT_CLASSIFICATION	 done grouping partitions within tabledir 
WITHOUT_CLASSIFICATION	 initialize the merge operators first 
WITHOUT_CLASSIFICATION	 this session bad dont allow reuse just convert normal get 
WITHOUT_CLASSIFICATION	 nulls repeating 
WITHOUT_CLASSIFICATION	 special handling for druid rules here otherwise planner will add druid rules with logical builder 
WITHOUT_CLASSIFICATION	 some the partitions miss stats 
WITHOUT_CLASSIFICATION	 know rowset has only one element 
WITHOUT_CLASSIFICATION	 note there are many different onsuccessonfailure callbacks floating around that this will probably called twice for the done state this given the sync 
WITHOUT_CLASSIFICATION	 import static import static 
WITHOUT_CLASSIFICATION	 gmt gmt americanewyork est 
WITHOUT_CLASSIFICATION	 symlink file contains first file from first dir and second file from second dir 
WITHOUT_CLASSIFICATION	 tblname 
WITHOUT_CLASSIFICATION	 decrement only element was removed 
WITHOUT_CLASSIFICATION	 child map join 
WITHOUT_CLASSIFICATION	 this many bytes are necessary store the reversed nanoseconds 
WITHOUT_CLASSIFICATION	 not order preserving 
WITHOUT_CLASSIFICATION	 gen tree from resolved parse tree 
WITHOUT_CLASSIFICATION	 pattern 
WITHOUT_CLASSIFICATION	 for auto reduce parallelism max reducers requested 
WITHOUT_CLASSIFICATION	 get locations again and make sure theyre the same 
WITHOUT_CLASSIFICATION	 statistics stored metastore 
WITHOUT_CLASSIFICATION	 optional sourcestateproto state 
WITHOUT_CLASSIFICATION	 can invalidate the entry now but calling removeentry requires write lock and may already have read lock taken now add entriestoremove delete later 
WITHOUT_CLASSIFICATION	 limit factor too big 
WITHOUT_CLASSIFICATION	 useexactbytes 
WITHOUT_CLASSIFICATION	 this going slow hold 
WITHOUT_CLASSIFICATION	 optional int vertexparallelism 
WITHOUT_CLASSIFICATION	 column name the second group from current match 
WITHOUT_CLASSIFICATION	 return how the list columns passed match return nomatch either the list empty null there mismatch for and return nomatch return completematch both the lists are nonempty and are same return prefixcolmatch list strict subset list and return prefixcolmatch list strict subset list for and return completematch prefixcolmatch and prefixcolmatch respectively 
WITHOUT_CLASSIFICATION	 there could some spilled partitions which needs cleaned 
WITHOUT_CLASSIFICATION	 require delete privilege this insertoverwrite 
WITHOUT_CLASSIFICATION	 testparam 
WITHOUT_CLASSIFICATION	 merge join into multijoin operators possible 
WITHOUT_CLASSIFICATION	 create walker which walks the tree dfs manner while maintaining the operator stack the dispatcher generates the plan from the operator tree 
WITHOUT_CLASSIFICATION	 the partition does not have partition level privilege table level 
WITHOUT_CLASSIFICATION	 copy all the properties 
WITHOUT_CLASSIFICATION	 the cache entry has just been invalidated need for the scheduled invalidation 
WITHOUT_CLASSIFICATION	 also set this the thread contextclassloader new threads will inherit this class loader and propagate into newly created configurations those 
WITHOUT_CLASSIFICATION	 read configuration for the target path first from jobconf then from table properties 
WITHOUT_CLASSIFICATION	 empty list non partitioned 
WITHOUT_CLASSIFICATION	 mark the mapredwork and filesinkoperator for gathering stats 
WITHOUT_CLASSIFICATION	 allow numeric string 
WITHOUT_CLASSIFICATION	 valiade 
WITHOUT_CLASSIFICATION	 bother about generating schema only schema retriever class wasnt provided 
WITHOUT_CLASSIFICATION	 quarter granularity 
WITHOUT_CLASSIFICATION	 read should get rows immutable mutable 
WITHOUT_CLASSIFICATION	 prepare the bloom filter 
WITHOUT_CLASSIFICATION	 all rows qualify 
WITHOUT_CLASSIFICATION	 the sign the string for required because prepares arguments expecting sign will fail prepare the arguments correctly without the sign present 
WITHOUT_CLASSIFICATION	 now lets load this file into new hive table 
WITHOUT_CLASSIFICATION	 end readonlysublist 
WITHOUT_CLASSIFICATION	 initialize udf which will output the return type for the udf 
WITHOUT_CLASSIFICATION	 since this special cased when rewritten subqueryremoverule 
WITHOUT_CLASSIFICATION	 query the hive query string select from src associated with this set tasks logs 
WITHOUT_CLASSIFICATION	 ignore nondirectory files 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring javalangstring 
WITHOUT_CLASSIFICATION	 fkname 
WITHOUT_CLASSIFICATION	 populating the empty string bytes putting static since should immutable and can shared 
WITHOUT_CLASSIFICATION	 this member has information for data type conversion not defined there conversion 
WITHOUT_CLASSIFICATION	 the destination file exists for some reason delete 
WITHOUT_CLASSIFICATION	 for the file size check 
WITHOUT_CLASSIFICATION	 the reducer contains groupby which needs restored 
WITHOUT_CLASSIFICATION	 replace prs with crs and remove operator sequence from prs crs 
WITHOUT_CLASSIFICATION	 set periodic progress reporting case the udtf doesnt output rows 
WITHOUT_CLASSIFICATION	 swap the fields with the passed orcstruct 
WITHOUT_CLASSIFICATION	 noop for session killed 
WITHOUT_CLASSIFICATION	 first infer the type object 
WITHOUT_CLASSIFICATION	 nonconstant nonprimitive constants 
WITHOUT_CLASSIFICATION	 dont want set autocommit truefalse get mixed with set hivefoobar 
WITHOUT_CLASSIFICATION	 add jar current thread class loader dynamically and add jar paths jobconf spark may need load classes from this jar other threads 
WITHOUT_CLASSIFICATION	 pound statement ifelseendif 
WITHOUT_CLASSIFICATION	 construct one location map not exists 
WITHOUT_CLASSIFICATION	 findwriteslot slot slot tripleindex tripleindex empty 
WITHOUT_CLASSIFICATION	 some other operation progress using the same lock subsequent fragmentcomplete expected come 
WITHOUT_CLASSIFICATION	 but this tricky implement and well leave future work for now 
WITHOUT_CLASSIFICATION	 key key key 
WITHOUT_CLASSIFICATION	 slow way get the number decimal digits 
WITHOUT_CLASSIFICATION	 external table 
WITHOUT_CLASSIFICATION	 nothing two messages have canceled each other before could react 
WITHOUT_CLASSIFICATION	 previous row was for large bytes value use smallbuffer possible 
WITHOUT_CLASSIFICATION	 for use that the minimum equal key can advanced 
WITHOUT_CLASSIFICATION	 had the put succeeded for our new buffer would have refcount from put and from notifyreused call above old buffer now has the from put new buffer not cache releasebuffer will decref the buffer and also deallocate 
WITHOUT_CLASSIFICATION	 form key object array 
WITHOUT_CLASSIFICATION	 execute the udf 
WITHOUT_CLASSIFICATION	 after column not null but did not find 
WITHOUT_CLASSIFICATION	 assumes the lists are sorted 
WITHOUT_CLASSIFICATION	 change the current thread name include parent thread executed thread pool useful extract logs specific job request and helpful debug job issues 
WITHOUT_CLASSIFICATION	 subject list exceptions writeidlist not show above example 
WITHOUT_CLASSIFICATION	 dont bail failure try detail below 
WITHOUT_CLASSIFICATION	 tez needs its own scratch dir per session todo delink from sessionstate tezsession can linked different hive sessions via the pool 
WITHOUT_CLASSIFICATION	 common name constants for event messages 
WITHOUT_CLASSIFICATION	 used hashbased groupby mode hash partials 
WITHOUT_CLASSIFICATION	 even the cleanup throws some exception will continue 
WITHOUT_CLASSIFICATION	 the sql should completed now 
WITHOUT_CLASSIFICATION	 those top layer reducesinkoperators 
WITHOUT_CLASSIFICATION	 curr value becomes old and viceversa 
WITHOUT_CLASSIFICATION	 test repeating left 
WITHOUT_CLASSIFICATION	 none the partitions will dumped the partitions list was empty 
WITHOUT_CLASSIFICATION	 does vectorization use stripped char values 
WITHOUT_CLASSIFICATION	 this messagetype only has one optional field whose name mapcol original type map 
WITHOUT_CLASSIFICATION	 create enough elementconverters note have have separate elementconverter for each element because the elementconverters can reuse the internal object its not safe use the same elementconverter convert multiple elements 
WITHOUT_CLASSIFICATION	 remoteexception with will thrown the file currently held writer 
WITHOUT_CLASSIFICATION	 set new and verify get 
WITHOUT_CLASSIFICATION	 convert ast expr exprnode 
WITHOUT_CLASSIFICATION	 any name does not matter 
WITHOUT_CLASSIFICATION	 reduceside join use mrstyle shuffle 
WITHOUT_CLASSIFICATION	 insert overwrite create some invalid deltas and import into nonmm table 
WITHOUT_CLASSIFICATION	 check can process not the index distinct 
WITHOUT_CLASSIFICATION	 preserve partitioning and ordering 
WITHOUT_CLASSIFICATION	 row group position within stripe 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 read state 
WITHOUT_CLASSIFICATION	 sort the list requested 
WITHOUT_CLASSIFICATION	 passed but super slow 
WITHOUT_CLASSIFICATION	 required required required required 
WITHOUT_CLASSIFICATION	 clear gworkmap 
WITHOUT_CLASSIFICATION	 convert the table acid 
WITHOUT_CLASSIFICATION	 most the method got skipped but still need handle the duck 
WITHOUT_CLASSIFICATION	 setup serde 
WITHOUT_CLASSIFICATION	 type major since theres base yet 
WITHOUT_CLASSIFICATION	 populate other data structures 
WITHOUT_CLASSIFICATION	 call increasebufferspace will ensure that buffer points byte with sufficient space for the specified size 
WITHOUT_CLASSIFICATION	 create the dummy aggregation 
WITHOUT_CLASSIFICATION	 used give unique name each subquery currently there can most subqueries query the where clause and the having clause 
WITHOUT_CLASSIFICATION	 load bytescolumnvector copying large data enough force the buffer expand 
WITHOUT_CLASSIFICATION	 these are suffixes attached intermediate directory names used the 
WITHOUT_CLASSIFICATION	 shouldnt getting called hive but somehow does should just set all the configurations for input and output 
WITHOUT_CLASSIFICATION	 add keys reduce keys 
WITHOUT_CLASSIFICATION	 checktgt calls ugirelogin only after checking close tgt expiry hadoop relogin actually done only every minutes hadoop 
WITHOUT_CLASSIFICATION	 caches disabled nodes for quicker lookups and ensures request node which was skipped does not out order 
WITHOUT_CLASSIFICATION	 rowidoffset could all files before current one are empty 
WITHOUT_CLASSIFICATION	 since enforcing precision and scale can cause hivedecimal become null must read enforce here and either return null buffer the result 
WITHOUT_CLASSIFICATION	 send only the state has changed 
WITHOUT_CLASSIFICATION	 perf times 
WITHOUT_CLASSIFICATION	 mserdeinfo serdeinfo should same well 
WITHOUT_CLASSIFICATION	 now positions contains all the distinct positions need first sort them group set and then get their position later 
WITHOUT_CLASSIFICATION	 the operator tree till the sink operator has already been processed while fetching the next row fetch from the priority queue possibly containing multiple files the small table given file the big table now process the remaining tree look comments dummystoreoperator for additional explanation 
WITHOUT_CLASSIFICATION	 the event will not sent ats there are too many outstanding work submissions 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the avro deserializer would deserialize our object and return back list object that hive can operate here should getting the same object back 
WITHOUT_CLASSIFICATION	 susbset files for the partition are sufficient for the optimization 
WITHOUT_CLASSIFICATION	 note critical this here that logj reinitialized before any the other core hive classes are loaded 
WITHOUT_CLASSIFICATION	 nodemap registration 
WITHOUT_CLASSIFICATION	 create all the files this required because empty files need created for empty buckets 
WITHOUT_CLASSIFICATION	 end hiverelmdcostjava 
WITHOUT_CLASSIFICATION	 the only difference numeric types pick the method with the smallest overall numeric type 
WITHOUT_CLASSIFICATION	 empty batch will appear the end the stream 
WITHOUT_CLASSIFICATION	 this true then there data the batch have hit the end input 
WITHOUT_CLASSIFICATION	 now could get previous and next day figure our how many hours were inserted removed and from which the days etc but this point our gun pointing straight our foot 
WITHOUT_CLASSIFICATION	 valid range rangerows between preceding and preceding for preceding case 
WITHOUT_CLASSIFICATION	 add empty line 
WITHOUT_CLASSIFICATION	 all good 
WITHOUT_CLASSIFICATION	 binary 
WITHOUT_CLASSIFICATION	 interestingly decimal means decimal 
WITHOUT_CLASSIFICATION	 look for databases without pattern 
WITHOUT_CLASSIFICATION	 the sortmerger heap data structure that stores pair deleterecordkey deletereadervalue each node and ordered deleterecordkey the deletereadervalue the actual wrapper class that has the reference the underlying delta file that being read and its corresponding deleterecordkey the smallest record for that file each iteration this loop extractpoll the minimum deleterecordkey pair once have processed that deleterecordkey advance the pointer for the corresponding deletereadervalue the underlying file itself has more records then remove that pair from the heap else add the updated pair back the heap 
WITHOUT_CLASSIFICATION	 multikey specific imports 
WITHOUT_CLASSIFICATION	 strictly not required just for consistency 
WITHOUT_CLASSIFICATION	 new tai lue letter low kva bytes 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautilmap 
WITHOUT_CLASSIFICATION	 during the tests run with prealloc the logs windows systems prealloc was seen take seconds resulting test failure client timeout first session set env and directly order handle static initgc issues 
WITHOUT_CLASSIFICATION	 partialcount 
WITHOUT_CLASSIFICATION	 make decimal batch with three columns including two for inputs and one for the result 
WITHOUT_CLASSIFICATION	 this point weve found the fork the pipeline that has the pruning child plan 
WITHOUT_CLASSIFICATION	 create the default properties object 
WITHOUT_CLASSIFICATION	 cant set constructor due circular dependency 
WITHOUT_CLASSIFICATION	 get the distribute aliases these are aliased the entries the select list 
WITHOUT_CLASSIFICATION	 process location onebyone 
WITHOUT_CLASSIFICATION	 the planner seems pull this one out 
WITHOUT_CLASSIFICATION	 simple pattern hmsnnnnnnnnn 
WITHOUT_CLASSIFICATION	 signature and generate field expressions for those 
WITHOUT_CLASSIFICATION	 skip the transaction under evaluation already committed 
WITHOUT_CLASSIFICATION	 copy logintimeout from driver manager thrift timeout needs millis 
WITHOUT_CLASSIFICATION	 all evaluation should processed here for valid aliasfiltertags for mapjoin filter tag precalculated mapredlocaltask and stored with value 
WITHOUT_CLASSIFICATION	 the arg self 
WITHOUT_CLASSIFICATION	 both sides 
WITHOUT_CLASSIFICATION	 parse from creation metadata 
WITHOUT_CLASSIFICATION	 normal insert 
WITHOUT_CLASSIFICATION	 since key expression can calculation and the key will into scratch column need the mapping and type information 
WITHOUT_CLASSIFICATION	 make sure that not change anything there anything wrong 
WITHOUT_CLASSIFICATION	 set the correct last repl return the user 
WITHOUT_CLASSIFICATION	 expand and write result 
WITHOUT_CLASSIFICATION	 now insert the new buffer its place and restore heap property 
WITHOUT_CLASSIFICATION	 this case have scale before division otherwise might lose precision 
WITHOUT_CLASSIFICATION	 now try reuse with other sessions remaining should still work 
WITHOUT_CLASSIFICATION	 verifysetup set true all the test setup will perform additional verifications well which useful verify that our setup occurred correctly when developing and debugging tests these verifications however not test any new functionality for replication and thus are not relevant for testing replication itself for steady state want this false 
WITHOUT_CLASSIFICATION	 test that are cleaning aborted transactions with components left txncomponents put one aborted transaction with entry txncomponents make sure dont accidently clean too 
WITHOUT_CLASSIFICATION	 copy entire string value 
WITHOUT_CLASSIFICATION	 puts long little endian order 
WITHOUT_CLASSIFICATION	 this the only public constructor filesplit 
WITHOUT_CLASSIFICATION	 save prev val the key threadlocal 
WITHOUT_CLASSIFICATION	 persist the column statistics object the metastore note this function shared for both table and partition column stats 
WITHOUT_CLASSIFICATION	 create new the first time fire the rule 
WITHOUT_CLASSIFICATION	 subsequent instances when its taken off the queue 
WITHOUT_CLASSIFICATION	 first entry count 
WITHOUT_CLASSIFICATION	 age 
WITHOUT_CLASSIFICATION	 the sorting property not obeyed 
WITHOUT_CLASSIFICATION	 the code point exists deletion set need emit out anything for this code point 
WITHOUT_CLASSIFICATION	 create tables verify query 
WITHOUT_CLASSIFICATION	 was able execute something before the last blacklist reset the exponent 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 convert date value days 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 test that the whole things works when theres nothing the queue this just survival test 
WITHOUT_CLASSIFICATION	 need propagate this the responder 
WITHOUT_CLASSIFICATION	 there distinctfuncexp add all parameters the reducekeys 
WITHOUT_CLASSIFICATION	 validate all integer type values are stored correctly 
WITHOUT_CLASSIFICATION	 output record readers 
WITHOUT_CLASSIFICATION	 regen plan from optimized ast 
WITHOUT_CLASSIFICATION	 for each aggregation 
WITHOUT_CLASSIFICATION	 array full 
WITHOUT_CLASSIFICATION	 have just removed the session from the same pool dont check concurrency here 
WITHOUT_CLASSIFICATION	 field node get amyfield from 
WITHOUT_CLASSIFICATION	 add new synthetic columns for projections not provided select 
WITHOUT_CLASSIFICATION	 aggregate does not change input ordering corvars will 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 restore broken links between operators and remove the branch from the original tree 
WITHOUT_CLASSIFICATION	 compile internal will automatically reset the perf logger 
WITHOUT_CLASSIFICATION	 dump and load only second insert records 
WITHOUT_CLASSIFICATION	 here may checking level lock against table level lock alternatively could have used intention locks for example request for lock table would cause lock that contains the table similarly partition level 
WITHOUT_CLASSIFICATION	 and not 
WITHOUT_CLASSIFICATION	 todo handle quoted tablenames 
WITHOUT_CLASSIFICATION	 zero and above numbers indicate big table key needed for small table result area 
WITHOUT_CLASSIFICATION	 todo 
WITHOUT_CLASSIFICATION	 traversing origin find exprnodedesc sources and replaces with exprnodedesc targets having same index 
WITHOUT_CLASSIFICATION	 equivalent aliases for the column 
WITHOUT_CLASSIFICATION	 see can load all the delete events from all the delete deltas memory 
WITHOUT_CLASSIFICATION	 from key and value tabledesc 
WITHOUT_CLASSIFICATION	 float min and max 
WITHOUT_CLASSIFICATION	 case cross join disable hybrid grace hash join 
WITHOUT_CLASSIFICATION	 columnstatisticsobj with info about its table partition table partitioned 
WITHOUT_CLASSIFICATION	 write the terminating null byte 
WITHOUT_CLASSIFICATION	 means index doesnt exist 
WITHOUT_CLASSIFICATION	 cast 
WITHOUT_CLASSIFICATION	 overflowbatchsize overflow 
WITHOUT_CLASSIFICATION	 extract the hex digits num into value from right left 
WITHOUT_CLASSIFICATION	 explicitly remove the setting lastreplid from the object parameters loadtask going run multiple times and explicit logic place which prevents updates tables when level last repl set and create alterdatabasetask the end processing database 
WITHOUT_CLASSIFICATION	 rewrite into query tokquery tokfrom join tokinsert tokdestination tokdir toktmpfile tokselect 
WITHOUT_CLASSIFICATION	 need fix create the two replacement project 
WITHOUT_CLASSIFICATION	 all tests are identical the other seek tests 
WITHOUT_CLASSIFICATION	 batches will sized 
WITHOUT_CLASSIFICATION	 reuse the reencoder evolved schema create and store new encoder the map for reuse 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 try with chunked streams 
WITHOUT_CLASSIFICATION	 keep asis 
WITHOUT_CLASSIFICATION	 todo this boolean flag set only stats annotation this point 
WITHOUT_CLASSIFICATION	 table locks for this 
WITHOUT_CLASSIFICATION	 the operators specified depth and removed from the tree 
WITHOUT_CLASSIFICATION	 key out range for whole hash table 
WITHOUT_CLASSIFICATION	 now different from 
WITHOUT_CLASSIFICATION	 have mindful order during filtering are not returning all partitions 
WITHOUT_CLASSIFICATION	 add hive function names for functions that arent infix operators add open 
WITHOUT_CLASSIFICATION	 however these expressions should not considered valid expressions for separation 
WITHOUT_CLASSIFICATION	 fall through acquire 
WITHOUT_CLASSIFICATION	 always reschedule the next callable irrespective task count case new tasks come later 
WITHOUT_CLASSIFICATION	 since user names need valid unix user names per ieee std they cannot contain comma can safely split above string comma 
WITHOUT_CLASSIFICATION	 for all practical purposes code point fancy name for character java char data type can store characters that require bits less however the unicode specification has changed allow for characters whose representation requires more than bits therefore need represent each character called code point from hereon int more details 
WITHOUT_CLASSIFICATION	 this map defines the progression casts numeric types 
WITHOUT_CLASSIFICATION	 look for tables without pattern 
WITHOUT_CLASSIFICATION	 columns are output from the join from the different reduce sinks the order their 
WITHOUT_CLASSIFICATION	 leading space significant 
WITHOUT_CLASSIFICATION	 this appears leave the remove transaction inconsistent state but the heartbeat now cancelled and will eventually time out 
WITHOUT_CLASSIFICATION	 column name not contained needed column list then partition column not need evaluate partition columns filter expression since will taken care partitio pruner 
WITHOUT_CLASSIFICATION	 only single subquery expr supported 
WITHOUT_CLASSIFICATION	 char and varchar types can specified with maximum length 
WITHOUT_CLASSIFICATION	 close the existing ctx etc before compiling new query but does not destroy driver 
WITHOUT_CLASSIFICATION	 ideally should just call here but that wont work since needs job object instead jobcontext which are handed here 
WITHOUT_CLASSIFICATION	 trimfalse 
WITHOUT_CLASSIFICATION	 have checked all the parents for the index position 
WITHOUT_CLASSIFICATION	 note get query here rather than the caller where would more correct because know which exact query intend kill this valid because are not expecting query change never reuse the session for which 
WITHOUT_CLASSIFICATION	 calculate the std result when count public vectorization code can use etc 
WITHOUT_CLASSIFICATION	 make sure aborted txns dont redflag basexxxx hive 
WITHOUT_CLASSIFICATION	 transformation outer query left join inner query correlated predicate 
WITHOUT_CLASSIFICATION	 make ssl connection 
WITHOUT_CLASSIFICATION	 previous batch was the last group batches remember the next the first batch new group batches 
WITHOUT_CLASSIFICATION	 nothing because and and and not supports null value evaluation note the future all udfs that treats null value unknown both parameters and return values should derive from common base class udfnullasunknown instead listing the classes here would test whether class derived from that base class all childs are null set unknown true 
WITHOUT_CLASSIFICATION	 unixtimestampargs tounixtimestampargs 
WITHOUT_CLASSIFICATION	 druid only support appending more partitions linear and numbered shardspecs 
WITHOUT_CLASSIFICATION	 first check the two table scan operators can actually merged 
WITHOUT_CLASSIFICATION	 create empty output object which will populated when convert invoked 
WITHOUT_CLASSIFICATION	 iterative through the children dfs manner see there more than table alias 
WITHOUT_CLASSIFICATION	 obtain list col stats use default they are not available 
WITHOUT_CLASSIFICATION	 position doesnt make sense for async reader chunk order arbitrary 
WITHOUT_CLASSIFICATION	 preallocated member for remembering the big tables selected array the beginning the process method before applying any filter for outer join need remember which rows did not match since they will appear the outer join result with nulls for the 
WITHOUT_CLASSIFICATION	 validate that can add partition without escaping escaping was originally intended avoid creating invalid hdfs paths however escape the hdfs path that deem invalid but hdfs actually supports possible create hdfs paths with unprintable characters like ascii metastore will create another directory instead the one are trying repair here 
WITHOUT_CLASSIFICATION	 should the same the moveworks sourcedir 
WITHOUT_CLASSIFICATION	 there need for the user specify mapjoin for 
WITHOUT_CLASSIFICATION	 compaction doesnt work under transaction and hence pass for current txn 
WITHOUT_CLASSIFICATION	 release initial refcounts 
WITHOUT_CLASSIFICATION	 the registers 
WITHOUT_CLASSIFICATION	 check the specified partitions 
WITHOUT_CLASSIFICATION	 statsdesc 
WITHOUT_CLASSIFICATION	 use when merging variance and partialcount and mergecount note mergecount and mergesum not include partialcount and partialsum yet 
WITHOUT_CLASSIFICATION	 evaluate then expression only and copy all its results 
WITHOUT_CLASSIFICATION	 use loadtask 
WITHOUT_CLASSIFICATION	 theres fraction part return immediately avoid the cost divide 
WITHOUT_CLASSIFICATION	 either the slice comes entirely after the end split following gap cached data the split ends the middle the slice its the same the startix logic wrt the partial match either dont want cannot use this theres need distinguish these two cases for now 
WITHOUT_CLASSIFICATION	 random 
WITHOUT_CLASSIFICATION	 semanticanalyzer 
WITHOUT_CLASSIFICATION	 this regex bit lax order compensate for lack any escaping done amazon for example useragent string can have double quotes 
WITHOUT_CLASSIFICATION	 replace the filter expression reference output the join 
WITHOUT_CLASSIFICATION	 java primitive type 
WITHOUT_CLASSIFICATION	 skipping columns since partition level field schemas are the same table levels skipping partition keys since the same table level partition keys 
WITHOUT_CLASSIFICATION	 gettable invoked after fetching the table names 
WITHOUT_CLASSIFICATION	 add child project rel needed generate output input sel rel 
WITHOUT_CLASSIFICATION	 create the new rowschema for the projected column 
WITHOUT_CLASSIFICATION	 create dummy partitions 
WITHOUT_CLASSIFICATION	 child the name the column 
WITHOUT_CLASSIFICATION	 full decimal maximum digits lower longs digits here 
WITHOUT_CLASSIFICATION	 use this copy method when the source batch safe and will remain around until the target batch finished any bytes column vector values will referenced the target column instead copying 
WITHOUT_CLASSIFICATION	 finally submit the job 
WITHOUT_CLASSIFICATION	 read the first characters from the url 
WITHOUT_CLASSIFICATION	 and initiates the sasl handshake 
WITHOUT_CLASSIFICATION	 convert decimal into the scratch buffer without allocating byte each time for better performance 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 two way left outer right outer join take selectivity only for 
WITHOUT_CLASSIFICATION	 column projection 
WITHOUT_CLASSIFICATION	 mysql returns the string not wellformed numeric value but decided return null instead which more conservative 
WITHOUT_CLASSIFICATION	 repl status 
WITHOUT_CLASSIFICATION	 gby operator the operator 
WITHOUT_CLASSIFICATION	 hash map overhead 
WITHOUT_CLASSIFICATION	 scratch dir initially 
WITHOUT_CLASSIFICATION	 run the worker explicitly order get the reference the compactor job 
WITHOUT_CLASSIFICATION	 now start with everything and test losing stuff 
WITHOUT_CLASSIFICATION	 default assume can user directsql thats kind the point 
WITHOUT_CLASSIFICATION	 only teztask sets this and then removes when done dont expect see 
WITHOUT_CLASSIFICATION	 incorrect precision expected xxxxx yyy but was xxxxx yyy 
WITHOUT_CLASSIFICATION	 verify when third argument repeating 
WITHOUT_CLASSIFICATION	 metrics system will get this via reflection 
WITHOUT_CLASSIFICATION	 like type 
WITHOUT_CLASSIFICATION	 dont clear the hash table reuse possible will take care 
WITHOUT_CLASSIFICATION	 same but repeating value null 
WITHOUT_CLASSIFICATION	 should get back the latest reader schema 
WITHOUT_CLASSIFICATION	 drop table without saving trash setting the purge option 
WITHOUT_CLASSIFICATION	 success 
WITHOUT_CLASSIFICATION	 the cannot inlined need the hasnext evaluated post the current retrieved 
WITHOUT_CLASSIFICATION	 test for null partition value map 
WITHOUT_CLASSIFICATION	 clone make sure new prop doesnt leak 
WITHOUT_CLASSIFICATION	 put the query user not llap user into the message and token 
WITHOUT_CLASSIFICATION	 got error attempting ssclose then its not likely that sserr valid were back systemerr also dont change the return code simply log warning and return whatever return code expected already 
WITHOUT_CLASSIFICATION	 following sequence 
WITHOUT_CLASSIFICATION	 note the only sane case where this can happen the nonpool one should get rid nonpool case perf doesnt matter might well open get time and then call update like the else can happen the user sets the tez flag after the session was established 
WITHOUT_CLASSIFICATION	 tried all back original code for error message 
WITHOUT_CLASSIFICATION	 parents arent llap neither should the child 
WITHOUT_CLASSIFICATION	 will update current number open txns back 
WITHOUT_CLASSIFICATION	 omitting zone time part allowed 
WITHOUT_CLASSIFICATION	 conflicting operations proceed with the rest commit sequence 
WITHOUT_CLASSIFICATION	 returns false there selectexpr that not constant aggr 
WITHOUT_CLASSIFICATION	 the subquery 
WITHOUT_CLASSIFICATION	 for backward compatibility 
WITHOUT_CLASSIFICATION	 over all the keys and get the size the fields fixed length keep 
WITHOUT_CLASSIFICATION	 nonjavadoc this provides lazyshort like class which can initialized from data stored binary format see int int 
WITHOUT_CLASSIFICATION	 because should impossible get incompatible outputs 
WITHOUT_CLASSIFICATION	 collect columns copy from the big table batch the overflow batch 
WITHOUT_CLASSIFICATION	 escaping happened need copy bytebybyte set the length first 
WITHOUT_CLASSIFICATION	 will used estimate num nulls 
WITHOUT_CLASSIFICATION	 test for null input strings 
WITHOUT_CLASSIFICATION	 self describing need send column info per partition since its not used anyway 
WITHOUT_CLASSIFICATION	 dont emit usertimestamp info test mode that the test golden output file fixed 
WITHOUT_CLASSIFICATION	 list sparkworkdependency 
WITHOUT_CLASSIFICATION	 could not renewed return that information 
WITHOUT_CLASSIFICATION	 map that keeps track the last operator task the following work 
WITHOUT_CLASSIFICATION	 test that not changing the database and the function name but only other parameters like 
WITHOUT_CLASSIFICATION	 reset table params 
WITHOUT_CLASSIFICATION	 deep copy expr node desc 
WITHOUT_CLASSIFICATION	 use not private because the copyonwrite irrelevant for deleted file 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 virtual trailing zeroes 
WITHOUT_CLASSIFICATION	 nothing cleanup 
WITHOUT_CLASSIFICATION	 then scale with 
WITHOUT_CLASSIFICATION	 through the reduce keys and find the matching columns the reduce values 
WITHOUT_CLASSIFICATION	 nesting not allowed 
WITHOUT_CLASSIFICATION	 add key key slot slot pairindex pairindex found key 
WITHOUT_CLASSIFICATION	 for debug tracing information about the map reduce task operator operator class etc 
WITHOUT_CLASSIFICATION	 special handling 
WITHOUT_CLASSIFICATION	 create walker which walks the tree dfs manner while maintaining 
WITHOUT_CLASSIFICATION	 cartesian product not supported strict mode 
WITHOUT_CLASSIFICATION	 the expected tags from the parent operators see processop before 
WITHOUT_CLASSIFICATION	 mimicking behaviour createtabledesc tabledesc creation returning null table description for output 
WITHOUT_CLASSIFICATION	 nonjavadoc see int javalangstring 
WITHOUT_CLASSIFICATION	 call lhs and rhs and and join the results with optype string 
WITHOUT_CLASSIFICATION	 process multikey outer join vectorized row batch 
WITHOUT_CLASSIFICATION	 there should only one mrinput 
WITHOUT_CLASSIFICATION	 then check distinct key 
WITHOUT_CLASSIFICATION	 initialize deleteeventwriter not yet done lazy initialization 
WITHOUT_CLASSIFICATION	 dont log exception here 
WITHOUT_CLASSIFICATION	 the root might have changed because tree modifications compute the new root for this tree and set the aststr 
WITHOUT_CLASSIFICATION	 apply rest the configuration only hiveserver 
WITHOUT_CLASSIFICATION	 cant divide null 
WITHOUT_CLASSIFICATION	 set java key provider for encrypted hdfs cluster 
WITHOUT_CLASSIFICATION	 try get default value only this default constraint 
WITHOUT_CLASSIFICATION	 the header look for use xxsrfheader this null methods not filter default getoptionsheadtrace null 
WITHOUT_CLASSIFICATION	 vectormapoperator 
WITHOUT_CLASSIFICATION	 print header vertices status total completed running pending failed killed 
WITHOUT_CLASSIFICATION	 the eventual goal monitor the progress all the tasks not only the map reduce task the execute method the tasks will return immediately and return task specific handle monitor the progress that task right now the behavior kind broken execdrivers execute method calls progress instead should invoked driver 
WITHOUT_CLASSIFICATION	 accumulo ranges 
WITHOUT_CLASSIFICATION	 convert the join operator bucket mapjoin join operator 
WITHOUT_CLASSIFICATION	 common comparison class for charvarchar string 
WITHOUT_CLASSIFICATION	 skip the same value avgdistinct true 
WITHOUT_CLASSIFICATION	 can safely convert the join map join 
WITHOUT_CLASSIFICATION	 will retrieve stats from the metastore only for columns that are not cached 
WITHOUT_CLASSIFICATION	 could make some assumptions given how the reader currently does the work consecutive chunks etc blocks and columns stored offset order the lists but wont just save all the chunk boundaries and lengths for now 
WITHOUT_CLASSIFICATION	 assert that there one partition present and had hcat instrumentation inserted when was created 
WITHOUT_CLASSIFICATION	 source operator get the number entries 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 step replace the corresponding part childmrworks mapwork 
WITHOUT_CLASSIFICATION	 transaction batch size case 
WITHOUT_CLASSIFICATION	 repeat the same check for droptable 
WITHOUT_CLASSIFICATION	 table exists 
WITHOUT_CLASSIFICATION	 should not get here 
WITHOUT_CLASSIFICATION	 conditions 
WITHOUT_CLASSIFICATION	 setref used below and this safe because the reference data owned this column vector this column vector gets reused the whole thing reused together there danger dangling reference 
WITHOUT_CLASSIFICATION	 form result from lower middle and middle words 
WITHOUT_CLASSIFICATION	 the new base dir now has two bucket files since the delta dir has two bucket files 
WITHOUT_CLASSIFICATION	 proceed only wed actually succeeded anyway otherwise 
WITHOUT_CLASSIFICATION	 testparam 
WITHOUT_CLASSIFICATION	 excludedprovidedby framework excludedconfigured 
WITHOUT_CLASSIFICATION	 hcat output format related errors 
WITHOUT_CLASSIFICATION	 indicate last batch current group 
WITHOUT_CLASSIFICATION	 helper function retrieve the basename local resource 
WITHOUT_CLASSIFICATION	 since currentreadblock may assigned currentwriteblock need store 
WITHOUT_CLASSIFICATION	 this will throw expected exception since client communicating with the wrong http service endpoint 
WITHOUT_CLASSIFICATION	 throw hiveexception the tablepartition archived 
WITHOUT_CLASSIFICATION	 find out database name and table name target table 
WITHOUT_CLASSIFICATION	 need some value that indicates null 
WITHOUT_CLASSIFICATION	 done have bytes continue reading this buffer 
WITHOUT_CLASSIFICATION	 update old data with values for the new schema columns 
WITHOUT_CLASSIFICATION	 get the sort order 
WITHOUT_CLASSIFICATION	 are just relay send unpause encoded data producer 
WITHOUT_CLASSIFICATION	 the tablescanoperators needed columns are just the data columns 
WITHOUT_CLASSIFICATION	 for nonacid tables paths all data files are getoriginalfiles list 
WITHOUT_CLASSIFICATION	 verify found them all 
WITHOUT_CLASSIFICATION	 close output stream open 
WITHOUT_CLASSIFICATION	 tokalterviewas 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the api that finds the jar being used this class disk 
WITHOUT_CLASSIFICATION	 number columns pertaining keys vectorized row batch 
WITHOUT_CLASSIFICATION	 because every value will null 
WITHOUT_CLASSIFICATION	 inputoutput settings 
WITHOUT_CLASSIFICATION	 cte 
WITHOUT_CLASSIFICATION	 that will not too far from the correct digit later 
WITHOUT_CLASSIFICATION	 infovalue 
WITHOUT_CLASSIFICATION	 doublecheck the header under lock 
WITHOUT_CLASSIFICATION	 now add the corvars from the input starting from position oldgroupkeycount 
WITHOUT_CLASSIFICATION	 use junits assume skip running this fixture against any storage formats whose serde the disabled serdes list 
WITHOUT_CLASSIFICATION	 hive has max limit for strings 
WITHOUT_CLASSIFICATION	 for snapshot isolation dont care about txns greater than current txn and stop here also need not include current txn exceptions list 
WITHOUT_CLASSIFICATION	 have emulate distinct otherwise tables with the same name may returned 
WITHOUT_CLASSIFICATION	 true insert overwrite 
WITHOUT_CLASSIFICATION	 able report progress 
WITHOUT_CLASSIFICATION	 dont use assertfail are catching assertion errors 
WITHOUT_CLASSIFICATION	 the object that determines equal key series 
WITHOUT_CLASSIFICATION	 collect column access information 
WITHOUT_CLASSIFICATION	 decide whether this already hashmap keys hashmap are deepcopied version and need use 
WITHOUT_CLASSIFICATION	 task lock but acquires lock the scheduler 
WITHOUT_CLASSIFICATION	 noop 
WITHOUT_CLASSIFICATION	 this internal error something odd happened with reflection log and dont output the bean 
WITHOUT_CLASSIFICATION	 routines for copying between 
WITHOUT_CLASSIFICATION	 there are fewer than leadamt values leadwindow start reading from the first position otherwise the window starts from nextposinwindow 
WITHOUT_CLASSIFICATION	 this point have seen the exponent letter and have decimal information isnegative precision integerdigitcount and fast fast fast after determine the exponent will appropriate scaling and fill fastresult 
WITHOUT_CLASSIFICATION	 map 
WITHOUT_CLASSIFICATION	 create the destination does not exist 
WITHOUT_CLASSIFICATION	 special handling for timezone 
WITHOUT_CLASSIFICATION	 lazybinary seems work better with row object array instead java object 
WITHOUT_CLASSIFICATION	 dealing with views 
WITHOUT_CLASSIFICATION	 maybe valid too expensive check without parse 
WITHOUT_CLASSIFICATION	 calculate relative offset 
WITHOUT_CLASSIFICATION	 rows are combination the ondisk hashmap and the sidefile 
WITHOUT_CLASSIFICATION	 the vertex that this operator belongs 
WITHOUT_CLASSIFICATION	 funcname 
WITHOUT_CLASSIFICATION	 since dont have nonnative passthru version vectorptfoperator not have enableconditionsmet like have for etc 
WITHOUT_CLASSIFICATION	 stateful 
WITHOUT_CLASSIFICATION	 check query results cache the case that row column maskingfiltering was required not support caching 
WITHOUT_CLASSIFICATION	 handle the case like sumlagf over aggregation function includes laglead call 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 build not null conditions 
WITHOUT_CLASSIFICATION	 obtain col stats for partitioned table 
WITHOUT_CLASSIFICATION	 check access columns from columnaccessinfo 
WITHOUT_CLASSIFICATION	 such abc 
WITHOUT_CLASSIFICATION	 the key wasnt present the mapping and the function didnt return default value ignore and use our default 
WITHOUT_CLASSIFICATION	 binaryval 
WITHOUT_CLASSIFICATION	 wrapper extends qlmetadatapartition for easy construction syntax 
WITHOUT_CLASSIFICATION	 subquery either where lhs subquery form where exists subquery form first case lhs should not bypassed 
WITHOUT_CLASSIFICATION	 this just for debug 
WITHOUT_CLASSIFICATION	 local path doesnt depend drone variables 
WITHOUT_CLASSIFICATION	 insert overwrite acid table from source table 
WITHOUT_CLASSIFICATION	 could not have removed the pool for this session would have canceled the init 
WITHOUT_CLASSIFICATION	 query 
WITHOUT_CLASSIFICATION	 this table cannot big table 
WITHOUT_CLASSIFICATION	 map splits map splits 
WITHOUT_CLASSIFICATION	 copy jar dfs 
WITHOUT_CLASSIFICATION	 visiblefortesting 
WITHOUT_CLASSIFICATION	 test that jdbc does not allow shell commands starting with 
WITHOUT_CLASSIFICATION	 the join keys matches the skewed keys use the table skewed keys 
WITHOUT_CLASSIFICATION	 the first byte the vint the vint itself indicating that there second vint but the nanoseconds field actually 
WITHOUT_CLASSIFICATION	 this test the parameter value denotes the method which needs throw error 
WITHOUT_CLASSIFICATION	 first read the header due orc estimates zcr etc this can complex 
WITHOUT_CLASSIFICATION	 you change this function remove the ignore from test these changes mysql and mssql use the state code for rollback postgres uses and oracle seems return different sqlstates and messages each time ive tried capture the different error messages there appear fewer different error messages than sql states derby and newer mysql driver use the new 
WITHOUT_CLASSIFICATION	 now reinitialize batch simulate batchobject reuse 
WITHOUT_CLASSIFICATION	 remove all detached objects from the cache since the transaction being rolled back they are longer relevant and this prevents them from reattaching future transactions 
WITHOUT_CLASSIFICATION	 were pretty screwed cant load the default conf vars 
WITHOUT_CLASSIFICATION	 current hashmap use 
WITHOUT_CLASSIFICATION	 call here because this point the has been set 
WITHOUT_CLASSIFICATION	 start tests that check values from pig that are out range for target column 
WITHOUT_CLASSIFICATION	 form the expression node corresponding column 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 truncate reopening fileoutputstream 
WITHOUT_CLASSIFICATION	 first find the path searched 
WITHOUT_CLASSIFICATION	 update the database cache 
WITHOUT_CLASSIFICATION	 present the child hence add child project rel 
WITHOUT_CLASSIFICATION	 clusterby 
WITHOUT_CLASSIFICATION	 add signature 
WITHOUT_CLASSIFICATION	 singlecolumn string get key 
WITHOUT_CLASSIFICATION	 parse the string determine column level storage type for primitive types for variable length string format storage for fixed width binary storage bytes for table storage type which defaults utf string string data always stored the default escaped storage format the data types byte short int long float and double have binary byte oriented storage option 
WITHOUT_CLASSIFICATION	 append mode 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring javalangstring javalangstring javautillist 
WITHOUT_CLASSIFICATION	 right now they come from jpoxproperties 
WITHOUT_CLASSIFICATION	 template classnameprefix returntype funcname 
WITHOUT_CLASSIFICATION	 use only reducer for order 
WITHOUT_CLASSIFICATION	 this point dont have anything special this case just run through the regular paces creating new task 
WITHOUT_CLASSIFICATION	 connection above 
WITHOUT_CLASSIFICATION	 continue the next code point 
WITHOUT_CLASSIFICATION	 open the client transport 
WITHOUT_CLASSIFICATION	 euro sign bytes 
WITHOUT_CLASSIFICATION	 operator check uses struct 
WITHOUT_CLASSIFICATION	 restrictionm disallow nested subquery expressions 
WITHOUT_CLASSIFICATION	 can safely convert the join map join 
WITHOUT_CLASSIFICATION	 extract the partitions keys segments granularity and partition key any 
WITHOUT_CLASSIFICATION	 not use the new cache buffers for the actual read given the way read api therefore dont need handle cache collisions just decref all the buffers 
WITHOUT_CLASSIFICATION	 this happens case map join operations the tree looks like this are here perhaps mapjoin are the pointed above and may have already visited the following the have already generated work for the tsrs need hook the current work this generated work 
WITHOUT_CLASSIFICATION	 suppress useless evaluation 
WITHOUT_CLASSIFICATION	 kerberos 
WITHOUT_CLASSIFICATION	 there predicate partitioning column need all partitions this case 
WITHOUT_CLASSIFICATION	 driver not initialized 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 hiveserver using hiveconf this combine paths 
WITHOUT_CLASSIFICATION	 since hivedecimal now uses fasthivedecimal which stores decimal digits per long lets test edge conditions here 
WITHOUT_CLASSIFICATION	 walk through the projection list and replace the column names with the expressions from the original update under the tokselect see above the structure looks like tokselect tokselexpr expr tokselexpr expr 
WITHOUT_CLASSIFICATION	 clone the search ast apply all rewrites the clone 
WITHOUT_CLASSIFICATION	 how many times well sleep before giving 
WITHOUT_CLASSIFICATION	 bunch these are hivemetastoreclient but not imetastoreclient have marked these deprecated and not updated them for the catalogs really want support them should add them imetastoreclient 
WITHOUT_CLASSIFICATION	 select from src lateral view udtf mytable join src not supported instead the lateral view must subquery select from select from src lateral view udtf mytable join src 
WITHOUT_CLASSIFICATION	 load jars under the 
WITHOUT_CLASSIFICATION	 proper children the union 
WITHOUT_CLASSIFICATION	 change curr ops row resolvers tab aliases subq alias 
WITHOUT_CLASSIFICATION	 nullindicator after the transformation 
WITHOUT_CLASSIFICATION	 only seal those partitions that havent been spilled and cleared because once hashmap cleared will become unusable 
WITHOUT_CLASSIFICATION	 generate the hiveconfargs after potentially adding the jars 
WITHOUT_CLASSIFICATION	 the execution engine set the mapreduce env with the credential store password 
WITHOUT_CLASSIFICATION	 replicationspeckey scopekey 
WITHOUT_CLASSIFICATION	 table bigtables then its output big 
WITHOUT_CLASSIFICATION	 try fold key and key not null key where can note key and key not null cannot folded 
WITHOUT_CLASSIFICATION	 server thread pool 
WITHOUT_CLASSIFICATION	 process else statement 
WITHOUT_CLASSIFICATION	 bgenjjtree enumdeflist 
WITHOUT_CLASSIFICATION	 validate reserved values 
WITHOUT_CLASSIFICATION	 handle case with nulls dont function the value null because the data may undefined for null value 
WITHOUT_CLASSIFICATION	 this table not yet loaded cache the prewarm thread working this tables database lets move this table the top stack that gets loaded the cache faster and available for subsequent requests 
WITHOUT_CLASSIFICATION	 copy intervening noncrlf characters but not including current index 
WITHOUT_CLASSIFICATION	 stringsplit returns single empty result for splitting the empty 
WITHOUT_CLASSIFICATION	 alternate unused 
WITHOUT_CLASSIFICATION	 try get prime number table size have less dependence good hash function 
WITHOUT_CLASSIFICATION	 represents ptf invocation captures function name and alias the partitioning details about its input its arguments the astnodes representing the arguments are captured here reference its input 
WITHOUT_CLASSIFICATION	 remove from cache materialized view 
WITHOUT_CLASSIFICATION	 found remove and its child and connect its parent 
WITHOUT_CLASSIFICATION	 batches will sized 
WITHOUT_CLASSIFICATION	 run cleaner shouldnt impact anything 
WITHOUT_CLASSIFICATION	 for case conversion convert both values common type and then compare 
WITHOUT_CLASSIFICATION	 each partition maintains large properties 
WITHOUT_CLASSIFICATION	 since warehouse path nonqualified the table should located second filesystem 
WITHOUT_CLASSIFICATION	 sanity check for overlap with regions already being expanded 
WITHOUT_CLASSIFICATION	 construct temp table name 
WITHOUT_CLASSIFICATION	 subsequent hashes are used generate bits within block words 
WITHOUT_CLASSIFICATION	 because there only one for analyze statement can get 
WITHOUT_CLASSIFICATION	 only for live instances 
WITHOUT_CLASSIFICATION	 the tasks are not ready yet the task eligible for preemptable 
WITHOUT_CLASSIFICATION	 hive variables 
WITHOUT_CLASSIFICATION	 this can happen for numbers less than for rawprecision scale this case well set the type have the same precision the scale 
WITHOUT_CLASSIFICATION	 the percentage maximum allocated memory that triggers job tracker this could overridden thru the jobconf 
WITHOUT_CLASSIFICATION	 find the privileges that are looking for 
WITHOUT_CLASSIFICATION	 lazy binary value serializer 
WITHOUT_CLASSIFICATION	 test that existing sharedread with new exclusive coalesces 
WITHOUT_CLASSIFICATION	 since previously opened txn was killed 
WITHOUT_CLASSIFICATION	 may need update the conditional tasks list this happens when common map join task exists the task list and has already been processed such case the current task the map join task and need replace with its parent the small table task 
WITHOUT_CLASSIFICATION	 other fields are skipped for this case 
WITHOUT_CLASSIFICATION	 dont fail execution due counters just dont print summary info 
WITHOUT_CLASSIFICATION	 only check hostport pair valid wheter the file exist not does not matter 
WITHOUT_CLASSIFICATION	 add partition event 
WITHOUT_CLASSIFICATION	 combo literal set url set none 
WITHOUT_CLASSIFICATION	 return empty string 
WITHOUT_CLASSIFICATION	 bail exception out the loop 
WITHOUT_CLASSIFICATION	 called stop the query running clean query results and release resources 
WITHOUT_CLASSIFICATION	 find tables which name contains tofind the default database 
WITHOUT_CLASSIFICATION	 key aggregate partition values column name and the value the col stat object 
WITHOUT_CLASSIFICATION	 try this map 
WITHOUT_CLASSIFICATION	 string enclosed single quotes 
WITHOUT_CLASSIFICATION	 requirements for smb sorted their keys both sides and bucketed get key columns 
WITHOUT_CLASSIFICATION	 choose array size have two hash tables hold entries the sum the two should have bit more than twice much space the minimum required 
WITHOUT_CLASSIFICATION	 this before checking failedupdate because that might break the iterator 
WITHOUT_CLASSIFICATION	 the output needed for the qfile results 
WITHOUT_CLASSIFICATION	 the zookeeper connection use 
WITHOUT_CLASSIFICATION	 this version the loop eliminates condition check and branch and measurably faster 
WITHOUT_CLASSIFICATION	 disable new tasks from being submitted 
WITHOUT_CLASSIFICATION	 create the temporary file its corresponding filesinkoperaotr and 
WITHOUT_CLASSIFICATION	 remove failures for tasks that succeeded 
WITHOUT_CLASSIFICATION	 get the set all partition columns custom path 
WITHOUT_CLASSIFICATION	 strip the column name the targetid 
WITHOUT_CLASSIFICATION	 divide down just before round point get round digit 
WITHOUT_CLASSIFICATION	 setautocommit called and the autocommit mode not changed the call noop 
WITHOUT_CLASSIFICATION	 logbase col special case and will implemented separately from this template 
WITHOUT_CLASSIFICATION	 catch the exceptions every other metastore could stopped well log least there slight possibility find out about this 
WITHOUT_CLASSIFICATION	 the table already present 
WITHOUT_CLASSIFICATION	 remove additional elements the list reused 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 allocate the bean the beginning 
WITHOUT_CLASSIFICATION	 set the wrong type parameters for prepared sql 
WITHOUT_CLASSIFICATION	 druid storage timestamp column name 
WITHOUT_CLASSIFICATION	 batchindex big table 
WITHOUT_CLASSIFICATION	 use path relative datadir directory not specified 
WITHOUT_CLASSIFICATION	 check input pruning enough 
WITHOUT_CLASSIFICATION	 columns being updated update expressions setrcols last param null because use actual expressions 
WITHOUT_CLASSIFICATION	 conversion the target data type requires helper target writable few cases 
WITHOUT_CLASSIFICATION	 partition null either these then they are claiming lock the whole table and need check otherwise 
WITHOUT_CLASSIFICATION	 the columnencoding column name and type are all irrelevant this point just need the cfcq 
WITHOUT_CLASSIFICATION	 the offset was never added offset filesize 
WITHOUT_CLASSIFICATION	 filter tags for objects 
WITHOUT_CLASSIFICATION	 now the other enum possibility 
WITHOUT_CLASSIFICATION	 the list servers the can locate hive username for use when creating the user not for connecting hive password for use when creating the user not for connecting hive database for use when creating the user not for connecting 
WITHOUT_CLASSIFICATION	 need connect this cloned parent work with the corresponding child work 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 big alias partitioned table its partition spec bucket number 
WITHOUT_CLASSIFICATION	 most accurate domain cardinality would source column ndv available 
WITHOUT_CLASSIFICATION	 will enabled the customvertex 
WITHOUT_CLASSIFICATION	 from precision scale 
WITHOUT_CLASSIFICATION	 ctas path insert into filedirectory 
WITHOUT_CLASSIFICATION	 the free list level the blocks from which need merge 
WITHOUT_CLASSIFICATION	 tasksidemetadata set rowgroupoffsets null 
WITHOUT_CLASSIFICATION	 this update statement thus any isolation level will take write locks will block 
WITHOUT_CLASSIFICATION	 order convert from integer float correctly need apply the float cast not the double cast hive 
WITHOUT_CLASSIFICATION	 logger debug message from oproc after logj initialize properly 
WITHOUT_CLASSIFICATION	 the planner puts constant field for the dummy grouping set will overwrite 
WITHOUT_CLASSIFICATION	 use common decimal binary conversion method share with fastbigintegerbytes 
WITHOUT_CLASSIFICATION	 compression buffer size should only set compression enabled 
WITHOUT_CLASSIFICATION	 start instance hiveserver which uses minimr 
WITHOUT_CLASSIFICATION	 invalidate the entry rely query cleanup remove from lookup 
WITHOUT_CLASSIFICATION	 list 
WITHOUT_CLASSIFICATION	 collect the needed columns from all the aliases and create ored filter 
WITHOUT_CLASSIFICATION	 check that property that begins the same also hidden 
WITHOUT_CLASSIFICATION	 might not able assign all rows because input nulls start tracking any unassigned rows 
WITHOUT_CLASSIFICATION	 for complex types like struct map etc not support need writer that does nothing assume the vectorizer class has not validated the query actually try and use the complex types they show inputobjinspector and need 
WITHOUT_CLASSIFICATION	 original scan only 
WITHOUT_CLASSIFICATION	 timestamp column type druid timestamp with local timezone represents specific instant time thus have this value and need extract the granularity split the data when are storing druid however druid stores the data utc thus need apply the following logic the data extract the granularity correctly read the timestamp with local timezone value extract utc epoch millis from timestamp with local timezone cast the long timestamp apply the granularity function the timestamp value that way utc and pst same instant will end the same druid segment 
WITHOUT_CLASSIFICATION	 generate new cookie and add the response 
WITHOUT_CLASSIFICATION	 guard because returns null children available bug 
WITHOUT_CLASSIFICATION	 reconstruct join tree 
WITHOUT_CLASSIFICATION	 filter timestamp against timestamp long seconds and double seconds with fractional nanoseconds filter timestampcol timestampcolumn filter timestampcol longdoublecolumn filter longdoublecol timestampcolumn filter timestampcol timestampscalar filter timestampcol longdoublescalar filter longdoublecol timestampscalar filter timestampscalar timestampcolumn filter timestampscalar longdoublecolumn filter longdoublescalar timestampcolumn 
WITHOUT_CLASSIFICATION	 set the required field 
WITHOUT_CLASSIFICATION	 enable trash can tested 
WITHOUT_CLASSIFICATION	 union all insert for nonmm tables subquery creates another subdirectory the end for each union queries 
WITHOUT_CLASSIFICATION	 might have generated dynamic partition operator chain since were removing the reduce sink need remove that too 
WITHOUT_CLASSIFICATION	 add the layout the queryid appender 
WITHOUT_CLASSIFICATION	 note like vectorizer this assumes partition columns after data columns 
WITHOUT_CLASSIFICATION	 put sample data the columns 
WITHOUT_CLASSIFICATION	 set the correct position 
WITHOUT_CLASSIFICATION	 add the partition expressions the order there order and validate order spec 
WITHOUT_CLASSIFICATION	 use the example from hive where the integer digits the result exceed the enforced precisionscale 
WITHOUT_CLASSIFICATION	 handle cancellation the promise 
WITHOUT_CLASSIFICATION	 enc colix allencgeti 
WITHOUT_CLASSIFICATION	 track you walk the tree there operator along the way that changes the rows from the table through joins aggregations only allowed operators are selects and filters 
WITHOUT_CLASSIFICATION	 before after 
WITHOUT_CLASSIFICATION	 convert the join operator sortmerge join operator 
WITHOUT_CLASSIFICATION	 per the javadocs condition not depend the condition alone start gate since spurious wake ups are possible 
WITHOUT_CLASSIFICATION	 set the index table information 
WITHOUT_CLASSIFICATION	 conversion needed 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 hash function should map the long value hence hash value has nonnegative 
WITHOUT_CLASSIFICATION	 explicitly disable bit packing 
WITHOUT_CLASSIFICATION	 such database 
WITHOUT_CLASSIFICATION	 based userspecified parameters check the hash table needs 
WITHOUT_CLASSIFICATION	 for complex type helper object that describes elements keyvalue pairs fields 
WITHOUT_CLASSIFICATION	 logs 
WITHOUT_CLASSIFICATION	 dont need the check for utnull here because well give the real type deserialization and the object inspector will never see the actual union 
WITHOUT_CLASSIFICATION	 ignore the tag passed which should not what want 
WITHOUT_CLASSIFICATION	 prspgbycrscgby 
WITHOUT_CLASSIFICATION	 process singlecolumn string outer join vectorized row batch 
WITHOUT_CLASSIFICATION	 step fill stuff local work 
WITHOUT_CLASSIFICATION	 besteffort check see the comment the method 
WITHOUT_CLASSIFICATION	 interrupt all threads and verify get and expected message also raise kill operations and ensure that retries keep the time out occupied for sec 
WITHOUT_CLASSIFICATION	 conditionaltask 
WITHOUT_CLASSIFICATION	 try with null void 
WITHOUT_CLASSIFICATION	 this one uses the arcsin method involves more pisum sum nnnnnn sum note that split that each term not overflown 
WITHOUT_CLASSIFICATION	 mix binarynonbinary args 
WITHOUT_CLASSIFICATION	 its resulted from rsdedup optimization which removes following under some condition 
WITHOUT_CLASSIFICATION	 doing major compaction its possible where full compliment bucket files not required tez that basex doesnt have file for bucket 
WITHOUT_CLASSIFICATION	 generic lookup 
WITHOUT_CLASSIFICATION	 class factory 
WITHOUT_CLASSIFICATION	 for reasons dont understand and too lazy debug the moment the 
WITHOUT_CLASSIFICATION	 partitioned specified for partitioned table lets fetch all 
WITHOUT_CLASSIFICATION	 remainder dividend 
WITHOUT_CLASSIFICATION	 there need continue processing branch 
WITHOUT_CLASSIFICATION	 functions 
WITHOUT_CLASSIFICATION	 read the column value 
WITHOUT_CLASSIFICATION	 have splitupdate turned for this table then the delta events have already been split into two directories deltaxy and deletedeltaxy when you have splitupdate turned the insert events deltaxy directory and all the delete events deletexy update event will generate two events delete event for the old record that put into deletedeltaxy followed insert event for the updated record put into the usual deltaxy therefore everything inside deltaxy insert event and all the files deltaxy can treated like base files hence each these are added baseororiginalfiles list 
WITHOUT_CLASSIFICATION	 add same jar multiple times and check that dependencies are added only once 
WITHOUT_CLASSIFICATION	 request 
WITHOUT_CLASSIFICATION	 partitions specified partitions inside tablespec 
WITHOUT_CLASSIFICATION	 generated earlier get possible nulls 
WITHOUT_CLASSIFICATION	 sets and might the last one call make sure setting false 
WITHOUT_CLASSIFICATION	 loginfohar file harfile 
WITHOUT_CLASSIFICATION	 and fetch the sql operation log with fetchnext orientation 
WITHOUT_CLASSIFICATION	 get the highvalue 
WITHOUT_CLASSIFICATION	 should not get any rows 
WITHOUT_CLASSIFICATION	 scale down 
WITHOUT_CLASSIFICATION	 and for complex types also leave the children types place 
WITHOUT_CLASSIFICATION	 the subquery identifier from 
WITHOUT_CLASSIFICATION	 test uri with dbname 
WITHOUT_CLASSIFICATION	 sleep for and cancel again 
WITHOUT_CLASSIFICATION	 cost cost writing intermediary results local cost reading from local for transferring gby 
WITHOUT_CLASSIFICATION	 source local then source files wont deleted and have delete them here 
WITHOUT_CLASSIFICATION	 through the map and print out the stuff 
WITHOUT_CLASSIFICATION	 null first default for ascending order 
WITHOUT_CLASSIFICATION	 adapted from only check privileges for loadadddfscompile and admin privileges 
WITHOUT_CLASSIFICATION	 always use foreach action submit rdd graph would only trigger one job 
WITHOUT_CLASSIFICATION	 sourcedb 
WITHOUT_CLASSIFICATION	 builderliteraltrue variablesset 
WITHOUT_CLASSIFICATION	 serde 
WITHOUT_CLASSIFICATION	 set union operator child each leftop and rightop 
WITHOUT_CLASSIFICATION	 internal fields 
WITHOUT_CLASSIFICATION	 static partition without list bucketing 
WITHOUT_CLASSIFICATION	 cast long get rid periodic decimal 
WITHOUT_CLASSIFICATION	 remove from the list 
WITHOUT_CLASSIFICATION	 unexpected metric type 
WITHOUT_CLASSIFICATION	 get non null row count from root column get max vector batches 
WITHOUT_CLASSIFICATION	 comparisons come from the correlatorrel 
WITHOUT_CLASSIFICATION	 from tez eventually changes over the llap protocol and protocolbuffers 
WITHOUT_CLASSIFICATION	 cvalue map rowvalues assertequals cvaluesize assertequalsx assertequalsy 
WITHOUT_CLASSIFICATION	 test executed times worst case original retries 
WITHOUT_CLASSIFICATION	 char starts index and with length covering the rest the array 
WITHOUT_CLASSIFICATION	 hivehome not defined file not found hivehomeconf then load default ivysettingsxml from class loader 
WITHOUT_CLASSIFICATION	 there should calls create partitions with batch sizes 
WITHOUT_CLASSIFICATION	 check whether log file created test running 
WITHOUT_CLASSIFICATION	 handle skewed value skewed value add directory path unless value false 
WITHOUT_CLASSIFICATION	 number headers smallest blocks per target block next free list from which will splitting 
WITHOUT_CLASSIFICATION	 create default database inside the catalog 
WITHOUT_CLASSIFICATION	 second row 
WITHOUT_CLASSIFICATION	 perfloggeroptimizer baseplan hepplanbaseplan true mdprovider executorprovider perfloggeroptimizer calcite prejoin ordering transformation push down semi joins 
WITHOUT_CLASSIFICATION	 add filter just scan the keys that pick everything 
WITHOUT_CLASSIFICATION	 other counter sources currently used llap 
WITHOUT_CLASSIFICATION	 now know where put row 
WITHOUT_CLASSIFICATION	 mapping from constraint name list unique constraints 
WITHOUT_CLASSIFICATION	 for managed tables make sure the file formats match 
WITHOUT_CLASSIFICATION	 semijoin attempted then replace the condition with minmax filter and bloom filter else 
WITHOUT_CLASSIFICATION	 break polling times out 
WITHOUT_CLASSIFICATION	 create gsscontext for authentication with the service 
WITHOUT_CLASSIFICATION	 notify clear pending events any 
WITHOUT_CLASSIFICATION	 not part 
WITHOUT_CLASSIFICATION	 assumes line would never null when this method called 
WITHOUT_CLASSIFICATION	 decimal 
WITHOUT_CLASSIFICATION	 output type information 
WITHOUT_CLASSIFICATION	 test dropping fields first middle last 
WITHOUT_CLASSIFICATION	 fail transactional property set invalid value 
WITHOUT_CLASSIFICATION	 input metrics 
WITHOUT_CLASSIFICATION	 create the table 
WITHOUT_CLASSIFICATION	 called latemapjoin processor for example 
WITHOUT_CLASSIFICATION	 bail out 
WITHOUT_CLASSIFICATION	 filtercorrelaterule rule mistakenly pushes filter consiting correlated vars top logicalcorrelate within left input for scalar corr queries which causes exception during decorrelation this has been disabled for now 
WITHOUT_CLASSIFICATION	 our aggregation buffer has nothing just copy over other deserializing the arraylist pairs into array coord objects 
WITHOUT_CLASSIFICATION	 nothing this property not specified empty 
WITHOUT_CLASSIFICATION	 process each level parallel 
WITHOUT_CLASSIFICATION	 determine there match between big table row and the corresponding hashtable three states can returned match match found nomatch match found from the specified partition spill the specified partition has been spilled disk and not available the evaluation for this big table row will postponed 
WITHOUT_CLASSIFICATION	 nothing for null object 
WITHOUT_CLASSIFICATION	 optimize physical tree translate target execution engine 
WITHOUT_CLASSIFICATION	 not sequential 
WITHOUT_CLASSIFICATION	 position first row 
WITHOUT_CLASSIFICATION	 read friendly string 
WITHOUT_CLASSIFICATION	 mapping from operator the columns which its output sorted 
WITHOUT_CLASSIFICATION	 implementing 
WITHOUT_CLASSIFICATION	 future decide how ask input file format what vectorization features supports 
WITHOUT_CLASSIFICATION	 will bloomfilter bytewritable 
WITHOUT_CLASSIFICATION	 run given query and validate expected result 
WITHOUT_CLASSIFICATION	 for bytes type can mapped decimal 
WITHOUT_CLASSIFICATION	 the transactional listener response will set already the event there not need pass the response the nontransactional listener 
WITHOUT_CLASSIFICATION	 keysi for the ith join operator key list 
WITHOUT_CLASSIFICATION	 default behavior when neither hivejobcredstore location set nor this case hadoop credential provider configured job config should use that else should remain unset 
WITHOUT_CLASSIFICATION	 now compact compaction produces single range for both delta and delete delta that both delta and deletedeltas would compacted into delta and deletedelta 
WITHOUT_CLASSIFICATION	 test reset 
WITHOUT_CLASSIFICATION	 noone could have moved have the heap lock 
WITHOUT_CLASSIFICATION	 populate the complete query with provided prefix and suffix 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader long 
WITHOUT_CLASSIFICATION	 skip escape 
WITHOUT_CLASSIFICATION	 more places get the schema from give may have reencode later 
WITHOUT_CLASSIFICATION	 join condition must equality predicate both sides must reference column needed flip the columns 
WITHOUT_CLASSIFICATION	 these are the output columns for the small table and the outer small table keys 
WITHOUT_CLASSIFICATION	 shortcut for hdfs 
WITHOUT_CLASSIFICATION	 now remove all the unions throw away any branch thats not reachable from the current set roots the reason that those branches will handled 
WITHOUT_CLASSIFICATION	 resolve all the kill query requests flight nothing below can affect them 
WITHOUT_CLASSIFICATION	 tablescan with same alias 
WITHOUT_CLASSIFICATION	 pab 
WITHOUT_CLASSIFICATION	 see can use reencoding read the format thru elevator 
WITHOUT_CLASSIFICATION	 pass along hashcode avoid recalculation 
WITHOUT_CLASSIFICATION	 base javaobject primitives javafieldref entry javaobject javafieldref 
WITHOUT_CLASSIFICATION	 mock out the predicate handler because its just easier 
WITHOUT_CLASSIFICATION	 count position 
WITHOUT_CLASSIFICATION	 retrieve the tables from the metastore batches alleviate memory constraints 
WITHOUT_CLASSIFICATION	 remember the condition variables for explain regardless 
WITHOUT_CLASSIFICATION	 this point weve verified the types are correct 
WITHOUT_CLASSIFICATION	 start small table random generation from beginning 
WITHOUT_CLASSIFICATION	 create dummy partitions 
WITHOUT_CLASSIFICATION	 add empty stats object for each column 
WITHOUT_CLASSIFICATION	 use remove instead get that not parsed again 
WITHOUT_CLASSIFICATION	 testlazybinaryfast source rows serde serdefewer primitivetypeinfos useincludecolumns false dowritefewercolumns true 
WITHOUT_CLASSIFICATION	 nop theres caching 
WITHOUT_CLASSIFICATION	 configuration 
WITHOUT_CLASSIFICATION	 change the join operator reflect this info 
WITHOUT_CLASSIFICATION	 local dirs 
WITHOUT_CLASSIFICATION	 this will only available when are doing table load only replication not otherwise 
WITHOUT_CLASSIFICATION	 multikey hash map based the 
WITHOUT_CLASSIFICATION	 contains 
WITHOUT_CLASSIFICATION	 should never get here 
WITHOUT_CLASSIFICATION	 for future use 
WITHOUT_CLASSIFICATION	 insert transaction entries into minhistorylevel 
WITHOUT_CLASSIFICATION	 find the partition will working with there one 
WITHOUT_CLASSIFICATION	 stage started but not complete 
WITHOUT_CLASSIFICATION	 the table scan for big table then skip 
WITHOUT_CLASSIFICATION	 print all results for standalone select statement 
WITHOUT_CLASSIFICATION	 just digits 
WITHOUT_CLASSIFICATION	 the number columns output the udtf 
WITHOUT_CLASSIFICATION	 updates the references that are present every operand till now 
WITHOUT_CLASSIFICATION	 oozie does not change the service field the token hence default token generation will have value new text hiveclient will look for use with service 
WITHOUT_CLASSIFICATION	 make sure were locking the whole table since this dynamic partitioning 
WITHOUT_CLASSIFICATION	 must set isnulli false make sure gets initialized case set nonulls true 
WITHOUT_CLASSIFICATION	 after catching oom java says undefined behavior dont even try clean can get stuck shutdown 
WITHOUT_CLASSIFICATION	 create table related objects 
WITHOUT_CLASSIFICATION	 max characters when auto generating the column name with func name 
WITHOUT_CLASSIFICATION	 dbname 
WITHOUT_CLASSIFICATION	 now set some tree properties related multiinsert 
WITHOUT_CLASSIFICATION	 since integer always some products here are not included 
WITHOUT_CLASSIFICATION	 assertassertequals 
WITHOUT_CLASSIFICATION	 requires calculate stats new and old have different fast stats 
WITHOUT_CLASSIFICATION	 create new vectorization context create new projection but keep same output column manager must inherited track the scratch the columns 
WITHOUT_CLASSIFICATION	 call further down rely upon opabort 
WITHOUT_CLASSIFICATION	 value 
WITHOUT_CLASSIFICATION	 given work descriptor and the taskname for the work this responsible check each mapjoinop for cross products the analyze call returns the warnings list for the taskname the stagename for tez the vertex name 
WITHOUT_CLASSIFICATION	 store this the udf context can get later 
WITHOUT_CLASSIFICATION	 make sure all the partitions have the catalog set well 
WITHOUT_CLASSIFICATION	 return output null because additional work needed 
WITHOUT_CLASSIFICATION	 complex types map list struct union 
WITHOUT_CLASSIFICATION	 make expression for default value 
WITHOUT_CLASSIFICATION	 this copy genericudfnvl which builtin well make generic custom udf for test purposes 
WITHOUT_CLASSIFICATION	 dont compare locations because the location can still empty the preevent listener before created 
WITHOUT_CLASSIFICATION	 addpartition 
WITHOUT_CLASSIFICATION	 clone configuration before modifying pertask basis 
WITHOUT_CLASSIFICATION	 isolated from the other transaction related rpc calls 
WITHOUT_CLASSIFICATION	 run worker delete aborted transactions delta directory 
WITHOUT_CLASSIFICATION	 acquiredat 
WITHOUT_CLASSIFICATION	 cset doesnt reset millis 
WITHOUT_CLASSIFICATION	 output sorted 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 expect correlated variables hivefilter only for now also check for case where operator has inputs tablescan 
WITHOUT_CLASSIFICATION	 this table has keys 
WITHOUT_CLASSIFICATION	 try read from the cache first 
WITHOUT_CLASSIFICATION	 support for schema evolution 
WITHOUT_CLASSIFICATION	 constructing the row objectinspector the row consists some set primitive columns each column will java object primitive type 
WITHOUT_CLASSIFICATION	 char test 
WITHOUT_CLASSIFICATION	 generate groupbyoperator 
WITHOUT_CLASSIFICATION	 possible that all the async methods returned the same thread because the session with registry data and stuff was available the pool this happens well take the session out here and cancel the init skip 
WITHOUT_CLASSIFICATION	 this offer will accepted and evicted 
WITHOUT_CLASSIFICATION	 this test method here initial call parsedriver and prevent any tests with timeouts the first 
WITHOUT_CLASSIFICATION	 insert into appends old version 
WITHOUT_CLASSIFICATION	 druid query 
WITHOUT_CLASSIFICATION	 substitution option hivevar 
WITHOUT_CLASSIFICATION	 separate required columns non partition and partition cols 
WITHOUT_CLASSIFICATION	 update cross size 
WITHOUT_CLASSIFICATION	 send out the actual submitworkrequest 
WITHOUT_CLASSIFICATION	 the sort order contains whether the sorting happening ascending descending 
WITHOUT_CLASSIFICATION	 evaluate else expression only and copy all its results 
WITHOUT_CLASSIFICATION	 filter may have sensitive information not send debug 
WITHOUT_CLASSIFICATION	 now compact one important thing note this test that minor compaction always produces deltaxy and counterpart deletedeltaxy even when there are deletedelta events such choice has been made simplify processing 
WITHOUT_CLASSIFICATION	 physical files are resides local file system the similar location 
WITHOUT_CLASSIFICATION	 may happen that know wont use some cache buffers anymore the alternative that will use the same buffers for other streams separate calls 
WITHOUT_CLASSIFICATION	 the key not found mapcolumnvector set the output null columnvector 
WITHOUT_CLASSIFICATION	 txnids 
WITHOUT_CLASSIFICATION	 was deleted during the transaction 
WITHOUT_CLASSIFICATION	 key nodes candidate list 
WITHOUT_CLASSIFICATION	 the extra parameters will added server side check that the required ones are present 
WITHOUT_CLASSIFICATION	 remove semijoin there any the semijoin branch can potentially create task level cycle with the hashjoin except when dynamically partitioned hash 
WITHOUT_CLASSIFICATION	 search mapping for any strings and return their output columns 
WITHOUT_CLASSIFICATION	 should true for sure because already checked before 
WITHOUT_CLASSIFICATION	 put now available buffered batch end 
WITHOUT_CLASSIFICATION	 expressions are not supported currently without alias 
WITHOUT_CLASSIFICATION	 default utc utc 
WITHOUT_CLASSIFICATION	 get any new notification events that have been since the last time checked and pass them the event handlers 
WITHOUT_CLASSIFICATION	 iterate over each clause 
WITHOUT_CLASSIFICATION	 check non null 
WITHOUT_CLASSIFICATION	 return subquery 
WITHOUT_CLASSIFICATION	 class store necessary information for attempt log 
WITHOUT_CLASSIFICATION	 remember map joins encounter them 
WITHOUT_CLASSIFICATION	 verify that the table created successfully 
WITHOUT_CLASSIFICATION	 set lambda the heap size becomes lru 
WITHOUT_CLASSIFICATION	 all the fastsetfrom methods require the caller pass fastresult parameter has been reset for better performance 
WITHOUT_CLASSIFICATION	 sequence number used name vertices map reduce 
WITHOUT_CLASSIFICATION	 overlay hivesitexml exists 
WITHOUT_CLASSIFICATION	 create remote metastore 
WITHOUT_CLASSIFICATION	 may add noandstop future where combine impossible and other should not base 
WITHOUT_CLASSIFICATION	 specialized class for native vectorized reduce sink that reducing uniform hash multiple key columns single nonlong nonstring column 
WITHOUT_CLASSIFICATION	 relationship 
WITHOUT_CLASSIFICATION	 nulls the join keys 
WITHOUT_CLASSIFICATION	 add cstatstask dependent all the nonstatsleaftasks 
WITHOUT_CLASSIFICATION	 now this should block until unlocks 
WITHOUT_CLASSIFICATION	 put the mapping task aliases 
WITHOUT_CLASSIFICATION	 this only called for replication that handles tables need for mmctx 
WITHOUT_CLASSIFICATION	 case repeating has nulls 
WITHOUT_CLASSIFICATION	 check file system permission 
WITHOUT_CLASSIFICATION	 create snapshot 
WITHOUT_CLASSIFICATION	 should convert 
WITHOUT_CLASSIFICATION	 add all the public member classes that implement evaluator 
WITHOUT_CLASSIFICATION	 need filter those that have been pushed already stored the join and those that were already the subtree rooted child 
WITHOUT_CLASSIFICATION	 renewer 
WITHOUT_CLASSIFICATION	 running queued 
WITHOUT_CLASSIFICATION	 important sorting here retain order its used match with values runtime 
WITHOUT_CLASSIFICATION	 fetch across schemas 
WITHOUT_CLASSIFICATION	 ensure filters are not set from previous pushfilters 
WITHOUT_CLASSIFICATION	 the message from remote exception includes the entire stack the error thrown from hive based the remote exception needs only the first line 
WITHOUT_CLASSIFICATION	 cluster than the default one but least for the default case wed have covered 
WITHOUT_CLASSIFICATION	 mapping from column name check expr 
WITHOUT_CLASSIFICATION	 our original foo should the wrapper 
WITHOUT_CLASSIFICATION	 checkcorrect codec 
WITHOUT_CLASSIFICATION	 reset for filling 
WITHOUT_CLASSIFICATION	 hiveserver configs that this instance will publish zookeeper that the clients can read these and configure themselves properly 
WITHOUT_CLASSIFICATION	 basic test 
WITHOUT_CLASSIFICATION	 for each partition spec get the partition 
WITHOUT_CLASSIFICATION	 next locate the aggregation buffer set for each key 
WITHOUT_CLASSIFICATION	 txn started implicitly previous statement 
WITHOUT_CLASSIFICATION	 process user groups for which doas authorized 
WITHOUT_CLASSIFICATION	 the task will either killed already the process completing which will trigger the next scheduling run result available slots being higher than 
WITHOUT_CLASSIFICATION	 normalize label row 
WITHOUT_CLASSIFICATION	 vieworiginaltext 
WITHOUT_CLASSIFICATION	 add limit order bys without limit can disabled for safety reasons 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautilcalendar 
WITHOUT_CLASSIFICATION	 install the jaas configuration for the runtime 
WITHOUT_CLASSIFICATION	 create and set provider 
WITHOUT_CLASSIFICATION	 close 
WITHOUT_CLASSIFICATION	 node became available enable the node and try scheduling 
WITHOUT_CLASSIFICATION	 get all target paths first because the number total target paths used determine number splits each target path 
WITHOUT_CLASSIFICATION	 vint 
WITHOUT_CLASSIFICATION	 handle the special cases here perhaps could have more general structure even configurable set like storage handlers but for now only have one 
WITHOUT_CLASSIFICATION	 attempt make the path case does not exist before check 
WITHOUT_CLASSIFICATION	 the values from timestampgetnanos 
WITHOUT_CLASSIFICATION	 function correctly 
WITHOUT_CLASSIFICATION	 outerjoinpos otherposfilterlen otherposfilterlen 
WITHOUT_CLASSIFICATION	 hive depends filesplits wrap hbasesplit 
WITHOUT_CLASSIFICATION	 for and condition cascadingly update stats 
WITHOUT_CLASSIFICATION	 walk over all the sources which are guaranteed reduce sink operators the join outputs concatenation all the inputs 
WITHOUT_CLASSIFICATION	 unknown unknown 
WITHOUT_CLASSIFICATION	 cookie based authentication allowed generate ticket only when necessary the necessary condition either when there are server side cookies the cookiestore which can send back when the server returns error code 
WITHOUT_CLASSIFICATION	 not delete for tables either want the file succeed must delete explicitly before proceeding the merge fails 
WITHOUT_CLASSIFICATION	 unpartitioned table filters 
WITHOUT_CLASSIFICATION	 create the delta directory dont worry already exists that likely means another task got first then move each the buckets would more efficient try move the delta with its buckets but that harder make race condition proof 
WITHOUT_CLASSIFICATION	 adding this child the union later 
WITHOUT_CLASSIFICATION	 the schema after like this all keys sumc sumvcolc the column size the same unioncolumnsize for except distinct add filter then add the project for except all add project change all keys then add the udtf 
WITHOUT_CLASSIFICATION	 use the session the one supplied constructor 
WITHOUT_CLASSIFICATION	 perform the data read asynchronously 
WITHOUT_CLASSIFICATION	 the default unless serde overrides 
WITHOUT_CLASSIFICATION	 connect after the lifetime there should not any failures 
WITHOUT_CLASSIFICATION	 actual batch size that will used 
WITHOUT_CLASSIFICATION	 close one connection verify still one left 
WITHOUT_CLASSIFICATION	 store into configuration 
WITHOUT_CLASSIFICATION	 change the selected vector 
WITHOUT_CLASSIFICATION	 records will emitted from hive 
WITHOUT_CLASSIFICATION	 otherwise expect the user already logged 
WITHOUT_CLASSIFICATION	 there are any open txns then the minimum minopentxnid from minhistorylevel table 
WITHOUT_CLASSIFICATION	 send task off another jvm 
WITHOUT_CLASSIFICATION	 generate test jar files 
WITHOUT_CLASSIFICATION	 input job properties 
WITHOUT_CLASSIFICATION	 and remember link between event and table scan 
WITHOUT_CLASSIFICATION	 checked partitions matching specification are marked archived the metadata they are and their levels are the same would set later means previous run failed and have the recovery 
WITHOUT_CLASSIFICATION	 validate that setop feasible according hive using type 
WITHOUT_CLASSIFICATION	 check same filter exists already 
WITHOUT_CLASSIFICATION	 stats key prefix 
WITHOUT_CLASSIFICATION	 need set type name should always decimal 
WITHOUT_CLASSIFICATION	 the basic idea for cbo support udtf treat udtf special project ast return path just need generate selexpr just need remember the expressions and the alias return path need generate sel and then udtf following old semantic analyzer 
WITHOUT_CLASSIFICATION	 doharcheckfsharfile 
WITHOUT_CLASSIFICATION	 the implementation balks when this method invoked multiple times 
WITHOUT_CLASSIFICATION	 taskstatus 
WITHOUT_CLASSIFICATION	 function create subcache 
WITHOUT_CLASSIFICATION	 once the conversion done can set the partitioner bucket cols the small table 
WITHOUT_CLASSIFICATION	 can just use setkeyprovider 
WITHOUT_CLASSIFICATION	 index means this key 
WITHOUT_CLASSIFICATION	 transaction manager the driver has been initialized with can null this set then this transaction manager will used during query rather than using the current sessions transaction manager this might needed situation where driver nested within already running driverquery the nested driver requires separate transaction manager not conflict with the outer driverquery which using the session 
WITHOUT_CLASSIFICATION	 cancel other tasks 
WITHOUT_CLASSIFICATION	 reserve space for potential future list 
WITHOUT_CLASSIFICATION	 schedulingpolicy 
WITHOUT_CLASSIFICATION	 choose max 
WITHOUT_CLASSIFICATION	 this better generic struct with constant values the children 
WITHOUT_CLASSIFICATION	 vectorized doesnt adjust usage for the keys while processing the batch 
WITHOUT_CLASSIFICATION	 required for mdc based routing appender that child threads can inherit the mdc context 
WITHOUT_CLASSIFICATION	 outer join are going add the inspector from the inner side but the key value will come from the outer side need create converter from inputoi outputoi 
WITHOUT_CLASSIFICATION	 just safe about numrows 
WITHOUT_CLASSIFICATION	 hive has concept avros fixed type fixed arrays bytes 
WITHOUT_CLASSIFICATION	 worstcase hash aggregation disabled 
WITHOUT_CLASSIFICATION	 because inverse 
WITHOUT_CLASSIFICATION	 partitions 
WITHOUT_CLASSIFICATION	 commenthello there propertiesab 
WITHOUT_CLASSIFICATION	 should only have one aggregate 
WITHOUT_CLASSIFICATION	 acidmm tables then need find the valid state wrt given validwriteidlist 
WITHOUT_CLASSIFICATION	 default partitions not defined 
WITHOUT_CLASSIFICATION	 include specified but this module not the set 
WITHOUT_CLASSIFICATION	 the operator not rexcall type fail fall through add this condition the list nonequijoin conditions 
WITHOUT_CLASSIFICATION	 use the source ordering flavor for the mapping 
WITHOUT_CLASSIFICATION	 insert current common join task conditional task 
WITHOUT_CLASSIFICATION	 equals 
WITHOUT_CLASSIFICATION	 returns value null 
WITHOUT_CLASSIFICATION	 try factoring out common filter elements separating deterministic nondeterministic udf this needs run before ppd that ppd can add onclauses for old style join syntax select from join where rxrx and 
WITHOUT_CLASSIFICATION	 test that existing sharedread partition with new exclusive coalesces 
WITHOUT_CLASSIFICATION	 failed compacts left and other since only have failed ones here 
WITHOUT_CLASSIFICATION	 output the exit code 
WITHOUT_CLASSIFICATION	 create transactional table 
WITHOUT_CLASSIFICATION	 try allocate memory havent allocated all the way maxsize yet very rare 
WITHOUT_CLASSIFICATION	 multibyte characters with blank ranges 
WITHOUT_CLASSIFICATION	 only expect transactional components 
WITHOUT_CLASSIFICATION	 blockedbyextid 
WITHOUT_CLASSIFICATION	 set the memory treshold that get before need flush 
WITHOUT_CLASSIFICATION	 namemethod name constant java string constant text stringwritable 
WITHOUT_CLASSIFICATION	 list operation for which log 
WITHOUT_CLASSIFICATION	 all precision has been lost result 
WITHOUT_CLASSIFICATION	 embedded metastore mode 
WITHOUT_CLASSIFICATION	 use when calculating intermediate variance and count note count has been incremented sum included value 
WITHOUT_CLASSIFICATION	 get the file status upfront for all partitions beneficial cases blob storage systems 
WITHOUT_CLASSIFICATION	 need add twice 
WITHOUT_CLASSIFICATION	 now make sure its array doubles floats dont allow integer types here 
WITHOUT_CLASSIFICATION	 object receive results reading decoded variable length int long 
WITHOUT_CLASSIFICATION	 are assuming the updateerror bad and just try kill 
WITHOUT_CLASSIFICATION	 creating stats table not exists 
WITHOUT_CLASSIFICATION	 generate the temporary file must the same file system the current destination 
WITHOUT_CLASSIFICATION	 unique set for operation when run from base encoded value 
WITHOUT_CLASSIFICATION	 write the key out 
WITHOUT_CLASSIFICATION	 get different dates 
WITHOUT_CLASSIFICATION	 return the desired vectorexpression found otherwise return null cause 
WITHOUT_CLASSIFICATION	 stay with multikey 
WITHOUT_CLASSIFICATION	 perform major compaction nothing should change both deltas and base dirs should have the same name 
WITHOUT_CLASSIFICATION	 need scale down thisscale rightscale newscale 
WITHOUT_CLASSIFICATION	 verify throws exception 
WITHOUT_CLASSIFICATION	 object constructed from output wdw fns before put the wdw processing partition set 
WITHOUT_CLASSIFICATION	 return new hash map result implementation specific object the object can used access the values when there match access spill information when the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 constructing the row objectinspector the row consists some string columns each column will java 
WITHOUT_CLASSIFICATION	 get jobids from job status dir 
WITHOUT_CLASSIFICATION	 output header 
WITHOUT_CLASSIFICATION	 sql usage inside larger transaction droptable may not desirable because some databases postgres abort the entire transaction when any query fails 
WITHOUT_CLASSIFICATION	 scalar subquery 
WITHOUT_CLASSIFICATION	 have already locked the table ddlsemanticanalyzer dont again here 
WITHOUT_CLASSIFICATION	 the subquery alias 
WITHOUT_CLASSIFICATION	 bloom filter false positive probability 
WITHOUT_CLASSIFICATION	 the table exists and found valid create table event then need drop the table first and then create this case possible the event sequence droptablet createtablet need drop here handle the case where the previous incremental load created the table but 
WITHOUT_CLASSIFICATION	 grant option revoke remove the whole role 
WITHOUT_CLASSIFICATION	 operator specific logic goes here 
WITHOUT_CLASSIFICATION	 todo change type the one the table schema 
WITHOUT_CLASSIFICATION	 create new join 
WITHOUT_CLASSIFICATION	 operation with insertupdate 
WITHOUT_CLASSIFICATION	 noop sba does not attempt authorize auth api call allow 
WITHOUT_CLASSIFICATION	 the same thing setchildren when there nothing read the setchildren method initializes the object inspector needed the operators based path and partition information which dont have this case 
WITHOUT_CLASSIFICATION	 are making what are trying more explicit theres union alias 
WITHOUT_CLASSIFICATION	 cancel other futures 
WITHOUT_CLASSIFICATION	 cannot call class testclidriver since thats the name the generated code for the scriptbased testing 
WITHOUT_CLASSIFICATION	 sync total record length key portion length 
WITHOUT_CLASSIFICATION	 hard know exactly for decimals 
WITHOUT_CLASSIFICATION	 allow string double conversion 
WITHOUT_CLASSIFICATION	 check for rounding 
WITHOUT_CLASSIFICATION	 dont need lookup ordercolumnidbyname because know must 
WITHOUT_CLASSIFICATION	 this will used rexnodeconverter create cor var 
WITHOUT_CLASSIFICATION	 check our config value first explicitly avoiding getting the default value for now dont want our default override hive set value 
WITHOUT_CLASSIFICATION	 append the deserialized standard object row using the current batch size 
WITHOUT_CLASSIFICATION	 ideally these properties should part llapdameonconf rather than hiveconf 
WITHOUT_CLASSIFICATION	 create new schema that subset original 
WITHOUT_CLASSIFICATION	 typespecific handling done here 
WITHOUT_CLASSIFICATION	 for example original max dist min rss schema key max min 
WITHOUT_CLASSIFICATION	 tracks various maps for dagcompletions this setup here since statechange messages 
WITHOUT_CLASSIFICATION	 bugbug somewhat fragile below substring expression 
WITHOUT_CLASSIFICATION	 the dbtype hive this setting the information schema hive will set the default jdbc url and driver overriden command line options passed url and driver 
WITHOUT_CLASSIFICATION	 unlikely but log the actual values case one the two was emptynull 
WITHOUT_CLASSIFICATION	 the buffer was pointing smallbuffer then nextfree keeps track the current state the free index for smallbuffer now need save this value smallbuffernextfree 
WITHOUT_CLASSIFICATION	 insert sparkhashtablesink and dummy operators 
WITHOUT_CLASSIFICATION	 create identity projection 
WITHOUT_CLASSIFICATION	 push first record group 
WITHOUT_CLASSIFICATION	 called explicitly through dynamic return false default 
WITHOUT_CLASSIFICATION	 this simulates the completion txnididtxnupdate 
WITHOUT_CLASSIFICATION	 param basedir not null its either tablepartition root folder basexxxx its basexxxx its dirstosearch else the actual original files all leaves recursively are the dirstosearch list 
WITHOUT_CLASSIFICATION	 udaf present and column expression map empty then must full aggregation query like count which case number 
WITHOUT_CLASSIFICATION	 detecting failed executions exceptions thrown the operator tree 
WITHOUT_CLASSIFICATION	 create table with multiple partitions 
WITHOUT_CLASSIFICATION	 string tests 
WITHOUT_CLASSIFICATION	 there should now directories the location 
WITHOUT_CLASSIFICATION	 build the path from bottom pick list bucketing subdirectories 
WITHOUT_CLASSIFICATION	 obtain metastore clients 
WITHOUT_CLASSIFICATION	 the methods older and newer match 
WITHOUT_CLASSIFICATION	 nulls come first otherwise nulls come last 
WITHOUT_CLASSIFICATION	 just access key and value ensure they are correct 
WITHOUT_CLASSIFICATION	 check there are enough entries the tree constitute hint 
WITHOUT_CLASSIFICATION	 fix for sfnet bug 
WITHOUT_CLASSIFICATION	 length green 
WITHOUT_CLASSIFICATION	 use the minmax instead the byte range 
WITHOUT_CLASSIFICATION	 first qualify 
WITHOUT_CLASSIFICATION	 dont print full exception trace debug not 
WITHOUT_CLASSIFICATION	 and check hdfs before and after 
WITHOUT_CLASSIFICATION	 decimal classes cannot converted printf convert them doubles 
WITHOUT_CLASSIFICATION	 the offsets are the same assume our initial jump did not cross any dst boundaries 
WITHOUT_CLASSIFICATION	 create lot locks 
WITHOUT_CLASSIFICATION	 collection methods 
WITHOUT_CLASSIFICATION	 this simulates the completion txnididtxnupdate 
WITHOUT_CLASSIFICATION	 expected 
WITHOUT_CLASSIFICATION	 for nonmm tables the final destination partition directory created during move task via rename for tables the final destination partition directory created the tasks themselves 
WITHOUT_CLASSIFICATION	 move task will create final path 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 when there are exceptions this has called always make sure incompatible files are moved properly the destination path 
WITHOUT_CLASSIFICATION	 remains here the legacy the original higherlevel interface getinstance 
WITHOUT_CLASSIFICATION	 already processed skip 
WITHOUT_CLASSIFICATION	 groupnames 
WITHOUT_CLASSIFICATION	 check that the change stuck 
WITHOUT_CLASSIFICATION	 unsupported 
WITHOUT_CLASSIFICATION	 test using loadfilework 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 update 
WITHOUT_CLASSIFICATION	 default type all string 
WITHOUT_CLASSIFICATION	 plan 
WITHOUT_CLASSIFICATION	 some other task 
WITHOUT_CLASSIFICATION	 make location hints 
WITHOUT_CLASSIFICATION	 now grant all privs admin 
WITHOUT_CLASSIFICATION	 note this does not work for embedded channels 
WITHOUT_CLASSIFICATION	 perform kerberos login using the hadoop shim api the configuration available 
WITHOUT_CLASSIFICATION	 aggregate itself should not reference cor vars 
WITHOUT_CLASSIFICATION	 regardless other criteria ducks are always more important than nonducks 
WITHOUT_CLASSIFICATION	 link queryid txnid 
WITHOUT_CLASSIFICATION	 get the serde parameters 
WITHOUT_CLASSIFICATION	 find all root tss and add all data sizes not adding other stats rows col stats since only data size used here 
WITHOUT_CLASSIFICATION	 load the test files into tables 
WITHOUT_CLASSIFICATION	 union expr for distinct keys 
WITHOUT_CLASSIFICATION	 avoid calculating modulo 
WITHOUT_CLASSIFICATION	 set the operator plan before setting splits the 
WITHOUT_CLASSIFICATION	 add this filter for deletion does not have nonfinal candidates 
WITHOUT_CLASSIFICATION	 determine the index expr child schema note calcite can not take compound exprs without being 
WITHOUT_CLASSIFICATION	 columns not expressions yes proceed 
WITHOUT_CLASSIFICATION	 test for only partnames being empty 
WITHOUT_CLASSIFICATION	 the matched field leaf which means all leaves are required not need deeper 
WITHOUT_CLASSIFICATION	 and return all the dummy parent 
WITHOUT_CLASSIFICATION	 are just converting common merge join operator the shuffle join mapreduce case 
WITHOUT_CLASSIFICATION	 dynamicpartitionctx indicate that needs dynamically partitioned 
WITHOUT_CLASSIFICATION	 some ddl task that directly executes teztask does not setup context and hence triggercontext setting queryid messed some ddl tasks have executionid instead proper queryid 
WITHOUT_CLASSIFICATION	 date value boolean doesnt make any sense 
WITHOUT_CLASSIFICATION	 are not using the key and value contexts nor support mapjoinkey 
WITHOUT_CLASSIFICATION	 should not have more than load file for ctas 
WITHOUT_CLASSIFICATION	 set input format information necessary 
WITHOUT_CLASSIFICATION	 set the configuration parameters 
WITHOUT_CLASSIFICATION	 bbhashcode 
WITHOUT_CLASSIFICATION	 get the list task 
WITHOUT_CLASSIFICATION	 try qualifying with current name for permanent functions 
WITHOUT_CLASSIFICATION	 wait before sending another heartbeat otherwise consider oob heartbeat 
WITHOUT_CLASSIFICATION	 may need convert common type compare 
WITHOUT_CLASSIFICATION	 first column empty 
WITHOUT_CLASSIFICATION	 partitioned table 
WITHOUT_CLASSIFICATION	 the queue does not have capacity does not throw rejection instead will return the task with the lowest priority which could the task which currently being processed 
WITHOUT_CLASSIFICATION	 will true there are null entries 
WITHOUT_CLASSIFICATION	 caller must make sure product inputs not too big 
WITHOUT_CLASSIFICATION	 copied over from 
WITHOUT_CLASSIFICATION	 enforce hive defaults 
WITHOUT_CLASSIFICATION	 consider query like select mapjoinsubq from select akey avalue from tbl subq join select akey avalue from tbl subq subqkey subqkey aliastoopinfo contains the selectoperator for subq and subq need traverse the tree using tableaccessanalyzer get the base table the object being mapjoined base table then aliastoopinfo contains the tablescanoperator and tableaccessanalyzer noop 
WITHOUT_CLASSIFICATION	 nothing that can really about 
WITHOUT_CLASSIFICATION	 since the oldname table not under its database see hive the renamed oldname table will keep its location after hive changed check the existence the newname table and its name instead verifying its location 
WITHOUT_CLASSIFICATION	 iterate through each token and create appropriate object here 
WITHOUT_CLASSIFICATION	 assumes the query has already been compiled 
WITHOUT_CLASSIFICATION	 case ctas statement 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 total completed running pending failed killed 
WITHOUT_CLASSIFICATION	 allow lookup query string 
WITHOUT_CLASSIFICATION	 right trim and truncate byte array maximum number characters and return byte array with only the trimmed and truncated bytes 
WITHOUT_CLASSIFICATION	 minimum rows per stripe 
WITHOUT_CLASSIFICATION	 container prewarming tell the how many containers need 
WITHOUT_CLASSIFICATION	 ignore the exception this may caused external jars 
WITHOUT_CLASSIFICATION	 bucket count for test tables set for easier debugging 
WITHOUT_CLASSIFICATION	 this function serves the wrapper 
WITHOUT_CLASSIFICATION	 verify that can create table with ifof some custom nonexistent format 
WITHOUT_CLASSIFICATION	 when dowritefewercolumns try read more fields than exist buffer 
WITHOUT_CLASSIFICATION	 ids were all aborted and the metadata cleaned would lose the record the aborted ids this case are not able determine the new writeidlist has equivalent commit state compared the previous writeidlists 
WITHOUT_CLASSIFICATION	 add the checkpoint key the database binding current dump directory retry using same dump shall skip database object update 
WITHOUT_CLASSIFICATION	 recurse into memoized decorator 
WITHOUT_CLASSIFICATION	 this deletes the side file 
WITHOUT_CLASSIFICATION	 insert them all before the get requests from this iteration 
WITHOUT_CLASSIFICATION	 the first query has full batches and the second query only has batch which only contains member 
WITHOUT_CLASSIFICATION	 scratch column information 
WITHOUT_CLASSIFICATION	 there any unknown partition create mapreduce job for the filter prune correctly 
WITHOUT_CLASSIFICATION	 mux operator with parent 
WITHOUT_CLASSIFICATION	 dont override confvars with null values 
WITHOUT_CLASSIFICATION	 least one mrtezspark job 
WITHOUT_CLASSIFICATION	 number objects the block before spilled 
WITHOUT_CLASSIFICATION	 filter the partitions show based supplied spec 
WITHOUT_CLASSIFICATION	 replace original avgx with sumx countx 
WITHOUT_CLASSIFICATION	 alter table tbl via objectstore 
WITHOUT_CLASSIFICATION	 the set dynamic partitions 
WITHOUT_CLASSIFICATION	 middle word 
WITHOUT_CLASSIFICATION	 compare required privileges and available privileges for each hive object 
WITHOUT_CLASSIFICATION	 compose the seconds field from two parts the lowest bits come from the first four 
WITHOUT_CLASSIFICATION	 are recordwriterclose make sense that the context would taskinputoutput 
WITHOUT_CLASSIFICATION	 existing thrift data 
WITHOUT_CLASSIFICATION	 default the bounds checking for maximum number dynamic partitions disabled 
WITHOUT_CLASSIFICATION	 suffix should timestamp 
WITHOUT_CLASSIFICATION	 find out the vertex for the big table 
WITHOUT_CLASSIFICATION	 this the only place where isquery set true defaults false 
WITHOUT_CLASSIFICATION	 assumes serialized dags within and reset structures after each dag completes 
WITHOUT_CLASSIFICATION	 calcite stores timestamp with local timezone utc internally thus when bring back need add the utc suffix 
WITHOUT_CLASSIFICATION	 create the functions and reload them from the metastore 
WITHOUT_CLASSIFICATION	 insert into values gets written into insert from select dummytable this table dummy and has stats 
WITHOUT_CLASSIFICATION	 set rhsexp picks the next char from the token stream 
WITHOUT_CLASSIFICATION	 nonvectorized regular acid reader 
WITHOUT_CLASSIFICATION	 make filter pushdown information available getsplits 
WITHOUT_CLASSIFICATION	 return true this any kind float 
WITHOUT_CLASSIFICATION	 compute locally and assign 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 each row 
WITHOUT_CLASSIFICATION	 minimum seconds 
WITHOUT_CLASSIFICATION	 check that change the hidden list should fail 
WITHOUT_CLASSIFICATION	 the same for rolling the key recreate the fsm with only the key 
WITHOUT_CLASSIFICATION	 have traits and table info present the traits know the exact number buckets else choose the largest number estimated 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 filter mode the column must boolean 
WITHOUT_CLASSIFICATION	 for now make sure that serde exists 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 reattach all registered listeners 
WITHOUT_CLASSIFICATION	 serialize table definition deserialize using the target hcatclient instance 
WITHOUT_CLASSIFICATION	 this method tries convert join smb this done based traits the sorted columns are the same the join columns then can convert the join smb otherwise retain the bucket map join still more efficient than regular join 
WITHOUT_CLASSIFICATION	 given tokselect this checks there subquery top level expression else throws error 
WITHOUT_CLASSIFICATION	 the update failed and could retried 
WITHOUT_CLASSIFICATION	 propagate constants 
WITHOUT_CLASSIFICATION	 default treat the table single column col 
WITHOUT_CLASSIFICATION	 data types 
WITHOUT_CLASSIFICATION	 set the credential provider passwords found there job specific password the credential provider location set directly the execute method localsparkclient 
WITHOUT_CLASSIFICATION	 generic udfs 
WITHOUT_CLASSIFICATION	 throw new 
WITHOUT_CLASSIFICATION	 this may need change the implementation changes 
WITHOUT_CLASSIFICATION	 case theres delay for the heartbeat and the delay long enough trigger the reaper then the txn will time out and aborted here just dont send the heartbeat all infinite delay 
WITHOUT_CLASSIFICATION	 does not support timestamp 
WITHOUT_CLASSIFICATION	 base javaobject primitives javafieldref javaarray entry javaobject javafieldref primitives 
WITHOUT_CLASSIFICATION	 total running 
WITHOUT_CLASSIFICATION	 the aggregation buffers use for each key present the batch 
WITHOUT_CLASSIFICATION	 array structures containing the ngram and its estimated frequency 
WITHOUT_CLASSIFICATION	 read authorization does not work with defaultlegacy authorization mode chicken and egg problem granting select privilege database the grant statement would invoke getdatabase which needs select privilege 
WITHOUT_CLASSIFICATION	 for each take the target mapwork and see dependent sparktask 
WITHOUT_CLASSIFICATION	 this hook verifies that the location every partition the inputs and outputs does not start with the location the table very simple check make sure the location not subdirectory 
WITHOUT_CLASSIFICATION	 propagate reporter and output collector all operators 
WITHOUT_CLASSIFICATION	 verify the auth should fail 
WITHOUT_CLASSIFICATION	 convert the complex lazybinary objects standard java objects downstream operators like filesinkoperator can serialize complex objects the form they expect java objects 
WITHOUT_CLASSIFICATION	 converts date timestamptz 
WITHOUT_CLASSIFICATION	 the binarysortable serialization the current key 
WITHOUT_CLASSIFICATION	 each the errorheuristics repeat for all the lines the log 
WITHOUT_CLASSIFICATION	 the operator type 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream 
WITHOUT_CLASSIFICATION	 when the repeated match due filtering need restore the selected information 
WITHOUT_CLASSIFICATION	 modify insert branch condition particular need modify the 
WITHOUT_CLASSIFICATION	 operation may have been cancelled another thread 
WITHOUT_CLASSIFICATION	 fetch operator not vectorized and such turn vectorization flag off that 
WITHOUT_CLASSIFICATION	 convert integer value seconds since the epoch timestamp value for use long column vector which represented nanoseconds since the epoch 
WITHOUT_CLASSIFICATION	 updaterule 
WITHOUT_CLASSIFICATION	 track still have the entire part 
WITHOUT_CLASSIFICATION	 cookie based authentication when using http transport 
WITHOUT_CLASSIFICATION	 the table does not have any partitions 
WITHOUT_CLASSIFICATION	 setup output stream redirect output 
WITHOUT_CLASSIFICATION	 insert some data this will generate only insert deltas and delete deltas delta 
WITHOUT_CLASSIFICATION	 pick unknown case and let and operator handle the rest 
WITHOUT_CLASSIFICATION	 dont use this one 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 create structs 
WITHOUT_CLASSIFICATION	 not inner join not push the 
WITHOUT_CLASSIFICATION	 make sure small table bytescolumnvectors have room for string values the big table and 
WITHOUT_CLASSIFICATION	 use the positions only pick the partitioncols which are required the small table side 
WITHOUT_CLASSIFICATION	 test whether that held 
WITHOUT_CLASSIFICATION	 for other registered patterns find exact matches 
WITHOUT_CLASSIFICATION	 create string appender capture log output 
WITHOUT_CLASSIFICATION	 skip processing has done first before continuing 
WITHOUT_CLASSIFICATION	 sorting columns the parent are more specific than those the child but sorting order the child more specific than that the parent 
WITHOUT_CLASSIFICATION	 instance tsocket this also not set when kerberos used 
WITHOUT_CLASSIFICATION	 having 
WITHOUT_CLASSIFICATION	 optional string uniquenodeid 
WITHOUT_CLASSIFICATION	 could here either because its unpartitioned table because there are pruning predicates partitioned table 
WITHOUT_CLASSIFICATION	 todo enable caching for queries with maskingfiltering 
WITHOUT_CLASSIFICATION	 publish new segments metadata storage 
WITHOUT_CLASSIFICATION	 other exceptions which defaults 
WITHOUT_CLASSIFICATION	 not making configurable for perf reasons avoid checks 
WITHOUT_CLASSIFICATION	 prepare children 
WITHOUT_CLASSIFICATION	 this creates and publish new segment 
WITHOUT_CLASSIFICATION	 somebody took away our unwanted ducks 
WITHOUT_CLASSIFICATION	 num reduce sinks hardcoded because has parents 
WITHOUT_CLASSIFICATION	 write keyvalue pairs one one 
WITHOUT_CLASSIFICATION	 when indicator null 
WITHOUT_CLASSIFICATION	 project 
WITHOUT_CLASSIFICATION	 the output buffer used serialize value into 
WITHOUT_CLASSIFICATION	 note that and have table with common name 
WITHOUT_CLASSIFICATION	 collect information vectorptfdesc that doesnt need the use this information for validation later when creating the vector operator create additional object vectorptfinfo 
WITHOUT_CLASSIFICATION	 not used 
WITHOUT_CLASSIFICATION	 test table with portion 
WITHOUT_CLASSIFICATION	 want signal error the function doesnt exist and were 
WITHOUT_CLASSIFICATION	 mix functions for 
WITHOUT_CLASSIFICATION	 now validate for the table 
WITHOUT_CLASSIFICATION	 ptf need selectop 
WITHOUT_CLASSIFICATION	 have their permissions mimic the table permissions 
WITHOUT_CLASSIFICATION	 local mode implies that scheme should file can change this going forward 
WITHOUT_CLASSIFICATION	 reach here means needs table authorization check and the table authorization may already happened because other 
WITHOUT_CLASSIFICATION	 modified 
WITHOUT_CLASSIFICATION	 met are not going try merge 
WITHOUT_CLASSIFICATION	 clean anything from the txns table that has components left txncomponents 
WITHOUT_CLASSIFICATION	 the mapjoin operator will encountered many times times for nway join since reducesink operator not allowed before mapjoin the task for the mapjoin will always root task the task corresponding the mapjoin converted root task when the operator encountered for the first time when the operator encountered subsequently the current task merged with the root task for the mapjoin note that possible that the mapjoin task may performed bucketized mapside join sortmerge join the map join operator enhanced contain the bucketing info when encountered 
WITHOUT_CLASSIFICATION	 for partitioned table always track writes partition level never table and for non partitioned always table level thus the same table should never have entries with partition key and 
WITHOUT_CLASSIFICATION	 longs 
WITHOUT_CLASSIFICATION	 there only one destination query try push where predicates join conditions 
WITHOUT_CLASSIFICATION	 this used get hold reference during the current creation tasks and initialized with tasks such that will non consequential any operations done with task tracker compositions 
WITHOUT_CLASSIFICATION	 bgenjjtree const 
WITHOUT_CLASSIFICATION	 package and compress all the hashtable files archive file 
WITHOUT_CLASSIFICATION	 need iterate more when threshold reached beneficial especially for object stores 
WITHOUT_CLASSIFICATION	 also clone the colexprmap default need deep copy 
WITHOUT_CLASSIFICATION	 this operator has been removed remove from the list existing operators 
WITHOUT_CLASSIFICATION	 add via objectstore 
WITHOUT_CLASSIFICATION	 test that fetching nonexistent tablename yields objectnotfound 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 spill tables are 
WITHOUT_CLASSIFICATION	 otherwise have failed the callback has taken care the failure 
WITHOUT_CLASSIFICATION	 returns the node the top the stack and remove from the stack 
WITHOUT_CLASSIFICATION	 ignore object fail not admin succeed admin 
WITHOUT_CLASSIFICATION	 map that keeps track the last operator task the work 
WITHOUT_CLASSIFICATION	 set columns read conf 
WITHOUT_CLASSIFICATION	 preserve the selected reference and size values generated 
WITHOUT_CLASSIFICATION	 use the target directory not specified 
WITHOUT_CLASSIFICATION	 obtain token directly invoking the metastore operationwithout going through the thrift interface obtaining token makes the secret manager aware the user and that gave the token the user also set the authentication method explicitly kerberos since the metastore checks whether the authentication method kerberos not for getdelegationtoken and the testcases dont use kerberos this needs done 
WITHOUT_CLASSIFICATION	 fieldindex becomes simple note that pos starts from while fieldindex starts from 
WITHOUT_CLASSIFICATION	 for small table parents that have already been processed need add the tag the work the reduce work that contains this map join this was not being done for normal mapjoins where the small table typically 
WITHOUT_CLASSIFICATION	 note check for existence deletedeltafile required because may not have 
WITHOUT_CLASSIFICATION	 make sure well use different plan path from the original one 
WITHOUT_CLASSIFICATION	 testtable 
WITHOUT_CLASSIFICATION	 case partition have move each file 
WITHOUT_CLASSIFICATION	 attempt acquire write resources waiting they are not available 
WITHOUT_CLASSIFICATION	 you store the materialized view 
WITHOUT_CLASSIFICATION	 primitive types 
WITHOUT_CLASSIFICATION	 decided not reposition and reread the buffer copy with will still correctly positioned for the next field 
WITHOUT_CLASSIFICATION	 mix functions for 
WITHOUT_CLASSIFICATION	 description 
WITHOUT_CLASSIFICATION	 left child 
WITHOUT_CLASSIFICATION	 indicates temporary error not corruption rethrow this exception 
WITHOUT_CLASSIFICATION	 scale 
WITHOUT_CLASSIFICATION	 with ptfs there maybe more note for ptfchains 
WITHOUT_CLASSIFICATION	 add write hooks needed 
WITHOUT_CLASSIFICATION	 tolerate repeated use big table column 
WITHOUT_CLASSIFICATION	 check configs are hidden 
WITHOUT_CLASSIFICATION	 werent provided any actual qualifier name set these 
WITHOUT_CLASSIFICATION	 ends getting rid project since not used further the tree 
WITHOUT_CLASSIFICATION	 call hiveclosecurrent that closes the hms connection causes hms connection leaks otherwise 
WITHOUT_CLASSIFICATION	 run rules aid translation from calcite tree hive tree 
WITHOUT_CLASSIFICATION	 the form partition not stripping quotes here need use while framing partition clause insert query 
WITHOUT_CLASSIFICATION	 early exit getting file lengths can expensive object stores 
WITHOUT_CLASSIFICATION	 newinstance should always the same type object this 
WITHOUT_CLASSIFICATION	 now notify the executorservice that the task has moved finishable state 
WITHOUT_CLASSIFICATION	 add some columns 
WITHOUT_CLASSIFICATION	 hadoop this what controls the timeout 
WITHOUT_CLASSIFICATION	 implements 
WITHOUT_CLASSIFICATION	 the first argument const then just set the flag and continue 
WITHOUT_CLASSIFICATION	 vrb mode process the vrbs with cache data the new cache data coming later 
WITHOUT_CLASSIFICATION	 replicate the remaining insert overwrite operation the table 
WITHOUT_CLASSIFICATION	 creating path expensive cache the corresponding path object normalizedpaths 
WITHOUT_CLASSIFICATION	 remove the condition replacing with true 
WITHOUT_CLASSIFICATION	 catch the exception log and rethrow 
WITHOUT_CLASSIFICATION	 need enforce precisionscale here 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream int 
WITHOUT_CLASSIFICATION	 will estimate collection object only its field 
WITHOUT_CLASSIFICATION	 project any correlated variables the input wants pass along 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 empty map case 
WITHOUT_CLASSIFICATION	 the owner can change also owner might appear user grants well keep owner privileges separate from usergrants 
WITHOUT_CLASSIFICATION	 whether the variable was true when the vectorizer class evaluated vectorizing this node when vectorized input file format looks this flag can determine whether should operate vectorized not some modes the node can vectorized but use row serialization 
WITHOUT_CLASSIFICATION	 logwarnno partition found genereated dynamic partitioning loadpath with dynspecdynpathspec 
WITHOUT_CLASSIFICATION	 called for each partition big table and populates mapping for each file the partition 
WITHOUT_CLASSIFICATION	 per jdbc spec section the driver implementation understands the url will return connection object otherwise returns null 
WITHOUT_CLASSIFICATION	 get substring 
WITHOUT_CLASSIFICATION	 mapreduce job 
WITHOUT_CLASSIFICATION	 report failure the main thread 
WITHOUT_CLASSIFICATION	 initialize reporters 
WITHOUT_CLASSIFICATION	 the only conf allowed have the metastore pwd keyname the hidden list configuration value 
WITHOUT_CLASSIFICATION	 care only about openaborted txns below currenttxn and hence the size should determined for the exceptions list the currenttxn will missing opentxns list only rare case like txn aborted and compactor actually cleans the aborted txns for such cases get negative value for sizetohwm with found position for currenttxn and 
WITHOUT_CLASSIFICATION	 need stay out the way any sequences used the underlying database otherwise the next time the client tries add catalog well get error there should never billions catalogs well shift our sequence number 
WITHOUT_CLASSIFICATION	 noop testing events only 
WITHOUT_CLASSIFICATION	 save the vector description for the explain 
WITHOUT_CLASSIFICATION	 handle table populate aliases appropriately leftaliases should contain the first table rightaliases should contain all other tables and basesrc should contain all tables 
WITHOUT_CLASSIFICATION	 compute collations 
WITHOUT_CLASSIFICATION	 this lossy invert the function above which produces hashcode which collides with the current winner the register lose all higher bits but get all bits useful for lesser pbit options 
WITHOUT_CLASSIFICATION	 stop tracking the fragment and rethrow the error 
WITHOUT_CLASSIFICATION	 check different filesystems 
WITHOUT_CLASSIFICATION	 constructors are marked private use create methods 
WITHOUT_CLASSIFICATION	 manufacture statsaggregator 
WITHOUT_CLASSIFICATION	 only need write out close the deletedelta there have been any 
WITHOUT_CLASSIFICATION	 deserialize the fields into the overflow batch using the buffered batch column map 
WITHOUT_CLASSIFICATION	 check the table directory 
WITHOUT_CLASSIFICATION	 some the data set the server side reset those 
WITHOUT_CLASSIFICATION	 session identifier 
WITHOUT_CLASSIFICATION	 update mappings oldinput newinput newproject oldinput newinput transformed oldinput newproject 
WITHOUT_CLASSIFICATION	 replace existing table 
WITHOUT_CLASSIFICATION	 first time this seen log 
WITHOUT_CLASSIFICATION	 toklateralview tokselect tokselexpr tokfunction identifierinline valuesclause identifier tablealias 
WITHOUT_CLASSIFICATION	 linking these two operator declares that they are representing the same thing currently important because statistincs are actually gather for newop but the lookup done using oldop 
WITHOUT_CLASSIFICATION	 additional data structures needed for the join optimization 
WITHOUT_CLASSIFICATION	 template classname valuetype operatorsymbol descriptionname descriptionvalue 
WITHOUT_CLASSIFICATION	 successfully scheduled 
WITHOUT_CLASSIFICATION	 groupby query results records 
WITHOUT_CLASSIFICATION	 set appropriate ownerperms the dir only need recurse 
WITHOUT_CLASSIFICATION	 nothing set 
WITHOUT_CLASSIFICATION	 didnt seem useful create another constants class just for these though 
WITHOUT_CLASSIFICATION	 delete table data 
WITHOUT_CLASSIFICATION	 due the way use the allocationfree cast from hivedecimalwriter decimal not have the luxury bytebuffer 
WITHOUT_CLASSIFICATION	 the output objectinspector 
WITHOUT_CLASSIFICATION	 only print out one task because thats good enough for debugging 
WITHOUT_CLASSIFICATION	 create row per table name 
WITHOUT_CLASSIFICATION	 use boundarytype boundaryamt sort key order behavior case preceding unb any any error preceding unsigned int null desc end partitionsize asc end preceding unsigned int not null desc scan backward until row such that rsk rsk bndamt end ridx preceding unsigned int not null asc scan backward until row such that rsk rsk bndamt end ridx current row null any scan forward until row such that rsk not null end ridx current row not null any scan forward until row such that rsk rsk end ridx following unb any any end partitionsize following unsigned int null desc end partitionsize asc scan forward until row such that rsk not null end ridx following unsigned int not null desc scan forward until row such rsk rsk bndamt end ridx asc scan forward until row such rsk rsk bndamt end ridx 
WITHOUT_CLASSIFICATION	 check that the agg the following type 
WITHOUT_CLASSIFICATION	 this null only read the isnull byte for level because the toplevel struct can never null 
WITHOUT_CLASSIFICATION	 variance check 
WITHOUT_CLASSIFICATION	 iterate thru all the filecaches this besteffort these superlonglived iterators affect the map some bad way 
WITHOUT_CLASSIFICATION	 write file 
WITHOUT_CLASSIFICATION	 add the privileges not supported the list privileges supported implementation defined 
WITHOUT_CLASSIFICATION	 the code inside the attribute getter threw exception log and fall back the class name 
WITHOUT_CLASSIFICATION	 initialize complete map reduce configuration 
WITHOUT_CLASSIFICATION	 current hive parquet timestamp implementation stores timestamps utc but other components not this case skip timestamp conversion this file written version hive before hive file metadata will not contain the writer timezone convert the timestamp the system reader time zone file written current hive implementation convert timestamps the writer time zone order emulate time zone agnostic behavior 
WITHOUT_CLASSIFICATION	 the table sorted partition column not valid for sorting 
WITHOUT_CLASSIFICATION	 after committing the initial txns and updating current number open txns back 
WITHOUT_CLASSIFICATION	 get udaf info using udaf evaluator 
WITHOUT_CLASSIFICATION	 default dont convert unix 
WITHOUT_CLASSIFICATION	 timeout for the iteration case asynchronous execute 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 noncbo path retries execute subqueries and throws completely different exceptionerror eclipse the original error message avoid executing subqueries noncbo 
WITHOUT_CLASSIFICATION	 create archived version the partition directory ending thats the same level the partition does not already exist does exist assume the dir good 
WITHOUT_CLASSIFICATION	 noticed that also suffer from the same issue hive only want call field inited when its nonnull check twice make sure get null both times 
WITHOUT_CLASSIFICATION	 this can only happen case failure read some data but didnt decompress deallocate the buffer directly not decref 
WITHOUT_CLASSIFICATION	 flush the print stream doesnt include output from the last command 
WITHOUT_CLASSIFICATION	 test february nonleap year viewd due days diff from 
WITHOUT_CLASSIFICATION	 timeouts are bad mmmkay 
WITHOUT_CLASSIFICATION	 tokenstrform 
WITHOUT_CLASSIFICATION	 convert the set into list 
WITHOUT_CLASSIFICATION	 set child environment setting hadoopclientopts will replaced with hadoopopts updated too since hadoopclientopts appended hadoopopts most cases this way the local task jvm can 
WITHOUT_CLASSIFICATION	 the channel listener instantiates the rpc instance when the connection established 
WITHOUT_CLASSIFICATION	 assuming that this closes the underlying streams 
WITHOUT_CLASSIFICATION	 restrictions correlated expression outer query must not contain unqualified column references disabled its obvious allow unqualified refs 
WITHOUT_CLASSIFICATION	 column family mapped mapstringstring 
WITHOUT_CLASSIFICATION	 nothing can here just proceed normally from now 
WITHOUT_CLASSIFICATION	 verify that hiveserver config not loaded 
WITHOUT_CLASSIFICATION	 open transactions 
WITHOUT_CLASSIFICATION	 not reduce the input size bail out 
WITHOUT_CLASSIFICATION	 prevents task from being processed multiple times 
WITHOUT_CLASSIFICATION	 rootpath 
WITHOUT_CLASSIFICATION	 testing with multibyte string 
WITHOUT_CLASSIFICATION	 bgenjjtree constlist 
WITHOUT_CLASSIFICATION	 the current nonnull key position 
WITHOUT_CLASSIFICATION	 implicit cast needed 
WITHOUT_CLASSIFICATION	 conf validator already checks this will never trigger usually 
WITHOUT_CLASSIFICATION	 ensure the session open and has the necessary local resources 
WITHOUT_CLASSIFICATION	 test conversion longstring 
WITHOUT_CLASSIFICATION	 might visiting twice because reutilization intermediary results that the case not need anything because either have already connected this operator will connect subsequent pass 
WITHOUT_CLASSIFICATION	 skip tokquery 
WITHOUT_CLASSIFICATION	 register both not fire the rule them again 
WITHOUT_CLASSIFICATION	 dag might have been killed lets try get vertex state from before dying 
WITHOUT_CLASSIFICATION	 bigtablefound means weve encountered table thats bigger than the 
WITHOUT_CLASSIFICATION	 has tag need set later 
WITHOUT_CLASSIFICATION	 mask this digit 
WITHOUT_CLASSIFICATION	 first add all children this work into queue processed later 
WITHOUT_CLASSIFICATION	 compare with tenscale example tenscale will zero after scaling 
WITHOUT_CLASSIFICATION	 this decoded compression buffer add 
WITHOUT_CLASSIFICATION	 set total number rows from all memory partitions 
WITHOUT_CLASSIFICATION	 verbose mode print update per recordprintinterval records 
WITHOUT_CLASSIFICATION	 map work starts with table scan operators 
WITHOUT_CLASSIFICATION	 this point everything the list going have refcount one unless failed between the allocation and the incref for single item should 
WITHOUT_CLASSIFICATION	 underscoreint 
WITHOUT_CLASSIFICATION	 exhausted all delete records return 
WITHOUT_CLASSIFICATION	 special treatment for filter operator that ignores the dpp predicates 
WITHOUT_CLASSIFICATION	 create row related objects 
WITHOUT_CLASSIFICATION	 reuse existing text member varchar writable 
WITHOUT_CLASSIFICATION	 for native vectorized map join require the key serde binarysortableserde note the may not really get nativelyvectorized later but changing serde wont hurt correctness 
WITHOUT_CLASSIFICATION	 update the partition columns small table ensure correct routing hash tables 
WITHOUT_CLASSIFICATION	 escape the escape 
WITHOUT_CLASSIFICATION	 preallocated member for storing index into the hashmultisetresults for each spilled row 
WITHOUT_CLASSIFICATION	 determine mapping between project input and output fields hive sort always based rexinputref only need check project can contain all the positions that sort needs 
WITHOUT_CLASSIFICATION	 adjacencylist 
WITHOUT_CLASSIFICATION	 close client session 
WITHOUT_CLASSIFICATION	 these parameters controls the maximum number concurrent job submitstatuslist operations templeton service more number concurrent requests comes then they will rejected with busyexception 
WITHOUT_CLASSIFICATION	 output has nonulls set false set the isnull false carefully 
WITHOUT_CLASSIFICATION	 recursively call the join the other rhs tables 
WITHOUT_CLASSIFICATION	 the actual size will assigned setchildreninfo after reading complete 
WITHOUT_CLASSIFICATION	 not primitive check struct and can infer common class 
WITHOUT_CLASSIFICATION	 check any the txns the list committed yes throw exception 
WITHOUT_CLASSIFICATION	 tried scheduling everything that could scheduled this loop 
WITHOUT_CLASSIFICATION	 the job for compaction 
WITHOUT_CLASSIFICATION	 destf 
WITHOUT_CLASSIFICATION	 test for duplicate publish this will either fail job creation time and throw exception will fail runtime and fail the job 
WITHOUT_CLASSIFICATION	 there should only directory left basexxxxxxx 
WITHOUT_CLASSIFICATION	 the plan for this reducer does not exist initialize the plan 
WITHOUT_CLASSIFICATION	 test replicated drop should drop this time since replstateid evid 
WITHOUT_CLASSIFICATION	 high word 
WITHOUT_CLASSIFICATION	 minutes 
WITHOUT_CLASSIFICATION	 insert select operator here used the columnpruner reduce 
WITHOUT_CLASSIFICATION	 create split for the previous unfinished stripe 
WITHOUT_CLASSIFICATION	 could acquire table level sharedwrite intead 
WITHOUT_CLASSIFICATION	 first one will fail count 
WITHOUT_CLASSIFICATION	 create all nulls key 
WITHOUT_CLASSIFICATION	 ordering 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 high and middle word must zero check for overflow digits lower word 
WITHOUT_CLASSIFICATION	 external llap clients would need set llapzkregistryuser the llap daemon user hive rather than relying 
WITHOUT_CLASSIFICATION	 the vertex cannot configured until all dataevents are seen 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 swap debug options hadoopclientopts those that the child jvm should have 
WITHOUT_CLASSIFICATION	 which can affect the working all downstream transformations 
WITHOUT_CLASSIFICATION	 estimate the same way for compressed and uncompressed for now 
WITHOUT_CLASSIFICATION	 optional int 
WITHOUT_CLASSIFICATION	 any child work for this work already added the targetwork earlier should connect this work with 
WITHOUT_CLASSIFICATION	 reader creation updates hdfs counters dont here 
WITHOUT_CLASSIFICATION	 second granularity 
WITHOUT_CLASSIFICATION	 try nonchunked stream there should issues assuming flushed the streams before closing 
WITHOUT_CLASSIFICATION	 compose query that select transactions containing update 
WITHOUT_CLASSIFICATION	 delegate the new api 
WITHOUT_CLASSIFICATION	 col 
WITHOUT_CLASSIFICATION	 use spark rdd async action submit job its the only way get jobid now 
WITHOUT_CLASSIFICATION	 invoke the right unpack method depending data type the column 
WITHOUT_CLASSIFICATION	 couldnt jdoql filter pushdown get names via normal means 
WITHOUT_CLASSIFICATION	 core pool size 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 prefix used auto generated column aliases this should started with 
WITHOUT_CLASSIFICATION	 nothing compact update expr with compacted children 
WITHOUT_CLASSIFICATION	 case when user has not specified any ingestion state the current command there kafka supervisor running then keep last known state start otherwise stop 
WITHOUT_CLASSIFICATION	 this local file 
WITHOUT_CLASSIFICATION	 not applicable 
WITHOUT_CLASSIFICATION	 this mapping collects all the configuration variables which have been set the user explicitly either via set the cli the hiveconf option system property mapping from the variable name its value note that user repeatedly 
WITHOUT_CLASSIFICATION	 avoid traversing the tree later save memory this could array byte arrays 
WITHOUT_CLASSIFICATION	 fetch the row inserted before schema altered and verify 
WITHOUT_CLASSIFICATION	 call the metastore get the status all known compactions completed get purged eventually 
WITHOUT_CLASSIFICATION	 txnid means its select iud which does not write acid table insert overwrite table partitionp select from and autocommittrue 
WITHOUT_CLASSIFICATION	 allowcomplex 
WITHOUT_CLASSIFICATION	 temporarily 
WITHOUT_CLASSIFICATION	 see hcatalog 
WITHOUT_CLASSIFICATION	 casts 
WITHOUT_CLASSIFICATION	 also print out the generic lineage information there any 
WITHOUT_CLASSIFICATION	 this tells the pending update any that whatever doing irrelevant and also makes sure dont take the duck back twice this called twice 
WITHOUT_CLASSIFICATION	 save previous longword 
WITHOUT_CLASSIFICATION	 initialize some variables which used initialized commonjoinoperator 
WITHOUT_CLASSIFICATION	 the default fraction 
WITHOUT_CLASSIFICATION	 return the passed string value 
WITHOUT_CLASSIFICATION	 cast int double 
WITHOUT_CLASSIFICATION	 remove 
WITHOUT_CLASSIFICATION	 prepare output set the projections 
WITHOUT_CLASSIFICATION	 dont deal with columns rhs set expression since the whole expr part the rewritten sql statement and thus handled semanticanalzyer nor have figure which cols rhs are from source and which from target 
WITHOUT_CLASSIFICATION	 type intervaldaytime 
WITHOUT_CLASSIFICATION	 this function for internal use only 
WITHOUT_CLASSIFICATION	 cache mkeygroup 
WITHOUT_CLASSIFICATION	 the current filters use follows evfilter eventutilsandfilter tblnameorpattern eventto test each those three filters and then test andfilter itself 
WITHOUT_CLASSIFICATION	 define how pass options the child process launching client local mode the driver options need passed directly the command line otherwise 
WITHOUT_CLASSIFICATION	 cleanup pathtopartitioninfo 
WITHOUT_CLASSIFICATION	 adjust noconditional task size threshold for llap 
WITHOUT_CLASSIFICATION	 direct and not memory mapped 
WITHOUT_CLASSIFICATION	 remove distinctcolindices set reducer reset keys 
WITHOUT_CLASSIFICATION	 trigger post compilation hook note that the compilation fails here then beforeafter execution hook will never executed 
WITHOUT_CLASSIFICATION	 original files delta directory deletedelta directory and base directory 
WITHOUT_CLASSIFICATION	 create new operator which share the table desc 
WITHOUT_CLASSIFICATION	 finally create the vertex 
WITHOUT_CLASSIFICATION	 not use datetime tests avoid result changes 
WITHOUT_CLASSIFICATION	 keep track all the contexts that are created this query can clear them when finish execution 
WITHOUT_CLASSIFICATION	 contract eof differs between datainput and inputstream 
WITHOUT_CLASSIFICATION	 get the partition columns from the end derivedschema 
WITHOUT_CLASSIFICATION	 smaller prefix mdhash and later will stored such staging stats table when stats gets aggregated statstask only the keys that starts with prefix will fetched now that hashed smaller prefix will not retrieved from staging table and hence not aggregated avoid this issue will remove the taskid from the key which redundant anyway 
WITHOUT_CLASSIFICATION	 the file held writer will throw 
WITHOUT_CLASSIFICATION	 cache uses allocator allocate and deallocate create allocator and then caches 
WITHOUT_CLASSIFICATION	 this test with hdfs acls will only work filesystemaccess available the version hadoop used build hive 
WITHOUT_CLASSIFICATION	 block until all semaphore resources are released outstanding async writes 
WITHOUT_CLASSIFICATION	 try singular 
WITHOUT_CLASSIFICATION	 parenttblname 
WITHOUT_CLASSIFICATION	 archiving unarchiving process 
WITHOUT_CLASSIFICATION	 the epoch 
WITHOUT_CLASSIFICATION	 the api authorizer use the session state getauthorizer return null here disable authorization use api the the additional authorization checks happening hcatalog are designed work with storage based authorization client side should not try doing additional checks authorizer use the recommended configuration use storage based authorization metastore server however user define custom authorization will honored 
WITHOUT_CLASSIFICATION	 dont allow swapping between virtual and materialized view replace 
WITHOUT_CLASSIFICATION	 server will create new threads max necessary after idle period will destroy threads keep the number threads the 
WITHOUT_CLASSIFICATION	 table tstage 
WITHOUT_CLASSIFICATION	 dont use the hadoopjobexechooks for local tasks 
WITHOUT_CLASSIFICATION	 try preempting task that higher priority task can take its place 
WITHOUT_CLASSIFICATION	 evaluate the column boolean converting necessary 
WITHOUT_CLASSIFICATION	 expect the start and count divisible step 
WITHOUT_CLASSIFICATION	 repeating case for first boolean flag argument 
WITHOUT_CLASSIFICATION	 both cases move the file under destf 
WITHOUT_CLASSIFICATION	 stream offset relation the stripe figure out which columns have present stream 
WITHOUT_CLASSIFICATION	 set the fetch formatter noop for the listsinkoperator since well write out formatted thrift objects sequencefile 
WITHOUT_CLASSIFICATION	 isextended 
WITHOUT_CLASSIFICATION	 declare this method final for performance reasons 
WITHOUT_CLASSIFICATION	 multikey specific members 
WITHOUT_CLASSIFICATION	 copy the data the buffer 
WITHOUT_CLASSIFICATION	 use protected for the fields the fasthivedecimalimpl class can access them other classes including hivedecimal should not access these fields directly 
WITHOUT_CLASSIFICATION	 todo pass this exception 
WITHOUT_CLASSIFICATION	 use the rowid directly 
WITHOUT_CLASSIFICATION	 finally start the server 
WITHOUT_CLASSIFICATION	 dont record encodings for unneeded columns 
WITHOUT_CLASSIFICATION	 skip overwriting exisiting table object which present because was added after prewarm started 
WITHOUT_CLASSIFICATION	 sanity checks 
WITHOUT_CLASSIFICATION	 ensure there operation related object leak 
WITHOUT_CLASSIFICATION	 not actually getter 
WITHOUT_CLASSIFICATION	 this correlation muxoperators 
WITHOUT_CLASSIFICATION	 verify zerodivide result for position 
WITHOUT_CLASSIFICATION	 parse until field separator currentlevel 
WITHOUT_CLASSIFICATION	 spark local mode need search added files root directory 
WITHOUT_CLASSIFICATION	 the following loop should create stripes the orc file 
WITHOUT_CLASSIFICATION	 read split 
WITHOUT_CLASSIFICATION	 put the keyvalue into the map 
WITHOUT_CLASSIFICATION	 not using position alias and number 
WITHOUT_CLASSIFICATION	 binary sortable key serializer 
WITHOUT_CLASSIFICATION	 must deterministic order map for consistent qtest output across java versions 
WITHOUT_CLASSIFICATION	 for unit tests 
WITHOUT_CLASSIFICATION	 pass lineagestate when driver instantiates another driver run 
WITHOUT_CLASSIFICATION	 still nothing raise exception 
WITHOUT_CLASSIFICATION	 this operation 
WITHOUT_CLASSIFICATION	 needed intercept readclassandobject 
WITHOUT_CLASSIFICATION	 production thisname basetype maptype settype listtype 
WITHOUT_CLASSIFICATION	 partname 
WITHOUT_CLASSIFICATION	 currently getprimarykeys always returns empty resultset for hive 
WITHOUT_CLASSIFICATION	 generate groupbyoperator for mapside partial aggregation 
WITHOUT_CLASSIFICATION	 the general case this set restricts automatic type conversion just these functions 
WITHOUT_CLASSIFICATION	 the default hook 
WITHOUT_CLASSIFICATION	 currently avgdistinct not supported partitionevaluator 
WITHOUT_CLASSIFICATION	 otherwise the registry has not been initialized skip for the time being 
WITHOUT_CLASSIFICATION	 lazy object inspectors for stringcharvarchar will all cached the same map 
WITHOUT_CLASSIFICATION	 see 
WITHOUT_CLASSIFICATION	 getfunctions 
WITHOUT_CLASSIFICATION	 skip the step connect the metastore 
WITHOUT_CLASSIFICATION	 grab round digit from middle word 
WITHOUT_CLASSIFICATION	 this full outer join this can never mapjoin any type return false 
WITHOUT_CLASSIFICATION	 include failedupdate only after looking all the tasks the same priority 
WITHOUT_CLASSIFICATION	 get evaluator for string concatenation expression 
WITHOUT_CLASSIFICATION	 issue would happen there was tiny delay the network dont care 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 for partitions flag controlling whether the current table specs are used 
WITHOUT_CLASSIFICATION	 generate the temporary file 
WITHOUT_CLASSIFICATION	 all must selected otherwise size would zero repeating property will not change 
WITHOUT_CLASSIFICATION	 reset keyinitedmapsize flag since may set true the case previous empty entry 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 remove the dummy store operator from the tree 
WITHOUT_CLASSIFICATION	 check the function really removed 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 addbasefilet 
WITHOUT_CLASSIFICATION	 have set the bucketing columns differently for update and deletes 
WITHOUT_CLASSIFICATION	 here are some negative cases below 
WITHOUT_CLASSIFICATION	 use linkedhashset give predictable display order 
WITHOUT_CLASSIFICATION	 note could skip creating the table and just add table type stuff directly the 
WITHOUT_CLASSIFICATION	 this the same the setchildren method below but for empty tables takes care the following create the right object inspector set the childrenoptooi with the object inspector ensure that the initialization happens correctly 
WITHOUT_CLASSIFICATION	 varchar should take string length into account varchar varchar varchar 
WITHOUT_CLASSIFICATION	 confirm the batch sizes were expected 
WITHOUT_CLASSIFICATION	 txns 
WITHOUT_CLASSIFICATION	 discard the blocks 
WITHOUT_CLASSIFICATION	 partition columns occur data want remove them 
WITHOUT_CLASSIFICATION	 note pass null factory because allocate objects here could also pass percall factory that would set filekey set after put 
WITHOUT_CLASSIFICATION	 same getrecordreader 
WITHOUT_CLASSIFICATION	 check the cache first 
WITHOUT_CLASSIFICATION	 the deserializer responsible for actually reading each record from the stream 
WITHOUT_CLASSIFICATION	 case select the data size does not change 
WITHOUT_CLASSIFICATION	 init parse context 
WITHOUT_CLASSIFICATION	 sort the objects first you are guaranteed that partition being locked the table has already been locked 
WITHOUT_CLASSIFICATION	 revoke with grant option only remove the grant option but keep the role 
WITHOUT_CLASSIFICATION	 create output row objectinspector 
WITHOUT_CLASSIFICATION	 generate possibly get from cached result parent sparktran 
WITHOUT_CLASSIFICATION	 succeeded state 
WITHOUT_CLASSIFICATION	 this function returns the grouping sets along with the grouping expressions even rollups and cubes are present the query they are converted 
WITHOUT_CLASSIFICATION	 worthwhile only more than split and filesplit 
WITHOUT_CLASSIFICATION	 exclude insert queries 
WITHOUT_CLASSIFICATION	 add tinyint values 
WITHOUT_CLASSIFICATION	 key val 
WITHOUT_CLASSIFICATION	 verify that the actual action also went through 
WITHOUT_CLASSIFICATION	 after one exception everything expected run 
WITHOUT_CLASSIFICATION	 skip the next child since already took care 
WITHOUT_CLASSIFICATION	 excepted 
WITHOUT_CLASSIFICATION	 lets add lot constant rows test the rle 
WITHOUT_CLASSIFICATION	 set the cookie max age very low value that 
WITHOUT_CLASSIFICATION	 change the key need 
WITHOUT_CLASSIFICATION	 nothing there not index definition for this table 
WITHOUT_CLASSIFICATION	 first breaking the filter conditions into equality comparisons between rightjoinkeysfrom the original filterinputrel and correlatedjoinkeys correlatedjoinkeys can expressions while rightjoinkeys need input 
WITHOUT_CLASSIFICATION	 get most the fields for the ids provided 
WITHOUT_CLASSIFICATION	 this should called rarely enough for now its just lock every time 
WITHOUT_CLASSIFICATION	 the object count longwritable sum resulttype reused during evaluating 
WITHOUT_CLASSIFICATION	 all parents should reduce sinks pick the one just walked choose the number reducers the joinunion case they will all sortorder case where matters there will only one parent 
WITHOUT_CLASSIFICATION	 clear out any parents reducer the root 
WITHOUT_CLASSIFICATION	 final string 
WITHOUT_CLASSIFICATION	 wait for stream threads finish 
WITHOUT_CLASSIFICATION	 would possible support this but this such pointless command 
WITHOUT_CLASSIFICATION	 ssilent 
WITHOUT_CLASSIFICATION	 release all the locks acquired for this object this becomes important for multitable inserts when one branch may take much more time than the others better release the lock for this particular insert the other option wait for all the branches finish set true which will mean that the first multiinsert results will available when all the branches multitable 
WITHOUT_CLASSIFICATION	 replace earlier element must have lower offset 
WITHOUT_CLASSIFICATION	 validate unset non existed table properties 
WITHOUT_CLASSIFICATION	 first exact match and then prefix matching the latter due input dir could dirdspart where part not part partition 
WITHOUT_CLASSIFICATION	 set the relevant information the configuration for the accumuloinputformat 
WITHOUT_CLASSIFICATION	 todo vcpu settings possibly when drfa works right 
WITHOUT_CLASSIFICATION	 allocate triples cannot above highest integer power 
WITHOUT_CLASSIFICATION	 use numdistinctvalues possible 
WITHOUT_CLASSIFICATION	 replace the map join operator localmapjoin operator the operator tree 
WITHOUT_CLASSIFICATION	 path format 
WITHOUT_CLASSIFICATION	 partition columns are repeated test element 
WITHOUT_CLASSIFICATION	 swap and thx 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite table with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 alter partitioned table rename partition 
WITHOUT_CLASSIFICATION	 row sufficient know have kill the query 
WITHOUT_CLASSIFICATION	 average value size will sum all sizes aggregation buffers 
WITHOUT_CLASSIFICATION	 call addtranslation just get the assertions for overlap 
WITHOUT_CLASSIFICATION	 set local work 
WITHOUT_CLASSIFICATION	 normalize was used then should have the same sign this currently working with that assumption 
WITHOUT_CLASSIFICATION	 validate the items are only constants 
WITHOUT_CLASSIFICATION	 indicates malformed version 
WITHOUT_CLASSIFICATION	 for query the type insert overwrite table select from subq union all subqu subq and subq write directories parentchild and parentchild respectively and union removed the movetask that follows subq and subq tasks moves the directory parent 
WITHOUT_CLASSIFICATION	 convert lower case case are getting from serde 
WITHOUT_CLASSIFICATION	 callback method used subclasses set the outputoi the evaluator 
WITHOUT_CLASSIFICATION	 the real implementation for the instanceset instanceset has its own copy the cache yet completely depends the parent every other aspect and thus unneeded 
WITHOUT_CLASSIFICATION	 from preacid insert 
WITHOUT_CLASSIFICATION	 there should different txn ids associated with each lock 
WITHOUT_CLASSIFICATION	 master thread methods 
WITHOUT_CLASSIFICATION	 list columns comma separated 
WITHOUT_CLASSIFICATION	 followed each element 
WITHOUT_CLASSIFICATION	 definitely byte most bytes fall here 
WITHOUT_CLASSIFICATION	 supports all types 
WITHOUT_CLASSIFICATION	 create standard settable union object inspector 
WITHOUT_CLASSIFICATION	 see also code clidriverjava 
WITHOUT_CLASSIFICATION	 this chicanery get around the fact that the table needs final order 
WITHOUT_CLASSIFICATION	 nothing special needs done for grouping sets this the final group operator and multiple rows corresponding the grouping sets have been generated upstream however addition job has been created handle grouping sets additional rows corresponding grouping sets need created here 
WITHOUT_CLASSIFICATION	 ignore other types for purposes authorization 
WITHOUT_CLASSIFICATION	 closed from orc writer still need the data not discard anything 
WITHOUT_CLASSIFICATION	 input type date epochdays 
WITHOUT_CLASSIFICATION	 cpu cost sorting cost for each relation 
WITHOUT_CLASSIFICATION	 char 
WITHOUT_CLASSIFICATION	 todo the name from the creation metadata for any the tables has changed 
WITHOUT_CLASSIFICATION	 this wont usually called otherwise 
WITHOUT_CLASSIFICATION	 the file whence this split 
WITHOUT_CLASSIFICATION	 pop list map 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktable call check side file for mockmocktbl call open mockmocktbl call check side file for mockmocktbl 
WITHOUT_CLASSIFICATION	 find the right 
WITHOUT_CLASSIFICATION	 operations will lost once owning session closed 
WITHOUT_CLASSIFICATION	 then try serdeproperties 
WITHOUT_CLASSIFICATION	 now only used for bucket mapjoin there exactly one event the list 
WITHOUT_CLASSIFICATION	 any the fields struct are representing null then return true 
WITHOUT_CLASSIFICATION	 class 
WITHOUT_CLASSIFICATION	 asts are slightly different 
WITHOUT_CLASSIFICATION	 whether the method takes variablelength arguments whether the method takes array like object string etc the last argument 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 for performance dont check that that the fieldref isnt recid everytime just assume that the caller used and thus doesnt have that fieldref 
WITHOUT_CLASSIFICATION	 recheck 
WITHOUT_CLASSIFICATION	 currently none 
WITHOUT_CLASSIFICATION	 note that set basic stats false will wipe out column stats too 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream int 
WITHOUT_CLASSIFICATION	 subtraction with overflow check overflow produces null output 
WITHOUT_CLASSIFICATION	 ugi for the hivehost kerberos principal 
WITHOUT_CLASSIFICATION	 archiving was done this upper level every matched partition would archived not being archived means archiving was done neither this nor upper level 
WITHOUT_CLASSIFICATION	 enabled for acid case and the file format orc 
WITHOUT_CLASSIFICATION	 create lazystruct with serialized string with expected separators 
WITHOUT_CLASSIFICATION	 outputstream null means need process for explain formatted 
WITHOUT_CLASSIFICATION	 prefix used specify module specific properties mainly avoid conflicts with older unittests properties 
WITHOUT_CLASSIFICATION	 the aggregation buffer already contains partial histogram therefore need merge histograms using algorithm from the benhaim and tomtov paper 
WITHOUT_CLASSIFICATION	 incase acid the file orc the extension not relevant and should not inherited 
WITHOUT_CLASSIFICATION	 new partition for example 
WITHOUT_CLASSIFICATION	 colvals 
WITHOUT_CLASSIFICATION	 true 
WITHOUT_CLASSIFICATION	 bucketed mapjoin cannot performed 
WITHOUT_CLASSIFICATION	 constants for sparse encoding 
WITHOUT_CLASSIFICATION	 another thread might have already created these tables 
WITHOUT_CLASSIFICATION	 optional int version 
WITHOUT_CLASSIFICATION	 theres bug ctor where seconds are not converted 
WITHOUT_CLASSIFICATION	 clear timing this threads hive object before proceeding 
WITHOUT_CLASSIFICATION	 assign tables without nested column pruning info the default conf 
WITHOUT_CLASSIFICATION	 requires calculate stats new partition doesnt have 
WITHOUT_CLASSIFICATION	 test repeating case for null value 
WITHOUT_CLASSIFICATION	 singlecolumn long specific save key 
WITHOUT_CLASSIFICATION	 this table working with acid semantics turn off merging 
WITHOUT_CLASSIFICATION	 get right table alias 
WITHOUT_CLASSIFICATION	 initialize the list event handlers 
WITHOUT_CLASSIFICATION	 since key expression can calculation and the key will into scratch column 
WITHOUT_CLASSIFICATION	 nothing vectorized expression that passes all rows through 
WITHOUT_CLASSIFICATION	 load the current incremental dump and ensure does nothing and lastreplid remains same 
WITHOUT_CLASSIFICATION	 important important important the keys used store info into the job configuration any new keys are added the hcatstorer needs updated the hcatstorer updates the job configuration the backend insert these keys avoid having call setoutput from the backend which would cause metastore call 
WITHOUT_CLASSIFICATION	 there equivalent version return that else return this version 
WITHOUT_CLASSIFICATION	 now set one column nullable 
WITHOUT_CLASSIFICATION	 test the unsecure base case when neither hadoop nor jobspecific credential provider set 
WITHOUT_CLASSIFICATION	 regular create table ddl 
WITHOUT_CLASSIFICATION	 removing semijoin optimization when may not beneficial 
WITHOUT_CLASSIFICATION	 more then one part exist 
WITHOUT_CLASSIFICATION	 skip cardinality violation clause 
WITHOUT_CLASSIFICATION	 and not using this privilege mapping but might make sense move here 
WITHOUT_CLASSIFICATION	 nanos converted millis 
WITHOUT_CLASSIFICATION	 sleep fails should exit now before things get worse 
WITHOUT_CLASSIFICATION	 parser enforces that table alias added but check again 
WITHOUT_CLASSIFICATION	 make sure the basic query properties are initialized 
WITHOUT_CLASSIFICATION	 bround without digits 
WITHOUT_CLASSIFICATION	 the hivejardir can determined once per client 
WITHOUT_CLASSIFICATION	 clear the mapredwork output file from outputs for ctas ddlwork the tail the chain will have the output 
WITHOUT_CLASSIFICATION	 allocate the buffers prepare cache keys 
WITHOUT_CLASSIFICATION	 compress fastbitset bytes 
WITHOUT_CLASSIFICATION	 pbb 
WITHOUT_CLASSIFICATION	 cannot rebuild not materialized view 
WITHOUT_CLASSIFICATION	 get notifications from metastore 
WITHOUT_CLASSIFICATION	 clean the cache 
WITHOUT_CLASSIFICATION	 need reinitialize the currentusername currentroles fields 
WITHOUT_CLASSIFICATION	 cluster 
WITHOUT_CLASSIFICATION	 create table can work against 
WITHOUT_CLASSIFICATION	 ideally this should never happen and this should assert 
WITHOUT_CLASSIFICATION	 but default partition 
WITHOUT_CLASSIFICATION	 should not done for semijoin since will change the semantics invert join inputs this done because otherwise the semanticanalyzer methods merge joins will not kick 
WITHOUT_CLASSIFICATION	 negative scale decimals 
WITHOUT_CLASSIFICATION	 required bytes 
WITHOUT_CLASSIFICATION	 get the vint that represents the map size 
WITHOUT_CLASSIFICATION	 sarg present get relevant stripe metadata from cache readers 
WITHOUT_CLASSIFICATION	 byteswritable valuebyteswritable byteswritable valuewritable collect keywritable valuewritable 
WITHOUT_CLASSIFICATION	 here means last txn the batch resolved but the close hasnt been called yet there nothing heartbeat 
WITHOUT_CLASSIFICATION	 tolerance check float equality 
WITHOUT_CLASSIFICATION	 compute stats before compaction 
WITHOUT_CLASSIFICATION	 swap the join 
WITHOUT_CLASSIFICATION	 position before the last written value 
WITHOUT_CLASSIFICATION	 followed key and nonkey input columns some may missing 
WITHOUT_CLASSIFICATION	 nonbean fields needed during compilation 
WITHOUT_CLASSIFICATION	 convert the remainders into list that are anded together 
WITHOUT_CLASSIFICATION	 size means nopooling case passthru 
WITHOUT_CLASSIFICATION	 note critical this here that logj reinitialized 
WITHOUT_CLASSIFICATION	 note that inputs and outputs can changed when the query gets executed 
WITHOUT_CLASSIFICATION	 output string such replacement exists emit out the original input code point 
WITHOUT_CLASSIFICATION	 ignore constant 
WITHOUT_CLASSIFICATION	 start inclusive infinity 
WITHOUT_CLASSIFICATION	 avoid concurrent modification 
WITHOUT_CLASSIFICATION	 create virtual row type for project only rename fields 
WITHOUT_CLASSIFICATION	 the removal before change the element avoid invalid queue ordering 
WITHOUT_CLASSIFICATION	 second copy red different object 
WITHOUT_CLASSIFICATION	 nothing actually hashes bucket updatedelete deltas dont have 
WITHOUT_CLASSIFICATION	 this not likely the right thing for compaction original files when there are copyn files 
WITHOUT_CLASSIFICATION	 table not partitioned 
WITHOUT_CLASSIFICATION	 know never went that far when were inserting 
WITHOUT_CLASSIFICATION	 this case are actually scaling dont have complicated things because doing scalingup after multiplication doesnt affect overflow doesnt happen happens anyways 
WITHOUT_CLASSIFICATION	 try find this input rel the position cor var 
WITHOUT_CLASSIFICATION	 create input byteswritable this would have capacity greater than length 
WITHOUT_CLASSIFICATION	 check have right delete delta files after minor compaction 
WITHOUT_CLASSIFICATION	 remove the node has expired 
WITHOUT_CLASSIFICATION	 bucketing version skip 
WITHOUT_CLASSIFICATION	 maximum length seen far 
WITHOUT_CLASSIFICATION	 convert list above set was created using setsunion for reasons explained there 
WITHOUT_CLASSIFICATION	 foreignkeys 
WITHOUT_CLASSIFICATION	 provide the path the field the error message 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 this admittedly bit simple seems allow old stats attributes kept the new values not overwrite them 
WITHOUT_CLASSIFICATION	 set nonhdfs tables external unless transactional should have been checked before this 
WITHOUT_CLASSIFICATION	 set the number buckets here ensure creation empty buckets 
WITHOUT_CLASSIFICATION	 see 
WITHOUT_CLASSIFICATION	 corrupt last entry 
WITHOUT_CLASSIFICATION	 todo check whether rowgroupoffsets can null then need apply the predicate push down filter 
WITHOUT_CLASSIFICATION	 volatile because heartbeat may different thread updates this are piggybacking 
WITHOUT_CLASSIFICATION	 position the row schema the filesink operator 
WITHOUT_CLASSIFICATION	 can just stop all the sessions 
WITHOUT_CLASSIFICATION	 length mixedup 
WITHOUT_CLASSIFICATION	 save original values 
WITHOUT_CLASSIFICATION	 changes the type the input references adjust nullability 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that outer join singlecolumn long using hash map 
WITHOUT_CLASSIFICATION	 for multiinsert query currently only optimize the from clause hence introduce multiinsert token top however first need reset existing token insert using would equivalent but use avoid setting the property multiple times 
WITHOUT_CLASSIFICATION	 verify orc sarg still works 
WITHOUT_CLASSIFICATION	 opening the meta table ensures that cluster running 
WITHOUT_CLASSIFICATION	 simple case union 
WITHOUT_CLASSIFICATION	 single string key hash multiset optimized for vector map join the key will deserialized and just the bytes will stored 
WITHOUT_CLASSIFICATION	 determine there only one tablescanoperator currently map vectorization not try vectorize multiple input trees 
WITHOUT_CLASSIFICATION	 gets new templeton controller objects 
WITHOUT_CLASSIFICATION	 set not null constraint name null before sending listener 
WITHOUT_CLASSIFICATION	 given session the name can fixed across all invocations 
WITHOUT_CLASSIFICATION	 lets use the remaining space column progress bar 
WITHOUT_CLASSIFICATION	 mapper can span partitions make sure splits does not contain multiple oplist combination this done using the map pathfilter 
WITHOUT_CLASSIFICATION	 nonjavadoc this provides lazyhivedecimal like class which can initialized from data stored binary format see int int 
WITHOUT_CLASSIFICATION	 from here evaluate the auto mode 
WITHOUT_CLASSIFICATION	 update source info the state succeeded 
WITHOUT_CLASSIFICATION	 these bottom layer reducesinkoperators 
WITHOUT_CLASSIFICATION	 delink union 
WITHOUT_CLASSIFICATION	 rounding 
WITHOUT_CLASSIFICATION	 need pass all the columns used the where clauses reduce values 
WITHOUT_CLASSIFICATION	 opening the meta table ensures that cluster running 
WITHOUT_CLASSIFICATION	 now take local offset midnight utc say are that means surprise 
WITHOUT_CLASSIFICATION	 reject all paths force continue when more paths should throw exception 
WITHOUT_CLASSIFICATION	 kerberos 
WITHOUT_CLASSIFICATION	 that are not our wordlist table and column names 
WITHOUT_CLASSIFICATION	 have too many results return null for full scan 
WITHOUT_CLASSIFICATION	 add not operator the beginning this for the cloned operator because 
WITHOUT_CLASSIFICATION	 start index query that equivalent using query internal offset 
WITHOUT_CLASSIFICATION	 this happens when the code inside the jmx bean setter from the java docs threw exception log and fall back the class name 
WITHOUT_CLASSIFICATION	 imported serdeformat null then set just 
WITHOUT_CLASSIFICATION	 bounded queue could specified here but that will lead blocking unbounded and will release soft referenced kryo instances under 
WITHOUT_CLASSIFICATION	 build keys grouping set starting position 
WITHOUT_CLASSIFICATION	 there are sample cols and bucket cols then throw error 
WITHOUT_CLASSIFICATION	 raise error could not find the column 
WITHOUT_CLASSIFICATION	 offset because the first frames are just calls get down here 
WITHOUT_CLASSIFICATION	 each group walk from most recent and count occurences each state type once you have counted enough for each state satisfy retention policy delete all other instances this status 
WITHOUT_CLASSIFICATION	 casesensitive string found 
WITHOUT_CLASSIFICATION	 created using field name 
WITHOUT_CLASSIFICATION	 let caller set negative sign necessary 
WITHOUT_CLASSIFICATION	 execute select statement and verify the result set should two rows 
WITHOUT_CLASSIFICATION	 delete functions created the tests enough remove functions from the default database other databases are dropped 
WITHOUT_CLASSIFICATION	 expected result entry the recordidentifier data entry file before compact 
WITHOUT_CLASSIFICATION	 verify that the beginning entry the only one that matches 
WITHOUT_CLASSIFICATION	 assumes one instance this singlethreaded compilation for each query 
WITHOUT_CLASSIFICATION	 system registry should not used check persistent functions see ispermanentfunc 
WITHOUT_CLASSIFICATION	 estimated number bytes needed 
WITHOUT_CLASSIFICATION	 inactive nodes restart every call 
WITHOUT_CLASSIFICATION	 framework 
WITHOUT_CLASSIFICATION	 binary arithmetic operator 
WITHOUT_CLASSIFICATION	 pick the length the first ptn expect all ptns listed have the same number keyvals 
WITHOUT_CLASSIFICATION	 create new vertex 
WITHOUT_CLASSIFICATION	 delta base 
WITHOUT_CLASSIFICATION	 nonskewed value add list for later handle default directory 
WITHOUT_CLASSIFICATION	 update for next iteration 
WITHOUT_CLASSIFICATION	 copy java object because that saves object creation time note that average the number copies logn thats not very important 
WITHOUT_CLASSIFICATION	 iterate through the batch and for each owid rowid the batch check deleted not 
WITHOUT_CLASSIFICATION	 track the variable length keys 
WITHOUT_CLASSIFICATION	 map from partid stattypevalue 
WITHOUT_CLASSIFICATION	 map the newly allocated write ids against the list txns which doesnt have preallocated 
WITHOUT_CLASSIFICATION	 roundlongcol returns long and noop will not implemented here 
WITHOUT_CLASSIFICATION	 process multikey leftsemi join vectorized row batch 
WITHOUT_CLASSIFICATION	 resourceplan 
WITHOUT_CLASSIFICATION	 operators for which the optimization will successful 
WITHOUT_CLASSIFICATION	 for dummy partitions only partition name needed 
WITHOUT_CLASSIFICATION	 serde helper methods 
WITHOUT_CLASSIFICATION	 set some parameters for prepared sql not all them 
WITHOUT_CLASSIFICATION	 groupby operator reducer may not processed parallel skip optimizing 
WITHOUT_CLASSIFICATION	 parse analyze optimize and compile 
WITHOUT_CLASSIFICATION	 again with correct output type 
WITHOUT_CLASSIFICATION	 specify the default logj properties file 
WITHOUT_CLASSIFICATION	 loop over the given inlist elements 
WITHOUT_CLASSIFICATION	 recurse over the subqueries fill the subquery part the plan 
WITHOUT_CLASSIFICATION	 are only interested exprnodecolumndesc 
WITHOUT_CLASSIFICATION	 create znode under the rootnamespace parent for this instance the server znode name 
WITHOUT_CLASSIFICATION	 extrapolation needed for some columns this case least column status for partition missing 
WITHOUT_CLASSIFICATION	 there insert the subquery 
WITHOUT_CLASSIFICATION	 env interface mock out dealing with environment variables this allows interface with environment vars through beelineopts while allowing tests mock out env setting needed 
WITHOUT_CLASSIFICATION	 update the time hasnt been specified 
WITHOUT_CLASSIFICATION	 single item clause transform from equals except complex types 
WITHOUT_CLASSIFICATION	 remember used this one the query 
WITHOUT_CLASSIFICATION	 for each dynamic partition check needs merged 
WITHOUT_CLASSIFICATION	 this conversion frequently used this should optimized converting decimal from the input bytes directly without making new string 
WITHOUT_CLASSIFICATION	 this should only ever called testing scenarios there should not any other users the cache its entries this may mess cleanup 
WITHOUT_CLASSIFICATION	 the given value type lazyobject then only try and convert that form 
WITHOUT_CLASSIFICATION	 build hivetox column mapping 
WITHOUT_CLASSIFICATION	 timer that reports every minutes the jobtracker this ensures that even the operator returning rows for greater than that duration progress report sent the tracker that the tracker does not think that the job dead 
WITHOUT_CLASSIFICATION	 creating the context can create bunch files make sure clear out 
WITHOUT_CLASSIFICATION	 get and process the current datum 
WITHOUT_CLASSIFICATION	 keep toread list for future use dont extract 
WITHOUT_CLASSIFICATION	 cost cost transferring small tables join node 
WITHOUT_CLASSIFICATION	 primitives except binary 
WITHOUT_CLASSIFICATION	 same test above but with jars sharing dependencies 
WITHOUT_CLASSIFICATION	 this where the spilling may happen again 
WITHOUT_CLASSIFICATION	 note might want smarter threadids are low and there more arenas than threads 
WITHOUT_CLASSIFICATION	 node 
WITHOUT_CLASSIFICATION	 this operator 
WITHOUT_CLASSIFICATION	 col 
WITHOUT_CLASSIFICATION	 add children self the front the queue that order 
WITHOUT_CLASSIFICATION	 set seconds have wait for least 
WITHOUT_CLASSIFICATION	 note that cannot allow users provide app since providing somebody elses appid would give one llap token and splits for that app could verify somehow yarn token nothing can udf could get from client already running yarn such the clients running yarn will have two app ids aware 
WITHOUT_CLASSIFICATION	 negative max cache size means unbounded 
WITHOUT_CLASSIFICATION	 switch iterate foreign keys 
WITHOUT_CLASSIFICATION	 there are uncompacted original files they will included compaction 
WITHOUT_CLASSIFICATION	 tests for listpartition partitions boolean ifnotexists boolean needresults method 
WITHOUT_CLASSIFICATION	 test serialization and deserialization with different schemas 
WITHOUT_CLASSIFICATION	 are moving the partition across filesystem boundaries inherit from the table properties otherwise same filesystem use the original partition location see hive and hive for background 
WITHOUT_CLASSIFICATION	 mutable threadsafe map store instances 
WITHOUT_CLASSIFICATION	 add column info corresponding partition columns 
WITHOUT_CLASSIFICATION	 default type table 
WITHOUT_CLASSIFICATION	 case now with entries try the above settings 
WITHOUT_CLASSIFICATION	 blurb list 
WITHOUT_CLASSIFICATION	 process deregister 
WITHOUT_CLASSIFICATION	 case column stats hash aggregation grouping sets 
WITHOUT_CLASSIFICATION	 prevents throwing exceptions our raw store calls since were not using rawstoreproxy 
WITHOUT_CLASSIFICATION	 string type hive represented varchar with precision integermaxvalue turn the max varchar precision should however the value not used for validation but rather only internally the optimizer know the max precision supported the system thus varchar precision should fall between 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 column name within regex pattern with its corresponding value provided 
WITHOUT_CLASSIFICATION	 auth has been initialized 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautilcalendar 
WITHOUT_CLASSIFICATION	 such job 
WITHOUT_CLASSIFICATION	 submit the job 
WITHOUT_CLASSIFICATION	 try replace bucket map join with sorted merge map join 
WITHOUT_CLASSIFICATION	 other formats can converted insertonly transactional tables 
WITHOUT_CLASSIFICATION	 not update metrics didnt update removal 
WITHOUT_CLASSIFICATION	 hadoop fixed version alpha 
WITHOUT_CLASSIFICATION	 moving tablespartitions depend the dependencytask 
WITHOUT_CLASSIFICATION	 single byte array value hash map based the since does not interpret the key binarysortable optimize this case and just reference the byte array key directly for the lookup instead serializing the byte array into binarysortable rely just doing byte array equality comparisons 
WITHOUT_CLASSIFICATION	 parse the json map 
WITHOUT_CLASSIFICATION	 make copy the statements result schema since may 
WITHOUT_CLASSIFICATION	 nop 
WITHOUT_CLASSIFICATION	 the number children slots used 
WITHOUT_CLASSIFICATION	 remove from the runningtasklist 
WITHOUT_CLASSIFICATION	 present 
WITHOUT_CLASSIFICATION	 embedded metastore used per config far 
WITHOUT_CLASSIFICATION	 heartbeats the txn table this commits not enter with any state 
WITHOUT_CLASSIFICATION	 since the user running the test wont belong nonexistent group foobargroup the call will fail 
WITHOUT_CLASSIFICATION	 nothing here 
WITHOUT_CLASSIFICATION	 this not join condition will get handled predicate pushdown 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 type date longcolumnvector storing epoch days minus type date produces type intervaldaytime storing nanosecond interval longs 
WITHOUT_CLASSIFICATION	 add procedures they can invoked functions 
WITHOUT_CLASSIFICATION	 singlecolumn long specific lookup key 
WITHOUT_CLASSIFICATION	 here are disconnecting root with its parents however need save few information since future may reach the parent operators via different path and may need connect parent works with the work associated 
WITHOUT_CLASSIFICATION	 this used for tests where theres always just one batch work and the checks after the batch the check will only come the end queueing 
WITHOUT_CLASSIFICATION	 report forward 
WITHOUT_CLASSIFICATION	 found semijoin branch there can more than one semijoin branch coming from the parent 
WITHOUT_CLASSIFICATION	 assume that norgs value only set from sarg filter and for all columns intermediate changes for individual columns will unset values the array skip this case for column read could probably specialcase just like encodedreaderimpl but for now its not that important 
WITHOUT_CLASSIFICATION	 this will null slave nodes 
WITHOUT_CLASSIFICATION	 this point tasks running both priority 
WITHOUT_CLASSIFICATION	 create our vector map join optimized hash table variation above the map join table container 
WITHOUT_CLASSIFICATION	 hdfs session path 
WITHOUT_CLASSIFICATION	 qualifiers 
WITHOUT_CLASSIFICATION	 this isnt really used for anything 
WITHOUT_CLASSIFICATION	 find proxy user any from query param 
WITHOUT_CLASSIFICATION	 must struct 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 schedule low pri first when high pri scheduled takes away the duck from the low pri task when the high pri finishes low pri gets the duck back 
WITHOUT_CLASSIFICATION	 compute join keys and store reducekeys 
WITHOUT_CLASSIFICATION	 add new item the spark work 
WITHOUT_CLASSIFICATION	 increase the row count 
WITHOUT_CLASSIFICATION	 repl export has repllastid and replscopeall import repl dump table has repllastid will likely 
WITHOUT_CLASSIFICATION	 update the log level for the specified logger 
WITHOUT_CLASSIFICATION	 expression udf not permanent udf 
WITHOUT_CLASSIFICATION	 dont want put any limits this task this essential before start processing new database events 
WITHOUT_CLASSIFICATION	 local file system path for spilled hashmap status hashmap true disk false memory when theres enough memory cannot create hashmap used create empty same above same above how many rows saved the ondisk hashmap disk 
WITHOUT_CLASSIFICATION	 write the null byte every eight elements this the last element and serialize the 
WITHOUT_CLASSIFICATION	 traverse the byte buffer containing the input string one code point time 
WITHOUT_CLASSIFICATION	 undone col col scalar col col scalar classname long 
WITHOUT_CLASSIFICATION	 convert the keyvalue pairs 
WITHOUT_CLASSIFICATION	 try again with include vector 
WITHOUT_CLASSIFICATION	 runs stat against the servers 
WITHOUT_CLASSIFICATION	 one row existence 
WITHOUT_CLASSIFICATION	 the one created here will not added 
WITHOUT_CLASSIFICATION	 least the header should fit 
WITHOUT_CLASSIFICATION	 have mintxn maxtxn and ismajor jobconf could figure out exactly what the dir name that want rename leave for another day 
WITHOUT_CLASSIFICATION	 unfortunately cannot directly read protected field nonthis object 
WITHOUT_CLASSIFICATION	 the block being moved the move will release memory 
WITHOUT_CLASSIFICATION	 put partial aggregation results reducevalues 
WITHOUT_CLASSIFICATION	 stored here only async operation context 
WITHOUT_CLASSIFICATION	 use calcite cost model for view rewriting 
WITHOUT_CLASSIFICATION	 cannot optimize any others 
WITHOUT_CLASSIFICATION	 booleanstats 
WITHOUT_CLASSIFICATION	 this was the only predicate set filter expression const 
WITHOUT_CLASSIFICATION	 preallocated member for storing any nonspills nonmatches merged row indexes during 
WITHOUT_CLASSIFICATION	 this hash function returns the same result doublehashcode while returns different result 
WITHOUT_CLASSIFICATION	 max file num and size used single copy after that distcp used 
WITHOUT_CLASSIFICATION	 nonnull column alias 
WITHOUT_CLASSIFICATION	 zero result 
WITHOUT_CLASSIFICATION	 synthetic predicates from semijoin opt should not affect stats 
WITHOUT_CLASSIFICATION	 this where counters are logged 
WITHOUT_CLASSIFICATION	 see the comment 
WITHOUT_CLASSIFICATION	 opnodes type always either kwexists kwin never notexists notin figure this out need check its grand parents parent 
WITHOUT_CLASSIFICATION	 clear rounding portion high longword and add right scale roundmultiplyfactor middle and lower longwords result 
WITHOUT_CLASSIFICATION	 the function distinct partial aggregation has not been done the client side distpartagg set the client letting know that partial aggregation has not been done for select countbc countdistinct group for countbc partial aggregation has been performed then directly look for countbc otherwise look for for distincts partial aggregation never performed the client side always look for the parameters 
WITHOUT_CLASSIFICATION	 extract the sequence number this ephemeralsequential znode 
WITHOUT_CLASSIFICATION	 the extra non existing values will ignored 
WITHOUT_CLASSIFICATION	 additional data structures needed for the join optimization through hive 
WITHOUT_CLASSIFICATION	 could just tolowercase here and let qualify but lets proper 
WITHOUT_CLASSIFICATION	 not ctas dont need further and just return 
WITHOUT_CLASSIFICATION	 remember which reducesinks weve already connected 
WITHOUT_CLASSIFICATION	 override public partitionid hashpartition return 
WITHOUT_CLASSIFICATION	 create dummy database and couple dummy tables 
WITHOUT_CLASSIFICATION	 call open mockmocktbl 
WITHOUT_CLASSIFICATION	 tracks only running portion the query 
WITHOUT_CLASSIFICATION	 object inspectors for input rows 
WITHOUT_CLASSIFICATION	 and finally lets check output sizes 
WITHOUT_CLASSIFICATION	 print vertexname 
WITHOUT_CLASSIFICATION	 test alter table with schema change 
WITHOUT_CLASSIFICATION	 case when user ctrlc twice kill hive cli jvm want release locks 
WITHOUT_CLASSIFICATION	 only the column stats available update the data size from the column stats 
WITHOUT_CLASSIFICATION	 read totalmonths from datainput 
WITHOUT_CLASSIFICATION	 spit marked isoriginal but its not immediate child partition nor base delta this should never happen 
WITHOUT_CLASSIFICATION	 blockcompressed should always false 
WITHOUT_CLASSIFICATION	 order its just row 
WITHOUT_CLASSIFICATION	 lenvalue 
WITHOUT_CLASSIFICATION	 add children taskstovisit 
WITHOUT_CLASSIFICATION	 create new 
WITHOUT_CLASSIFICATION	 apply the rules the spec fill any missing pieces every window specification also validate that the effective specification valid the rules applied are for wdw specs that refer window defns inherit missing components window spec with parition spec partitioned constantnumber for missing wdw frames for frames with only start boundary completely specify them the rules link validate the effective window frames with the rules link validatewindowframe there order then add the partition expressions the order 
WITHOUT_CLASSIFICATION	 store the chunk indices split file that way several callers are reading the same file they can separately store and remove the relevant parts the index 
WITHOUT_CLASSIFICATION	 invert words 
WITHOUT_CLASSIFICATION	 sort the list get sorted deterministic output for ease testing 
WITHOUT_CLASSIFICATION	 suppose its the first major compaction only have deltas 
WITHOUT_CLASSIFICATION	 long masks and values 
WITHOUT_CLASSIFICATION	 join which are part join keys 
WITHOUT_CLASSIFICATION	 current write ids are not valid 
WITHOUT_CLASSIFICATION	 empty rows for each table 
WITHOUT_CLASSIFICATION	 value for kale 
WITHOUT_CLASSIFICATION	 need partitions for firing events and for result needs mpartitions drop great maybe could bypass fetching mpartitions issuing direct sql deletes 
WITHOUT_CLASSIFICATION	 change lock manager otherwise unittest doesnt through 
WITHOUT_CLASSIFICATION	 not control character nothing 
WITHOUT_CLASSIFICATION	 functiontype 
WITHOUT_CLASSIFICATION	 emit the rest word 
WITHOUT_CLASSIFICATION	 run cleaner delete rows for the aborted transaction 
WITHOUT_CLASSIFICATION	 add new rel its the maps 
WITHOUT_CLASSIFICATION	 table alias small input file name big target file names small 
WITHOUT_CLASSIFICATION	 this will happen case joins the current plan can thrown away after being merged with the original plan 
WITHOUT_CLASSIFICATION	 long columncolumn 
WITHOUT_CLASSIFICATION	 write back previous fields null byte 
WITHOUT_CLASSIFICATION	 serialize the union 
WITHOUT_CLASSIFICATION	 for internal getacidstate purposes and means the delta dir had statement 
WITHOUT_CLASSIFICATION	 load into compressed buf first 
WITHOUT_CLASSIFICATION	 stores all the downloaded resources key and the jars which depend these resources values form list used for deleting transitive dependencies 
WITHOUT_CLASSIFICATION	 verify that hive caching the object inspectors for 
WITHOUT_CLASSIFICATION	 explicitly using only the start offset split and not the length splits generated block boundaries and stripe boundaries can vary slightly try hashing both the same node there the drawback potentially hashing the same data multiple nodes though when large split sent node and second invocation uses smaller chunks the previous large split and send them different nodes 
WITHOUT_CLASSIFICATION	 blank byte new tai lue letter low bytes 
WITHOUT_CLASSIFICATION	 validation 
WITHOUT_CLASSIFICATION	 create http client from the configs 
WITHOUT_CLASSIFICATION	 try opportunistically for the common case the samesized allocated buddy 
WITHOUT_CLASSIFICATION	 token expiration 
WITHOUT_CLASSIFICATION	 that explain doesnt leak tmp tables 
WITHOUT_CLASSIFICATION	 the exception not nodeexists rethrow 
WITHOUT_CLASSIFICATION	 determine the query qualifies reduce input size for limit the query only qualifies when there are only one top operator and there transformer udtf and block sampling used 
WITHOUT_CLASSIFICATION	 finishes inside 
WITHOUT_CLASSIFICATION	 then write chunk bytes 
WITHOUT_CLASSIFICATION	 dropfunction 
WITHOUT_CLASSIFICATION	 number buckets 
WITHOUT_CLASSIFICATION	 were hijacking the big table evaluators replace them with our own custom ones 
WITHOUT_CLASSIFICATION	 not allowed 
WITHOUT_CLASSIFICATION	 were supporting dynamic service discovery well add the service uri for this hiveserver instance zookeeper znode 
WITHOUT_CLASSIFICATION	 fill colname 
WITHOUT_CLASSIFICATION	 take what all input formats support and eliminate any them not enabled the hive variable 
WITHOUT_CLASSIFICATION	 always want read all the delete delta files 
WITHOUT_CLASSIFICATION	 init input object inspectors 
WITHOUT_CLASSIFICATION	 how many blocks this size comprise arena 
WITHOUT_CLASSIFICATION	 reset buffer 
WITHOUT_CLASSIFICATION	 the loaded hash table empty for some conditions can skip processing the big table rows 
WITHOUT_CLASSIFICATION	 serialize work 
WITHOUT_CLASSIFICATION	 fill the temp list before merging sparse map 
WITHOUT_CLASSIFICATION	 null byte start 
WITHOUT_CLASSIFICATION	 the serde part from testlazysimpleserde 
WITHOUT_CLASSIFICATION	 this point hinted semijoin case has been handled already check big table big enough that runtime filtering 
WITHOUT_CLASSIFICATION	 not always allow but always return true because later subq remove rule will generate diff plan for this case 
WITHOUT_CLASSIFICATION	 rowid also found 
WITHOUT_CLASSIFICATION	 this assumes splitting 
WITHOUT_CLASSIFICATION	 translate window spec 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 get length word 
WITHOUT_CLASSIFICATION	 true same value repeats for whole column vector vector holds the repeating value 
WITHOUT_CLASSIFICATION	 default tokenstore memorytokenstore 
WITHOUT_CLASSIFICATION	 fetch table alias 
WITHOUT_CLASSIFICATION	 retrieve not null constraints 
WITHOUT_CLASSIFICATION	 set output collector for any reduce sink operators the pipeline 
WITHOUT_CLASSIFICATION	 preserve the old behavior failing when cannot write even deletedata and even the table external that might not make any sense 
WITHOUT_CLASSIFICATION	 consider query like select count from select key count from select mapjoina akey key avalue val bvalue val from tbl join tbl akey bkey subq group key subq the table alias should subqsubqa which needs fetched from topops 
WITHOUT_CLASSIFICATION	 but snapshot not valid recompile the query 
WITHOUT_CLASSIFICATION	 using init instead this because the new operation that needs run before delegating the other ctor and this messes chaining ctors 
WITHOUT_CLASSIFICATION	 add fsd that the loadtask compilation could fix its path avoid the move 
WITHOUT_CLASSIFICATION	 map from tablename task columnstatstask which includes basicstatstask 
WITHOUT_CLASSIFICATION	 reset correct http path 
WITHOUT_CLASSIFICATION	 for the following method 
WITHOUT_CLASSIFICATION	 this can change based new splits 
WITHOUT_CLASSIFICATION	 dfs 
WITHOUT_CLASSIFICATION	 proceed ast contains partition not exists 
WITHOUT_CLASSIFICATION	 vertices elapsed time contains footersummary startime 
WITHOUT_CLASSIFICATION	 the below loop may perform bad when the destination file already exists and has too many copy files well desired approach was call listfiles and get complete list files from the destination and check whether the file exists not that list however millions files could live the destination directory and concurrent situations this can cause oom problems ill leave the below loop for now until better approach found 
WITHOUT_CLASSIFICATION	 replimports are replaceinto unless the event insertinto 
WITHOUT_CLASSIFICATION	 hive date representable pig datetime 
WITHOUT_CLASSIFICATION	 table and will exist and partition india will exist rest failed operation failed 
WITHOUT_CLASSIFICATION	 tests 
WITHOUT_CLASSIFICATION	 use threads resolve directories into splits 
WITHOUT_CLASSIFICATION	 process groupby pattern 
WITHOUT_CLASSIFICATION	 must called under the epic lock 
WITHOUT_CLASSIFICATION	 call open read data split mockmocktable 
WITHOUT_CLASSIFICATION	 are doing update delete the number columns the table will not match the number columns the file sink for update there will one too many because the rowid and the case the delete there will just the rowid which dont need worry about from lineage perspective 
WITHOUT_CLASSIFICATION	 print the results 
WITHOUT_CLASSIFICATION	 dont allow turning auto parallel once has been explicitly turned off that avoid scenarios where auto parallelism could break assumptions about number reducers hash function 
WITHOUT_CLASSIFICATION	 the bottom layer reducesinkoperators these reducesinkoperators are used record the boundary this subtree which can evaluated single 
WITHOUT_CLASSIFICATION	 this shouldnt ever happen 
WITHOUT_CLASSIFICATION	 unionfield 
WITHOUT_CLASSIFICATION	 reevaluate expression current row trigger the lazy object caches reset the current row 
WITHOUT_CLASSIFICATION	 reuse record reader 
WITHOUT_CLASSIFICATION	 order clause 
WITHOUT_CLASSIFICATION	 instance fields 
WITHOUT_CLASSIFICATION	 return true false based whether bucketed mapjoin can converted successfully sortmerge map join operator the following checks are performed the mapjoin under consideration bucketed mapjoin all the tables are sorted same order such that join columns equal prefix the sort columns 
WITHOUT_CLASSIFICATION	 trailing blank field 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautilcalendar 
WITHOUT_CLASSIFICATION	 convenient constructor for initial batch creation takes 
WITHOUT_CLASSIFICATION	 test that zerodivide produces null for all output values 
WITHOUT_CLASSIFICATION	 check that write still valid 
WITHOUT_CLASSIFICATION	 evaluate the aggregators 
WITHOUT_CLASSIFICATION	 matches only joinoperators which are reducers rather than map joins smb map joins etc 
WITHOUT_CLASSIFICATION	 temp partition input path does not match exist temp path 
WITHOUT_CLASSIFICATION	 munging innerschemas 
WITHOUT_CLASSIFICATION	 approximate 
WITHOUT_CLASSIFICATION	 reset for the new partition 
WITHOUT_CLASSIFICATION	 filtered 
WITHOUT_CLASSIFICATION	 fill high long from some middle long 
WITHOUT_CLASSIFICATION	 store the orc configuration from the first file all other files should 
WITHOUT_CLASSIFICATION	 unionfield 
WITHOUT_CLASSIFICATION	 join not left right outer bail out any sort column not part the input where the 
WITHOUT_CLASSIFICATION	 there are functions doesnt matter much whether aggregate the inputs before the join because there will not any functions experiencing cartesian product effect but finding out whether the input already unique requires call arecolumnsunique that currently until calcite make metadata more robust fixed places heavy load the metadata system choose imagine the the input already unique which untrue but harmless 
WITHOUT_CLASSIFICATION	 export valid directories with modified name they dont look like basesdeltas could also dump the delta contents all together and rename the files names collide 
WITHOUT_CLASSIFICATION	 overridden and used mode 
WITHOUT_CLASSIFICATION	 case the empty grouping set preset but output has done the summary row still needs emitted 
WITHOUT_CLASSIFICATION	 aborted break out the loop and cancel all subsequent futures 
WITHOUT_CLASSIFICATION	 maxcollen 
WITHOUT_CLASSIFICATION	 the nonmm path only finds new partitions looking the temp path produce the same effect will find all the partitions affected this txn note ignore the statement here because its currently irrelevant for movetask where this used always want load everything also the only case where have multiple statements anyway union 
WITHOUT_CLASSIFICATION	 only retrieve the materialization corresponding the rebuild turn pass true for the parameter cannot allow the materialization contents stale for rebuild want use 
WITHOUT_CLASSIFICATION	 the value for the constant does not matter replaced the grouping set value for the actual implementation 
WITHOUT_CLASSIFICATION	 the columns used the join appear the output the aggregate 
WITHOUT_CLASSIFICATION	 test that locking database prevents locking tables the database 
WITHOUT_CLASSIFICATION	 must ensure the exactness the doubles fractional portion the fraction part will converted and significantly reduce the savings from binary serialization 
WITHOUT_CLASSIFICATION	 outputs empty which means this create table happens the current 
WITHOUT_CLASSIFICATION	 clean the staging table 
WITHOUT_CLASSIFICATION	 this needs major compaction 
WITHOUT_CLASSIFICATION	 allocate writeid txn under hwm this will get greater than txn hwm 
WITHOUT_CLASSIFICATION	 support for authorization partitions needs added 
WITHOUT_CLASSIFICATION	 start the creation znodes 
WITHOUT_CLASSIFICATION	 note cannot use copyselected below since whole column operation 
WITHOUT_CLASSIFICATION	 get the output objectinspector from the return type 
WITHOUT_CLASSIFICATION	 and put writeentity for postexec hook 
WITHOUT_CLASSIFICATION	 derby commandline parser 
WITHOUT_CLASSIFICATION	 destpath directory exists rename call will move the srcpath into destpath without failing check before renaming 
WITHOUT_CLASSIFICATION	 used for avoid extra byte copy 
WITHOUT_CLASSIFICATION	 updated only when thread has failed 
WITHOUT_CLASSIFICATION	 all columns have primitive 
WITHOUT_CLASSIFICATION	 unionfield 
WITHOUT_CLASSIFICATION	 tblvalidwriteids 
WITHOUT_CLASSIFICATION	 truncate table 
WITHOUT_CLASSIFICATION	 interleaved writes both batches 
WITHOUT_CLASSIFICATION	 governs remotefetchinput behaviour set true well assume that the input has files file present which lists the actual input files copy and well pull each those read set false itll behave traditional copytask 
WITHOUT_CLASSIFICATION	 extract the group positions that are part the collations and sort them they respect 
WITHOUT_CLASSIFICATION	 try transform predicates filter into simpler clauses first 
WITHOUT_CLASSIFICATION	 the equality implemented fully the greaterthanlessthan values not implement transitive relation 
WITHOUT_CLASSIFICATION	 outputformat 
WITHOUT_CLASSIFICATION	 this join has already been processed 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 nonblocking execute 
WITHOUT_CLASSIFICATION	 nothing for other modes 
WITHOUT_CLASSIFICATION	 parse key 
WITHOUT_CLASSIFICATION	 the subtree gather the references 
WITHOUT_CLASSIFICATION	 now for each entry the queue see all the associated locks are clear 
WITHOUT_CLASSIFICATION	 fullyspecified partition 
WITHOUT_CLASSIFICATION	 twice more skip dedup 
WITHOUT_CLASSIFICATION	 for now theres nothing special return addedvals just return the footer 
WITHOUT_CLASSIFICATION	 now create the new project 
WITHOUT_CLASSIFICATION	 for now require select with grant 
WITHOUT_CLASSIFICATION	 now test that dont timeout locks should not 
WITHOUT_CLASSIFICATION	 implementations may choose override this 
WITHOUT_CLASSIFICATION	 have set for each partition too 
WITHOUT_CLASSIFICATION	 output file system information 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 with nulls 
WITHOUT_CLASSIFICATION	 forward the current keys needed for sortbased aggregation 
WITHOUT_CLASSIFICATION	 local time zone store separately because calendar would clone 
WITHOUT_CLASSIFICATION	 write out the first bits worth data 
WITHOUT_CLASSIFICATION	 should this escaped string 
WITHOUT_CLASSIFICATION	 reset member variables dont get halfconstructed state 
WITHOUT_CLASSIFICATION	 note that the state the failed service still inited and not started even though the last service not started completely still call stop all services including failed service make sure cleanup happens 
WITHOUT_CLASSIFICATION	 iterator cursor the currblock size current read block append cursor the lastblock for the row object inspector for the row 
WITHOUT_CLASSIFICATION	 lock part txn heartbeat info txn record 
WITHOUT_CLASSIFICATION	 make sure the null flag agrees 
WITHOUT_CLASSIFICATION	 alert already running low memory starting with low memory will lead frequent auto flush 
WITHOUT_CLASSIFICATION	 add this condition the list nonequijoin conditions 
WITHOUT_CLASSIFICATION	 backtracking from 
WITHOUT_CLASSIFICATION	 list tezworkdependency 
WITHOUT_CLASSIFICATION	 create the project before 
WITHOUT_CLASSIFICATION	 now have written all information about the next value work the new value 
WITHOUT_CLASSIFICATION	 tez task were currently processing 
WITHOUT_CLASSIFICATION	 set version 
WITHOUT_CLASSIFICATION	 allow stateful functions the select list but nowhere else 
WITHOUT_CLASSIFICATION	 accurate byte value cannot obtained 
WITHOUT_CLASSIFICATION	 didnt find the token cant proceed log the tokens for debugging 
WITHOUT_CLASSIFICATION	 the key missing shouldnt able verify 
WITHOUT_CLASSIFICATION	 note the materialized view does not contain table that contained the query not need check whether that specific table outdated not rewriting produced those cases because that additional table joined with the existing tables with appendcolumns only join pkfk not null 
WITHOUT_CLASSIFICATION	 true only one value null 
WITHOUT_CLASSIFICATION	 initialize schema 
WITHOUT_CLASSIFICATION	 exponent 
WITHOUT_CLASSIFICATION	 type affinity does not help when multiple methods have the same type affinity 
WITHOUT_CLASSIFICATION	 retention 
WITHOUT_CLASSIFICATION	 convert valuelist array for the 
WITHOUT_CLASSIFICATION	 map needs two separators key and keyvalue pair 
WITHOUT_CLASSIFICATION	 save compiletime perflogging for webui executiontime perf logs are done either another threads perflogger reset perflogger 
WITHOUT_CLASSIFICATION	 this point the number reducers precisely defined the plan 
WITHOUT_CLASSIFICATION	 allowed operations intervalyearmonth intervalyearmonth intervalyearmonth date intervalyearmonth date operands not reversible timestamp intervalyearmonth timestamp operands not reversible intervaldaytime intervaldaytime intervaldaytime date intervalyearmonth timestamp operands not reversible timestamp intervalyearmonth timestamp operands not reversible timestamp timestamp intervaldaytime date date intervaldaytime timestamp date intervaldaytime operands reversible 
WITHOUT_CLASSIFICATION	 for performance reasons not want chase the values the end determine the count use hasrows and issinglerow instead 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 special handling for serde reader text llap file format version null then are processing text llap which case get vectors instead streams vectors contain instance decimalcolumnvector should use decimalstreamreader which acts wrapper around vectors 
WITHOUT_CLASSIFICATION	 simulate missing table scenario renaming couple tables 
WITHOUT_CLASSIFICATION	 generate table access stats required 
WITHOUT_CLASSIFICATION	 will always excuse the first error can decide single 
WITHOUT_CLASSIFICATION	 app never seen previous dag has been unregistered 
WITHOUT_CLASSIFICATION	 not affected the not about transactional 
WITHOUT_CLASSIFICATION	 need the directory hdfs which shall put all these files 
WITHOUT_CLASSIFICATION	 set third argument 
WITHOUT_CLASSIFICATION	 preallocated member for storing the physical batch index rows that need spilled 
WITHOUT_CLASSIFICATION	 verify all the aborted write ids are replicated the replicated 
WITHOUT_CLASSIFICATION	 compare stats obj ensure what get what wrote 
WITHOUT_CLASSIFICATION	 add token only already doesnt exist 
WITHOUT_CLASSIFICATION	 allow estimated stats for the columns then shall set the boolean true since otherwise will throw exception because columns with stimated stats are actually added the list columns that not contain stats 
WITHOUT_CLASSIFICATION	 dont make copy dont have noinspection unchecked 
WITHOUT_CLASSIFICATION	 cannot convert map join weve already chosen big table size and theres another one thats bigger 
WITHOUT_CLASSIFICATION	 decimal place 
WITHOUT_CLASSIFICATION	 optional bool result 
WITHOUT_CLASSIFICATION	 wrapper class for reading and writing metadata about dump responsible for dumpmetadata files 
WITHOUT_CLASSIFICATION	 not throw exception table does not exist 
WITHOUT_CLASSIFICATION	 cannot delimited split for some commands like dfs cat that prints the contents file which may have different delimiter will split only when the resultschema has more than column 
WITHOUT_CLASSIFICATION	 loads all the delete events from all the delete deltas into memory prevent outofmemory errors this check rough heuristic that prevents creation object this class the total number delete events exceed this value default has been set million delete events per bucket 
WITHOUT_CLASSIFICATION	 the the job this tracking node represents 
WITHOUT_CLASSIFICATION	 keys are the column names basically this maps the position the column 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream long 
WITHOUT_CLASSIFICATION	 get tokens for all other known fss since hive tables may result different ones 
WITHOUT_CLASSIFICATION	 for some reason this just locks the table alter table add this partition then end locking both table and partition with shareread plan has readentities same for other locks below 
WITHOUT_CLASSIFICATION	 level 
WITHOUT_CLASSIFICATION	 input gby different than the source columns find the same column input 
WITHOUT_CLASSIFICATION	 conjuctive elements 
WITHOUT_CLASSIFICATION	 key contains scheme such pfile and want only the path portion fix hive 
WITHOUT_CLASSIFICATION	 empty array 
WITHOUT_CLASSIFICATION	 key correct 
WITHOUT_CLASSIFICATION	 lastaccesstime 
WITHOUT_CLASSIFICATION	 support for statistics 
WITHOUT_CLASSIFICATION	 with the fast hash table implementation currently not support hybrid grace hash join 
WITHOUT_CLASSIFICATION	 llap server depends hive execution the reverse cannot true create the singleton once daemon startup the said singleton serves the interface 
WITHOUT_CLASSIFICATION	 finally monitor will print progress until the job done 
WITHOUT_CLASSIFICATION	 table columns mapping 
WITHOUT_CLASSIFICATION	 always generate list with least value 
WITHOUT_CLASSIFICATION	 lock was outdated and was removed then maybe another transaction picked 
WITHOUT_CLASSIFICATION	 unscaledvalue negativefalse fractionaldigits unscaledvalue negativetrue fractionaldigits unscaledvalue negativefalse fractionaldigits exponent unscaledvalue negativefalse fractionaldigits 
WITHOUT_CLASSIFICATION	 druidoutputformat will write segments intermediate directory 
WITHOUT_CLASSIFICATION	 get the first valid row the batch still available 
WITHOUT_CLASSIFICATION	 create hepplanner 
WITHOUT_CLASSIFICATION	 todo this method ever called more than one jar getting the dir and the 
WITHOUT_CLASSIFICATION	 this based the existing valid write list that was built for select query therefore assume all the aborted txns etc were already accounted for all adjust the high watermark only include contiguous txns 
WITHOUT_CLASSIFICATION	 add writeentity for each matching partition 
WITHOUT_CLASSIFICATION	 may happen that theres not enough memory instantiate hashmap for the partition that case dont create the hashmap but pretend the hashmap directly spilled 
WITHOUT_CLASSIFICATION	 update the property before offering 
WITHOUT_CLASSIFICATION	 this plan projecta all gby keys rewritten nullable projexpr aggregate groupbyall left input refs aggrewritten expression agg projectb rewriten original projected exprs join loj cond true leftinputrel rightinputrel 
WITHOUT_CLASSIFICATION	 tests that doing tablelevel repl load updates table repllastid but not dblevel repllastid 
WITHOUT_CLASSIFICATION	 required optional optional optional optional 
WITHOUT_CLASSIFICATION	 long and double are handled using descriptors string needs specially handled 
WITHOUT_CLASSIFICATION	 preserve the original configuration 
WITHOUT_CLASSIFICATION	 try this list 
WITHOUT_CLASSIFICATION	 passed the unparsed name here getpartitionsps expects parse 
WITHOUT_CLASSIFICATION	 create walker which walks the tree dfs manner while maintaining the operator stack the dispatcher 
WITHOUT_CLASSIFICATION	 ctas with acid target table 
WITHOUT_CLASSIFICATION	 copy null 
WITHOUT_CLASSIFICATION	 the current plan can thrown away after being merged with the union plan 
WITHOUT_CLASSIFICATION	 update jobconf using mrinput info like filename comes via this 
WITHOUT_CLASSIFICATION	 filtering the server and have fall back client path 
WITHOUT_CLASSIFICATION	 the logger name not found root logger returned dont want change root logger level since user either requested new logger specified invalid input which will add the logger that user requested 
WITHOUT_CLASSIFICATION	 end conversion 
WITHOUT_CLASSIFICATION	 this normal insert delta which only has insert events and hence all the files 
WITHOUT_CLASSIFICATION	 looking for map reduce 
WITHOUT_CLASSIFICATION	 someone else replacedremoved paralleladded stale value try again max confusion 
WITHOUT_CLASSIFICATION	 get options from arguments 
WITHOUT_CLASSIFICATION	 less than 
WITHOUT_CLASSIFICATION	 finally create the outer struct contain the key value structs 
WITHOUT_CLASSIFICATION	 build rel for clause 
WITHOUT_CLASSIFICATION	 for writes transaction batch not closed yet 
WITHOUT_CLASSIFICATION	 from 
WITHOUT_CLASSIFICATION	 suffix reduce len 
WITHOUT_CLASSIFICATION	 floattype treated doubletype 
WITHOUT_CLASSIFICATION	 there may more data after the gap 
WITHOUT_CLASSIFICATION	 following fields for displaying queries webui 
WITHOUT_CLASSIFICATION	 update them all 
WITHOUT_CLASSIFICATION	 the current task root task 
WITHOUT_CLASSIFICATION	 walk down expression see which arguments are actually used 
WITHOUT_CLASSIFICATION	 set writeentity for replication 
WITHOUT_CLASSIFICATION	 have processed this the previous run after has already queued the message 
WITHOUT_CLASSIFICATION	 how much have minimum size completely memory blowout factor datasize memory size 
WITHOUT_CLASSIFICATION	 hadoop property names set templeton logic 
WITHOUT_CLASSIFICATION	 this for reconciling hbasestoragehandler for use hcatalog 
WITHOUT_CLASSIFICATION	 returns fileid for smbjoin which consists part result file name 
WITHOUT_CLASSIFICATION	 foo bar blah form 
WITHOUT_CLASSIFICATION	 case last row was large bytes value 
WITHOUT_CLASSIFICATION	 roots 
WITHOUT_CLASSIFICATION	 char starts from index and total length should bytes max 
WITHOUT_CLASSIFICATION	 some inputs will probably never actually happen 
WITHOUT_CLASSIFICATION	 will keycolx valuecolx 
WITHOUT_CLASSIFICATION	 create permanent function 
WITHOUT_CLASSIFICATION	 request interceptor for any request preprocessing logic 
WITHOUT_CLASSIFICATION	 similarly need mapping since value expression can calculation and the value will into scratch column 
WITHOUT_CLASSIFICATION	 has the permissions the table dir 
WITHOUT_CLASSIFICATION	 for udfs that expect primitive types like int instead integer intwritable this will catch the the exception that happens they are passed null value then the default null handling logic will apply and the result will null 
WITHOUT_CLASSIFICATION	 entries the vgby are flushed 
WITHOUT_CLASSIFICATION	 next bits are used locate offset within longword 
WITHOUT_CLASSIFICATION	 could also join with acid tables only get tables with outdated stats 
WITHOUT_CLASSIFICATION	 return garbage value metrics havent been initialized that callers dont have keep checking the resulting value null 
WITHOUT_CLASSIFICATION	 remove the comments 
WITHOUT_CLASSIFICATION	 check groupby empty and there other cols aggr this should only happen when newparent constant 
WITHOUT_CLASSIFICATION	 replicate only one insert into operation the table 
WITHOUT_CLASSIFICATION	 need check the druid metadata 
WITHOUT_CLASSIFICATION	 timer that tops rptimer after long timeout 
WITHOUT_CLASSIFICATION	 remove entire priority level its been emptied 
WITHOUT_CLASSIFICATION	 dont lock files directories also skip locking temp tables 
WITHOUT_CLASSIFICATION	 initialize singlecolumn string members for this specialized class 
WITHOUT_CLASSIFICATION	 create copy the function descriptor 
WITHOUT_CLASSIFICATION	 warn the user bytes per reducer much larger than memory per task 
WITHOUT_CLASSIFICATION	 nothing here this not invoked the logj framework should likely not the logj interface 
WITHOUT_CLASSIFICATION	 hive only supports primitive map keys 
WITHOUT_CLASSIFICATION	 parent stats are not populated yet 
WITHOUT_CLASSIFICATION	 allocate and initialize new conf since test can 
WITHOUT_CLASSIFICATION	 match there filter sqcountcheck right input join which left input another join 
WITHOUT_CLASSIFICATION	 splits are equal number files worst case 
WITHOUT_CLASSIFICATION	 unit test convenience method for putting the key into the hash table using the actual type 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 accurate int value cannot obtained 
WITHOUT_CLASSIFICATION	 save some positional state 
WITHOUT_CLASSIFICATION	 they gave value but not time unit assume the default time unit 
WITHOUT_CLASSIFICATION	 create the converters 
WITHOUT_CLASSIFICATION	 scale down factor account for approximate values 
WITHOUT_CLASSIFICATION	 plugin interface for storage handler which supports input estimation 
WITHOUT_CLASSIFICATION	 allocate pairs cannot above highest integer power 
WITHOUT_CLASSIFICATION	 relogin with kerberos this makes sure all daemons have the same login user 
WITHOUT_CLASSIFICATION	 with nulls 
WITHOUT_CLASSIFICATION	 assuming grouping enabled always 
WITHOUT_CLASSIFICATION	 per split strategy basis and has same for all the files that strategy 
WITHOUT_CLASSIFICATION	 read many records because sometimes the recordreader for the format test behaves different with one record than bunch records 
WITHOUT_CLASSIFICATION	 todo test dropping nonempty catalog 
WITHOUT_CLASSIFICATION	 draw and return order further run should return last returned 
WITHOUT_CLASSIFICATION	 equal maps 
WITHOUT_CLASSIFICATION	 need reset true case previous aggregateproject has set false 
WITHOUT_CLASSIFICATION	 evaluate the result given partition and the row number process 
WITHOUT_CLASSIFICATION	 error 
WITHOUT_CLASSIFICATION	 driver class 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 work 
WITHOUT_CLASSIFICATION	 code below shameless borrowed from hadoop streaming 
WITHOUT_CLASSIFICATION	 each iteration cleans the file cache single unit unlike the orc cache 
WITHOUT_CLASSIFICATION	 delimiter check dot delimited qualified names 
WITHOUT_CLASSIFICATION	 keeping mintxnid atomic shared with heartbeat thread 
WITHOUT_CLASSIFICATION	 setting the default batch size makes the memory check rows work the same the row row writer was the default the smallest stripe size would rows which changes the output some the tests 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the hash table slots for long key hash table each slot longs and the array sized the slot pair nonzero reference word the first value bytes and the long value 
WITHOUT_CLASSIFICATION	 corresponds semanalyzer 
WITHOUT_CLASSIFICATION	 have check here since invalid decref will overflow 
WITHOUT_CLASSIFICATION	 batches will sized 
WITHOUT_CLASSIFICATION	 found stale value cannot incref try replace with new value 
WITHOUT_CLASSIFICATION	 since renewal kerberos authenticated token may not cached 
WITHOUT_CLASSIFICATION	 check this grant statement will end creating cycle 
WITHOUT_CLASSIFICATION	 have keep least branch before support empty values hive 
WITHOUT_CLASSIFICATION	 bgenjjtree enumdef 
WITHOUT_CLASSIFICATION	 update statistics based column statistics conditions keeps adding the stats independently this may result number rows getting more than the input rows which case stats need not updated 
WITHOUT_CLASSIFICATION	 check that the table valid under strict managed tables mode 
WITHOUT_CLASSIFICATION	 the applicationlevel name 
WITHOUT_CLASSIFICATION	 dedup file list 
WITHOUT_CLASSIFICATION	 table was renamed 
WITHOUT_CLASSIFICATION	 only need aborted since dont consider anything above minopenwriteid 
WITHOUT_CLASSIFICATION	 this point should add any relevant jars that would needed for the udf 
WITHOUT_CLASSIFICATION	 the index 
WITHOUT_CLASSIFICATION	 list terminal operation states measure only completed counts for operations these states 
WITHOUT_CLASSIFICATION	 increment the counters only when there are violations 
WITHOUT_CLASSIFICATION	 varchar char length 
WITHOUT_CLASSIFICATION	 set dont repeat this initialization 
WITHOUT_CLASSIFICATION	 should able execute without failure the session whose transport has been closed 
WITHOUT_CLASSIFICATION	 operation fails invalid input 
WITHOUT_CLASSIFICATION	 handle sqlline command beeline which starts with and does not end with 
WITHOUT_CLASSIFICATION	 right now only one parent 
WITHOUT_CLASSIFICATION	 check here for each dir were copying out see already exists error out also treat dynwrites writes immutable tables dryrun true immutable true 
WITHOUT_CLASSIFICATION	 filter operator 
WITHOUT_CLASSIFICATION	 allow for empty string etc 
WITHOUT_CLASSIFICATION	 get metastorethrift privilege object using metastore api 
WITHOUT_CLASSIFICATION	 transitive 
WITHOUT_CLASSIFICATION	 the output partial aggregation struct containing long count and doubles sum and variance 
WITHOUT_CLASSIFICATION	 then drop the database 
WITHOUT_CLASSIFICATION	 the failure occurred before even made entry compactionqueue 
WITHOUT_CLASSIFICATION	 add alias aliastoopinfo and optoalias 
WITHOUT_CLASSIFICATION	 the old new output position mapping will the same that 
WITHOUT_CLASSIFICATION	 date integer internally 
WITHOUT_CLASSIFICATION	 queue notify generate the next batch rows 
WITHOUT_CLASSIFICATION	 task execution time out configured for submit operation then job may need killed execution time out these parameters controls the maximum number retries and retry wait time seconds for executing the time out task 
WITHOUT_CLASSIFICATION	 the next line works 
WITHOUT_CLASSIFICATION	 call open read data split mockmocktable call call call 
WITHOUT_CLASSIFICATION	 check appropriate codec available 
WITHOUT_CLASSIFICATION	 value simpleentry rowcount 
WITHOUT_CLASSIFICATION	 the random values must between and distributed uniformly the average value large set should about verify close this value 
WITHOUT_CLASSIFICATION	 will only interrupt checking the lowestlevel operator for multiple joins 
WITHOUT_CLASSIFICATION	 test decimal scalar divided column this tests the primary logic for template 
WITHOUT_CLASSIFICATION	 verify 
WITHOUT_CLASSIFICATION	 populate the operator 
WITHOUT_CLASSIFICATION	 metadata should get created 
WITHOUT_CLASSIFICATION	 the operation metastore fails dont anything change management but fail the metastore transaction having copy the jar change management not going 
WITHOUT_CLASSIFICATION	 function setup locks 
WITHOUT_CLASSIFICATION	 first argument hiveversion compatible argument dbversion greater than equal check the compatible case 
WITHOUT_CLASSIFICATION	 not this the identity the rule will nothing 
WITHOUT_CLASSIFICATION	 get synchronized wrapper the meta store remote 
WITHOUT_CLASSIFICATION	 try return stuff that was killed from under should noop 
WITHOUT_CLASSIFICATION	 compile another query 
WITHOUT_CLASSIFICATION	 operator with enough children 
WITHOUT_CLASSIFICATION	 the work that cannot done via async calls 
WITHOUT_CLASSIFICATION	 the column has been obtained from cache 
WITHOUT_CLASSIFICATION	 lets write more bytes the files test that estimator actually working returning the file size not from the filesystem 
WITHOUT_CLASSIFICATION	 last block affect all bits all the case statements fall through 
WITHOUT_CLASSIFICATION	 test gettables with table name pattern 
WITHOUT_CLASSIFICATION	 now add enough failed compactions ensure will attempt delete enough for this but also want enough tickle the code 
WITHOUT_CLASSIFICATION	 lets wait the async ops before continuing 
WITHOUT_CLASSIFICATION	 sequence file read 
WITHOUT_CLASSIFICATION	 column names 
WITHOUT_CLASSIFICATION	 only recompute stats after major compact they existed before 
WITHOUT_CLASSIFICATION	 need reload 
WITHOUT_CLASSIFICATION	 for acid table insert overwrite shouldnt replace the table content keep the old 
WITHOUT_CLASSIFICATION	 toss timestamp and date 
WITHOUT_CLASSIFICATION	 call droppartition each the tables partitions follow the procedure for cleanly dropping partitions 
WITHOUT_CLASSIFICATION	 impl notificationfetcher 
WITHOUT_CLASSIFICATION	 extract columns missing current keyvalue 
WITHOUT_CLASSIFICATION	 create tables one partitioned and other not also have both types full acid and tables 
WITHOUT_CLASSIFICATION	 first compare the length and then compare the directory name 
WITHOUT_CLASSIFICATION	 note that will called before this thus some type promotionconversion may occur short integer should refactor this that its hapenning one place per moduleproduct that are integrating with all pig conversion should done here etc 
WITHOUT_CLASSIFICATION	 since guaranteed produce most one row 
WITHOUT_CLASSIFICATION	 without vectorization 
WITHOUT_CLASSIFICATION	 signing not required for tez 
WITHOUT_CLASSIFICATION	 hive ast right child join cannot another join thus need introduce project top but only need the additional project the left child another join too not astconverter will swap the join inputs leaving the join operator the left also parent hivesemijoin since astconverter wont swap inputs then this will help triggering multijoin recognition methods that are embedded semanticanalyzer 
WITHOUT_CLASSIFICATION	 ensures that the list doesnt have dups and keeps track directories have created 
WITHOUT_CLASSIFICATION	 alter partition 
WITHOUT_CLASSIFICATION	 find how much compressed data was added for this column 
WITHOUT_CLASSIFICATION	 test the vectorized udf adaptor verify that custom legacy and generic udfs can run vectorized mode 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 row was processed 
WITHOUT_CLASSIFICATION	 nothing here 
WITHOUT_CLASSIFICATION	 first just allocate just the projection columns will using 
WITHOUT_CLASSIFICATION	 tabletype specified was null need figure out what type was 
WITHOUT_CLASSIFICATION	 hive has max limit for binary 
WITHOUT_CLASSIFICATION	 evaluate union object 
WITHOUT_CLASSIFICATION	 this turns splitupdate udi 
WITHOUT_CLASSIFICATION	 split each row duplicate which will cause update into rows and augment with col which has insert update 
WITHOUT_CLASSIFICATION	 get row resolvers column map for original left and right input 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 cas race look again 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 nothing far and shouldnt called 
WITHOUT_CLASSIFICATION	 create bloom filter with same number bits but different hash functions 
WITHOUT_CLASSIFICATION	 one input path would mean only one map task 
WITHOUT_CLASSIFICATION	 copy 
WITHOUT_CLASSIFICATION	 mark any scratch small table scratch columns that would normally receive copy the key null too 
WITHOUT_CLASSIFICATION	 single mapreduce job launched 
WITHOUT_CLASSIFICATION	 deserializes bit decimals the maximum bit precision decimal digits note major assumption the input decimal has already been bounds checked and least has precision not bounds check here for better performance you can bounds check beforehand with mathabsdecimallong 
WITHOUT_CLASSIFICATION	 month granularity 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 modifier letter triangular colon bytes 
WITHOUT_CLASSIFICATION	 global contains excludes individual modules can only contain additional excludes 
WITHOUT_CLASSIFICATION	 clean the database 
WITHOUT_CLASSIFICATION	 values colexpmap and rowschema 
WITHOUT_CLASSIFICATION	 become part the record otherwise will just write over later 
WITHOUT_CLASSIFICATION	 dont compact and opened 
WITHOUT_CLASSIFICATION	 for rsselrs case reducer operator reducer task cannot null task compiler 
WITHOUT_CLASSIFICATION	 once the feature stable 
WITHOUT_CLASSIFICATION	 synchronized lock 
WITHOUT_CLASSIFICATION	 some other things that could added here model cost cost computingsending partial bloomfilter results bloomfiltersize mappers for reduceside join add the cost the semijoin table scandependent tablescans 
WITHOUT_CLASSIFICATION	 register for notifications inside the lock should avoid races with happens different submission thread avoid register running for this task 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 want avoid expiring locks for txn expiring the txn itself 
WITHOUT_CLASSIFICATION	 add another value 
WITHOUT_CLASSIFICATION	 register the shard sub type used the mapper 
WITHOUT_CLASSIFICATION	 determine which stripes read based the split 
WITHOUT_CLASSIFICATION	 create operation log root directory operation logging enabled 
WITHOUT_CLASSIFICATION	 longer the wait time for attempt wrt its start time higher the priority gets 
WITHOUT_CLASSIFICATION	 first breaking the filter conditions into equality comparisons between rightjoinkeysfrom the original filterinputrel and correlatedjoinkeys correlatedjoinkeys can only rexfieldaccess while rightjoinkeys can 
WITHOUT_CLASSIFICATION	 save the original job tracker 
WITHOUT_CLASSIFICATION	 zero need shiftscale 
WITHOUT_CLASSIFICATION	 create test input file with specified number rows 
WITHOUT_CLASSIFICATION	 returning void because ignore this production 
WITHOUT_CLASSIFICATION	 currently using the format 
WITHOUT_CLASSIFICATION	 range check needed 
WITHOUT_CLASSIFICATION	 test nonvectorized acid combine 
WITHOUT_CLASSIFICATION	 determine bit width for bitpacking and encode header 
WITHOUT_CLASSIFICATION	 boolean value match for char field 
WITHOUT_CLASSIFICATION	 bump its internal version 
WITHOUT_CLASSIFICATION	 this removed using poll because there can case where there partitions iterator empty but because both the producer and consumer are started simultaneously the while loop will execute because producer not terminated but wont produce anything queue will empty and then should only wait for specific time before continuing the next loop cycle will fail 
WITHOUT_CLASSIFICATION	 clear out isnull array 
WITHOUT_CLASSIFICATION	 adds the missing schemeauthority for the new table location 
WITHOUT_CLASSIFICATION	 first child subquery second child alias set the node interest and the subquery 
WITHOUT_CLASSIFICATION	 return key from any the readers 
WITHOUT_CLASSIFICATION	 the incoming split may not file split when are regrouping tezgroupedsplits the case smb join this case can early exit not doing the calculation for bucketsizemap each bucket will assume can fill availableslots waves preset for smb join 
WITHOUT_CLASSIFICATION	 getposition different columns should never give the same value 
WITHOUT_CLASSIFICATION	 string group comparison 
WITHOUT_CLASSIFICATION	 fetch task query 
WITHOUT_CLASSIFICATION	 note that deletedelta should not read when minor compacted deletedelta present 
WITHOUT_CLASSIFICATION	 traverse data and masks array together check for set bits 
WITHOUT_CLASSIFICATION	 set sessionusername dfs querytab 
WITHOUT_CLASSIFICATION	 copy the files from different source file systems one destination directory 
WITHOUT_CLASSIFICATION	 otherwise didnt understand mark maybe 
WITHOUT_CLASSIFICATION	 exclusive locks occur before shared locks 
WITHOUT_CLASSIFICATION	 now insert from union here create data files sub dirs 
WITHOUT_CLASSIFICATION	 found map objectinspector grab the objectinspector for the value and initialize aptly 
WITHOUT_CLASSIFICATION	 need unique ids refer each minmax key value the 
WITHOUT_CLASSIFICATION	 tokfrom subtree 
WITHOUT_CLASSIFICATION	 preserve only partitioning 
WITHOUT_CLASSIFICATION	 drop database everything all meta tables should disappear 
WITHOUT_CLASSIFICATION	 run common join task 
WITHOUT_CLASSIFICATION	 replace the node place 
WITHOUT_CLASSIFICATION	 matches times one time the original node one time the new node created the rule 
WITHOUT_CLASSIFICATION	 nonmm case 
WITHOUT_CLASSIFICATION	 groupby reorders the keys emitted hence the keycols would change 
WITHOUT_CLASSIFICATION	 shutdown hook clean resources process end 
WITHOUT_CLASSIFICATION	 simple one long key map join benchmarks build with mvn clean install dskiptests pdistitests main hive directory from itestshivejmh directory run java jar targetbenchmarksjar inner innerbigonly leftsemi outer rowmodehashmap rowmodeoptimized vectorpassthrough nativevectorfast 
WITHOUT_CLASSIFICATION	 filter timestamp against timestamp interval day time against interval day time 
WITHOUT_CLASSIFICATION	 disable sargs for deleteeventreaders sargs have meaning 
WITHOUT_CLASSIFICATION	 for rule mapjoinmapjoin have child mapjoin the the current mapjoin local work will put the current mapjoin the rejected list 
WITHOUT_CLASSIFICATION	 hivedecimal suppresses trailing zeroes 
WITHOUT_CLASSIFICATION	 different kinds vectorized reading supported read the vectorized input file format which returns vectorizedrowbatch the row read using deserialize each row into the vectorizedrowbatch and read using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow 
WITHOUT_CLASSIFICATION	 set servers idle timeout very low value 
WITHOUT_CLASSIFICATION	 used check recursive cte invocations similar viewsexpanded 
WITHOUT_CLASSIFICATION	 serialize bytes 
WITHOUT_CLASSIFICATION	 unquoted space 
WITHOUT_CLASSIFICATION	 that know the type table are creating acidmm match what was exported 
WITHOUT_CLASSIFICATION	 dont add partition data already exists 
WITHOUT_CLASSIFICATION	 fix the input column numbers and output column numbers 
WITHOUT_CLASSIFICATION	 setobject the yet unknown type javautildate 
WITHOUT_CLASSIFICATION	 truncate the excess chars fit the character length also make sure take supplementary chars into account 
WITHOUT_CLASSIFICATION	 row limit does not match currently not merge 
WITHOUT_CLASSIFICATION	 pass 
WITHOUT_CLASSIFICATION	 build plan 
WITHOUT_CLASSIFICATION	 the total size local tables after merge localworks larger than the limit set 
WITHOUT_CLASSIFICATION	 there should original bucket files the location and 
WITHOUT_CLASSIFICATION	 suppress empty column map 
WITHOUT_CLASSIFICATION	 the data shuffle 
WITHOUT_CLASSIFICATION	 all other cases throw exception its whitelist allowed operations 
WITHOUT_CLASSIFICATION	 create the hadoop archive 
WITHOUT_CLASSIFICATION	 contains aggregate and not full acid table not rewrite need merge support 
WITHOUT_CLASSIFICATION	 window spec with parition spec partitioned constantnumber 
WITHOUT_CLASSIFICATION	 minor optimization avoiding creating new objects 
WITHOUT_CLASSIFICATION	 alias not fully qualified 
WITHOUT_CLASSIFICATION	 passing null matches everything 
WITHOUT_CLASSIFICATION	 check whether this input operator produces output has residual not skip this output will add select top the join 
WITHOUT_CLASSIFICATION	 now propagate the constant from the parent the child 
WITHOUT_CLASSIFICATION	 target paths last component also the column family name 
WITHOUT_CLASSIFICATION	 various restrictions 
WITHOUT_CLASSIFICATION	 handle stop this process from the outside needed 
WITHOUT_CLASSIFICATION	 have estimator for this type assume low overhead and hope for the best 
WITHOUT_CLASSIFICATION	 use boundarytype boundary amt sort key order behavior case preceding unb any any start preceding unsigned int null asc start desc scan backwards row such that rsk not null start ridx preceding unsigned int not null desc scan backwards until row such that rsk rsk amt start ridx preceding unsigned int not null asc scan backward until row such that rsk rsk bndamt start ridx current row null any scan backwards until row such that rsk not null start ridx current row not null any scan backwards until row such rsk rsk start ridx following unb any any error following unsigned int null desc start partitionsize asc scan forward until such that rsk not null start ridx following unsigned int not null desc scan forward until row such that rsk rsk amt start ridx asc scan forward until row such that rsk rsk amt 
WITHOUT_CLASSIFICATION	 task requested host got host 
WITHOUT_CLASSIFICATION	 authorize the operation 
WITHOUT_CLASSIFICATION	 that operator writes into the bucketsort columns for that data 
WITHOUT_CLASSIFICATION	 projection that casts proj expr nullable type 
WITHOUT_CLASSIFICATION	 ascending 
WITHOUT_CLASSIFICATION	 partition keys 
WITHOUT_CLASSIFICATION	 add new node the cache 
WITHOUT_CLASSIFICATION	 relying the rpc threads keep the service alive 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 execute malformed query 
WITHOUT_CLASSIFICATION	 partstats 
WITHOUT_CLASSIFICATION	 todo even listener for check new true this 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 assume splits will never start the middle the stripe 
WITHOUT_CLASSIFICATION	 reset the buffer were going use 
WITHOUT_CLASSIFICATION	 test that existing exclusive table with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 minimize allocations 
WITHOUT_CLASSIFICATION	 change column 
WITHOUT_CLASSIFICATION	 increments one hms connection hiveget 
WITHOUT_CLASSIFICATION	 call open read data split mockmocktable 
WITHOUT_CLASSIFICATION	 long scalarcolumn 
WITHOUT_CLASSIFICATION	 filesystemcache 
WITHOUT_CLASSIFICATION	 timestamps are not supported both dates were changed 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 insert overwrite table 
WITHOUT_CLASSIFICATION	 get sitexml loaded 
WITHOUT_CLASSIFICATION	 containerendtaskend invocation 
WITHOUT_CLASSIFICATION	 link the rpc and the promise that events from one are propagated the other needed 
WITHOUT_CLASSIFICATION	 note this will determine the order columns the result for now the columns for each table will together the order the tables well the columns within each table deterministic but undefined stores them the order addition 
WITHOUT_CLASSIFICATION	 reorder tags need 
WITHOUT_CLASSIFICATION	 keep draining the queue the same session 
WITHOUT_CLASSIFICATION	 room for optimization since cannot convert empty project operator 
WITHOUT_CLASSIFICATION	 the first thread detect the error cleanup old connection reconnect 
WITHOUT_CLASSIFICATION	 only bonecp should return true 
WITHOUT_CLASSIFICATION	 dont add the partition table created during the execution the input source 
WITHOUT_CLASSIFICATION	 cancel the watchkey since the output dir has been found 
WITHOUT_CLASSIFICATION	 collect keyvalues for this row 
WITHOUT_CLASSIFICATION	 update largest relation 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 true only one date null 
WITHOUT_CLASSIFICATION	 setup values registry 
WITHOUT_CLASSIFICATION	 ignore and break 
WITHOUT_CLASSIFICATION	 mapper can span partitions combine into few one split subject the pathfilters set using combinecreatepool 
WITHOUT_CLASSIFICATION	 hint disable mapjoin 
WITHOUT_CLASSIFICATION	 just compare the magnitudes signums set 
WITHOUT_CLASSIFICATION	 dont propagate the error termination was done part closing the client 
WITHOUT_CLASSIFICATION	 default runs slightly over day long 
WITHOUT_CLASSIFICATION	 include the original blank value longminvalue the negatives make sure get 
WITHOUT_CLASSIFICATION	 previous record the write buffers see writebuffers javadoc 
WITHOUT_CLASSIFICATION	 pool exhausted return new object 
WITHOUT_CLASSIFICATION	 pattern for keyvaluekeyvalue 
WITHOUT_CLASSIFICATION	 check the output fixacidkeyindex should indicate the index was invalid 
WITHOUT_CLASSIFICATION	 now its time rewrite the aggregate 
WITHOUT_CLASSIFICATION	 left repeats and null 
WITHOUT_CLASSIFICATION	 use that one mapper processes exactly one file 
WITHOUT_CLASSIFICATION	 iterate through the ophandles and close their operations 
WITHOUT_CLASSIFICATION	 create segment file the destination location with linearshardspec 
WITHOUT_CLASSIFICATION	 return metadataonly 
WITHOUT_CLASSIFICATION	 unpartitioned table 
WITHOUT_CLASSIFICATION	 subquery project 
WITHOUT_CLASSIFICATION	 required required required 
WITHOUT_CLASSIFICATION	 fetch the first group for all small table aliases 
WITHOUT_CLASSIFICATION	 test that partition key not allowed data 
WITHOUT_CLASSIFICATION	 grab the tag and the field 
WITHOUT_CLASSIFICATION	 the hadoop cluster secure kerberos login for the service from the keytab 
WITHOUT_CLASSIFICATION	 matched instance not leader 
WITHOUT_CLASSIFICATION	 try reconnect child job one found 
WITHOUT_CLASSIFICATION	 groupby query results 
WITHOUT_CLASSIFICATION	 lost statistics optraits through cloning try get them back 
WITHOUT_CLASSIFICATION	 this async method always launch threads even for single task 
WITHOUT_CLASSIFICATION	 for now just decimal inputs and decimal boolean output 
WITHOUT_CLASSIFICATION	 executorservice for sending heartbeat metastore periodically 
WITHOUT_CLASSIFICATION	 given list partstats this function will give you aggr stats 
WITHOUT_CLASSIFICATION	 this means the column was not included the projection from the underlying read 
WITHOUT_CLASSIFICATION	 generate the token for query user applies all splits 
WITHOUT_CLASSIFICATION	 for each source write get the appropriate lock type its overwrite need get exclusive lock its insert overwrite than need shared its update delete then 
WITHOUT_CLASSIFICATION	 there are filter operators the pipeline 
WITHOUT_CLASSIFICATION	 check filter input contains correlation 
WITHOUT_CLASSIFICATION	 analyzecreateview uses thisast but dophase doesnt only reset here 
WITHOUT_CLASSIFICATION	 make sure session init gets stuck init 
WITHOUT_CLASSIFICATION	 double wait time until min 
WITHOUT_CLASSIFICATION	 want use the because otherwise will return for two objects that have different object inspectors and the roir will help convert both values common type that they can compared reasonably 
WITHOUT_CLASSIFICATION	 update the nextlevel with newly discovered subdirectories from the above 
WITHOUT_CLASSIFICATION	 create the list needed 
WITHOUT_CLASSIFICATION	 dont expect cache requests from the middle 
WITHOUT_CLASSIFICATION	 theres some special handling for dummyops required mapjoins wont properly initialized their dummy parents arent initialized since cloned the plan 
WITHOUT_CLASSIFICATION	 zero 
WITHOUT_CLASSIFICATION	 message needed 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 enable blobstore optimizations for the rest tests 
WITHOUT_CLASSIFICATION	 null hiveconf passed jdbc driver side code since driver side supposed independent conf object create new hiveconf object here this case 
WITHOUT_CLASSIFICATION	 the version doesnt exist then create 
WITHOUT_CLASSIFICATION	 the counters are missing there point trying print progress 
WITHOUT_CLASSIFICATION	 need clean data directory ensure that there interference from old runs cleaning happening here allow debugging case tests fail dont have clean logs since append mode 
WITHOUT_CLASSIFICATION	 create walker which walks the tree dfs manner while maintaining the operator stack the dispatcher generates the plan from the operator tree 
WITHOUT_CLASSIFICATION	 replicate insert event and verify 
WITHOUT_CLASSIFICATION	 decimal conversion instead 
WITHOUT_CLASSIFICATION	 both are nonempty only copy now 
WITHOUT_CLASSIFICATION	 the jdoexception may wrapped further metaexception 
WITHOUT_CLASSIFICATION	 trigger rewriting remove union branch with 
WITHOUT_CLASSIFICATION	 all txns below minuncommittedtxnid are either committed emptyaborted are allowed 
WITHOUT_CLASSIFICATION	 should now have new lock acidtblpart 
WITHOUT_CLASSIFICATION	 nonacid 
WITHOUT_CLASSIFICATION	 check whether the shuffle version compatible 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 already setup the create method 
WITHOUT_CLASSIFICATION	 the skewedvalues contains and the user looking for positions the result would get the skewed key values that are part the join key param skewedvalueslist list all the skewed values param positionskewedkeys the requested positions return sublist skewed values with the positions present 
WITHOUT_CLASSIFICATION	 queries without source table currently are not supported cbo 
WITHOUT_CLASSIFICATION	 start third batch but dont close this delta will ignored compaction since 
WITHOUT_CLASSIFICATION	 the cookie based authentication already enabled parse the 
WITHOUT_CLASSIFICATION	 nope look see can find conf file finding our jar going one directory and looking for conf directory 
WITHOUT_CLASSIFICATION	 put the mapping from table scan operator partpruner map 
WITHOUT_CLASSIFICATION	 diff against table target 
WITHOUT_CLASSIFICATION	 location not shown test mode 
WITHOUT_CLASSIFICATION	 only database object updated 
WITHOUT_CLASSIFICATION	 csvreader will throw exception any separator quote escape the same but the csv format specifies that the escape character and quote char are the same very weird 
WITHOUT_CLASSIFICATION	 dont have many file formats that implement inputformatchecker wont holding 
WITHOUT_CLASSIFICATION	 move data from temp directory the actual table directory 
WITHOUT_CLASSIFICATION	 isallparts 
WITHOUT_CLASSIFICATION	 required required required required required required required required optional 
WITHOUT_CLASSIFICATION	 remote metastore mode 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this class should replaced with class once fixed this code should removed copy 
WITHOUT_CLASSIFICATION	 column statistics from different sources are put together and 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 should fail because the transactionbatch timed out 
WITHOUT_CLASSIFICATION	 whether the native vectorized map join operator has performed its common setup 
WITHOUT_CLASSIFICATION	 update stmt has pblah thus nothing actually update and generate empty dyn part list 
WITHOUT_CLASSIFICATION	 prewarm cachedstore 
WITHOUT_CLASSIFICATION	 checkh for and not the subquery must implicitly explicitly only contain one select item 
WITHOUT_CLASSIFICATION	 add uncovered acid delta splits 
WITHOUT_CLASSIFICATION	 make sure assign correct ids 
WITHOUT_CLASSIFICATION	 these namestypes are the data columns plus partition columns 
WITHOUT_CLASSIFICATION	 sleep before send checklock again but with back off 
WITHOUT_CLASSIFICATION	 try valid alter table partition key comment 
WITHOUT_CLASSIFICATION	 add small table result columns 
WITHOUT_CLASSIFICATION	 validate the update new column even old rows 
WITHOUT_CLASSIFICATION	 datastr not null and datastr not pattern 
WITHOUT_CLASSIFICATION	 expand the nested script the metadbtype set this setting the information schema hive that specifically means that the sql commands need adjusted for the underlying rdbms correct quotation strings etc 
WITHOUT_CLASSIFICATION	 populate source 
WITHOUT_CLASSIFICATION	 all the joins fit into half the memory lets safe and scale them out 
WITHOUT_CLASSIFICATION	 parse resultexpr str and setup 
WITHOUT_CLASSIFICATION	 creat default dir 
WITHOUT_CLASSIFICATION	 this class the payload for custom vertex serializes and deserializes numbuckets the number buckets the big table vertextype this the type vertex and differentiates between bucket map join and smb joins numinputs the number inputs that are directly connected the vertex mrinputmultimrinput case bucket map join always inputname this the name the input used case smb joins empty case bucketmapjoin 
WITHOUT_CLASSIFICATION	 repl imports are replaceimports and thus are idempotent note that this assumes that this importcommand running export dump created using export for replication the scope importcommand were eventually expand importing dumps created regular exports then this needs updating 
WITHOUT_CLASSIFICATION	 test rpcserveraddress not configured but server host configured 
WITHOUT_CLASSIFICATION	 make the list transactional tables list which are getting read written current txn 
WITHOUT_CLASSIFICATION	 note given that return pool sessions the pool the finally block below and that 
WITHOUT_CLASSIFICATION	 end input confirm got end stream indicator from server well done status from fragment execution 
WITHOUT_CLASSIFICATION	 are testing for both type modes always not passing that parameter for now 
WITHOUT_CLASSIFICATION	 serialize the result struct 
WITHOUT_CLASSIFICATION	 retain this digit 
WITHOUT_CLASSIFICATION	 tracks tasks which are running useful for selecting task preempt based when started 
WITHOUT_CLASSIFICATION	 maxcapacity should calculated based percentage memorythreshold which divide row size using long size 
WITHOUT_CLASSIFICATION	 delete clause 
WITHOUT_CLASSIFICATION	 unique key the leftinputrel 
WITHOUT_CLASSIFICATION	 validation substitute class name for thisclass only public fields and methods are versioned methods compare nonstatic return type name parameter types exceptions thrown fields compare nonstatic type name value when static 
WITHOUT_CLASSIFICATION	 decimal means decimal 
WITHOUT_CLASSIFICATION	 note the normalize call with rounding hivedecimal will currently reduce the precision and scale the value throwing away trailing zeroes this may may not desirable for the literals however this used the default behavior for explicit decimal literals keep this behavior for now 
WITHOUT_CLASSIFICATION	 this field not null 
WITHOUT_CLASSIFICATION	 regular singlepartition write into partitioned table 
WITHOUT_CLASSIFICATION	 safety check 
WITHOUT_CLASSIFICATION	 save the conf variable values that they can restored later 
WITHOUT_CLASSIFICATION	 initialization here adapted from method 
WITHOUT_CLASSIFICATION	 validate there added null for column 
WITHOUT_CLASSIFICATION	 get output committer 
WITHOUT_CLASSIFICATION	 call open mockmocktbl 
WITHOUT_CLASSIFICATION	 data read 
WITHOUT_CLASSIFICATION	 third row 
WITHOUT_CLASSIFICATION	 fatal errors happen should kill the job immediately rather than 
WITHOUT_CLASSIFICATION	 test basic right trim vector 
WITHOUT_CLASSIFICATION	 add unique element list per occurrence order skewed value occurrence order skewed value doesnt matter 
WITHOUT_CLASSIFICATION	 store char vector row batch with padding stripped 
WITHOUT_CLASSIFICATION	 rpc already handles retries will just try kill the session here this will cause the current query fail could instead keep retrying 
WITHOUT_CLASSIFICATION	 serialize numdistinctvalue estimator 
WITHOUT_CLASSIFICATION	 run hive metastore server 
WITHOUT_CLASSIFICATION	 mock operationmanager for session 
WITHOUT_CLASSIFICATION	 case data sorted time and extra hashing dimension see thus use segment partition addition time dimension data with the same and time interval will end the same segment 
WITHOUT_CLASSIFICATION	 check that writeid has been excluded check that the data sorted order 
WITHOUT_CLASSIFICATION	 should never happen since are reading bucketx written acid write 
WITHOUT_CLASSIFICATION	 not any blocking ops this thread 
WITHOUT_CLASSIFICATION	 however can constant too that case need track the column that originated from the input operator can propagate the aliases 
WITHOUT_CLASSIFICATION	 this batch only used vectorrow deserializer readers 
WITHOUT_CLASSIFICATION	 special handling for sql delete from table where 
WITHOUT_CLASSIFICATION	 show database level privileges 
WITHOUT_CLASSIFICATION	 add hivesitexml add this first that gets overridden the new metastore 
WITHOUT_CLASSIFICATION	 hiveserver specific configs 
WITHOUT_CLASSIFICATION	 create new outgoing vectorization context because column name map will change 
WITHOUT_CLASSIFICATION	 chars try keep cols aligned 
WITHOUT_CLASSIFICATION	 set current user session conf 
WITHOUT_CLASSIFICATION	 file pattern that set propertiesfile 
WITHOUT_CLASSIFICATION	 prspgbycrs 
WITHOUT_CLASSIFICATION	 ensure pig can write correctly smallinttinyint columns this means values within the 
WITHOUT_CLASSIFICATION	 alter all partitions 
WITHOUT_CLASSIFICATION	 create the row object 
WITHOUT_CLASSIFICATION	 final result 
WITHOUT_CLASSIFICATION	 deserialize the result 
WITHOUT_CLASSIFICATION	 temp tables that not through semanticanalyzer may not have location set here for example export acid tables generates query plan that creates temp table 
WITHOUT_CLASSIFICATION	 find which columns need update for this partition any 
WITHOUT_CLASSIFICATION	 and preserve rows only from left side 
WITHOUT_CLASSIFICATION	 create root scratchdir with write all that user impersonation has issues 
WITHOUT_CLASSIFICATION	 unionwork null means the first time need create union work object and add this work subsequent work should reference the union and not the actual work 
WITHOUT_CLASSIFICATION	 aggregation result null 
WITHOUT_CLASSIFICATION	 from txncomponents 
WITHOUT_CLASSIFICATION	 process the first node extract tablename 
WITHOUT_CLASSIFICATION	 now have table with data files multiple different levels 
WITHOUT_CLASSIFICATION	 checkexpression 
WITHOUT_CLASSIFICATION	 need add for the default supported local jar driver 
WITHOUT_CLASSIFICATION	 set the thread local address 
WITHOUT_CLASSIFICATION	 reimplemented use primitivecategory rather than typeinfo because typeinfos from the same qualified type varchar decimal should still seen equivalent 
WITHOUT_CLASSIFICATION	 init output 
WITHOUT_CLASSIFICATION	 func may null when gby closing see mvn test dqfileexplainuserq original behavior create fmsketch 
WITHOUT_CLASSIFICATION	 need make sure that all the field associated with the union are settable 
WITHOUT_CLASSIFICATION	 cant use the current table the big table but its too big for the map side 
WITHOUT_CLASSIFICATION	 cycle consists atleast one dynamic partition pruningdpp optimization and atleast one minmax optimization dpp better optimization unless ends scanning the bigger table for keys instead the smaller table 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject int 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 partitioning columns the child are assigned assign these the partitioning columns the parent 
WITHOUT_CLASSIFICATION	 loginfofound list record assumes are here after key compare 
WITHOUT_CLASSIFICATION	 new true this 
WITHOUT_CLASSIFICATION	 the product the toppermutation and bottompermutation yields the identity then can swap the join and remove the project 
WITHOUT_CLASSIFICATION	 synchronized locking itself 
WITHOUT_CLASSIFICATION	 when make new connection should get from minihs this time 
WITHOUT_CLASSIFICATION	 tablename and pattern 
WITHOUT_CLASSIFICATION	 after set originaldata null incref the buffer and the cleanup would decref note that this assumes the failure during incref means incref didnt occur 
WITHOUT_CLASSIFICATION	 data for hll bias correction 
WITHOUT_CLASSIFICATION	 directory 
WITHOUT_CLASSIFICATION	 basen cannot contain updatedelete original files are all insert and need compact only there are updatedelete events 
WITHOUT_CLASSIFICATION	 create aggregate top with the new aggregate list 
WITHOUT_CLASSIFICATION	 should generate infinf 
WITHOUT_CLASSIFICATION	 move onto the next null byte 
WITHOUT_CLASSIFICATION	 marked being read defaults true that the most common case 
WITHOUT_CLASSIFICATION	 extend any repeating values and nonulls indicator the inputs 
WITHOUT_CLASSIFICATION	 return genconvertcoldest tab tabledesc input arraysaslist convert the case update and delete the bucketing column always the first column and isnt the table info rather than asking the table for well construct ourself and send back this based the work done genconvertcol below 
WITHOUT_CLASSIFICATION	 these are global since orc reuses objects between stripes 
WITHOUT_CLASSIFICATION	 steps create the archive temporary folder move the archive dir intermediate dir that the same dir the original partition dir call the new dir intermediatearchive rename the original partition dir intermediate dir call the renamed dir rename intermediatearchive the original partition dir change the metadata delete the original partition files 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 remove 
WITHOUT_CLASSIFICATION	 parse the configuration parameters 
WITHOUT_CLASSIFICATION	 number reducers set default 
WITHOUT_CLASSIFICATION	 each bucket 
WITHOUT_CLASSIFICATION	 not match ignore the line return row with all nulls 
WITHOUT_CLASSIFICATION	 effect the input null because outofrange precisionscale 
WITHOUT_CLASSIFICATION	 has been committed all others open 
WITHOUT_CLASSIFICATION	 whether contains sort merge join operator 
WITHOUT_CLASSIFICATION	 always choose the function with least implicit conversions 
WITHOUT_CLASSIFICATION	 join operators which may converted commonjoinresolver 
WITHOUT_CLASSIFICATION	 confoverlay 
WITHOUT_CLASSIFICATION	 order facilitate partition pruning the where clauses together and put them the 
WITHOUT_CLASSIFICATION	 rewrite the ast replace tabref with maskingfiltering 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 insert mapside 
WITHOUT_CLASSIFICATION	 end the pools array 
WITHOUT_CLASSIFICATION	 send failover request again minihs and get failure 
WITHOUT_CLASSIFICATION	 got the expr for one full partition spec determine the prefix length 
WITHOUT_CLASSIFICATION	 save old values 
WITHOUT_CLASSIFICATION	 see 
WITHOUT_CLASSIFICATION	 need staging directories long single partition needed addition 
WITHOUT_CLASSIFICATION	 paths bucket files small table for current bucket file big table initializes fetchoperator for each file paths reuses fetchoperator possible currently number paths always the same bucket numbers are all the same over all partitions table 
WITHOUT_CLASSIFICATION	 initialize export path 
WITHOUT_CLASSIFICATION	 validate that the multijoin valid star join before returning 
WITHOUT_CLASSIFICATION	 unequal strings 
WITHOUT_CLASSIFICATION	 verifyrunselect from repldbname matview ptndata drivermirror 
WITHOUT_CLASSIFICATION	 caches objects before constructing forward cache 
WITHOUT_CLASSIFICATION	 add the rest the memory consumption 
WITHOUT_CLASSIFICATION	 verify the fetched log incrementally 
WITHOUT_CLASSIFICATION	 what are trying get the equivalent new dateymdgettime the local where ymd whatever represents how works this 
WITHOUT_CLASSIFICATION	 can create calcite isdistinctfrom operator for this but since our join reordering algo cant handle this anyway there advantage thisso bail out for now 
WITHOUT_CLASSIFICATION	 test when two jars with shared dependencies are added the classloader contains union the dependencies 
WITHOUT_CLASSIFICATION	 intersperse getat and next calls 
WITHOUT_CLASSIFICATION	 make sure the referenced schema exists 
WITHOUT_CLASSIFICATION	 will only throw jsonexception when statsputbasicstats true has duplicate key which not possible 
WITHOUT_CLASSIFICATION	 only mechanical data retrieval should remain here 
WITHOUT_CLASSIFICATION	 only column family 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this unsupported operator 
WITHOUT_CLASSIFICATION	 create walker which walks the tree bfs manner while maintaining the 
WITHOUT_CLASSIFICATION	 the target column list has the format targetwork 
WITHOUT_CLASSIFICATION	 note use col the key provided again col 
WITHOUT_CLASSIFICATION	 class read and reread the values 
WITHOUT_CLASSIFICATION	 nonjavadoc see utillist 
WITHOUT_CLASSIFICATION	 need make sure that the underlying fields are settable well hence the recursive call for each field note that equalscheck false while invoking getconvertedoi because need bypass the initial check 
WITHOUT_CLASSIFICATION	 reuse super renewal logic 
WITHOUT_CLASSIFICATION	 the applicationlevel name component name component description name for each metric record 
WITHOUT_CLASSIFICATION	 serialized sizes after serialization and deserialization should equal 
WITHOUT_CLASSIFICATION	 variables hold state from before flattening can easily restored 
WITHOUT_CLASSIFICATION	 required required required optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 first check the local cache 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 wed have thrown exception 
WITHOUT_CLASSIFICATION	 need sessionhooktest hivesite 
WITHOUT_CLASSIFICATION	 duplicate the value merging should remove duplicates 
WITHOUT_CLASSIFICATION	 sleep for expiry time and then fetch again 
WITHOUT_CLASSIFICATION	 upgrade schema from latest 
WITHOUT_CLASSIFICATION	 apply comparison rules 
WITHOUT_CLASSIFICATION	 for close the local work 
WITHOUT_CLASSIFICATION	 output also big the output size medium 
WITHOUT_CLASSIFICATION	 for left outer joins left alias sorted but right alias might not 
WITHOUT_CLASSIFICATION	 errors 
WITHOUT_CLASSIFICATION	 grow 
WITHOUT_CLASSIFICATION	 try roll the key none found 
WITHOUT_CLASSIFICATION	 dont construct illegal cache key 
WITHOUT_CLASSIFICATION	 try fold key and key not null key 
WITHOUT_CLASSIFICATION	 initializes them 
WITHOUT_CLASSIFICATION	 finishes the vectorization context after all the initial 
WITHOUT_CLASSIFICATION	 all the locks are created under this parent 
WITHOUT_CLASSIFICATION	 such abc 
WITHOUT_CLASSIFICATION	 preserve precision 
WITHOUT_CLASSIFICATION	 create single child representing the scratch column where will 
WITHOUT_CLASSIFICATION	 test strict locking mode backward compatible locking mode for nonacid resources with nonstrict mode insert got sharedread lock instead exclusive with acid semantics 
WITHOUT_CLASSIFICATION	 since are running the mapred task the same jvm should update the job conf 
WITHOUT_CLASSIFICATION	 the big table can divided buckets small tables 
WITHOUT_CLASSIFICATION	 update max max greater than the largest value seen far 
WITHOUT_CLASSIFICATION	 disable ansi sql arithmetic changes 
WITHOUT_CLASSIFICATION	 setting these parameters here just case that the code got changed future these are not missing 
WITHOUT_CLASSIFICATION	 boolean purposely excluded 
WITHOUT_CLASSIFICATION	 now run another compaction make sure empty dirs dont cause issues 
WITHOUT_CLASSIFICATION	 check potential trigger nullscan 
WITHOUT_CLASSIFICATION	 compress key and write key out 
WITHOUT_CLASSIFICATION	 gathering stats 
WITHOUT_CLASSIFICATION	 set readallcolumns false 
WITHOUT_CLASSIFICATION	 update file sink descriptor 
WITHOUT_CLASSIFICATION	 usually called after close commit rollback query and end the driver life cycle 
WITHOUT_CLASSIFICATION	 display error message for tasks with the highest failure count 
WITHOUT_CLASSIFICATION	 not set bit the null byte when are writing null 
WITHOUT_CLASSIFICATION	 reasons roots are data sources leaves are data sinks know 
WITHOUT_CLASSIFICATION	 for each path getsplits 
WITHOUT_CLASSIFICATION	 fop exists this not the top level filter and fop not 
WITHOUT_CLASSIFICATION	 create the required temporary file the hdfs location the destination 
WITHOUT_CLASSIFICATION	 set stats config for filesinkoperators which are cloned from the filesink 
WITHOUT_CLASSIFICATION	 all its parents operators are state close and called close children note close being called and its state being close difference since close could called but state not close one its parent not state close 
WITHOUT_CLASSIFICATION	 start delete from tab txn 
WITHOUT_CLASSIFICATION	 this invalid decimal value getting hivedecimal from will return null 
WITHOUT_CLASSIFICATION	 rows looked one repeated key are match but filtered out rows need generated nonmatches too 
WITHOUT_CLASSIFICATION	 connect via kerberos and get delegation token 
WITHOUT_CLASSIFICATION	 lest for now load data woverwrite not allowed txn hive 
WITHOUT_CLASSIFICATION	 adding postgres jdbc driver exists 
WITHOUT_CLASSIFICATION	 note that partitioning fields dont need change since either partitioned randomly all grouping keys distinct keys 
WITHOUT_CLASSIFICATION	 remote metastore situation 
WITHOUT_CLASSIFICATION	 authorization error not really expected filter call the impl should have just filtered out everything checkprivileges call would have already been made authorize this action 
WITHOUT_CLASSIFICATION	 the split doesnt exclusively serve one alias 
WITHOUT_CLASSIFICATION	 outer join involved 
WITHOUT_CLASSIFICATION	 only left input repeating and has nulls 
WITHOUT_CLASSIFICATION	 search for match the rhs table 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 this should block behind the lock 
WITHOUT_CLASSIFICATION	 sum lengths all values seen far 
WITHOUT_CLASSIFICATION	 the join operation the child not the same keys 
WITHOUT_CLASSIFICATION	 per length means null list 
WITHOUT_CLASSIFICATION	 add allnull record 
WITHOUT_CLASSIFICATION	 handle leadingtrailing whitespace 
WITHOUT_CLASSIFICATION	 format the stored statement 
WITHOUT_CLASSIFICATION	 todo more writers are added separate out 
WITHOUT_CLASSIFICATION	 condition for merging not met see genmrfilesink 
WITHOUT_CLASSIFICATION	 validate response 
WITHOUT_CLASSIFICATION	 sessionstate null this unlikely happen just case 
WITHOUT_CLASSIFICATION	 keys from fetchsampler are collected here 
WITHOUT_CLASSIFICATION	 note cache slices one one since need lock them before sending consumer could lock here then cache them together then unlock here and return 
WITHOUT_CLASSIFICATION	 the state only changes from truefalse once set false may not change back true 
WITHOUT_CLASSIFICATION	 uncaught exception handler that will set for all threads before execution 
WITHOUT_CLASSIFICATION	 expect base delta dir this list 
WITHOUT_CLASSIFICATION	 default children inputs 
WITHOUT_CLASSIFICATION	 exchange partition not allowed with transactional tables only source transactional table then target will see deleted rows too snapshot isolation applicable for nonacid tables only target transactional table then data would visible all ongoing transactions affecting the snapshot isolation both source and targets are transactional tables then target partition may have deltabase 
WITHOUT_CLASSIFICATION	 the dispatcher fires the processor corresponding the closest matching rule and passes the context along 
WITHOUT_CLASSIFICATION	 optional group containing multiple elements 
WITHOUT_CLASSIFICATION	 are done with the buffers unlike data blocks are also the consumer release 
WITHOUT_CLASSIFICATION	 small table 
WITHOUT_CLASSIFICATION	 two parts kerberos principal 
WITHOUT_CLASSIFICATION	 cte actually subquery 
WITHOUT_CLASSIFICATION	 execute select and verify that aborted operation not counted for table 
WITHOUT_CLASSIFICATION	 this should eventually hang the delay code from the background thread 
WITHOUT_CLASSIFICATION	 reuse the same hashmap reduce new object allocation this means counts can empty when there input data 
WITHOUT_CLASSIFICATION	 while 
WITHOUT_CLASSIFICATION	 quite reliable 
WITHOUT_CLASSIFICATION	 metaexception here really means classnotfound see the utility method any these happen that means can never succeed 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 initialize fetch work such that operator tree will constructed 
WITHOUT_CLASSIFICATION	 get the values repetition and definitionlevel 
WITHOUT_CLASSIFICATION	 instead maintaining complex state for the fetch the next group know for sure that the end all the values for given key will definitely reach the next key group 
WITHOUT_CLASSIFICATION	 write base 
WITHOUT_CLASSIFICATION	 spread bits adjacent longs default spreading hash bits within blocksize longs will make bloom filter cache friendly 
WITHOUT_CLASSIFICATION	 set bootstrap dump location used but for tablepartition missing 
WITHOUT_CLASSIFICATION	 collect column stats which need rewritten and remove old stats 
WITHOUT_CLASSIFICATION	 setup local dirs 
WITHOUT_CLASSIFICATION	 end entry reached 
WITHOUT_CLASSIFICATION	 make sure know saw error that dont recognize 
WITHOUT_CLASSIFICATION	 multikey specific save key and lookup 
WITHOUT_CLASSIFICATION	 for webui kept alive after queryplan freed 
WITHOUT_CLASSIFICATION	 note that enablebitvector does not apply here because columnstatisticsobj itself will tell whether bitvector null not and aggr logic can automatically apply 
WITHOUT_CLASSIFICATION	 pig script was successful 
WITHOUT_CLASSIFICATION	 repeat the procedure for the new select 
WITHOUT_CLASSIFICATION	 writeid 
WITHOUT_CLASSIFICATION	 reducer 
WITHOUT_CLASSIFICATION	 try some time zone boundaries 
WITHOUT_CLASSIFICATION	 removes any union operator and clones the plan 
WITHOUT_CLASSIFICATION	 theres key return 
WITHOUT_CLASSIFICATION	 init file contains incorrect row 
WITHOUT_CLASSIFICATION	 reset and add counters this can happen during start query session being moved another pool with its own set triggers 
WITHOUT_CLASSIFICATION	 folded the regex usually into 
WITHOUT_CLASSIFICATION	 build new table 
WITHOUT_CLASSIFICATION	 make sure that the user doesnt happen the super group 
WITHOUT_CLASSIFICATION	 the cache buffer comprises the tail the requested range and possibly overshoots the same above applies may throw cache buffer larger than the requested range and theres another range after this that starts the middle this cache buffer currently cache exact offsets the latter should never happen 
WITHOUT_CLASSIFICATION	 set data empty explicitly 
WITHOUT_CLASSIFICATION	 are trying check acls the workers directory which noone except should able write higherlevel directories shouldnt matter dont read them 
WITHOUT_CLASSIFICATION	 show create table more sensitive information includes table properties etc 
WITHOUT_CLASSIFICATION	 nontransient field used runtime kill task exceeded memory limits when running llap 
WITHOUT_CLASSIFICATION	 patterns isrepeating columns for boolean tristate null for others null somevalue nonulls sometimes false and there are nulls random selectedinuse too 
WITHOUT_CLASSIFICATION	 use the table default storage specification 
WITHOUT_CLASSIFICATION	 bgenjjtree typei 
WITHOUT_CLASSIFICATION	 make sure the arguments make sense 
WITHOUT_CLASSIFICATION	 valid schemes 
WITHOUT_CLASSIFICATION	 this backward compatible for nonacid resources acid semantics 
WITHOUT_CLASSIFICATION	 have ensured that the keys are columns 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 delete delta file with delete events 
WITHOUT_CLASSIFICATION	 this one the columns were setting record its position can come back later and patch add one the index because the select has the rowid the first column 
WITHOUT_CLASSIFICATION	 add stuff here implemented 
WITHOUT_CLASSIFICATION	 special case for root parent 
WITHOUT_CLASSIFICATION	 make sure that each session has its own udfclassloader for details see link udfclassloader 
WITHOUT_CLASSIFICATION	 currently this method only sets database functionname ownername ownertype classname 
WITHOUT_CLASSIFICATION	 first try selecting methods based the type affinity the arguments passed the candidate method arguments 
WITHOUT_CLASSIFICATION	 validate that join condition legal function refering both sides join only equi join todo join filter handling only supported for runtime supported for well 
WITHOUT_CLASSIFICATION	 now save stats for partition wont modify 
WITHOUT_CLASSIFICATION	 check that the tables used not resolve temp tables 
WITHOUT_CLASSIFICATION	 the collect method override for 
WITHOUT_CLASSIFICATION	 create the merge file work 
WITHOUT_CLASSIFICATION	 put all virtual columns rowresolver 
WITHOUT_CLASSIFICATION	 pkcolumnname 
WITHOUT_CLASSIFICATION	 fix needed due dependency for hbasemapreduce module 
WITHOUT_CLASSIFICATION	 return the mapping for table descriptor the expected table 
WITHOUT_CLASSIFICATION	 this mapping which keys will copied from the big table input and key expressions 
WITHOUT_CLASSIFICATION	 obtain filter for shared operator 
WITHOUT_CLASSIFICATION	 verify that driver works fine with latest schema 
WITHOUT_CLASSIFICATION	 shouldnt happen 
WITHOUT_CLASSIFICATION	 exception thrown looks good 
WITHOUT_CLASSIFICATION	 expected error should throw 
WITHOUT_CLASSIFICATION	 code borrowed from 
WITHOUT_CLASSIFICATION	 loginfopartition specgetkey 
WITHOUT_CLASSIFICATION	 write directly into our bytescolumnvector value buffer 
WITHOUT_CLASSIFICATION	 case find rows which have been deleted 
WITHOUT_CLASSIFICATION	 add sign byte since high bit 
WITHOUT_CLASSIFICATION	 when using only hbase then could change this 
WITHOUT_CLASSIFICATION	 this safe because positive 
WITHOUT_CLASSIFICATION	 key stored text format get bytes representation constant also text format 
WITHOUT_CLASSIFICATION	 avoid 
WITHOUT_CLASSIFICATION	 once are done processing the line restore the old handler 
WITHOUT_CLASSIFICATION	 acquired all the locks commit and return acquired 
WITHOUT_CLASSIFICATION	 make sure getting table the wrong catalog does not work 
WITHOUT_CLASSIFICATION	 dont expect conflicts from bad estimates 
WITHOUT_CLASSIFICATION	 need acquire lock twice the same object ensured that exclusive locks occur before shared locks the same object 
WITHOUT_CLASSIFICATION	 try with row file 
WITHOUT_CLASSIFICATION	 batch size and decaying factor 
WITHOUT_CLASSIFICATION	 generate the dummy driver using txt file 
WITHOUT_CLASSIFICATION	 there should still one request the locks still held 
WITHOUT_CLASSIFICATION	 for all the existing partitions check the value can type casted nonnull object 
WITHOUT_CLASSIFICATION	 before notify though lock the list lock cannot remove from the list 
WITHOUT_CLASSIFICATION	 for type casts 
WITHOUT_CLASSIFICATION	 need not traversed again 
WITHOUT_CLASSIFICATION	 this will insert and between the and its parent 
WITHOUT_CLASSIFICATION	 were faking out hive work through type system impedence mismatch pull out the backing array and convert list 
WITHOUT_CLASSIFICATION	 get the reflection methods from 
WITHOUT_CLASSIFICATION	 tablepropkey that was passed lead valid uri resolution update parts match the oldnnloc else add badrecords 
WITHOUT_CLASSIFICATION	 one session will running the other will queued 
WITHOUT_CLASSIFICATION	 check out the statistics 
WITHOUT_CLASSIFICATION	 get the big table row container 
WITHOUT_CLASSIFICATION	 dont acquire locks for any these have already asked for them ddlsemanticanalyzer 
WITHOUT_CLASSIFICATION	 verify that attempt was made schedule the task but the decision was skip scheduling 
WITHOUT_CLASSIFICATION	 mapreduce case need always clear mapreduce doesnt have object registry 
WITHOUT_CLASSIFICATION	 vertexs children vertex 
WITHOUT_CLASSIFICATION	 set the buffer that will receive the serialized data the output buffer will reset 
WITHOUT_CLASSIFICATION	 retry any other exception 
WITHOUT_CLASSIFICATION	 catalogs cannot parsed part the query seems bug 
WITHOUT_CLASSIFICATION	 dagclient such should have bearing jobclose 
WITHOUT_CLASSIFICATION	 get colstats for the original table column for selcol possible this would have 
WITHOUT_CLASSIFICATION	 clear all threadlocal cached mapworkreducework after plan generation this may executed pool thread 
WITHOUT_CLASSIFICATION	 want signal error the function doesnt exist and were configured not ignore this 
WITHOUT_CLASSIFICATION	 add the columns join filters 
WITHOUT_CLASSIFICATION	 used for create 
WITHOUT_CLASSIFICATION	 analyze table partition compute statistics the plan consists simple sparktask followed statstask the spark task just simple tablescanoperator 
WITHOUT_CLASSIFICATION	 compute the pseudorandom position from the above then derive the actual header 
WITHOUT_CLASSIFICATION	 partitions locations which might need deleted 
WITHOUT_CLASSIFICATION	 the stack contains either filter filter filter with the head the stack being the rightmost symbol just pop out the two elements from the top and the second one them not table scan then the operator the top 
WITHOUT_CLASSIFICATION	 the default such that there throttling 
WITHOUT_CLASSIFICATION	 report suspicious gaps writebuffers 
WITHOUT_CLASSIFICATION	 otherwise replace parent sibling 
WITHOUT_CLASSIFICATION	 the objectinspector for the current column 
WITHOUT_CLASSIFICATION	 check other parts 
WITHOUT_CLASSIFICATION	 get total size and individual aliass size 
WITHOUT_CLASSIFICATION	 run partition pruner get partitions 
WITHOUT_CLASSIFICATION	 get all cols 
WITHOUT_CLASSIFICATION	 fstrashintervalkey hadoop 
WITHOUT_CLASSIFICATION	 cant multiply null 
WITHOUT_CLASSIFICATION	 make the list transactional tables list which are getting written current txn 
WITHOUT_CLASSIFICATION	 check the lastrecordoutput 
WITHOUT_CLASSIFICATION	 open default connections which will used throughout the tests 
WITHOUT_CLASSIFICATION	 tblgetpath null for views 
WITHOUT_CLASSIFICATION	 table specified check all tables and all partitions 
WITHOUT_CLASSIFICATION	 match not supposed there 
WITHOUT_CLASSIFICATION	 this constant null 
WITHOUT_CLASSIFICATION	 the old instance has not been unregistered and the new instances has not registered yet 
WITHOUT_CLASSIFICATION	 the table the pending prewarm list move the top 
WITHOUT_CLASSIFICATION	 crossproduct keys really 
WITHOUT_CLASSIFICATION	 update the filesinkoperator include partition columns 
WITHOUT_CLASSIFICATION	 values will override any values set the underlying hadoop configuration 
WITHOUT_CLASSIFICATION	 possible for some request queued after main thread has decided kill this session the next iteration wed processing that request with irrelevant session 
WITHOUT_CLASSIFICATION	 there point trying validate further have type info about target field 
WITHOUT_CLASSIFICATION	 setup for input resultexpr select list 
WITHOUT_CLASSIFICATION	 not external table 
WITHOUT_CLASSIFICATION	 how run this test you can run this test via the command line mvn clean install java jar targetbenchmarksjar prof perf linux java jar targetbenchmarksjar prof perfnorm linux java jar targetbenchmarksjar prof perfasm linux java jar targetbenchmarksjar prof allocation counting via 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 infomessages 
WITHOUT_CLASSIFICATION	 with repeating value can finish all remaining rows 
WITHOUT_CLASSIFICATION	 views derive the column type from the base table definition the view definition can altered change the column types the column type compatibility checks should 
WITHOUT_CLASSIFICATION	 when people forget quote string opop null for example select from sometable where not 
WITHOUT_CLASSIFICATION	 close should just nothing 
WITHOUT_CLASSIFICATION	 get the available privileges from file system 
WITHOUT_CLASSIFICATION	 executor single thread can guarantee domain created before any ats entries 
WITHOUT_CLASSIFICATION	 called functions that transform the raw input this method invoked during translation and also when the operator initialized during runtime subclass must use this call setup the shape the raw input that fed the partitioning mechanics subsequent this call call getrawinputoi call the link must return the the output this function 
WITHOUT_CLASSIFICATION	 see this node toktableorcol find the value and put the list not recurse any children 
WITHOUT_CLASSIFICATION	 down the semaphore block until available 
WITHOUT_CLASSIFICATION	 update the count the number values seen far 
WITHOUT_CLASSIFICATION	 read type encoding 
WITHOUT_CLASSIFICATION	 these two structures track the list known nodes and the list nodes which are sending keepalive heartbeats 
WITHOUT_CLASSIFICATION	 cred provider doesnt have entry fall back conf 
WITHOUT_CLASSIFICATION	 add type params 
WITHOUT_CLASSIFICATION	 overwrite value 
WITHOUT_CLASSIFICATION	 aggregatedata already has the ndv the max all 
WITHOUT_CLASSIFICATION	 threadsafe 
WITHOUT_CLASSIFICATION	 seed with the buddy this block the first iteration would target this block 
WITHOUT_CLASSIFICATION	 null check because some test cases get null from msgetcatalog 
WITHOUT_CLASSIFICATION	 matching rule and passes the context along 
WITHOUT_CLASSIFICATION	 fraction digits continue into middle longword 
WITHOUT_CLASSIFICATION	 slow down the reducer that shufflebytes publishing and validation can happen adding sleep between multiple reduce stages 
WITHOUT_CLASSIFICATION	 thomas wangs integer hash function 
WITHOUT_CLASSIFICATION	 create resolver 
WITHOUT_CLASSIFICATION	 build aggregations 
WITHOUT_CLASSIFICATION	 its possible that parition column may have null value which case the row belongs the special partition 
WITHOUT_CLASSIFICATION	 order sourcecolumn 
WITHOUT_CLASSIFICATION	 the threshold size convert the join into mapjoin and dont create conditional task 
WITHOUT_CLASSIFICATION	 add views planner 
WITHOUT_CLASSIFICATION	 the the actual spark job 
WITHOUT_CLASSIFICATION	 class could not inited use our local copy 
WITHOUT_CLASSIFICATION	 test string literal string column comparison 
WITHOUT_CLASSIFICATION	 query info created sqloperation which will have start time the operation when jdbc statement not used queryinfo will null which case take creation driver instance query start time which also the time when query display object created 
WITHOUT_CLASSIFICATION	 reset the interrupt status 
WITHOUT_CLASSIFICATION	 inputsplitnum that contains the first row this block 
WITHOUT_CLASSIFICATION	 run with cascade 
WITHOUT_CLASSIFICATION	 restricted text for now this new feature only text files can sliced 
WITHOUT_CLASSIFICATION	 provide faster way write date without date object 
WITHOUT_CLASSIFICATION	 whether any error occurred during query compilation used for query lifetime hook 
WITHOUT_CLASSIFICATION	 preemption with ducks reversed 
WITHOUT_CLASSIFICATION	 this would attempt directory add watch and track 
WITHOUT_CLASSIFICATION	 add our conf file 
WITHOUT_CLASSIFICATION	 optimize for common case just one row for key container acts row 
WITHOUT_CLASSIFICATION	 when minor compacting write delete events separate file when splitupdate turned 
WITHOUT_CLASSIFICATION	 convert rexnode 
WITHOUT_CLASSIFICATION	 these arent real column refs instead they are special internal expressions used the representation aggregation 
WITHOUT_CLASSIFICATION	 should not happen 
WITHOUT_CLASSIFICATION	 help 
WITHOUT_CLASSIFICATION	 this has already been inspected and rejected 
WITHOUT_CLASSIFICATION	 binary type should not seen 
WITHOUT_CLASSIFICATION	 not created cannot remove 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 cant overwrite existing keys 
WITHOUT_CLASSIFICATION	 deleted the user will need call unarchive again clear those 
WITHOUT_CLASSIFICATION	 this expected these mock files are not valid orc file 
WITHOUT_CLASSIFICATION	 restore interrupt wont handle here 
WITHOUT_CLASSIFICATION	 mapside join exactly one table not present memory the client provides the list tables which can cached memory 
WITHOUT_CLASSIFICATION	 collect all branching operators 
WITHOUT_CLASSIFICATION	 create fake directory throw exception 
WITHOUT_CLASSIFICATION	 known 
WITHOUT_CLASSIFICATION	 map col keycol etc 
WITHOUT_CLASSIFICATION	 whether the cycle running 
WITHOUT_CLASSIFICATION	 event operators point table scan operators when cloning these need restore the original scan 
WITHOUT_CLASSIFICATION	 found least one children with mismatch 
WITHOUT_CLASSIFICATION	 unable find stats for column return null can build stats 
WITHOUT_CLASSIFICATION	 decimalstats 
WITHOUT_CLASSIFICATION	 data structures coming originally from qbjointree 
WITHOUT_CLASSIFICATION	 cancel the heartbeat 
WITHOUT_CLASSIFICATION	 append 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the state has changed during the update lets undo what just did 
WITHOUT_CLASSIFICATION	 this should never happen only schedule one attempt once 
WITHOUT_CLASSIFICATION	 need get state transition updates for the vertices that will send events once have received all events and vertex has succeeded can move the pruning 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 allow implicit numeric string conversion 
WITHOUT_CLASSIFICATION	 the registrator jar should already when not test mode 
WITHOUT_CLASSIFICATION	 writeid 
WITHOUT_CLASSIFICATION	 create one input split for each segment 
WITHOUT_CLASSIFICATION	 special char 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 second data dir contains files 
WITHOUT_CLASSIFICATION	 weve killed something and may want wait for die 
WITHOUT_CLASSIFICATION	 size surpasses limit cannot convert 
WITHOUT_CLASSIFICATION	 default 
WITHOUT_CLASSIFICATION	 copy the first entries 
WITHOUT_CLASSIFICATION	 test month diff with fraction considering time components 
WITHOUT_CLASSIFICATION	 replace existing view 
WITHOUT_CLASSIFICATION	 tez only the hash map might already cached the container run 
WITHOUT_CLASSIFICATION	 undone presumption append 
WITHOUT_CLASSIFICATION	 element for key long hash table hashmultiset 
WITHOUT_CLASSIFICATION	 data 
WITHOUT_CLASSIFICATION	 the destination table 
WITHOUT_CLASSIFICATION	 get close enough 
WITHOUT_CLASSIFICATION	 look all methods that generate values for explain 
WITHOUT_CLASSIFICATION	 check for fatal error again case occurred after the last check before the job completed 
WITHOUT_CLASSIFICATION	 set correct scheme and authority 
WITHOUT_CLASSIFICATION	 reverse place 
WITHOUT_CLASSIFICATION	 required required required required required required required optional optional 
WITHOUT_CLASSIFICATION	 generate groupby operator 
WITHOUT_CLASSIFICATION	 abstract function add httpauth header 
WITHOUT_CLASSIFICATION	 tokenowner 
WITHOUT_CLASSIFICATION	 mybool 
WITHOUT_CLASSIFICATION	 should set false when using 
WITHOUT_CLASSIFICATION	 build the path from bottom 
WITHOUT_CLASSIFICATION	 start explicit txn that txnmgr knows 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject 
WITHOUT_CLASSIFICATION	 create jointree structures fill them later 
WITHOUT_CLASSIFICATION	 called once the client 
WITHOUT_CLASSIFICATION	 decompress the data 
WITHOUT_CLASSIFICATION	 see expr already present reducekeys get index expr reducekeys 
WITHOUT_CLASSIFICATION	 are here then have established that firstrecordinbatch deleterecord now continue marking records which have been deleted until reach the end the batch exhaust all the delete records 
WITHOUT_CLASSIFICATION	 more queries can added here the future test acid joins 
WITHOUT_CLASSIFICATION	 table names with schema name necessary 
WITHOUT_CLASSIFICATION	 when dynamic partitioning used the recordwriter instance initialized here isnt used can use null thats because records cant written until the values the dynamic partitions are deduced that time new local instance recordwriter with the correct outputpath will constructed 
WITHOUT_CLASSIFICATION	 generate dummy preupgrade script with errors 
WITHOUT_CLASSIFICATION	 user sets default queue now 
WITHOUT_CLASSIFICATION	 same sign just add the absolute values 
WITHOUT_CLASSIFICATION	 recreate refresh jobconf currtask context 
WITHOUT_CLASSIFICATION	 class 
WITHOUT_CLASSIFICATION	 which the correlator 
WITHOUT_CLASSIFICATION	 materialization allowed not view definition 
WITHOUT_CLASSIFICATION	 statuscode 
WITHOUT_CLASSIFICATION	 find the first nonzero digit the last digits all are zero 
WITHOUT_CLASSIFICATION	 get text input format here can not determine file text according its content can test other file format can accept one other file format can accept this file treat this file text file although maybe not 
WITHOUT_CLASSIFICATION	 invalid table not partitioned but endpoints partitionvals not empty 
WITHOUT_CLASSIFICATION	 bloom filter rest 
WITHOUT_CLASSIFICATION	 need separate table for acid testing since has bucketed and has acid 
WITHOUT_CLASSIFICATION	 move next valid index 
WITHOUT_CLASSIFICATION	 export table tablename partition partcolumnvalue exporttargetpath 
WITHOUT_CLASSIFICATION	 first look the column from the source against which resolved wed later translated this into the column from proper input its valid todo excludecols may possible remove using the same 
WITHOUT_CLASSIFICATION	 for druid storage handler 
WITHOUT_CLASSIFICATION	 count characters forward and watch for final run pads 
WITHOUT_CLASSIFICATION	 call open mockmocktbl 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 original method used deepcopy the same here 
WITHOUT_CLASSIFICATION	 constant null just return 
WITHOUT_CLASSIFICATION	 buildvdirectly use druid default need configured user 
WITHOUT_CLASSIFICATION	 ptf handling ptfinvocationspec ptfdesc 
WITHOUT_CLASSIFICATION	 test for string type 
WITHOUT_CLASSIFICATION	 hive conf changed need get the hive again with 
WITHOUT_CLASSIFICATION	 hive has been filed for this 
WITHOUT_CLASSIFICATION	 default partition key 
WITHOUT_CLASSIFICATION	 required required required required required required required required 
WITHOUT_CLASSIFICATION	 all input columns are repeating just evaluate function for row the batch and set output repeating 
WITHOUT_CLASSIFICATION	 unsupported aggregation 
WITHOUT_CLASSIFICATION	 udf 
WITHOUT_CLASSIFICATION	 each side better have more either side unbalanced cannot convert this workaround for now right fix would refactor code the maprecordprocessor and with respect the sources 
WITHOUT_CLASSIFICATION	 make sure matching name but wrong type doesnt return 
WITHOUT_CLASSIFICATION	 the deletedelta should not read because greater than the high watermark 
WITHOUT_CLASSIFICATION	 task not allocated 
WITHOUT_CLASSIFICATION	 add hiveexec jar 
WITHOUT_CLASSIFICATION	 smallbuffer might still out space 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 show locks 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 deserialize and check 
WITHOUT_CLASSIFICATION	 track the dependencies for the view consider query like select from where view the form select from 
WITHOUT_CLASSIFICATION	 hostnameport 
WITHOUT_CLASSIFICATION	 this class used read one field time simple fields like long double int are read into primitive current members the nonsimple field types like date timestamp etc are read into current object that this method will allocate this method handles complex type fields recursively calling this method 
WITHOUT_CLASSIFICATION	 generate the map the inputoutput column name for the keys are about 
WITHOUT_CLASSIFICATION	 how many records already buffered 
WITHOUT_CLASSIFICATION	 output privileges and asks for selectnogrant input 
WITHOUT_CLASSIFICATION	 what were reading from disk originally 
WITHOUT_CLASSIFICATION	 specify the columns deserialize into array 
WITHOUT_CLASSIFICATION	 try allocate using basebuffer approach from each arena 
WITHOUT_CLASSIFICATION	 when file system cache disabled you get different filesystem objects for same file system cant used such cases filesystem api doesnt have equals function implemented using the uri for comparison filesystem already uses uriconfiguration for equality its cache once equality has been added hdfs should make use 
WITHOUT_CLASSIFICATION	 verify that can drain the pool then cycle the state not corrupted 
WITHOUT_CLASSIFICATION	 bring the server only after all other components have started 
WITHOUT_CLASSIFICATION	 check this mapjoin not split 
WITHOUT_CLASSIFICATION	 add the new operator child each the passed operators 
WITHOUT_CLASSIFICATION	 logical loop over the rows the batch since the batch may have selected use 
WITHOUT_CLASSIFICATION	 for tez route data from upstream vertex correctly the following vertex the output name the reduce sink needs setup appropriately the case reduce side merge work need ensure that the parent work that provides data this merge work setup point the right vertex name the main work name this case the big table work has already been created can hook the merge work items for the small table correctly 
WITHOUT_CLASSIFICATION	 http transport mode set the thread local address thrifthttpservlet 
WITHOUT_CLASSIFICATION	 the sixth will not combined because delete delta files that desired hive 
WITHOUT_CLASSIFICATION	 potentially wait the cache entry entry pending status blocking here can potentially dangerous for example the global compile lock used this will block all subsequent queries that try acquire the compile lock should not done unless parallel compilation enabled might not want block for explain queries well 
WITHOUT_CLASSIFICATION	 theres fixed number partition cols that might have filters avoid joining multiple times for one column there are several filters will keep numcols elements the list one for each column will fill with nulls put each join corresponding index when necessary and remove nulls the end 
WITHOUT_CLASSIFICATION	 need handle tables unsupported path 
WITHOUT_CLASSIFICATION	 any the partition requests are null then need pull all partition locks for this table 
WITHOUT_CLASSIFICATION	 purge 
WITHOUT_CLASSIFICATION	 undone for now 
WITHOUT_CLASSIFICATION	 disable trash hadoop fstrashintervalkey hadoop 
WITHOUT_CLASSIFICATION	 table scan has the table object and pruned partitions that has information such bucketing sorting etc that used later for optimization 
WITHOUT_CLASSIFICATION	 dont overwrite user choice transactional attribute explicitly set 
WITHOUT_CLASSIFICATION	 output entry should not null for null input for this particular generic udf 
WITHOUT_CLASSIFICATION	 then determine the local offset that magical time 
WITHOUT_CLASSIFICATION	 write the results the file 
WITHOUT_CLASSIFICATION	 this api changed from this wont even compile with but doesnt need since only run this preupgrade 
WITHOUT_CLASSIFICATION	 one single call get all column stats 
WITHOUT_CLASSIFICATION	 project only the correlated fields out each inputrel and join the projectrel together make sure the plan does not change terms join order join these rels based their occurrence cor var list which 
WITHOUT_CLASSIFICATION	 null firstlast 
WITHOUT_CLASSIFICATION	 normally worry about the blanket false being passed here and that itd need integrated into abort call for outputcommitter but the underlying recordwriter ignores and throws away its irrelevant 
WITHOUT_CLASSIFICATION	 important restore the batchs selected array 
WITHOUT_CLASSIFICATION	 pigs schema contain type information about maps keys and values its new column assume stringstring its existing return whatever contained the existing column 
WITHOUT_CLASSIFICATION	 remove col stats 
WITHOUT_CLASSIFICATION	 logdebugclassname logical logical batchindex batchindex new key currentkey savejoinresultname 
WITHOUT_CLASSIFICATION	 send done event which llaprecordreader expecting upon end input 
WITHOUT_CLASSIFICATION	 check owner has write permission else will have copy 
WITHOUT_CLASSIFICATION	 create more staging data and test load data overwrite 
WITHOUT_CLASSIFICATION	 very expensive sometimes 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream 
WITHOUT_CLASSIFICATION	 check scan grammar prevents coexit noscancolumns 
WITHOUT_CLASSIFICATION	 shift the remaining bins left one position 
WITHOUT_CLASSIFICATION	 stop the cachedstore cache update service well start explicitly control the test 
WITHOUT_CLASSIFICATION	 see discussion yarn for the memory accounting discussion 
WITHOUT_CLASSIFICATION	 does any result need emitted 
WITHOUT_CLASSIFICATION	 enabling this will cause test failures mac 
WITHOUT_CLASSIFICATION	 file required parameter 
WITHOUT_CLASSIFICATION	 check for partition key change when have order keys 
WITHOUT_CLASSIFICATION	 write value object that can inspected 
WITHOUT_CLASSIFICATION	 need drop the table 
WITHOUT_CLASSIFICATION	 staging area for results avoid new calls 
WITHOUT_CLASSIFICATION	 should this close updaters 
WITHOUT_CLASSIFICATION	 test serialization and deserialization with different schemas 
WITHOUT_CLASSIFICATION	 candidate 
WITHOUT_CLASSIFICATION	 not rexcall continue 
WITHOUT_CLASSIFICATION	 size stdout buffer bytes 
WITHOUT_CLASSIFICATION	 todo parse make sure its the only compacts file and contains alter table defaulttacid compact majoralter table defaulttacidpart partitionpy compact major 
WITHOUT_CLASSIFICATION	 username must present 
WITHOUT_CLASSIFICATION	 should not happen have accounted for all types 
WITHOUT_CLASSIFICATION	 long 
WITHOUT_CLASSIFICATION	 given key find the corresponding column name 
WITHOUT_CLASSIFICATION	 are rounding may introduce one more integer digit 
WITHOUT_CLASSIFICATION	 static partitions specified and hence all are dynamic partition keys and need part temp table input data file 
WITHOUT_CLASSIFICATION	 the may not tablescan for mapjoins consider the query select mapjoina count from join akey bkey 
WITHOUT_CLASSIFICATION	 add viewbased rewriting rules planner 
WITHOUT_CLASSIFICATION	 inner big table only join hash multiset 
WITHOUT_CLASSIFICATION	 case not list 
WITHOUT_CLASSIFICATION	 earlier version hive 
WITHOUT_CLASSIFICATION	 remove incomplete outputs some incomplete outputs may added the beginning for for dynamic partitions 
WITHOUT_CLASSIFICATION	 get the registry 
WITHOUT_CLASSIFICATION	 write buffer full read buffer isnt used switch buffer 
WITHOUT_CLASSIFICATION	 since componentize windowing need translate the partition order specs individual wfns 
WITHOUT_CLASSIFICATION	 decimalab type 
WITHOUT_CLASSIFICATION	 getcolumnsstring catalog string schemapattern string 
WITHOUT_CLASSIFICATION	 evaluate filter expression and update statistics 
WITHOUT_CLASSIFICATION	 llap off dont output 
WITHOUT_CLASSIFICATION	 equalkey match bytes 
WITHOUT_CLASSIFICATION	 note for now llap only supported tez tasks will never come others may added here although this only necessary have extra debug information 
WITHOUT_CLASSIFICATION	 check hashmap disk memory 
WITHOUT_CLASSIFICATION	 default user hasnt provided any optional constraint properties 
WITHOUT_CLASSIFICATION	 unique constaint violation incl unique key 
WITHOUT_CLASSIFICATION	 and accepts the first one clazzgetmethods returns 
WITHOUT_CLASSIFICATION	 handle the case select from select from the currentinput null check above needed the alias list that case would avt lookup would return null need further find the view inside 
WITHOUT_CLASSIFICATION	 need close the dummyops well the operator pipeline not considered closeddone unless all operators are 
WITHOUT_CLASSIFICATION	 open base txn which allocates write and then committed 
WITHOUT_CLASSIFICATION	 the data source will produce data ever ending want see that memory pressure kicks and some 
WITHOUT_CLASSIFICATION	 the queue should ignored 
WITHOUT_CLASSIFICATION	 child the optional comment the column 
WITHOUT_CLASSIFICATION	 this only there pre event listener registered avoid unnecessary metastore api call 
WITHOUT_CLASSIFICATION	 have reset the conf when change that the change takes affect 
WITHOUT_CLASSIFICATION	 hive compiler going remove inner order disable that optimization until then 
WITHOUT_CLASSIFICATION	 could also allow cutting off versions and other stuff provided that sha matches 
WITHOUT_CLASSIFICATION	 requested host still alive but cannot accept task pick the next available host consistent order 
WITHOUT_CLASSIFICATION	 the query materialization validation check only occurs cbo thus only cache results cbo was used 
WITHOUT_CLASSIFICATION	 verify handle the key column types for optimized table this the effectively the same check used hashtableloader 
WITHOUT_CLASSIFICATION	 remove the ddltime gets refreshed 
WITHOUT_CLASSIFICATION	 testing with repeating and nulls 
WITHOUT_CLASSIFICATION	 set the bit value not null 
WITHOUT_CLASSIFICATION	 read keys from token store 
WITHOUT_CLASSIFICATION	 now register permanent function 
WITHOUT_CLASSIFICATION	 make sure are checking the right latest compaction entry 
WITHOUT_CLASSIFICATION	 returns first one matches all the params 
WITHOUT_CLASSIFICATION	 select algorithm with min cost 
WITHOUT_CLASSIFICATION	 check possible drop default database 
WITHOUT_CLASSIFICATION	 write byte size the string which vint 
WITHOUT_CLASSIFICATION	 get the row structure 
WITHOUT_CLASSIFICATION	 for 
WITHOUT_CLASSIFICATION	 separate the base files into acid schema and nonacidoriginal schema files 
WITHOUT_CLASSIFICATION	 already added this column select list 
WITHOUT_CLASSIFICATION	 equals 
WITHOUT_CLASSIFICATION	 should not happen 
WITHOUT_CLASSIFICATION	 bags always contain tuples 
WITHOUT_CLASSIFICATION	 instruct exprnodedesc list for the current table alias 
WITHOUT_CLASSIFICATION	 look getting rid fractional digits that will now below hivedecimalmaxscale 
WITHOUT_CLASSIFICATION	 each evaluator has constant java object overhead 
WITHOUT_CLASSIFICATION	 test andor more 
WITHOUT_CLASSIFICATION	 masks for quicker extraction pprime qprime values 
WITHOUT_CLASSIFICATION	 dont need the buffer anymore 
WITHOUT_CLASSIFICATION	 looks like subq plan todo can collapse this part tree into single 
WITHOUT_CLASSIFICATION	 compactions are not happening 
WITHOUT_CLASSIFICATION	 jdo 
WITHOUT_CLASSIFICATION	 this method also initializes the consolereader which 
WITHOUT_CLASSIFICATION	 preserved initialization time have session use during resize 
WITHOUT_CLASSIFICATION	 more data 
WITHOUT_CLASSIFICATION	 append colnum make unique 
WITHOUT_CLASSIFICATION	 that can cancelled later from completedelegator 
WITHOUT_CLASSIFICATION	 not sequential with next 
WITHOUT_CLASSIFICATION	 the same thing that writerimpl does when writing the footer but the footer 
WITHOUT_CLASSIFICATION	 start the input and wait for ready event number mrinput expected 
WITHOUT_CLASSIFICATION	 create reader look footer need check side file since can only streaming ingest delta 
WITHOUT_CLASSIFICATION	 initialize table properties from the table parameters this required because the table may define certain table parameters that may required while writing the table parameter one such example 
WITHOUT_CLASSIFICATION	 empty maybe because cbo did not run fall back full select query 
WITHOUT_CLASSIFICATION	 bounds the column type are written and values outside throw exception 
WITHOUT_CLASSIFICATION	 constructing conditional task consisting move task and map reduce task 
WITHOUT_CLASSIFICATION	 has dynamic well static partitions 
WITHOUT_CLASSIFICATION	 check its 
WITHOUT_CLASSIFICATION	 initialize has not been called initialize has been called and close has not been called close has been called but one its parent not closed 
WITHOUT_CLASSIFICATION	 since noscan true table name command 
WITHOUT_CLASSIFICATION	 retry with different dump should fail 
WITHOUT_CLASSIFICATION	 get deterministic count number tasks for the vertex 
WITHOUT_CLASSIFICATION	 join cost 
WITHOUT_CLASSIFICATION	 test basic operation 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that outer join multikey using hash map 
WITHOUT_CLASSIFICATION	 fell through here this not valid type conversion 
WITHOUT_CLASSIFICATION	 assumptions precision scale scale 
WITHOUT_CLASSIFICATION	 statsobjold found can merge 
WITHOUT_CLASSIFICATION	 test addpartitions 
WITHOUT_CLASSIFICATION	 special case unsigned magnitude twos compliment adjustment 
WITHOUT_CLASSIFICATION	 remove all non alphanumeric letters replace whitespace spans with underscore 
WITHOUT_CLASSIFICATION	 preanalyze hook fired the middle these calls 
WITHOUT_CLASSIFICATION	 fail with good message 
WITHOUT_CLASSIFICATION	 check size data shuffle larger table less than given max size 
WITHOUT_CLASSIFICATION	 here need lock partition write and lock table read which should 
WITHOUT_CLASSIFICATION	 the prefix nonempty add the before set the mutation 
WITHOUT_CLASSIFICATION	 note israwformat invalid for nonorc tables will always return true were good 
WITHOUT_CLASSIFICATION	 singlecolumn long outer null detection 
WITHOUT_CLASSIFICATION	 set conf 
WITHOUT_CLASSIFICATION	 this test function that takes three different kinds arguments for use verify vectorized udf invocation 
WITHOUT_CLASSIFICATION	 default values 
WITHOUT_CLASSIFICATION	 exact type conversion get out 
WITHOUT_CLASSIFICATION	 create and load the input data into the hbase table 
WITHOUT_CLASSIFICATION	 output minus the distinct aggcalls input 
WITHOUT_CLASSIFICATION	 authorizer not set check for metastore authorizer 
WITHOUT_CLASSIFICATION	 keep the small table alias avoid concurrent modification exception 
WITHOUT_CLASSIFICATION	 tests with queries which can pushed down and executed with directsql but the number partitions which should fetched bigger than the maximum set the parameter 
WITHOUT_CLASSIFICATION	 need new run the constant folding because might have created lots and true and true conditions rather than run the full constant folding just need shortcut andor expressions 
WITHOUT_CLASSIFICATION	 fetchtask should not depend the plan 
WITHOUT_CLASSIFICATION	 sadly derby for update doesnt meant what should 
WITHOUT_CLASSIFICATION	 keeps track all events that need processed irrespective the source 
WITHOUT_CLASSIFICATION	 not zero 
WITHOUT_CLASSIFICATION	 should empty 
WITHOUT_CLASSIFICATION	 non default session nothing changes the user can continue use the existing session the sessionstate 
WITHOUT_CLASSIFICATION	 now the left join 
WITHOUT_CLASSIFICATION	 with feature multiple tasks may get into conflict creatingusing tmplocation and were generate the target dir the map task there easy way pass outputcommitter 
WITHOUT_CLASSIFICATION	 carrayarraystring 
WITHOUT_CLASSIFICATION	 create input stream given nameext and write sql statements 
WITHOUT_CLASSIFICATION	 hashcode 
WITHOUT_CLASSIFICATION	 ingest size bytes gets resetted flush whereas connection stats not 
WITHOUT_CLASSIFICATION	 account for maximum cache buffer size 
WITHOUT_CLASSIFICATION	 validate inputs and outputs have right protectmode execute the query 
WITHOUT_CLASSIFICATION	 have close the processors run method because tez closes inputs before calling close tez and might need read inputs when flush the pipeline 
WITHOUT_CLASSIFICATION	 only consider range operators havent already seen one 
WITHOUT_CLASSIFICATION	 all divides are the result column times col 
WITHOUT_CLASSIFICATION	 map type 
WITHOUT_CLASSIFICATION	 merge the target works the second dpp sink into the first dpp sink 
WITHOUT_CLASSIFICATION	 table exists 
WITHOUT_CLASSIFICATION	 flag indicate its the first time read parquet data page with this instance 
WITHOUT_CLASSIFICATION	 the fractional digits are gone clear remaining round digits 
WITHOUT_CLASSIFICATION	 alter partitioned tables partition set partition property 
WITHOUT_CLASSIFICATION	 already file with same checksum exists cmpath just ignore the copymove also mark the operation unsuccessful notify that file with same name already exist which will ensure the timestamp cmpath updated avoid cleanup cleaner 
WITHOUT_CLASSIFICATION	 check mapreduce path 
WITHOUT_CLASSIFICATION	 not included the input collations but can propagated this aggregate will enforce 
WITHOUT_CLASSIFICATION	 task requested host got host since host dead and host full 
WITHOUT_CLASSIFICATION	 verify that the whitlelist params can set 
WITHOUT_CLASSIFICATION	 find the positions the bucketed columns the table corresponding the select list consider the following scenario tkey value value bucketedsorted key into buckets tdummy key value value bucketedsorted key into buckets query like insert overwrite table select key value value from should optimized 
WITHOUT_CLASSIFICATION	 database database 
WITHOUT_CLASSIFICATION	 todo duplicated code for init method since vectorization reader path doesnt support nested column pruning far see hive 
WITHOUT_CLASSIFICATION	 vectorptfoperator native vectorized 
WITHOUT_CLASSIFICATION	 get all files from the src directory 
WITHOUT_CLASSIFICATION	 newcat 
WITHOUT_CLASSIFICATION	 set the log stream 
WITHOUT_CLASSIFICATION	 this will happen for count such cases arbitarily pick first element from srcrel 
WITHOUT_CLASSIFICATION	 ekoifman tree ext hiveunionsubdir hiveunionsubdir hiveunionsubdir directories files 
WITHOUT_CLASSIFICATION	 case column stats hash aggregation grouping sets 
WITHOUT_CLASSIFICATION	 count characters 
WITHOUT_CLASSIFICATION	 for outer joins should not exceed aliases short type 
WITHOUT_CLASSIFICATION	 using volatile instead locking updates this variable 
WITHOUT_CLASSIFICATION	 queryfile 
WITHOUT_CLASSIFICATION	 remove the reduce sink operator 
WITHOUT_CLASSIFICATION	 valuetypeptr 
WITHOUT_CLASSIFICATION	 where product the carry from 
WITHOUT_CLASSIFICATION	 get partition 
WITHOUT_CLASSIFICATION	 test that exclusive blocks read and exclusive 
WITHOUT_CLASSIFICATION	 set vector null verify correct null handling 
WITHOUT_CLASSIFICATION	 delayed tasks will not kick right now that will happen the scheduling loop 
WITHOUT_CLASSIFICATION	 total size the composite object 
WITHOUT_CLASSIFICATION	 undone method still under development 
WITHOUT_CLASSIFICATION	 rewrite logic pass along any correlated variables coming from the input 
WITHOUT_CLASSIFICATION	 definition this session not use and can longer use only affects the session pool can handle this inline 
WITHOUT_CLASSIFICATION	 required for jackson 
WITHOUT_CLASSIFICATION	 create test dbs and tables 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 statsobjold not found just use statsobjnew accurate 
WITHOUT_CLASSIFICATION	 make sure location string proper format 
WITHOUT_CLASSIFICATION	 this should really close zero 
WITHOUT_CLASSIFICATION	 normal case 
WITHOUT_CLASSIFICATION	 not need the lock for partitions since they are covered the table lock 
WITHOUT_CLASSIFICATION	 some logging and force log rollover 
WITHOUT_CLASSIFICATION	 the logical indices for reading with readfield 
WITHOUT_CLASSIFICATION	 must have the duck still should just the other task 
WITHOUT_CLASSIFICATION	 note may add async option future for now let the task fail for the user 
WITHOUT_CLASSIFICATION	 abortedbits 
WITHOUT_CLASSIFICATION	 all children are done need walk the children 
WITHOUT_CLASSIFICATION	 the row consists string columns double columns some unionint double columns only 
WITHOUT_CLASSIFICATION	 repeating null 
WITHOUT_CLASSIFICATION	 the event else noop 
WITHOUT_CLASSIFICATION	 give sequence number for all the partitions 
WITHOUT_CLASSIFICATION	 arbitrary 
WITHOUT_CLASSIFICATION	 object overhead bytes for long fasttime bytes for cdate 
WITHOUT_CLASSIFICATION	 create the filter operator and update the parents and children appropriately 
WITHOUT_CLASSIFICATION	 test alter database set location 
WITHOUT_CLASSIFICATION	 because row was updated and thus has different recordidentifier now 
WITHOUT_CLASSIFICATION	 time 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring 
WITHOUT_CLASSIFICATION	 recreate 
WITHOUT_CLASSIFICATION	 show all privileges 
WITHOUT_CLASSIFICATION	 formatteroff 
WITHOUT_CLASSIFICATION	 caching disabled for mapinput due hive 
WITHOUT_CLASSIFICATION	 derivedschema arraylist 
WITHOUT_CLASSIFICATION	 password may longer the conf use getpassword 
WITHOUT_CLASSIFICATION	 use the serialization option switch write primitive values either variable 
WITHOUT_CLASSIFICATION	 make sure the node got deleted 
WITHOUT_CLASSIFICATION	 see javadoc hbasecompositekey 
WITHOUT_CLASSIFICATION	 remainder 
WITHOUT_CLASSIFICATION	 nonstatic methods wrap the static methods enable testing 
WITHOUT_CLASSIFICATION	 match was found create new entries 
WITHOUT_CLASSIFICATION	 insert globally unique byte value every few entries that one can seek into the middle file and then synchronize with record starts and ends scanning for this value 
WITHOUT_CLASSIFICATION	 history file name 
WITHOUT_CLASSIFICATION	 this shows the relevant bits the original hash value and how the conversion moving bits from the index value over the leading zero computation 
WITHOUT_CLASSIFICATION	 new connection should able call describeuse function without issue 
WITHOUT_CLASSIFICATION	 partition must have least sdid and serdeid set nothing set its view 
WITHOUT_CLASSIFICATION	 whether series key null 
WITHOUT_CLASSIFICATION	 update number columns from sel 
WITHOUT_CLASSIFICATION	 get the union value 
WITHOUT_CLASSIFICATION	 returns the location disc the jar this class 
WITHOUT_CLASSIFICATION	 more 
WITHOUT_CLASSIFICATION	 skip trailing blank characters 
WITHOUT_CLASSIFICATION	 get binary service port 
WITHOUT_CLASSIFICATION	 truncation needed 
WITHOUT_CLASSIFICATION	 since rowids are not needed didnt create the columnvectors hold them but still have check the data being read committed far current reader transactions concerned since here are reading original schema file all rows have been created the same txn namely 
WITHOUT_CLASSIFICATION	 not found the cache must parameterized types create 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 open matches metastore state 
WITHOUT_CLASSIFICATION	 the original record was lost the deserialization just the correct way through objectinspectors 
WITHOUT_CLASSIFICATION	 isemptypartition false 
WITHOUT_CLASSIFICATION	 store table descriptor mapwork 
WITHOUT_CLASSIFICATION	 prepare empty routing table 
WITHOUT_CLASSIFICATION	 add partitions for the partitioned table 
WITHOUT_CLASSIFICATION	 pbb 
WITHOUT_CLASSIFICATION	 check are already current schema level 
WITHOUT_CLASSIFICATION	 the database name not changed during alter 
WITHOUT_CLASSIFICATION	 expected number droppartitions call 
WITHOUT_CLASSIFICATION	 get the cost the operator 
WITHOUT_CLASSIFICATION	 also ensures that heartbeat noop since client likely doing async 
WITHOUT_CLASSIFICATION	 set upper bound how much were willing push before should flush weve set the memory treshold each key distinct should not beyond keydata 
WITHOUT_CLASSIFICATION	 check streaming side 
WITHOUT_CLASSIFICATION	 source filesystem 
WITHOUT_CLASSIFICATION	 note that this depends the fact that noone this class calls anything but getconnection you want use any the logger wrap calls youll have implement them 
WITHOUT_CLASSIFICATION	 remove pwd from conf file that job tracker doesnt show this logs 
WITHOUT_CLASSIFICATION	 random test string 
WITHOUT_CLASSIFICATION	 check this subquery lateral view 
WITHOUT_CLASSIFICATION	 this for special case ensure unit tests pass 
WITHOUT_CLASSIFICATION	 its filesink bucketed files also use mrstyle shuffle 
WITHOUT_CLASSIFICATION	 dont constant folding here wait until the optimizer changed family related jiras hive hive and hive 
WITHOUT_CLASSIFICATION	 create dumpfile prefix needed create descriptor 
WITHOUT_CLASSIFICATION	 already deleted 
WITHOUT_CLASSIFICATION	 myenumlist 
WITHOUT_CLASSIFICATION	 create temp dir 
WITHOUT_CLASSIFICATION	 get first level array 
WITHOUT_CLASSIFICATION	 and does not call the real method explicitly unset the queue name here 
WITHOUT_CLASSIFICATION	 remember the input file formats validated and why 
WITHOUT_CLASSIFICATION	 add scale 
WITHOUT_CLASSIFICATION	 the kill failed and the user also thinks the session invalid restart 
WITHOUT_CLASSIFICATION	 epoch days since epoch 
WITHOUT_CLASSIFICATION	 working directory 
WITHOUT_CLASSIFICATION	 build the filter and add parameters linearly are traversing leaf nodes ltr 
WITHOUT_CLASSIFICATION	 identifier 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 tez used the base for tez ats acls exists honor get the same acls for tez ats entries and hive entries 
WITHOUT_CLASSIFICATION	 for group type need build the projected group type with required leaves 
WITHOUT_CLASSIFICATION	 verify that the property has been properly set while creating the 
WITHOUT_CLASSIFICATION	 class private variables 
WITHOUT_CLASSIFICATION	 stream buffers are arranged enum order stream kind 
WITHOUT_CLASSIFICATION	 lastanalyzed stored per column but thrift has per several get the lowest for now nobody actually uses this field 
WITHOUT_CLASSIFICATION	 add what found our type and table tables 
WITHOUT_CLASSIFICATION	 column name can anything since will named udtf clause 
WITHOUT_CLASSIFICATION	 noop for now 
WITHOUT_CLASSIFICATION	 append the first group within pattern 
WITHOUT_CLASSIFICATION	 create two connections 
WITHOUT_CLASSIFICATION	 multiplication with overflow check overflow produces null output 
WITHOUT_CLASSIFICATION	 also include the stillinmemory sidefile before has been truely spilled 
WITHOUT_CLASSIFICATION	 meets all requirements 
WITHOUT_CLASSIFICATION	 test invalid case 
WITHOUT_CLASSIFICATION	 try the repeating null case 
WITHOUT_CLASSIFICATION	 operator list 
WITHOUT_CLASSIFICATION	 todo this assumes indexes getrowindexes would match column ids 
WITHOUT_CLASSIFICATION	 clean prep 
WITHOUT_CLASSIFICATION	 check there segments load 
WITHOUT_CLASSIFICATION	 already have struct node for the current index insert the constant value into the corresponding struct node 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl call check side file for mockmocktbl 
WITHOUT_CLASSIFICATION	 job request got timed out job kill should have started return client with queueexception 
WITHOUT_CLASSIFICATION	 estimated count 
WITHOUT_CLASSIFICATION	 adjust the data bytes according any possible offset that was provided 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 rules how recurse the objectinspector based its type 
WITHOUT_CLASSIFICATION	 currently none 
WITHOUT_CLASSIFICATION	 can null for void type 
WITHOUT_CLASSIFICATION	 poll the operation status till the query completed 
WITHOUT_CLASSIFICATION	 locks not associated with txn 
WITHOUT_CLASSIFICATION	 the highwatermark should assuming currenttxn otherwise currenttxn and commits before then will see result which doesnt make sense for snapshot isolation course for read committed the list should include the latest committed set 
WITHOUT_CLASSIFICATION	 uncompressed sizes file and rows 
WITHOUT_CLASSIFICATION	 have found merge work corresponding this closing operator hook this work 
WITHOUT_CLASSIFICATION	 addpartitiondesc already has the right partition location 
WITHOUT_CLASSIFICATION	 check parts the error not the whole string not tightly couple the error message with test 
WITHOUT_CLASSIFICATION	 task state unkown 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl call check side file for mockmocktbl 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that outer join singlecolumn string using hash map 
WITHOUT_CLASSIFICATION	 for final and complete 
WITHOUT_CLASSIFICATION	 alters and replacements are not undoable theyve taken effect already they are retriable though creates are undoable but cannot differentiate between creates alters and replacements from command level 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 mode 
WITHOUT_CLASSIFICATION	 align multiples from origin 
WITHOUT_CLASSIFICATION	 allocatemap one txn per aborted writeid and abort the txn mark writeid aborted 
WITHOUT_CLASSIFICATION	 init udf 
WITHOUT_CLASSIFICATION	 check forcing the location required 
WITHOUT_CLASSIFICATION	 pattern 
WITHOUT_CLASSIFICATION	 partially contained topoffset toplimit bottomlimit topoffset bottomlimit 
WITHOUT_CLASSIFICATION	 scope openedclosed times 
WITHOUT_CLASSIFICATION	 partitionname keyvaluekeyvalue 
WITHOUT_CLASSIFICATION	 create row file and empty 
WITHOUT_CLASSIFICATION	 three array two int array 
WITHOUT_CLASSIFICATION	 are variables from constant 
WITHOUT_CLASSIFICATION	 all protocols 
WITHOUT_CLASSIFICATION	 todo expose all wmcontexts via jmx use 
WITHOUT_CLASSIFICATION	 time doesnt matter 
WITHOUT_CLASSIFICATION	 column stats 
WITHOUT_CLASSIFICATION	 data for the split fits the middle one two slices 
WITHOUT_CLASSIFICATION	 try split bigger blocks 
WITHOUT_CLASSIFICATION	 todo can pass custom things thru the progress 
WITHOUT_CLASSIFICATION	 this impossible read from this chunk 
WITHOUT_CLASSIFICATION	 unpartitioned table 
WITHOUT_CLASSIFICATION	 used for legacy hivedecimalv setscale compatibility for binary display serialization 
WITHOUT_CLASSIFICATION	 read the template into string expand and write 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 job callable task for job status operation overrides behavior execute get status job need override behavior cleanup there nothing done job sttaus operation timed out interrupted 
WITHOUT_CLASSIFICATION	 firstrow invoke underlying evaluator initialize skipnulls flag 
WITHOUT_CLASSIFICATION	 enum class 
WITHOUT_CLASSIFICATION	 creates connection hms and thus must occur after kerberos login above 
WITHOUT_CLASSIFICATION	 start server again 
WITHOUT_CLASSIFICATION	 create map for tracking gauges 
WITHOUT_CLASSIFICATION	 the time seconds converts milliseconds first 
WITHOUT_CLASSIFICATION	 either schema literal serialization class must provided 
WITHOUT_CLASSIFICATION	 get the relevant information for this column 
WITHOUT_CLASSIFICATION	 pass the validtxnlist and validtxnwriteidlist snapshot configurations corresponding the input query 
WITHOUT_CLASSIFICATION	 didnt see this lock when running delete stmt above but now showed should should never happen happened 
WITHOUT_CLASSIFICATION	 should get back null 
WITHOUT_CLASSIFICATION	 operation log configuration 
WITHOUT_CLASSIFICATION	 reserved much needed 
WITHOUT_CLASSIFICATION	 read prompt configuration and substitute variables 
WITHOUT_CLASSIFICATION	 returns true the readfield method supported 
WITHOUT_CLASSIFICATION	 integer parsing move next lower longword 
WITHOUT_CLASSIFICATION	 private final helper helper 
WITHOUT_CLASSIFICATION	 grimacing face ufc bytes 
WITHOUT_CLASSIFICATION	 return our known table name 
WITHOUT_CLASSIFICATION	 remove this backup server 
WITHOUT_CLASSIFICATION	 the planner will not include unneeded columns for reading but other parts execution may ask for them 
WITHOUT_CLASSIFICATION	 foreigntblname 
WITHOUT_CLASSIFICATION	 after constant folding child expression the return type udfcase might have changed recreate the expression 
WITHOUT_CLASSIFICATION	 value can anything use the obj inspector and respect binary 
WITHOUT_CLASSIFICATION	 divided max split size 
WITHOUT_CLASSIFICATION	 drop table ignore error 
WITHOUT_CLASSIFICATION	 generate the data 
WITHOUT_CLASSIFICATION	 the actual check should the compare the connection string the external tables 
WITHOUT_CLASSIFICATION	 columns from sel branch only and append all columns from udtf branch 
WITHOUT_CLASSIFICATION	 inner classes 
WITHOUT_CLASSIFICATION	 trim additional bytes 
WITHOUT_CLASSIFICATION	 generate parse context for optimizer physical compiler 
WITHOUT_CLASSIFICATION	 batch size and decaying factor 
WITHOUT_CLASSIFICATION	 not update metrics wed immediately add the session back are able remove 
WITHOUT_CLASSIFICATION	 return all the dependency urls 
WITHOUT_CLASSIFICATION	 outer and inner joins 
WITHOUT_CLASSIFICATION	 noop orcdatareaderref owned the parent object 
WITHOUT_CLASSIFICATION	 setup for input resultexpr select list 
WITHOUT_CLASSIFICATION	 test routines exercise vectorizedrowbatch filling column vectors with data and null values 
WITHOUT_CLASSIFICATION	 check whether the mapjoin bucketed mapjoin the above can ascertained checking the big table bucket small table buckets mapping the mapjoin descriptor first check this mapjoin operator already bucketmapjoin not not give 
WITHOUT_CLASSIFICATION	 runasync 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 bit mask generate positive bit longs from random signed longs 
WITHOUT_CLASSIFICATION	 have cases here all the data the cache always single slice disk read cache puts some data the cache always single slice disk read and single cache put data the cache multiple slices disk read and multiple cache puts 
WITHOUT_CLASSIFICATION	 can this happen delta cannot exceed 
WITHOUT_CLASSIFICATION	 the partition did not change the new partition should similar the original partition 
WITHOUT_CLASSIFICATION	 streaming evaluators fill their results during the evaluate call 
WITHOUT_CLASSIFICATION	 todo track stats rejections etc per host 
WITHOUT_CLASSIFICATION	 right now only support one special character more special characters can added accordingly the future note the following array updated please also sure update the configuration parameter documentation 
WITHOUT_CLASSIFICATION	 build keys grouping set starting position first add original keys 
WITHOUT_CLASSIFICATION	 inspect the test data 
WITHOUT_CLASSIFICATION	 this ast has only one child then column name specified 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 end synchronized 
WITHOUT_CLASSIFICATION	 drop table event 
WITHOUT_CLASSIFICATION	 wrapper extends qlmetadatatable for easy construction syntax 
WITHOUT_CLASSIFICATION	 drop the table 
WITHOUT_CLASSIFICATION	 accumulo token already configuration but the token isnt the job credentials like the 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 specify the columns deserialize into list 
WITHOUT_CLASSIFICATION	 tag the original file name know where the file comes from note currently only track the last known trace xattr has limited capacity shall revisit and store all original 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create some data 
WITHOUT_CLASSIFICATION	 right repeats 
WITHOUT_CLASSIFICATION	 use current for now 
WITHOUT_CLASSIFICATION	 processing will decref once and the last one will unlock the buffers 
WITHOUT_CLASSIFICATION	 dont support changing name type 
WITHOUT_CLASSIFICATION	 when type config set classic 
WITHOUT_CLASSIFICATION	 does the conversion string itself 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 this one can used deny permission for performing the operation 
WITHOUT_CLASSIFICATION	 process multikey inner join vectorized row batch 
WITHOUT_CLASSIFICATION	 datacolumnnums 
WITHOUT_CLASSIFICATION	 compare input 
WITHOUT_CLASSIFICATION	 setup qbjointree between subquery and its parent query the parent query the lhs the join the parent query represented the last operator needed process its from clause case single table query this will tablescan but can join operator the parent query contains join clauses case single source from clause the source could subquery ptf invocation setup the qbjointree with the above constrains place the lhs the qbjointree can another qbjointree the parent query operator joinoperator this case get its qbjointree from the joincontext the rhs always reference the subquery its alias obtained from the qbsubquery object the qbsubquery also provides the joining condition ast the joining condition has been transformed qbsubquery setup before this call the joining condition has any correlated predicates and predicate for joining the parent query expression with the subquery the qbsubquery also specifies what kind join construct given this information once initialize the qbjointree call the parsejoincondition method validate and parse join conditions 
WITHOUT_CLASSIFICATION	 and grand child 
WITHOUT_CLASSIFICATION	 this sql standard return statemaxnarray null the size zero 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite partition with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 this two functions are for use only the planner will fail task 
WITHOUT_CLASSIFICATION	 simple pattern 
WITHOUT_CLASSIFICATION	 cannot convert bucket map join cannot convert map join either based the size check can convert smb join 
WITHOUT_CLASSIFICATION	 create the mapjoin operator 
WITHOUT_CLASSIFICATION	 test repeating nonnull selection 
WITHOUT_CLASSIFICATION	 cardinality estimate from normalized bias corrected harmonic mean 
WITHOUT_CLASSIFICATION	 get the counters for the input vertex 
WITHOUT_CLASSIFICATION	 both classes access subclasses 
WITHOUT_CLASSIFICATION	 timeseries query results records types defined metastore 
WITHOUT_CLASSIFICATION	 private readstringresults readstringresults 
WITHOUT_CLASSIFICATION	 even the user isnt doing schema evolution cut the schema the desired size 
WITHOUT_CLASSIFICATION	 insert two rows into the table 
WITHOUT_CLASSIFICATION	 see hive 
WITHOUT_CLASSIFICATION	 set base the location that the input format reads the original files 
WITHOUT_CLASSIFICATION	 locks from different transactions detected from transaction and readonly query autocommit 
WITHOUT_CLASSIFICATION	 operator wants some work the beginning group the first group 
WITHOUT_CLASSIFICATION	 convert all nan values vector null should only used 
WITHOUT_CLASSIFICATION	 filter expression since will taken care partitio pruner 
WITHOUT_CLASSIFICATION	 dont store too many items the queue full well block the checker thread since the worker count determines how many queries can running parallel makes sense produce more work the backlog getting too long 
WITHOUT_CLASSIFICATION	 checking 
WITHOUT_CLASSIFICATION	 string query 
WITHOUT_CLASSIFICATION	 session handle should not null 
WITHOUT_CLASSIFICATION	 nothing will added the expression 
WITHOUT_CLASSIFICATION	 semijoin 
WITHOUT_CLASSIFICATION	 acending 
WITHOUT_CLASSIFICATION	 print foreign key containing parents 
WITHOUT_CLASSIFICATION	 iterate through children and push down not for each one 
WITHOUT_CLASSIFICATION	 realm ignored 
WITHOUT_CLASSIFICATION	 walk the operator tree the tablescan and build the mapping 
WITHOUT_CLASSIFICATION	 make ones complement masked only for the bytes read 
WITHOUT_CLASSIFICATION	 nothing done for filters the output schema does not change 
WITHOUT_CLASSIFICATION	 make initialdelay random number heartbeatinterval that lot queries land the server the same time and all get blocked lack resources that they all dont start heartbeating the same time 
WITHOUT_CLASSIFICATION	 arithmetic operations reset the results 
WITHOUT_CLASSIFICATION	 rewrite projects replace column references constants when possible 
WITHOUT_CLASSIFICATION	 resourceplans 
WITHOUT_CLASSIFICATION	 xxxx idx xxxx idx 
WITHOUT_CLASSIFICATION	 this primitive type 
WITHOUT_CLASSIFICATION	 add the log processor 
WITHOUT_CLASSIFICATION	 the alias already there then have conflict 
WITHOUT_CLASSIFICATION	 create provider 
WITHOUT_CLASSIFICATION	 this operator only process small tables read the keyvalue pairs load them into hashtable 
WITHOUT_CLASSIFICATION	 reset the bufferreader fetching from the beginning the file 
WITHOUT_CLASSIFICATION	 extract any drop privileges out required privileges 
WITHOUT_CLASSIFICATION	 without this becomes 
WITHOUT_CLASSIFICATION	 change for multiply value 
WITHOUT_CLASSIFICATION	 query should have fetch task 
WITHOUT_CLASSIFICATION	 cause cannot prune columns from udtf branch currently extract 
WITHOUT_CLASSIFICATION	 substituted 
WITHOUT_CLASSIFICATION	 count distinct with more that one argument not supported 
WITHOUT_CLASSIFICATION	 custom build arguments 
WITHOUT_CLASSIFICATION	 get node type 
WITHOUT_CLASSIFICATION	 has use linkedhashmap enforce order 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 will superceded credential provider will not superceded 
WITHOUT_CLASSIFICATION	 the bucketing and sorting positions should exactly match 
WITHOUT_CLASSIFICATION	 distinct lost position 
WITHOUT_CLASSIFICATION	 subquery rewriting needed 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 call function 
WITHOUT_CLASSIFICATION	 create 
WITHOUT_CLASSIFICATION	 string type never stored anything other than escaped string 
WITHOUT_CLASSIFICATION	 test params check that all the parameters empty table are retained asis may add beyond but not change values for any parameters that hive defines for empty table 
WITHOUT_CLASSIFICATION	 task requested host got host 
WITHOUT_CLASSIFICATION	 clear work from threadlocal after splits generated case thread reused pool 
WITHOUT_CLASSIFICATION	 can not use split function directly may quoted 
WITHOUT_CLASSIFICATION	 disabling vectorization this test yields incorrect results with vectorization 
WITHOUT_CLASSIFICATION	 the perbatch setup for left semi join 
WITHOUT_CLASSIFICATION	 get timestamp from string constant cast 
WITHOUT_CLASSIFICATION	 completion delete from tab txn 
WITHOUT_CLASSIFICATION	 construct map join and set the child operator tblscanop 
WITHOUT_CLASSIFICATION	 finished writing array contents 
WITHOUT_CLASSIFICATION	 for dynamic uris relookup there are new metastore locations 
WITHOUT_CLASSIFICATION	 unknown unknown 
WITHOUT_CLASSIFICATION	 cause timestamp object replaced buggy code with zerotimestamp 
WITHOUT_CLASSIFICATION	 gby for distinct after windowing 
WITHOUT_CLASSIFICATION	 this thismag thisscale right rightmag rightscale this right thismag rightmag thisscale rightscale need scale down thisscale rightscale newscale 
WITHOUT_CLASSIFICATION	 both children the expression should not literal 
WITHOUT_CLASSIFICATION	 empty ctor make jackson happy 
WITHOUT_CLASSIFICATION	 verify the directories table location 
WITHOUT_CLASSIFICATION	 try iso format 
WITHOUT_CLASSIFICATION	 this method converts from the string representation druid type the corresponding hive type 
WITHOUT_CLASSIFICATION	 indicates the maximum capacity the cache minimum value should the number threads 
WITHOUT_CLASSIFICATION	 note doesnt check for overflow could and with max refcount mask but the caller checks 
WITHOUT_CLASSIFICATION	 one time update issue when the new hive catalog created upgrade the script does not know the location the warehouse need update 
WITHOUT_CLASSIFICATION	 build the schema for this table which slightly different than the schema for the input table 
WITHOUT_CLASSIFICATION	 propagate null values for twoinput operator and set isrepeating and nonulls appropriately 
WITHOUT_CLASSIFICATION	 positioned first 
WITHOUT_CLASSIFICATION	 append leading needed 
WITHOUT_CLASSIFICATION	 longminvalue 
WITHOUT_CLASSIFICATION	 currently tez the flow events thus generate splits initialize vertex with parallelism info obtained from the generate splits phase the generate splits phase groups splits using the however for bucket map joins the grouping done this input format results incorrect results the grouper has knowledge buckets initially set the input format hiveinputformat dagutils for the case bucket map joins obtain ungrouped splits then group the splits corresponding buckets using the tez grouper which returns tezgroupedsplits 
WITHOUT_CLASSIFICATION	 stores each cells length column one dataoutputbuffer element 
WITHOUT_CLASSIFICATION	 pairwise columnhasnulls columnisrepeating columnhasnulls columnisrepeating 
WITHOUT_CLASSIFICATION	 groupingsets cube rollup were used account groupingid 
WITHOUT_CLASSIFICATION	 only need flip the msb 
WITHOUT_CLASSIFICATION	 service fresh conf for every testmethod 
WITHOUT_CLASSIFICATION	 note that the calls below will throw exception java securitymanager installed and configured forbid invoking setaccessible practice this not problem hive 
WITHOUT_CLASSIFICATION	 reset the execcontext for each new row 
WITHOUT_CLASSIFICATION	 the read field the union gives its tag 
WITHOUT_CLASSIFICATION	 disable expensive operations the metastore 
WITHOUT_CLASSIFICATION	 something else holds the lock the moment dont bother cleaning 
WITHOUT_CLASSIFICATION	 test null input 
WITHOUT_CLASSIFICATION	 read length 
WITHOUT_CLASSIFICATION	 adjacency list 
WITHOUT_CLASSIFICATION	 cluster worker end job submitted the cluster 
WITHOUT_CLASSIFICATION	 shutdown the current active one 
WITHOUT_CLASSIFICATION	 get the operation logs once and print then wait till progress bar update complete before printing the remaining logs 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 get the cpu counters gctimemillis cpumilliseconds 
WITHOUT_CLASSIFICATION	 shutdown the timeout thread any while closing this operation 
WITHOUT_CLASSIFICATION	 evaluate row 
WITHOUT_CLASSIFICATION	 out range due time 
WITHOUT_CLASSIFICATION	 sort before comparing with expected results 
WITHOUT_CLASSIFICATION	 clearing this before sending kill since canfinish will change false ideally this should state machine where kills are issued the executor and the structures are cleaned once all tasks complete new requests however 
WITHOUT_CLASSIFICATION	 undone need get the table schema inspector from selfdescribing input file formats like orc modify the orc serde instead for now this works 
WITHOUT_CLASSIFICATION	 digits 
WITHOUT_CLASSIFICATION	 fall back regular api and create statuses without 
WITHOUT_CLASSIFICATION	 the table does not define any transactional properties return default type 
WITHOUT_CLASSIFICATION	 trigger query hook before compilation 
WITHOUT_CLASSIFICATION	 update columnar lineage for each partition 
WITHOUT_CLASSIFICATION	 this node was previous union inputs but not this one 
WITHOUT_CLASSIFICATION	 broadcast data 
WITHOUT_CLASSIFICATION	 stores explain output 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 different batch for vectorized input file format readers they can their work overlapped with work the row collection that vectorrow deserialization does this allows the partitions mix modes for flush the previously batched rows file change 
WITHOUT_CLASSIFICATION	 not always there sessionstate sometimes exedriver directly invoked 
WITHOUT_CLASSIFICATION	 table level event that matches 
WITHOUT_CLASSIFICATION	 swsw lock are examining shared write 
WITHOUT_CLASSIFICATION	 avoid copy 
WITHOUT_CLASSIFICATION	 last row last batch determines isgroupresultnull and decimal lastvalue 
WITHOUT_CLASSIFICATION	 now that have found real data emit sign byte necessary 
WITHOUT_CLASSIFICATION	 create executor 
WITHOUT_CLASSIFICATION	 the existing entry already has grant new priv does not have grant update needs done 
WITHOUT_CLASSIFICATION	 skip some piece data 
WITHOUT_CLASSIFICATION	 useful for class generation via templates 
WITHOUT_CLASSIFICATION	 normally statskeypref will the same dirname but the latter 
WITHOUT_CLASSIFICATION	 todo make sure cleanup created dirs 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 castexpr coltype colname 
WITHOUT_CLASSIFICATION	 full acid export goes thru updatedelete analyzer 
WITHOUT_CLASSIFICATION	 tests the missing element layer detected multifield group 
WITHOUT_CLASSIFICATION	 only done when bucket map join only smb 
WITHOUT_CLASSIFICATION	 friday august 
WITHOUT_CLASSIFICATION	 generate split strategy for acid schema files any 
WITHOUT_CLASSIFICATION	 replacemode creates are really alters using createtabledesc 
WITHOUT_CLASSIFICATION	 create export task and add root task 
WITHOUT_CLASSIFICATION	 core pool size max pool size direct handoff 
WITHOUT_CLASSIFICATION	 new record reader 
WITHOUT_CLASSIFICATION	 weve already satisfied the number events were supposed deliver end 
WITHOUT_CLASSIFICATION	 create environment for 
WITHOUT_CLASSIFICATION	 the first argument null return null its okay for other arguments null which case null will printed 
WITHOUT_CLASSIFICATION	 create success file requested 
WITHOUT_CLASSIFICATION	 get the top operator and its child all operators have single parent 
WITHOUT_CLASSIFICATION	 todo handle cast windowing agg call 
WITHOUT_CLASSIFICATION	 archiving was done this upper level its level would lesser equal specification size not which means archiving this upper level 
WITHOUT_CLASSIFICATION	 need iterate twice since byteswritable doesnt support append 
WITHOUT_CLASSIFICATION	 processing the message that the successful init has queued for 
WITHOUT_CLASSIFICATION	 get too crazy 
WITHOUT_CLASSIFICATION	 the lazy struct object inspector 
WITHOUT_CLASSIFICATION	 call check existence side file for mockmocktbl 
WITHOUT_CLASSIFICATION	 string including style literal characters 
WITHOUT_CLASSIFICATION	 always mark llap 
WITHOUT_CLASSIFICATION	 test that new columns gets added table schema 
WITHOUT_CLASSIFICATION	 verify proxy user privilege the realuser for the proxyuser 
WITHOUT_CLASSIFICATION	 both sides are constants there nothing propagate 
WITHOUT_CLASSIFICATION	 add this node the parent node 
WITHOUT_CLASSIFICATION	 todo use final fields 
WITHOUT_CLASSIFICATION	 check the score for this method any better relative others 
WITHOUT_CLASSIFICATION	 the operator not rexcall type fail fall through 
WITHOUT_CLASSIFICATION	 check out the types 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that inner join singlecolumn string and only big table columns appear the join result hash multiset used 
WITHOUT_CLASSIFICATION	 when truncated included used its length must least the number source type infos 
WITHOUT_CLASSIFICATION	 cannot proceed and need tell the hive client that retries wont succeed either 
WITHOUT_CLASSIFICATION	 add new information from source target 
WITHOUT_CLASSIFICATION	 remove the subquery from the where clause tree return the remaining whereclause 
WITHOUT_CLASSIFICATION	 now deserialize 
WITHOUT_CLASSIFICATION	 here assume that upstream code may have parametrized the msg from errormsg want keep 
WITHOUT_CLASSIFICATION	 last value should present 
WITHOUT_CLASSIFICATION	 for each matching partition call getsplits the underlying inputformat 
WITHOUT_CLASSIFICATION	 from bit linear congruential generator 
WITHOUT_CLASSIFICATION	 constant propagation constant folding 
WITHOUT_CLASSIFICATION	 verify moveonlytask not optimized 
WITHOUT_CLASSIFICATION	 create add node for current pool 
WITHOUT_CLASSIFICATION	 and are sorted the same order 
WITHOUT_CLASSIFICATION	 get splits from accumulo 
WITHOUT_CLASSIFICATION	 determine two strings are equal from two byte arrays each with their own start position and length use lexicographic unsigned byte value order this whats used for utf sort order 
WITHOUT_CLASSIFICATION	 table using some custom format and its not the classpath wont mark the table for acid but today hive and earlier the only acid format 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 launch the parallel mode separate thread only for tasks 
WITHOUT_CLASSIFICATION	 build rel for limit clause 
WITHOUT_CLASSIFICATION	 lstring 
WITHOUT_CLASSIFICATION	 convert from java writable 
WITHOUT_CLASSIFICATION	 save the current script any 
WITHOUT_CLASSIFICATION	 messageformat 
WITHOUT_CLASSIFICATION	 column family qualifier dont want include the map 
WITHOUT_CLASSIFICATION	 case the sizes match preference given the table with fewer partitions 
WITHOUT_CLASSIFICATION	 the hiveserver instance running this service 
WITHOUT_CLASSIFICATION	 iterate through the index that add more children they dont get revisited 
WITHOUT_CLASSIFICATION	 write json the temp file 
WITHOUT_CLASSIFICATION	 should produce json 
WITHOUT_CLASSIFICATION	 skip not the one are looking for 
WITHOUT_CLASSIFICATION	 script preserves alias and value for columns related keys user can set this true 
WITHOUT_CLASSIFICATION	 deactivate currently active resource plan 
WITHOUT_CLASSIFICATION	 extra heartbeat logically harmless but 
WITHOUT_CLASSIFICATION	 otherwise write the file system implied the directory copy required may want revisit this policy future 
WITHOUT_CLASSIFICATION	 new new byte 
WITHOUT_CLASSIFICATION	 make the movetask the child the task 
WITHOUT_CLASSIFICATION	 message 
WITHOUT_CLASSIFICATION	 test getter for map object 
WITHOUT_CLASSIFICATION	 plug verifying metastore for testing directsql 
WITHOUT_CLASSIFICATION	 specialcase for orc 
WITHOUT_CLASSIFICATION	 the node should always known this point log occasionally not known 
WITHOUT_CLASSIFICATION	 currently sumdistinct not supported partitionevaluator 
WITHOUT_CLASSIFICATION	 assign row from array objects 
WITHOUT_CLASSIFICATION	 base path for repl load 
WITHOUT_CLASSIFICATION	 this vectorized code pattern says the input batch has nulls all nonulls true the input row not null copy the value otherwise have null input value the standard way mark null the output batch turn off nonulls indicating there least one null the batch and mark that row null when vectorized row batch reset nonulls set true and the isnull array zeroed grab the key index dont care about selected repeating since all keys the input batch are suppose the same 
WITHOUT_CLASSIFICATION	 dayssinceepoch 
WITHOUT_CLASSIFICATION	 test left input repeating 
WITHOUT_CLASSIFICATION	 this map task has filesinkoperator and bucketingsorting metadata can inferred about the data being written that operator these are mappings from the directory 
WITHOUT_CLASSIFICATION	 single table alias reference ignore and move the next expression node 
WITHOUT_CLASSIFICATION	 currently only support these noprecisionloss promotion data type conversions tinyint smallint tinyint int tinyint bigint smallint int smallint bigint int bigint float double since stare char without padding can become string implicitly char varchar string 
WITHOUT_CLASSIFICATION	 task requested host got host host and host are full 
WITHOUT_CLASSIFICATION	 test that only the fixed property forqueue used order determination not the dynamic call 
WITHOUT_CLASSIFICATION	 add support for configurable threads however should always enough 
WITHOUT_CLASSIFICATION	 confvarsscratchdir 
WITHOUT_CLASSIFICATION	 map that contains the rows for each alias 
WITHOUT_CLASSIFICATION	 might still able push the limit 
WITHOUT_CLASSIFICATION	 mapper can span multiple filespartitions the need changed the input file changed 
WITHOUT_CLASSIFICATION	 all files are needed meet the size limit disable optimization usually happens for empty tablepartition tablepartition with only one file disabling this optimization can avoid retrying the query there not sufficient rows 
WITHOUT_CLASSIFICATION	 find the parsed delta files 
WITHOUT_CLASSIFICATION	 add fake entries 
WITHOUT_CLASSIFICATION	 trigger clean errors for anyone who mixes identity with hosts 
WITHOUT_CLASSIFICATION	 same algorithm timestampwritable not currently importable here 
WITHOUT_CLASSIFICATION	 write nonnull element 
WITHOUT_CLASSIFICATION	 run cleaner should remove the delta dirs 
WITHOUT_CLASSIFICATION	 overflow checks 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 obtain stats for partition cols 
WITHOUT_CLASSIFICATION	 list 
WITHOUT_CLASSIFICATION	 fallback default 
WITHOUT_CLASSIFICATION	 are using the fact the input sorted 
WITHOUT_CLASSIFICATION	 writing both acid and nonacid resources the same txn tab write dynamic partition insert 
WITHOUT_CLASSIFICATION	 external table are done 
WITHOUT_CLASSIFICATION	 create empty invalid side file make sure getlogicallength throws 
WITHOUT_CLASSIFICATION	 specify that the results this query can cached 
WITHOUT_CLASSIFICATION	 test null dbname works default used 
WITHOUT_CLASSIFICATION	 are aborting only the current transaction move the min range for heartbeat disable heartbeat the current txn last the batch 
WITHOUT_CLASSIFICATION	 return mocked serdeinfo 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautilcalendar 
WITHOUT_CLASSIFICATION	 copy over the mandatory configs for the package 
WITHOUT_CLASSIFICATION	 generate row values 
WITHOUT_CLASSIFICATION	 update output row schema 
WITHOUT_CLASSIFICATION	 map from type name such int varchar the corresponding primitivetypeinfo 
WITHOUT_CLASSIFICATION	 everything qualifies rows all with value 
WITHOUT_CLASSIFICATION	 theres rexinputref the projected expressions return empty set 
WITHOUT_CLASSIFICATION	 check object 
WITHOUT_CLASSIFICATION	 expecting change the size internal structures 
WITHOUT_CLASSIFICATION	 proxy class within the tezapi package access package private methods 
WITHOUT_CLASSIFICATION	 estimate needed underlying aggbuffer for results for maxchain results underlying wdwsz maxchain underlying wdwsz 
WITHOUT_CLASSIFICATION	 need create the merge join work 
WITHOUT_CLASSIFICATION	 sort the splits that subsequent grouping consistent 
WITHOUT_CLASSIFICATION	 max length for varchar and char cases 
WITHOUT_CLASSIFICATION	 all are selected 
WITHOUT_CLASSIFICATION	 insert overwrite directory command there were bucketing list bucketing 
WITHOUT_CLASSIFICATION	 statementid 
WITHOUT_CLASSIFICATION	 save charvarchar string 
WITHOUT_CLASSIFICATION	 for null fields make valid max length 
WITHOUT_CLASSIFICATION	 trigger failover minihs without authorization header 
WITHOUT_CLASSIFICATION	 normilize table name for mapping 
WITHOUT_CLASSIFICATION	 dont currently allow imposition type 
WITHOUT_CLASSIFICATION	 array should have listtypeinfo within the list extract types 
WITHOUT_CLASSIFICATION	 locks txn outta here 
WITHOUT_CLASSIFICATION	 does the logger config look correct 
WITHOUT_CLASSIFICATION	 verifyrunselect from repldbname matview unptndata drivermirror 
WITHOUT_CLASSIFICATION	 delete remaining jars 
WITHOUT_CLASSIFICATION	 lock table dynamic partitions 
WITHOUT_CLASSIFICATION	 correlating variables are means for other relational expressions use fields 
WITHOUT_CLASSIFICATION	 serialize the row struct 
WITHOUT_CLASSIFICATION	 under the limit 
WITHOUT_CLASSIFICATION	 build partition strings 
WITHOUT_CLASSIFICATION	 not vrb mode the new cache data partially ready should use force the rest the data thru 
WITHOUT_CLASSIFICATION	 column not partition column for the table not allow partitions based complex list struct fields 
WITHOUT_CLASSIFICATION	 parse out ngrams update frequency counts 
WITHOUT_CLASSIFICATION	 remove the tag from key coming out reducer and store separate variable make copy for multiinsert with join case spark reuses input key from same parent 
WITHOUT_CLASSIFICATION	 this will take care mapping between input column names and output column names the returned column stats will have the output column names 
WITHOUT_CLASSIFICATION	 checkh subquery predicates cannot only refer outer query columns 
WITHOUT_CLASSIFICATION	 show cannot create child file 
WITHOUT_CLASSIFICATION	 iterate over the map and remove semijoin optimizations needed 
WITHOUT_CLASSIFICATION	 more than capacity 
WITHOUT_CLASSIFICATION	 check this udf has been provided with type params for the output varchar type 
WITHOUT_CLASSIFICATION	 execute final aggregation stage for simple fetch query fetch task 
WITHOUT_CLASSIFICATION	 try merge this join with the left child 
WITHOUT_CLASSIFICATION	 create the adaptor for this function call work vector mode 
WITHOUT_CLASSIFICATION	 get the vlong that represents the map size 
WITHOUT_CLASSIFICATION	 switch hivesitexml with remote metastore 
WITHOUT_CLASSIFICATION	 our one time process method initialization 
WITHOUT_CLASSIFICATION	 startdate sun letters day name 
WITHOUT_CLASSIFICATION	 delay with exponential backoff 
WITHOUT_CLASSIFICATION	 done with the row 
WITHOUT_CLASSIFICATION	 sleep for seconds 
WITHOUT_CLASSIFICATION	 udf like one user would create implementing the udf interface this used test the vectorized udf adaptor for legacystyle udfs 
WITHOUT_CLASSIFICATION	 file 
WITHOUT_CLASSIFICATION	 for table level load need not update replication state for the database 
WITHOUT_CLASSIFICATION	 dont think this can happen but just case 
WITHOUT_CLASSIFICATION	 this batch with the same column schema the big table batch that can used build join output results can create some join output results the big table batch will for better efficiency avoiding copying otherwise will use the 
WITHOUT_CLASSIFICATION	 perform some operations 
WITHOUT_CLASSIFICATION	 load the partition 
WITHOUT_CLASSIFICATION	 try with null args 
WITHOUT_CLASSIFICATION	 the fsop configuration for the fsop that going write initial data during ctas 
WITHOUT_CLASSIFICATION	 add all columns make vectorization context for the tablescan operator 
WITHOUT_CLASSIFICATION	 rounding fractional digits 
WITHOUT_CLASSIFICATION	 during mapreduce tasks there may not valid hiveconf from the sessionstate lookup and save any needed conf information during query compilation the hive conf where there should valid hiveconf from sessionstate plan serialization will ensure have access these values the mapreduce tasks 
WITHOUT_CLASSIFICATION	 leave the client time out 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 generate the right hand side the clause 
WITHOUT_CLASSIFICATION	 the windowingspec used for windowing clauses this 
WITHOUT_CLASSIFICATION	 trigger the transform 
WITHOUT_CLASSIFICATION	 processing completed 
WITHOUT_CLASSIFICATION	 multiple stripes 
WITHOUT_CLASSIFICATION	 insert the dummy store operator here 
WITHOUT_CLASSIFICATION	 load entire archive there some parallelism going you load more than partition the file name changes from run run between and and the data correct but this causes rowidbucketidfile names change 
WITHOUT_CLASSIFICATION	 here checked all parts and they are acid compatible make acid 
WITHOUT_CLASSIFICATION	 test behavior with nonchunked streams 
WITHOUT_CLASSIFICATION	 there are aborted txns then the minimum aborted txnid could the minuncommittedtxnid 
WITHOUT_CLASSIFICATION	 endtime 
WITHOUT_CLASSIFICATION	 the children type should converted return type 
WITHOUT_CLASSIFICATION	 unit tests can overwrite this affect default dump behaviour 
WITHOUT_CLASSIFICATION	 lower case rolename 
WITHOUT_CLASSIFICATION	 set our inputformat 
WITHOUT_CLASSIFICATION	 column types 
WITHOUT_CLASSIFICATION	 todo side the union has columns with the same name noone the higher level can refer them could change the alias the original node 
WITHOUT_CLASSIFICATION	 then make sure the file sink operators are set right 
WITHOUT_CLASSIFICATION	 there kryo which after timestamp becomes date get around this issue once kryo fixed the issue can simplify 
WITHOUT_CLASSIFICATION	 insert schema was specified 
WITHOUT_CLASSIFICATION	 describe how deserialize data back from user script 
WITHOUT_CLASSIFICATION	 sets the job state and result returns true status and result are set otherwise returns false 
WITHOUT_CLASSIFICATION	 table existed and okay replicate into not dropping and recreating 
WITHOUT_CLASSIFICATION	 locationuri 
WITHOUT_CLASSIFICATION	 most one alias unknown can safely regard big alias 
WITHOUT_CLASSIFICATION	 this genericudf cant pushed down 
WITHOUT_CLASSIFICATION	 only first stripe will satisfy condition and hence single split 
WITHOUT_CLASSIFICATION	 used for value registry 
WITHOUT_CLASSIFICATION	 view referring old database data 
WITHOUT_CLASSIFICATION	 for each table reference get the table name and the alias alias not present the table name 
WITHOUT_CLASSIFICATION	 hive this noop 
WITHOUT_CLASSIFICATION	 generate the service ticket for sending the server locking ensures the tokens are unique case concurrent requests 
WITHOUT_CLASSIFICATION	 make data consistent with encodings dont store useless information 
WITHOUT_CLASSIFICATION	 merges sampling data from previous and make partition keys for total sort 
WITHOUT_CLASSIFICATION	 resend existing value necessary 
WITHOUT_CLASSIFICATION	 only the admin allowed list privileges for any user 
WITHOUT_CLASSIFICATION	 nothing just retry 
WITHOUT_CLASSIFICATION	 create map capture object privileges corresponding privilege 
WITHOUT_CLASSIFICATION	 the method returns new writable 
WITHOUT_CLASSIFICATION	 include same hllastheartbeat condition case someone heartbeated since the select 
WITHOUT_CLASSIFICATION	 there can multiple instances per node 
WITHOUT_CLASSIFICATION	 int pos 
WITHOUT_CLASSIFICATION	 check make sure there are duplicate rowids hive 
WITHOUT_CLASSIFICATION	 oldreplstate lessthan newreplstate allow 
WITHOUT_CLASSIFICATION	 fixup numbers limit the range 
WITHOUT_CLASSIFICATION	 the first group 
WITHOUT_CLASSIFICATION	 check see this input job outputjob 
WITHOUT_CLASSIFICATION	 serialize the configuration once 
WITHOUT_CLASSIFICATION	 change correlator rel into join join all the correlated variables produced this correlator rel 
WITHOUT_CLASSIFICATION	 this secured cookie and the current connection nonsecured then skip this cookie need skip this cookie because the cookie replay will not transmitted the server 
WITHOUT_CLASSIFICATION	 avro requires nullable types defined unions some type and null this annoying and were going hide from the user 
WITHOUT_CLASSIFICATION	 drop partition after dump 
WITHOUT_CLASSIFICATION	 the max good the min too low 
WITHOUT_CLASSIFICATION	 list the loggers and their levels 
WITHOUT_CLASSIFICATION	 then create splits with the druid queries 
WITHOUT_CLASSIFICATION	 sort will try open the output file write mode windows need close first 
WITHOUT_CLASSIFICATION	 detect correlations 
WITHOUT_CLASSIFICATION	 now check the partition already exists not ahead error out immutable and mutable check that the partitions matches our current jobs tables check for compatibility compatible ignore and not add incompatible error out again 
WITHOUT_CLASSIFICATION	 otherwise try default timestamp parsing 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 try reduce 
WITHOUT_CLASSIFICATION	 bgenjjtree typemap 
WITHOUT_CLASSIFICATION	 the structure the ast for the rewritten insert statement tokquery tokfrom tokinsert tokinsertinto tokselect toksortby the following adds the tokwhere and its subtree from the original query child tokinsert which where would have landed had been there originally the string this way because its easy then turning the original ast back into string and reparsing have move the sortby over one grab and then push the second slot and put the where the first slot 
WITHOUT_CLASSIFICATION	 determine the user would need sign fragments 
WITHOUT_CLASSIFICATION	 write couple batches 
WITHOUT_CLASSIFICATION	 this constant bit misnomer since now always have txn context just means the operation such that dont care what tablespartitions affected doesnt trigger compaction conflict detection better name would nontransactional 
WITHOUT_CLASSIFICATION	 data 
WITHOUT_CLASSIFICATION	 integer 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 not supported 
WITHOUT_CLASSIFICATION	 unknown information judge 
WITHOUT_CLASSIFICATION	 finally hand off the stripe reader produce the data 
WITHOUT_CLASSIFICATION	 check permutation project 
WITHOUT_CLASSIFICATION	 heartbeat started 
WITHOUT_CLASSIFICATION	 there could case where join operators input are not map join with spark since following estimation statistics relies join operators having inputs reduced sink will not work for such cases should not try estimate stats 
WITHOUT_CLASSIFICATION	 pattern does not contain all date fields 
WITHOUT_CLASSIFICATION	 partition bucket file names and partition bucket number for 
WITHOUT_CLASSIFICATION	 verify drop table nonexisting table idempotent 
WITHOUT_CLASSIFICATION	 explaining this would really require picture basically the level lower than our level that means imagine tree are the leftmost leaf node the subtree under our sibling the tree wed need look the buddies that leftmost leaf block all the intermediate levels aka all intermediate levels the tree between this guy and our sibling including its own buddy its own level and for every subtree where our buddy not the same level does not cover the entire 
WITHOUT_CLASSIFICATION	 incorrect use this class 
WITHOUT_CLASSIFICATION	 write schema since need pull the data out see point above 
WITHOUT_CLASSIFICATION	 even for regular copy have use the same user permissions that distcp will use since hiveserver user might different that the super user required copy relevant files 
WITHOUT_CLASSIFICATION	 only release cache chunks not release proccachechunks they may not yet have data 
WITHOUT_CLASSIFICATION	 the indexes the delimiters 
WITHOUT_CLASSIFICATION	 after load shall see the overwritten data 
WITHOUT_CLASSIFICATION	 expected 
WITHOUT_CLASSIFICATION	 statementid 
WITHOUT_CLASSIFICATION	 process join values 
WITHOUT_CLASSIFICATION	 template classname valuetype outputtype outputtypeinspector 
WITHOUT_CLASSIFICATION	 create reducesinkop operator 
WITHOUT_CLASSIFICATION	 populate local work needed 
WITHOUT_CLASSIFICATION	 create fetchwork for partitioned table 
WITHOUT_CLASSIFICATION	 turn clientside authorization 
WITHOUT_CLASSIFICATION	 this the iow case 
WITHOUT_CLASSIFICATION	 the synchronization here not necessary but tests depend 
WITHOUT_CLASSIFICATION	 since theres close here maintain the initial read position between writes 
WITHOUT_CLASSIFICATION	 fields these aggregation classes 
WITHOUT_CLASSIFICATION	 actual result directory need move anything 
WITHOUT_CLASSIFICATION	 conversion possible for the reduce keys 
WITHOUT_CLASSIFICATION	 bugbug need deal with named type here look and proxy should raise exception this typedef since wont any children and thus can quickly find this comment and limitation 
WITHOUT_CLASSIFICATION	 dogetfooters 
WITHOUT_CLASSIFICATION	 the same day the month then time part should ignored 
WITHOUT_CLASSIFICATION	 with nulls and selected 
WITHOUT_CLASSIFICATION	 need clone some operator plans and remove union operators still 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 coming from bigtable side some bookkeeping and continue traversal 
WITHOUT_CLASSIFICATION	 about filtering 
WITHOUT_CLASSIFICATION	 must have most one child 
WITHOUT_CLASSIFICATION	 calculate result typeinfo 
WITHOUT_CLASSIFICATION	 print since otherwise exception lost 
WITHOUT_CLASSIFICATION	 update summary bitvector generate hash value the long value and mod bitvectorsize this implementation bitvectorsize 
WITHOUT_CLASSIFICATION	 hltxnid and hllockstate lockwaiting for multistatement txns where 
WITHOUT_CLASSIFICATION	 use get make sure variable substitution works 
WITHOUT_CLASSIFICATION	 meanwhile the init fails 
WITHOUT_CLASSIFICATION	 not need column reset since are carefully changing the output 
WITHOUT_CLASSIFICATION	 verify that ptned table property set worked 
WITHOUT_CLASSIFICATION	 this tmp table and thus session scoped and acid requires sql statement serial 
WITHOUT_CLASSIFICATION	 when there order specified add the partition expressions order expressions this implementation artifact for udafs that imply order like rank denserank depend the order expressions work internally pass the order expressions args these functions could change the translation that the functions are setup with partition expressions when the orderspec null but for now are setting orderspec that copies the partition expressions 
WITHOUT_CLASSIFICATION	 implicit type conversion hierarchy 
WITHOUT_CLASSIFICATION	 the data type primitive category the column being deserialized 
WITHOUT_CLASSIFICATION	 need include isinsideview inside digest differentiate direct 
WITHOUT_CLASSIFICATION	 will clone here will update bucket column key with its 
WITHOUT_CLASSIFICATION	 actually create the permanent function 
WITHOUT_CLASSIFICATION	 default list bucketing directory key internal use only not for client 
WITHOUT_CLASSIFICATION	 for queue size estimation purposes assume all columns have weight one and the following types are counted multiple columns this very primitive wanted make better 
WITHOUT_CLASSIFICATION	 were calling processop again process the leftover tuples know the row coming from the spilled matchfile need recreate hashmaprowgetter against new hashtables 
WITHOUT_CLASSIFICATION	 set the fetch formatter noop for the listsinkoperator since well read formatted thrift objects from the output sequencefile written tasks 
WITHOUT_CLASSIFICATION	 introduce select after the union 
WITHOUT_CLASSIFICATION	 from javasqltimestamp used vectorization serializable 
WITHOUT_CLASSIFICATION	 maxcomplexdepth 
WITHOUT_CLASSIFICATION	 local aliases need not hand over context further 
WITHOUT_CLASSIFICATION	 add the filter the queryid appender 
WITHOUT_CLASSIFICATION	 mapjoin table descriptor contains key descriptor which needs the field schema column type column name the column name not really used anywhere but needs passed use the string defined below for that 
WITHOUT_CLASSIFICATION	 set props for read 
WITHOUT_CLASSIFICATION	 read friendly string 
WITHOUT_CLASSIFICATION	 repeat times 
WITHOUT_CLASSIFICATION	 one each for min max and bloom filter 
WITHOUT_CLASSIFICATION	 make sure the ugi current 
WITHOUT_CLASSIFICATION	 reached the end the tag 
WITHOUT_CLASSIFICATION	 create view 
WITHOUT_CLASSIFICATION	 there remainder from numrowsnumbuckets then distribute increase the size the first rem buckets 
WITHOUT_CLASSIFICATION	 need convert the thrift type the sql type 
WITHOUT_CLASSIFICATION	 used handle skew join 
WITHOUT_CLASSIFICATION	 push down projections 
WITHOUT_CLASSIFICATION	 custom parameters 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 check julian days between jan and jan this method used test julian days between jan bce and jan since bce 
WITHOUT_CLASSIFICATION	 start monitoring the spark job returns when the spark job has completed failed 
WITHOUT_CLASSIFICATION	 count denserank and rank not care about column types the rest 
WITHOUT_CLASSIFICATION	 failure from not having permissions create table 
WITHOUT_CLASSIFICATION	 optimized for sequential key lookup 
WITHOUT_CLASSIFICATION	 security the path below the user path full access 
WITHOUT_CLASSIFICATION	 index into map 
WITHOUT_CLASSIFICATION	 maxrows 
WITHOUT_CLASSIFICATION	 detect queries the form select udtfcol looking for function the first child and then checking see the function generic udtf its not clean transform due 
WITHOUT_CLASSIFICATION	 convert from mapjoin bucket map join enabled 
WITHOUT_CLASSIFICATION	 hive values have copied and use 
WITHOUT_CLASSIFICATION	 test with multilevel scratch dir path 
WITHOUT_CLASSIFICATION	 clear the columnbuffers 
WITHOUT_CLASSIFICATION	 make cost based decision pick cheaper plan 
WITHOUT_CLASSIFICATION	 the dispatcher generates the plan from the operator tree 
WITHOUT_CLASSIFICATION	 keep track for error reporting 
WITHOUT_CLASSIFICATION	 ignored for some reason the bean was not found dont output 
WITHOUT_CLASSIFICATION	 save the current record the new extravalue for next time that 
WITHOUT_CLASSIFICATION	 now add cache 
WITHOUT_CLASSIFICATION	 todo 
WITHOUT_CLASSIFICATION	 for each node 
WITHOUT_CLASSIFICATION	 make sure originaldate midnight the local time zone since datewritablev will generate dates that time 
WITHOUT_CLASSIFICATION	 concurrency 
WITHOUT_CLASSIFICATION	 generated the log message 
WITHOUT_CLASSIFICATION	 index the next free block split 
WITHOUT_CLASSIFICATION	 any hive related operations like moving tables and files 
WITHOUT_CLASSIFICATION	 specification binary storage should not affect serde 
WITHOUT_CLASSIFICATION	 the automation the data warehouse the local file system based 
WITHOUT_CLASSIFICATION	 create mapred task for this work 
WITHOUT_CLASSIFICATION	 case 
WITHOUT_CLASSIFICATION	 current replication state must set the table object only for bootstrap dump event replication state will null case bootstrap dump 
WITHOUT_CLASSIFICATION	 tez were avoiding duplicate the file info fileinputformat 
WITHOUT_CLASSIFICATION	 sessionmanager initialized 
WITHOUT_CLASSIFICATION	 the parameter keys for the table statistics those keys are excluded from show create table command output 
WITHOUT_CLASSIFICATION	 the first position partition column 
WITHOUT_CLASSIFICATION	 sort the methods before omitting them 
WITHOUT_CLASSIFICATION	 inherit table properties into partition properties 
WITHOUT_CLASSIFICATION	 target isnull was copied beginning method 
WITHOUT_CLASSIFICATION	 lastfrom point the same text object which would make fromequalslastfrom always true 
WITHOUT_CLASSIFICATION	 prevnext are already checked the calls 
WITHOUT_CLASSIFICATION	 but soon became very fast decimal specific 
WITHOUT_CLASSIFICATION	 last partition key anything between key and end 
WITHOUT_CLASSIFICATION	 inclusive exclusive 
WITHOUT_CLASSIFICATION	 make random array byte arrays 
WITHOUT_CLASSIFICATION	 this makes that can back the tree later 
WITHOUT_CLASSIFICATION	 tables 
WITHOUT_CLASSIFICATION	 sortby desc 
WITHOUT_CLASSIFICATION	 test replicated drop should not drop because evid replstateid 
WITHOUT_CLASSIFICATION	 create the join predicate info object the object contains the join condition split accordingly the join condition not part the equijoin predicate the returned object will typed sqlkindother 
WITHOUT_CLASSIFICATION	 downloaded resources dir 
WITHOUT_CLASSIFICATION	 updatable map that holds instances the class 
WITHOUT_CLASSIFICATION	 first update buffer priority have just been using 
WITHOUT_CLASSIFICATION	 emulate biginteger serialization used lazybinary avro parquet and possibly others 
WITHOUT_CLASSIFICATION	 reload tables from the metastore 
WITHOUT_CLASSIFICATION	 set arguments 
WITHOUT_CLASSIFICATION	 dont want cache hits from llap for testing filesystem bytes read counters 
WITHOUT_CLASSIFICATION	 source 
WITHOUT_CLASSIFICATION	 since txnutilsgettxnstore calls txnhandlersetconf checkqfiletesthack which may change the values below two entries need avoid polluting the original values 
WITHOUT_CLASSIFICATION	 empty constructor for writable etc 
WITHOUT_CLASSIFICATION	 throw away lower digits 
WITHOUT_CLASSIFICATION	 mask udfs 
WITHOUT_CLASSIFICATION	 there was cluster state change make sure redistribute all the pools 
WITHOUT_CLASSIFICATION	 prepare 
WITHOUT_CLASSIFICATION	 mystringlist 
WITHOUT_CLASSIFICATION	 create default users 
WITHOUT_CLASSIFICATION	 notify have successfully copied the file 
WITHOUT_CLASSIFICATION	 set handling for low resource conditions 
WITHOUT_CLASSIFICATION	 caller should not try allocate another arena before waiting for the previous one 
WITHOUT_CLASSIFICATION	 this operator has materialized view below make its cost tiny and adjust the cost its 
WITHOUT_CLASSIFICATION	 our stats for ndv approx not accurate 
WITHOUT_CLASSIFICATION	 required optional required 
WITHOUT_CLASSIFICATION	 runas 
WITHOUT_CLASSIFICATION	 this method takes care bitflipping for descending order 
WITHOUT_CLASSIFICATION	 skip mode should not throw exception when invalid partition directory found should just ignore 
WITHOUT_CLASSIFICATION	 nonpartitioned table 
WITHOUT_CLASSIFICATION	 dont add this the resources because dont want read config values from but find because want remember where for later case anyone calls 
WITHOUT_CLASSIFICATION	 ignore the value got from optraits the logic below will fall back the estimate from numreducers 
WITHOUT_CLASSIFICATION	 the directory does not exist 
WITHOUT_CLASSIFICATION	 listhapeers 
WITHOUT_CLASSIFICATION	 hive jar 
WITHOUT_CLASSIFICATION	 cleanup session log directory 
WITHOUT_CLASSIFICATION	 pre all the fields are required and serialized order isrealthrift 
WITHOUT_CLASSIFICATION	 note the stats for acid tables not have any coordination with either hive acid logic like txn commits time outs etc nor the lower level sync metastore pertaining acid updates the are not themselves acid 
WITHOUT_CLASSIFICATION	 first partition key anything between key and first 
WITHOUT_CLASSIFICATION	 optional optional 
WITHOUT_CLASSIFICATION	 step reanalyze 
WITHOUT_CLASSIFICATION	 not understand why needed and wonder could combined with close 
WITHOUT_CLASSIFICATION	 found file depth which less than number partition keys 
WITHOUT_CLASSIFICATION	 preemption will finally registered deallocatetask result preemptcontainer that resets preemption info and allows additional tasks preempted required 
WITHOUT_CLASSIFICATION	 alter table tablename drop exists partition partitionspec partition partitionspec 
WITHOUT_CLASSIFICATION	 existing ngram just increment count 
WITHOUT_CLASSIFICATION	 the idea that this will use lockhandledbconn 
WITHOUT_CLASSIFICATION	 find out need throw away the tuple not 
WITHOUT_CLASSIFICATION	 copy current value not change current scale 
WITHOUT_CLASSIFICATION	 optional fragmentruntimeinfo fragmentruntimeinfo 
WITHOUT_CLASSIFICATION	 have storage specification for primitive column type 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 noop dont really write anything here 
WITHOUT_CLASSIFICATION	 theres race between removing the current task from the preemption queue and the actual scheduler attempting take element from the preemption queue make space for another task the current element removed make space that since the current task completing and will end making space for execution any kill message sent out the scheduler the task will ignored since the task knows has completed otherwise would not this callback the task removed from the queue result this callback and the scheduler happens the section where its looking for preemptible task the scheuler may end pulling the next preemptible task and killing extra preemption todo this potential extra preemption can avoided synchronizing the entire tryscheduling block this would essentially synchronize all operations would better see theres approach where multiple locks could used avoid single threaded operation checks available and preempts which could this task this task completes making space and removing the need for preemption 
WITHOUT_CLASSIFICATION	 find 
WITHOUT_CLASSIFICATION	 copy set deduped locks back original list 
WITHOUT_CLASSIFICATION	 none are null all are selected 
WITHOUT_CLASSIFICATION	 the number stripes should match the key index count 
WITHOUT_CLASSIFICATION	 special handling for timestamp column field name field type 
WITHOUT_CLASSIFICATION	 after the first child evaluated 
WITHOUT_CLASSIFICATION	 this nonnative table need set stats inaccurate 
WITHOUT_CLASSIFICATION	 thrift configs 
WITHOUT_CLASSIFICATION	 add partitions all tables 
WITHOUT_CLASSIFICATION	 here means open transaction but different queries 
WITHOUT_CLASSIFICATION	 bgenjjtree typei 
WITHOUT_CLASSIFICATION	 serde 
WITHOUT_CLASSIFICATION	 create dummy tablescanoperator for the file generated through filesinkop 
WITHOUT_CLASSIFICATION	 recheck locks which were waiting state should now acquired 
WITHOUT_CLASSIFICATION	 create views registry 
WITHOUT_CLASSIFICATION	 the plan needs broken only one the subqueries involve 
WITHOUT_CLASSIFICATION	 assertequals mapgetcapacity 
WITHOUT_CLASSIFICATION	 there group file need call chgrp 
WITHOUT_CLASSIFICATION	 not used for mock but 
WITHOUT_CLASSIFICATION	 buffers hold filter pushdown information 
WITHOUT_CLASSIFICATION	 either colstats null estimated 
WITHOUT_CLASSIFICATION	 check bigint implicitely cast double part comparison perform the check here instead guarantee only run once per operator 
WITHOUT_CLASSIFICATION	 dbtesttablep should also columns will fix separate ticket 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 exact match 
WITHOUT_CLASSIFICATION	 now have written some new data bkt and shows 
WITHOUT_CLASSIFICATION	 generate empty dyn part call 
WITHOUT_CLASSIFICATION	 shortcircuit quickly forward all rows 
WITHOUT_CLASSIFICATION	 tablehandle can null table doesnt exist 
WITHOUT_CLASSIFICATION	 this captures mapping hive type names hcat type names the long run should just use hive types directly but that larger refactoring effort for hcatpig mapping see for pighcat mapping see 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlref 
WITHOUT_CLASSIFICATION	 nanosecond interval primitives produces type timestamp 
WITHOUT_CLASSIFICATION	 every line below this identical for evaluatelong evaluatestring 
WITHOUT_CLASSIFICATION	 calite bug calcite 
WITHOUT_CLASSIFICATION	 lets write more bytes the files test that actually working returning the file size not from the filesystem 
WITHOUT_CLASSIFICATION	 round the specified number decimal places using halfeven round function 
WITHOUT_CLASSIFICATION	 add backup task runnable 
WITHOUT_CLASSIFICATION	 more than thread should call this close function 
WITHOUT_CLASSIFICATION	 use this constructor when only ascending sort order used default for ascending order null first 
WITHOUT_CLASSIFICATION	 dont think event notifications case failures are necessary but other hms operations make this call whether the event failed succeeded make this behavior consistent this call made for failed events also 
WITHOUT_CLASSIFICATION	 add new column 
WITHOUT_CLASSIFICATION	 jline will detect tab regular character 
WITHOUT_CLASSIFICATION	 restrictionh correlated sub queries cannot contain windowing clauses 
WITHOUT_CLASSIFICATION	 check static partition appear after dynamic partitions 
WITHOUT_CLASSIFICATION	 same hashs default seed 
WITHOUT_CLASSIFICATION	 the jsonobject for this vertex 
WITHOUT_CLASSIFICATION	 the ndv the minimum the and the 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that left semi join singlecolumn long using hash set 
WITHOUT_CLASSIFICATION	 findreadslot key key slot slot pairindex pairindex found key 
WITHOUT_CLASSIFICATION	 virtual columns 
WITHOUT_CLASSIFICATION	 fake two live session 
WITHOUT_CLASSIFICATION	 for each range have intersect them they dont overlap the range can discarded 
WITHOUT_CLASSIFICATION	 and produce the correlated variables the new output 
WITHOUT_CLASSIFICATION	 task contains operator which instructs 
WITHOUT_CLASSIFICATION	 perform another major compaction nothing should change both deltas and both base dirs should have the same name 
WITHOUT_CLASSIFICATION	 remove trailing 
WITHOUT_CLASSIFICATION	 todo should call this more often theory for date type time should never matter but 
WITHOUT_CLASSIFICATION	 this sync call that will feed data the consumer 
WITHOUT_CLASSIFICATION	 verify that rows were selected 
WITHOUT_CLASSIFICATION	 this transaction isnt skip over 
WITHOUT_CLASSIFICATION	 just check make sure base below not new 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 time 
WITHOUT_CLASSIFICATION	 mysql returns the string not wellformed double value but decided return null instead which more conservative 
WITHOUT_CLASSIFICATION	 matches rows matches rows 
WITHOUT_CLASSIFICATION	 timeout chosen make sure that even one iteration takes more than half the scripttimeout but less than scripttimeout will still 
WITHOUT_CLASSIFICATION	 update necessary 
WITHOUT_CLASSIFICATION	 suspect that like pushdown into jdo invalid see hive check for like here 
WITHOUT_CLASSIFICATION	 empty set cannot convert 
WITHOUT_CLASSIFICATION	 were using tss stats for mapjoin optimization check each branch and see theres any upstream operator join lateralview that can increase output data size 
WITHOUT_CLASSIFICATION	 each child should has its own outputobjinspector 
WITHOUT_CLASSIFICATION	 backtrack value columns crs prs 
WITHOUT_CLASSIFICATION	 the instance 
WITHOUT_CLASSIFICATION	 need get the columnaccessinfo and viewtotableschema for views 
WITHOUT_CLASSIFICATION	 remove the path with which alias associates 
WITHOUT_CLASSIFICATION	 sbappendchar 
WITHOUT_CLASSIFICATION	 recently evicted index used for next keyvalue count excluded rows from previous flush 
WITHOUT_CLASSIFICATION	 only columns present the batch and noncomplex types 
WITHOUT_CLASSIFICATION	 the same line 
WITHOUT_CLASSIFICATION	 one byte always available for writing 
WITHOUT_CLASSIFICATION	 internal name for expressions and estimate column statistics for expression 
WITHOUT_CLASSIFICATION	 only leader publishes instance uri endpoint which will used clients make connections via service discovery 
WITHOUT_CLASSIFICATION	 transfer columnvector objects from base batch outgoing batch 
WITHOUT_CLASSIFICATION	 update the output position for the cor vars only pass the cor 
WITHOUT_CLASSIFICATION	 make sure the vector was flattened 
WITHOUT_CLASSIFICATION	 test second argument with nulls 
WITHOUT_CLASSIFICATION	 create metrics directory not present 
WITHOUT_CLASSIFICATION	 stored directories dont care about the skew otherwise 
WITHOUT_CLASSIFICATION	 out range for whole batch 
WITHOUT_CLASSIFICATION	 simple distributeby goes here 
WITHOUT_CLASSIFICATION	 note may later have special logic pick old ams any 
WITHOUT_CLASSIFICATION	 get the job info from the configuration 
WITHOUT_CLASSIFICATION	 add the relevant database namespace writeentity 
WITHOUT_CLASSIFICATION	 nothing far 
WITHOUT_CLASSIFICATION	 context for current input file 
WITHOUT_CLASSIFICATION	 structentry 
WITHOUT_CLASSIFICATION	 table information yet looks like could valid 
WITHOUT_CLASSIFICATION	 egetkey alias can null case constant expressions see inputq 
WITHOUT_CLASSIFICATION	 check input can pruned 
WITHOUT_CLASSIFICATION	 operation not recognized set null and let upper level handle this case 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 then invalidate column stats 
WITHOUT_CLASSIFICATION	 need get footers 
WITHOUT_CLASSIFICATION	 test decimal scalar decimal column addition this used cover all the cases used the source code template 
WITHOUT_CLASSIFICATION	 regardless our matching result keep that information make multiple use 
WITHOUT_CLASSIFICATION	 there are aggregate functions grouping sets will need value generator 
WITHOUT_CLASSIFICATION	 assumption that environment has already been cleaned once globally hence each thread does not call cleanup and createsources again 
WITHOUT_CLASSIFICATION	 load the incremental dump and ensure does nothing and lastreplid remains same 
WITHOUT_CLASSIFICATION	 perform another major compaction 
WITHOUT_CLASSIFICATION	 this list may modified specific cli drivers mask strings that change every test 
WITHOUT_CLASSIFICATION	 generate input event 
WITHOUT_CLASSIFICATION	 there some information about the windowing functions that needs initialized during query compilation time and made available during the mapreduce tasks via plan serialization 
WITHOUT_CLASSIFICATION	 copied here until utility version this released orc 
WITHOUT_CLASSIFICATION	 the for the main map operator itself 
WITHOUT_CLASSIFICATION	 continue handle changes specific plan 
WITHOUT_CLASSIFICATION	 boolean 
WITHOUT_CLASSIFICATION	 various unsupported methods 
WITHOUT_CLASSIFICATION	 when the reducer encountered for the first time 
WITHOUT_CLASSIFICATION	 done for broadcast joins that includes the dummy parents 
WITHOUT_CLASSIFICATION	 continue range 
WITHOUT_CLASSIFICATION	 storage descriptor data 
WITHOUT_CLASSIFICATION	 for joinrs case its not possible generally merge child has less keypartition columns than parents 
WITHOUT_CLASSIFICATION	 gets swallowed remote mode 
WITHOUT_CLASSIFICATION	 need update the queryplans output well that postexec hook get executed this only needed for dynamic partitioning since for the the writeentity constructed compile time and the queryplan already contains that for writeentity creation deferred this stage need update 
WITHOUT_CLASSIFICATION	 although its likely valid exception will retry with cbo off anyway for tests would like avoid retrying catch cbo failures 
WITHOUT_CLASSIFICATION	 timestamp scalarscalar 
WITHOUT_CLASSIFICATION	 not forward compatible 
WITHOUT_CLASSIFICATION	 local mode outputcommitter hook not invoked hadoop 
WITHOUT_CLASSIFICATION	 setup mapjointables and serdes 
WITHOUT_CLASSIFICATION	 the way this works session pool will move back tez pool kill and will get reassigned back pool getrequest based user pool mapping only remove the session from active sessions list its pool will the queued getrequest processed 
WITHOUT_CLASSIFICATION	 get object cache 
WITHOUT_CLASSIFICATION	 make sure flow and double equality compare works 
WITHOUT_CLASSIFICATION	 negative test 
WITHOUT_CLASSIFICATION	 exprinfo the key 
WITHOUT_CLASSIFICATION	 its send cancel already completed future noop 
WITHOUT_CLASSIFICATION	 bail out empty list 
WITHOUT_CLASSIFICATION	 most the above will failed offers and takes due speed the thing 
WITHOUT_CLASSIFICATION	 there should call create partitions with batch sizes 
WITHOUT_CLASSIFICATION	 ignore changes the amount white space 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject 
WITHOUT_CLASSIFICATION	 new tai lue letter low bytes 
WITHOUT_CLASSIFICATION	 loop rewrite rest insert references 
WITHOUT_CLASSIFICATION	 the following steps seem roundabout but they are meant aid recovery failure occurs and keep consistent state the 
WITHOUT_CLASSIFICATION	 the value repeating use row 
WITHOUT_CLASSIFICATION	 match this configuration before merging else will not merged 
WITHOUT_CLASSIFICATION	 class expressionbuilder 
WITHOUT_CLASSIFICATION	 state close doesnt mean all children are also state close 
WITHOUT_CLASSIFICATION	 the vectorized mapoperator there are modes reading for vectorization one for the vectorized input file format which returns vectorizedrowbatch the row one for using deserialize each row into the vectorizedrowbatch currently these input file formats textfile sequencefile and one using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow this picks input file format not supported the other two 
WITHOUT_CLASSIFICATION	 probably not local filesystem need check 
WITHOUT_CLASSIFICATION	 enable trash can tested hadoop fstrashintervalkey hadoop 
WITHOUT_CLASSIFICATION	 case fail the reader sending back the error received from the reader event 
WITHOUT_CLASSIFICATION	 create the jetty server jetty conf file exists use that create server 
WITHOUT_CLASSIFICATION	 stripped down version fastsetfrombytes 
WITHOUT_CLASSIFICATION	 this hook verifies that the location every output table empty 
WITHOUT_CLASSIFICATION	 test lazybinaryserde 
WITHOUT_CLASSIFICATION	 the select list 
WITHOUT_CLASSIFICATION	 just rename the directory 
WITHOUT_CLASSIFICATION	 join has been automatically converted into sortmerge join create conditional task try mapside join with each table the big table similar hiveautoconvertjoin but only applicable joins which have been automatically converted sortmerge joins for hiveautoconvertjoin the backup task the mapreduce join whereas here the backup task the sortmerge join depending the inputs sortmerge join may faster slower than the mapside join the other advantage sortmerge join that the output also bucketed and sorted consider very big table say with buckets being joined with very small table say with buckets the sortmerge join may perform slower since will restricted mappers 
WITHOUT_CLASSIFICATION	 for now top level can have where clause predicate 
WITHOUT_CLASSIFICATION	 use this buffer hold columns cells value length for usages 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 ensure that dont try read any data case skip read 
WITHOUT_CLASSIFICATION	 doesnt matter wait for all inputs any input ready 
WITHOUT_CLASSIFICATION	 this the root the partition which the file located 
WITHOUT_CLASSIFICATION	 make sure the signature works 
WITHOUT_CLASSIFICATION	 root abstract class for hash table result 
WITHOUT_CLASSIFICATION	 verify droptable recycle table files 
WITHOUT_CLASSIFICATION	 are going use lbserde serialize values create for retrieval 
WITHOUT_CLASSIFICATION	 couldnt find the from that contains subquery replace with allcolref 
WITHOUT_CLASSIFICATION	 etl strategy requested through config 
WITHOUT_CLASSIFICATION	 hmmthis looks bit wierdsetup boots qtestutilthis part used beforeclass 
WITHOUT_CLASSIFICATION	 create rexnode for lhs 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this part reducekeys later used create column names strictly for nondistinct aggregates with parameters same distinct keys which expects col the end always append col the end instead coli 
WITHOUT_CLASSIFICATION	 load newthreshold newthreshold 
WITHOUT_CLASSIFICATION	 allocator uses memory manager request memory create the manager next 
WITHOUT_CLASSIFICATION	 reset ckpt and last repl keys empty set for allowing bootstrap load 
WITHOUT_CLASSIFICATION	 seems point the start the batch todo validate 
WITHOUT_CLASSIFICATION	 check reset operation 
WITHOUT_CLASSIFICATION	 set vector null object reference verify correct null handling 
WITHOUT_CLASSIFICATION	 join types should all the same for merging returns null 
WITHOUT_CLASSIFICATION	 communication error the default sql state override the sqlstate 
WITHOUT_CLASSIFICATION	 format the upgrade script name upgradexydbtypesql 
WITHOUT_CLASSIFICATION	 but the value rounded more scaling 
WITHOUT_CLASSIFICATION	 format are sure are getting the right value 
WITHOUT_CLASSIFICATION	 return all the known job ids for this user based the optional filter conditions example usages curl return all the job ids submitted hsubramaniyan curl return all the job ids that are visible hsubramaniyan curl return all the job ids for hsubramaniyan after job curl return the first atmost job ids submitted hsubramaniyan after job curl return the first atmost job ids submitted hsubramaniyan after sorting the job list lexicographically supporting pagination using jobid and numrecords parameters step get the start jobid jobxxx numrecords step issue curl command specifying the userdefined numrecords and jobid step list obtained from step has size equal numrecords retrieve the lists last record and get the job the last record jobyyyk else quit step set jobidjobyyyk and step param fields fields set the request will return full details the job fields missing will only return the job currently the value can only other values are not allowed and will throw exception param showall showall set true the request will return all jobs the user has permission view not only the jobs belonging the user param jobid jobid present the records whose job lexicographically greater than jobid are only returned for example jobid job the jobs whose job greater than job are returned the number records returned depends the value numrecords param numrecords the jobid and numrecords parameters are present the top numrecords records appearing after jobid will returned after sorting the job list lexicographically jobid parameter missing and numrecords present the top numrecords will returned after lexicographically sorting the job list jobid parameter present and numrecords missing all the records whose job greater than jobid are returned return list job items based the filter conditions specified the user 
WITHOUT_CLASSIFICATION	 get delegation tokens from hcat server and store them into the job these will used publish partitions hcat normally when the jobtracker hadoop mapreduce starts supporting renewal 
WITHOUT_CLASSIFICATION	 use the hints later top level 
WITHOUT_CLASSIFICATION	 combine the column field schemas and the partition keys create the whole schema 
WITHOUT_CLASSIFICATION	 this set not empty means need generate separate task for collecting 
WITHOUT_CLASSIFICATION	 need merge isdirect flag input even the newinput does not have parent 
WITHOUT_CLASSIFICATION	 supplying using this enforces identityequls matching which will most probably make the signature very unique 
WITHOUT_CLASSIFICATION	 the the jobhandle used track the actual spark job 
WITHOUT_CLASSIFICATION	 the lack special token 
WITHOUT_CLASSIFICATION	 number digits mantissa 
WITHOUT_CLASSIFICATION	 max time when waiting for read locks node list 
WITHOUT_CLASSIFICATION	 all fractional digits become integer digits 
WITHOUT_CLASSIFICATION	 most likely the user specified invalid partition 
WITHOUT_CLASSIFICATION	 dummy vertex for mergejoin branch 
WITHOUT_CLASSIFICATION	 best attempt shouldnt really kill dag for this 
WITHOUT_CLASSIFICATION	 number nodes stack current mark 
WITHOUT_CLASSIFICATION	 whether there tag added the end each key and the tag value 
WITHOUT_CLASSIFICATION	 copy the hive conf into the job conf and restore the backend context 
WITHOUT_CLASSIFICATION	 add some data and nulls 
WITHOUT_CLASSIFICATION	 get the table objects for this batch table names and get iterator 
WITHOUT_CLASSIFICATION	 adjust arrays 
WITHOUT_CLASSIFICATION	 nodes one them column and the other numeric const 
WITHOUT_CLASSIFICATION	 can happen with virtual columns would add the column its output columns but would not exist the grandparent output columns exprmap 
WITHOUT_CLASSIFICATION	 number elements map cannot determined this value will used 
WITHOUT_CLASSIFICATION	 child should join for this happen 
WITHOUT_CLASSIFICATION	 the only aba problem care about have another buffer there 
WITHOUT_CLASSIFICATION	 adjustable 
WITHOUT_CLASSIFICATION	 assuming returns all schemes that are accessed task side well not need way get all the schemes that are accessed the tez taskllap 
WITHOUT_CLASSIFICATION	 figure out subquery expression columns type 
WITHOUT_CLASSIFICATION	 individual columns are going pivoted hbase cells and for each row they need written out order column name sort the column names now creating mapping their column position however the first 
WITHOUT_CLASSIFICATION	 setup run concurrent operations 
WITHOUT_CLASSIFICATION	 scalar and trivial evaluate 
WITHOUT_CLASSIFICATION	 test that read columns are initially empty list 
WITHOUT_CLASSIFICATION	 found some columns user specified schema which are neither regular not dynamic partition columns 
WITHOUT_CLASSIFICATION	 the job argument passed that configuration overrides can used initialize the metastore configuration the special case embedded metastore hivemetastoreuris 
WITHOUT_CLASSIFICATION	 create selectlistoi 
WITHOUT_CLASSIFICATION	 principalgrants 
WITHOUT_CLASSIFICATION	 expected error 
WITHOUT_CLASSIFICATION	 cannot merge 
WITHOUT_CLASSIFICATION	 genericudf stateful have make copy here 
WITHOUT_CLASSIFICATION	 the record reader from which the record originated already seen and valid need reencode the record 
WITHOUT_CLASSIFICATION	 parameters for exporting metadata table drop requires the use the 
WITHOUT_CLASSIFICATION	 print parent where data comes from 
WITHOUT_CLASSIFICATION	 set and table 
WITHOUT_CLASSIFICATION	 according the javadoc getmax can return this case default this will probably never actually happen 
WITHOUT_CLASSIFICATION	 this hash function returns the same result stringhashcode when all characters are ascii while texthashcode always returns different result 
WITHOUT_CLASSIFICATION	 end struct 
WITHOUT_CLASSIFICATION	 load data 
WITHOUT_CLASSIFICATION	 custom composite key class provided return null 
WITHOUT_CLASSIFICATION	 for hbase storage handler 
WITHOUT_CLASSIFICATION	 likewise 
WITHOUT_CLASSIFICATION	 sign with different key 
WITHOUT_CLASSIFICATION	 gen join between outer operator and 
WITHOUT_CLASSIFICATION	 see class comment about refcounts 
WITHOUT_CLASSIFICATION	 determine type udaf this the genericudaf name 
WITHOUT_CLASSIFICATION	 left larger 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 verify that the names match the partitioned clause 
WITHOUT_CLASSIFICATION	 set the link between mapjoin and parent vertex 
WITHOUT_CLASSIFICATION	 newer versions and later support offsetfetch 
WITHOUT_CLASSIFICATION	 repeating 
WITHOUT_CLASSIFICATION	 did read all the data 
WITHOUT_CLASSIFICATION	 read the altered via cachedstore altered user from user user 
WITHOUT_CLASSIFICATION	 srctxntowriteidlist 
WITHOUT_CLASSIFICATION	 the join columns which are also skewed 
WITHOUT_CLASSIFICATION	 this succeeds aborttxn idempotent 
WITHOUT_CLASSIFICATION	 copy the bigtable values into the overflow batch since the overflow batch may not get flushed here must copy value 
WITHOUT_CLASSIFICATION	 use hiveinputformat any the paths not splittable 
WITHOUT_CLASSIFICATION	 check whether the join can combined with any its children 
WITHOUT_CLASSIFICATION	 case repeating nulls 
WITHOUT_CLASSIFICATION	 optional alias the column external name 
WITHOUT_CLASSIFICATION	 walk over all the sources which are guaranteed reduce sink operators 
WITHOUT_CLASSIFICATION	 when splitupdate enabled can choose not write any delta files when there are inserts such cases only the deletedeltas would written they are closed separately below 
WITHOUT_CLASSIFICATION	 sort all the inputs outputs lock needs acquired any partition read lock needs acquired all 
WITHOUT_CLASSIFICATION	 the input filesinkoperator dynamic partition enabled the tsmerge input schema needs include the partition column and the fsoutput should have 
WITHOUT_CLASSIFICATION	 note hive ast rowsrangephysical range values logical 
WITHOUT_CLASSIFICATION	 username 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check permutation project 
WITHOUT_CLASSIFICATION	 update maxlength length greater than the largest value seen far 
WITHOUT_CLASSIFICATION	 instantiated 
WITHOUT_CLASSIFICATION	 array null column values input objectinspectors 
WITHOUT_CLASSIFICATION	 value becomes zero for rounding beyond 
WITHOUT_CLASSIFICATION	 format the storage format statements 
WITHOUT_CLASSIFICATION	 dynamic partition usecase partition values were null not all were specified need figure out which keys are not specified 
WITHOUT_CLASSIFICATION	 longer best match more than one 
WITHOUT_CLASSIFICATION	 just bitwiseor the bits together size functions should the same 
WITHOUT_CLASSIFICATION	 this restricts macro creation privileged users 
WITHOUT_CLASSIFICATION	 for now for simplicity are doing just one directory one database come back use multiple databases once have the basic flow chain creating tasks place for database directory 
WITHOUT_CLASSIFICATION	 disk 
WITHOUT_CLASSIFICATION	 reach runlength here use the previous length and reset runlength 
WITHOUT_CLASSIFICATION	 were moving files around for acid write then the rules and paths are all different 
WITHOUT_CLASSIFICATION	 only consider the materialized view outdated forceoutdated true rebuild otherwise passed the test and use 
WITHOUT_CLASSIFICATION	 create 
WITHOUT_CLASSIFICATION	 for leaf dont anything 
WITHOUT_CLASSIFICATION	 verify cmrecyclepath api moves file cmroot dir 
WITHOUT_CLASSIFICATION	 default location hiveserver 
WITHOUT_CLASSIFICATION	 there are multiple aliases source not know how merge 
WITHOUT_CLASSIFICATION	 alternate 
WITHOUT_CLASSIFICATION	 flush memory limits were reached 
WITHOUT_CLASSIFICATION	 now only try the first partition the first partition doesnt contain enough size change normal mode 
WITHOUT_CLASSIFICATION	 await future result with timeout check the abort field occasionally its possible that the interrupt which comes along with abort suppressed some other operator 
WITHOUT_CLASSIFICATION	 this event can never occur does fail 
WITHOUT_CLASSIFICATION	 when either name value null the set method below will fail and throw 
WITHOUT_CLASSIFICATION	 there will not any tez job above this task 
WITHOUT_CLASSIFICATION	 the following data used compute partitioned tables ndv based partitions ndv when true global ndvs cannot accurately derived from partition ndvs because the domain column value two partitions can overlap there overlap then global ndv just the sum partition ndvs upperbound but there some overlay then global ndv can anywhere between sum partition ndvs overlap and same one the partition ndv domain column value all other partitions subset the domain value one the partition lowerboundbut under uniform distribution can roughly estimate the global ndv leveraging the minmax values and also guarantee that the estimation makes sense comparing the upperbound calculated sumnumdistincts and lowerbound calculated maxnumdistincts 
WITHOUT_CLASSIFICATION	 dump and load only truncate records 
WITHOUT_CLASSIFICATION	 same string 
WITHOUT_CLASSIFICATION	 none 
WITHOUT_CLASSIFICATION	 weve got what need mark the queue 
WITHOUT_CLASSIFICATION	 the keyhash missing the bloom filter then the value cannot exist any the spilled partition return nomatch 
WITHOUT_CLASSIFICATION	 windowing spec include the list further will examine its children ast nodes check whether there are aggregation functions within 
WITHOUT_CLASSIFICATION	 original bucket files should stay until cleaner kicks 
WITHOUT_CLASSIFICATION	 right trim and truncate slice byte array maximum number characters and place the result into element vector 
WITHOUT_CLASSIFICATION	 children 
WITHOUT_CLASSIFICATION	 the child selectoperator does not have the columnexprmap not need update the columnexprmap the parent selectoperator 
WITHOUT_CLASSIFICATION	 far same javamathbigdecimal but the scaling below specific ansi sql numeric 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this needed for tracking the dependencies for inputs along with their parents 
WITHOUT_CLASSIFICATION	 repartition new number splits 
WITHOUT_CLASSIFICATION	 directly deserialize with the caller reading fieldbyfield the lazysimple text serialization format the caller responsible for calling the read method for the right type each field after calling readnextfield reading some fields require results object receive value information separate results object created the caller initialization per different field even for the same type some type values are reference either bytes the deserialization buffer other type specific buffers those references are only valid until the next time set called 
WITHOUT_CLASSIFICATION	 match 
WITHOUT_CLASSIFICATION	 make sure map task environment points 
WITHOUT_CLASSIFICATION	 first parse the view query and create the materialization object 
WITHOUT_CLASSIFICATION	 represents collection rows that acted upon tablefunction windowfunction 
WITHOUT_CLASSIFICATION	 testing multibyte string with reference starting mid array 
WITHOUT_CLASSIFICATION	 group column found 
WITHOUT_CLASSIFICATION	 one the predicates then any other predicate with illegal add residual 
WITHOUT_CLASSIFICATION	 over all the destination tables 
WITHOUT_CLASSIFICATION	 second incremental 
WITHOUT_CLASSIFICATION	 schemaversion 
WITHOUT_CLASSIFICATION	 this compressed buffer need uncompress the buffer can comprise several disk ranges might need combine them 
WITHOUT_CLASSIFICATION	 simple rownum offset and rownum offset limit wont work will return nothing 
WITHOUT_CLASSIFICATION	 verify that there notifications available yet 
WITHOUT_CLASSIFICATION	 check each exprnodedesc 
WITHOUT_CLASSIFICATION	 move files 
WITHOUT_CLASSIFICATION	 update the connection 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaneturl 
WITHOUT_CLASSIFICATION	 two cases srcs has only src directory rename src directory destf also need copymove each file under the source directory avoid delete the destination directory the root hdfs encryption zone srcs must list files ensured 
WITHOUT_CLASSIFICATION	 curreader closed for only need footer buffer info from prereader 
WITHOUT_CLASSIFICATION	 wrap the inner parts the loop catch throwable that any errors the loop dont doom the entire thread 
WITHOUT_CLASSIFICATION	 take the empty buffer out the free list 
WITHOUT_CLASSIFICATION	 start heartbeat thread 
WITHOUT_CLASSIFICATION	 get the transactional tblproperties value 
WITHOUT_CLASSIFICATION	 duplicate names this should 
WITHOUT_CLASSIFICATION	 create task for this local work right now this local work shared 
WITHOUT_CLASSIFICATION	 only distinct nodes that are not part the key should added distexprnodes 
WITHOUT_CLASSIFICATION	 have just locked buffer that wasnt previously locked 
WITHOUT_CLASSIFICATION	 can use alter table partition rename convertnormalize the legacy partition column values should not enable the validation the old partition spec passed this command 
WITHOUT_CLASSIFICATION	 time which needs thread protected 
WITHOUT_CLASSIFICATION	 boolean long done with identityexpression boolean double done with standard long double cast see for remaining cast vectorexpression classes 
WITHOUT_CLASSIFICATION	 one element 
WITHOUT_CLASSIFICATION	 for now bigint going cast double throw error warning 
WITHOUT_CLASSIFICATION	 distinct value estimator 
WITHOUT_CLASSIFICATION	 since are using thrift part will not have the create time and last ddl time set since does not get updated the addpartition call likewise part and part set correctly that equals check doesnt fail 
WITHOUT_CLASSIFICATION	 removes tasks from the runninglist and sends out preempt request the system subsequent tasks will scheduled again once the deallocate request for the preempted 
WITHOUT_CLASSIFICATION	 update the existing row newlyconverted acid table 
WITHOUT_CLASSIFICATION	 single long value map optimized for vector map join 
WITHOUT_CLASSIFICATION	 operators for which there chance the optimization can applied 
WITHOUT_CLASSIFICATION	 could not find common category return null 
WITHOUT_CLASSIFICATION	 temporary till the external interface makes use single connection per instance 
WITHOUT_CLASSIFICATION	 make sure that the table alias and column alias are stored the column info 
WITHOUT_CLASSIFICATION	 these tests inherently cause exceptions written the test output logs this undesirable since you might appear someone looking the test output logs something failing when isnt 
WITHOUT_CLASSIFICATION	 note this relies the fact that always evict the entire column have the column data assume have all the streams need 
WITHOUT_CLASSIFICATION	 reserve spaces for the byte size the map which integer and takes four bytes 
WITHOUT_CLASSIFICATION	 are overwriting disable existing sources 
WITHOUT_CLASSIFICATION	 skip daytime part both dates are end the month 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 count the size for cost estimation later 
WITHOUT_CLASSIFICATION	 just created top level node for this jobid 
WITHOUT_CLASSIFICATION	 unique registered based submit response theoretically could get ping when the task valid but havent stored the unique yet tasknodeid null however the next heartbeats should get the value eventually and mark task alive 
WITHOUT_CLASSIFICATION	 hmm why dont many other operations here need locks 
WITHOUT_CLASSIFICATION	 can bail out 
WITHOUT_CLASSIFICATION	 saslkerberos properties 
WITHOUT_CLASSIFICATION	 for innersemi join for left outer join for right outer join 
WITHOUT_CLASSIFICATION	 optimize assuming that repeating listmap will run from from lengths the child vector sanity check the assumption that can start 
WITHOUT_CLASSIFICATION	 callback method used subclasses set the rawinputoi the evaluator 
WITHOUT_CLASSIFICATION	 optional string dagname 
WITHOUT_CLASSIFICATION	 reattempts are left upto the rpc layer theres failure reported after this mark all attempts running this node killed the node itself cannot killed from here thats only possible via the scheduler the assumption that theres failure communicate with the node will eventually timeout and more tasks will allocated 
WITHOUT_CLASSIFICATION	 now that have found real data emit sign byte necessary and negative fixup 
WITHOUT_CLASSIFICATION	 verify that new job requests have issues 
WITHOUT_CLASSIFICATION	 since warehouse path nonqualified the database should located second filesystem 
WITHOUT_CLASSIFICATION	 the rowid column string 
WITHOUT_CLASSIFICATION	 found suitable join keys add them key list ensuring that there nonequi join predicate appears the end the key list also mark the null filtering property 
WITHOUT_CLASSIFICATION	 used with primitive types 
WITHOUT_CLASSIFICATION	 metastore stats unavailable fallback old way 
WITHOUT_CLASSIFICATION	 age 
WITHOUT_CLASSIFICATION	 elt special case because can take variable number arguments 
WITHOUT_CLASSIFICATION	 varchar test 
WITHOUT_CLASSIFICATION	 reconfigure logj after settings via hiveconf are write into system properties 
WITHOUT_CLASSIFICATION	 try using both permanent functions 
WITHOUT_CLASSIFICATION	 need add all the estimates from the siblings this reduce sink 
WITHOUT_CLASSIFICATION	 does breadth first traversal the tasks 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 emptybuckets true 
WITHOUT_CLASSIFICATION	 generate single split typically happens when reading data out order queries order query returns rows files will exists input path 
WITHOUT_CLASSIFICATION	 todo ideally this should only need send the taskattemptid everything else should inferred from this passing parameters until theres some dag information stored and tracked the daemon 
WITHOUT_CLASSIFICATION	 the null union type should removed 
WITHOUT_CLASSIFICATION	 this percentage value between and 
WITHOUT_CLASSIFICATION	 required required optional optional optional optional 
WITHOUT_CLASSIFICATION	 keyvalueseparator seen all bytes belong key and 
WITHOUT_CLASSIFICATION	 todo these can eventually used replace 
WITHOUT_CLASSIFICATION	 want message sent when session commits thus run transacted mode 
WITHOUT_CLASSIFICATION	 note this sets loadfiletype incorrectly for acid that relevant for load 
WITHOUT_CLASSIFICATION	 has been rewritten apply rules postdecorrelation 
WITHOUT_CLASSIFICATION	 since updates the childoperators and parentoperators list place need make copy list iterator over them 
WITHOUT_CLASSIFICATION	 values should unique given how the checking and addormerge 
WITHOUT_CLASSIFICATION	 enums are one two avro types that hive doesnt have any native support for 
WITHOUT_CLASSIFICATION	 mssql specific parser 
WITHOUT_CLASSIFICATION	 lateral view outer not supported cbo 
WITHOUT_CLASSIFICATION	 todo ideally want tez use callablewithmdc that retains the mdc for threads created thread pool for now will push both dagid and queryid into ndc and the custom thread pool that use for task execution and llap will pop them 
WITHOUT_CLASSIFICATION	 define constants and local variables 
WITHOUT_CLASSIFICATION	 case broadcastjoin read the broadcast edge inputs possibly asynchronously 
WITHOUT_CLASSIFICATION	 extract name will need afterwards 
WITHOUT_CLASSIFICATION	 now check for overflow 
WITHOUT_CLASSIFICATION	 existence 
WITHOUT_CLASSIFICATION	 config parameter that suggests hcat that metastore clients not cached default false this parameter allows highlyparallel hcat usescases not gobble too many connections that 
WITHOUT_CLASSIFICATION	 remove the entry theres nothing left the specific priority level 
WITHOUT_CLASSIFICATION	 should still able get the session 
WITHOUT_CLASSIFICATION	 set values needed for numeric arithmetic udfs 
WITHOUT_CLASSIFICATION	 hasnext implies there some column the batch 
WITHOUT_CLASSIFICATION	 can not queue more requests queue full 
WITHOUT_CLASSIFICATION	 convert integer value milliseconds since the epoch timestamp value for use long column vector which represented nanoseconds since the epoch 
WITHOUT_CLASSIFICATION	 miscellaneous errors range 
WITHOUT_CLASSIFICATION	 found ugi perform doas 
WITHOUT_CLASSIFICATION	 check any right pair exists for left objects 
WITHOUT_CLASSIFICATION	 gen calcite plan 
WITHOUT_CLASSIFICATION	 test for valid values for both 
WITHOUT_CLASSIFICATION	 now working should sorted like delta delta delta delta for example and want end with the best set containing all relevant data delta delta 
WITHOUT_CLASSIFICATION	 use this copy method when the source batch may get reused before the target batch finished any bytes column vector values will copied the target value into the columns data buffer 
WITHOUT_CLASSIFICATION	 get array utf byte arrays from array strings 
WITHOUT_CLASSIFICATION	 and then compare the two tables 
WITHOUT_CLASSIFICATION	 special case date requires its own specific betweendynamicvalue class but derives from 
WITHOUT_CLASSIFICATION	 others should kept 
WITHOUT_CLASSIFICATION	 after that inputop the parent selop 
WITHOUT_CLASSIFICATION	 not real field 
WITHOUT_CLASSIFICATION	 initialize metrics first some metrics are for initialization stuff 
WITHOUT_CLASSIFICATION	 the maximum column length mfieldschemafname 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 try widening conversion otherwise fail union 
WITHOUT_CLASSIFICATION	 look through the file with rows selected 
WITHOUT_CLASSIFICATION	 select none child none child and none 
WITHOUT_CLASSIFICATION	 rowmode the expected value 
WITHOUT_CLASSIFICATION	 move all data from destsequencefile dest 
WITHOUT_CLASSIFICATION	 split join condition 
WITHOUT_CLASSIFICATION	 register that have visited this operator this rule 
WITHOUT_CLASSIFICATION	 the following complex type for special handling 
WITHOUT_CLASSIFICATION	 mixture input columns and new scratch columns for the aggregation output 
WITHOUT_CLASSIFICATION	 verify that are indeed doing acid write import 
WITHOUT_CLASSIFICATION	 dbpatterns 
WITHOUT_CLASSIFICATION	 thru the blocks add stuff results and prepare the decompression work see below 
WITHOUT_CLASSIFICATION	 the lowest digit power 
WITHOUT_CLASSIFICATION	 arrange result has longer digit tail and lines will shift the shift digits our addition and them into the result 
WITHOUT_CLASSIFICATION	 this has state cant cached 
WITHOUT_CLASSIFICATION	 not want the start group clear the storage 
WITHOUT_CLASSIFICATION	 next locate all the iterate methods for each these classes 
WITHOUT_CLASSIFICATION	 set the config value empty string which should result all catalogs being cached 
WITHOUT_CLASSIFICATION	 create and put hiverc sample file default directory 
WITHOUT_CLASSIFICATION	 this purely for testing convenience has bearing operations such list 
WITHOUT_CLASSIFICATION	 ssl settings 
WITHOUT_CLASSIFICATION	 get the distinct values the group fields and the arguments the agg functions 
WITHOUT_CLASSIFICATION	 catchall due some exec time dependencies session state that would cause otherwise 
WITHOUT_CLASSIFICATION	 are revoking another duck dont wait could also give the duck 
WITHOUT_CLASSIFICATION	 todo lossy conversion distance considered seconds 
WITHOUT_CLASSIFICATION	 add new operator cache work group existing operator group exists 
WITHOUT_CLASSIFICATION	 store the given token the ugi 
WITHOUT_CLASSIFICATION	 calculate the parameters 
WITHOUT_CLASSIFICATION	 get the parent victimrs 
WITHOUT_CLASSIFICATION	 columncount 
WITHOUT_CLASSIFICATION	 for cast constant operator all members the input list and return new list containing results 
WITHOUT_CLASSIFICATION	 insert reduce side with reduce side input 
WITHOUT_CLASSIFICATION	 only one distinct aggregate and one more nondistinct aggregates 
WITHOUT_CLASSIFICATION	 select all with not null child none child and null with and then expect the child not invoked 
WITHOUT_CLASSIFICATION	 positions variable arguments columns nonconstant expressions 
WITHOUT_CLASSIFICATION	 just can get the output type 
WITHOUT_CLASSIFICATION	 reads through undesired field data values are valid after this call designed for skipping columns that are not included 
WITHOUT_CLASSIFICATION	 these members have information for deserializing row into the vectorizedrowbatch columns say source because when there conversion are converting deserialized source into target data type 
WITHOUT_CLASSIFICATION	 partition columns are appended end only care about stats column 
WITHOUT_CLASSIFICATION	 createdat 
WITHOUT_CLASSIFICATION	 list doubles 
WITHOUT_CLASSIFICATION	 not want the end group cause checkandgenobject 
WITHOUT_CLASSIFICATION	 when there are partition and order clauses will have different partition expressions 
WITHOUT_CLASSIFICATION	 have remember 
WITHOUT_CLASSIFICATION	 the character set for encoding constant can optimize that 
WITHOUT_CLASSIFICATION	 this time even more inaccurate 
WITHOUT_CLASSIFICATION	 replace the distinct with the count aggregation 
WITHOUT_CLASSIFICATION	 for hive script operator 
WITHOUT_CLASSIFICATION	 technique 
WITHOUT_CLASSIFICATION	 since wont able update this add for now estimate usage this shouldnt much and this cache should remove later anyway 
WITHOUT_CLASSIFICATION	 for nonllap mode most these are not relevant only used shared scan optimizer 
WITHOUT_CLASSIFICATION	 since row mode takes everyone 
WITHOUT_CLASSIFICATION	 does need additional job 
WITHOUT_CLASSIFICATION	 todo set correct vendorcode field 
WITHOUT_CLASSIFICATION	 the current expression node does not have virtualpartition column 
WITHOUT_CLASSIFICATION	 create 
WITHOUT_CLASSIFICATION	 the current char will written out later 
WITHOUT_CLASSIFICATION	 examine the last child could alias 
WITHOUT_CLASSIFICATION	 returnafteruse will take care this 
WITHOUT_CLASSIFICATION	 decimal longwords 
WITHOUT_CLASSIFICATION	 not need anything the expression 
WITHOUT_CLASSIFICATION	 extraction need parsed 
WITHOUT_CLASSIFICATION	 the type cast udf for parameterized type then should implement the settableudf interface that can pass the params not sure this the cleanest solution but there does need way 
WITHOUT_CLASSIFICATION	 list struct 
WITHOUT_CLASSIFICATION	 valid merge register set size gets bigger dense automatically 
WITHOUT_CLASSIFICATION	 looks like subq plan 
WITHOUT_CLASSIFICATION	 should used for either sort merge join bucket map join 
WITHOUT_CLASSIFICATION	 well 
WITHOUT_CLASSIFICATION	 primitive 
WITHOUT_CLASSIFICATION	 char not between 
WITHOUT_CLASSIFICATION	 generate map join task for the big table 
WITHOUT_CLASSIFICATION	 output type intervaldaytime 
WITHOUT_CLASSIFICATION	 property speficied file found local file system use the specified file 
WITHOUT_CLASSIFICATION	 same commitdroptable where always delete the data accumulo table 
WITHOUT_CLASSIFICATION	 callers duplicate the buffer they have for bufferchunk dont have 
WITHOUT_CLASSIFICATION	 all must selected otherwise size would zero repeating property will not change 
WITHOUT_CLASSIFICATION	 requires that current directory exists 
WITHOUT_CLASSIFICATION	 the isnull check and work has already been performed 
WITHOUT_CLASSIFICATION	 copy the current object contents into the output only copy selected entries 
WITHOUT_CLASSIFICATION	 check for delegation token present add the header 
WITHOUT_CLASSIFICATION	 preserving the old logic hmm 
WITHOUT_CLASSIFICATION	 have already explored the stack deep enough but not have matching 
WITHOUT_CLASSIFICATION	 for nway join only spill big table rows once 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring 
WITHOUT_CLASSIFICATION	 update aggregate partition column stats for table cache 
WITHOUT_CLASSIFICATION	 add udaf 
WITHOUT_CLASSIFICATION	 datetime type isnt currently supported 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 helper classes for connparam comparison logics 
WITHOUT_CLASSIFICATION	 handle case with nulls dont function the value null save time because calling the function can expensive 
WITHOUT_CLASSIFICATION	 should constant column 
WITHOUT_CLASSIFICATION	 delete remaining directories for external tables can affect stats for following tests 
WITHOUT_CLASSIFICATION	 note are creating brand new the partition this going valid for acid 
WITHOUT_CLASSIFICATION	 than the configured the header size 
WITHOUT_CLASSIFICATION	 eat 
WITHOUT_CLASSIFICATION	 node exists throw exception 
WITHOUT_CLASSIFICATION	 part part 
WITHOUT_CLASSIFICATION	 for locks associated with txn always heartbeat txn and timeout based that 
WITHOUT_CLASSIFICATION	 clean time out locks from the database not associated with transactions locks for readonly autocommittrue statements this does commit and thus should done before any calls heartbeat that will leave 
WITHOUT_CLASSIFICATION	 only creates the expiration tracker expiration configured 
WITHOUT_CLASSIFICATION	 look for row like info query 
WITHOUT_CLASSIFICATION	 find operators work 
WITHOUT_CLASSIFICATION	 adding rowid field 
WITHOUT_CLASSIFICATION	 check owid outside the range all owids present 
WITHOUT_CLASSIFICATION	 also has one outer join filter associated with ack which making 
WITHOUT_CLASSIFICATION	 add dummy node cache partnames tabparttabpart 
WITHOUT_CLASSIFICATION	 testperformancetest 
WITHOUT_CLASSIFICATION	 its wrapped toplevel select star query skip ambiguity check for backward compatibility 
WITHOUT_CLASSIFICATION	 initialize any entries that could used output vector have false for null value 
WITHOUT_CLASSIFICATION	 should since the lock ephemeral and will eventually deleted when the query finishes and zookeeper session closed 
WITHOUT_CLASSIFICATION	 use for maponly merging 
WITHOUT_CLASSIFICATION	 there project top due nullability 
WITHOUT_CLASSIFICATION	 running example 
WITHOUT_CLASSIFICATION	 any sensible way for now the lock going epic 
WITHOUT_CLASSIFICATION	 does the row match the pattern represented this symbolfunction 
WITHOUT_CLASSIFICATION	 matchstats for each candidate 
WITHOUT_CLASSIFICATION	 temporarily holds location exponent string 
WITHOUT_CLASSIFICATION	 bytes remaining the current chunk data 
WITHOUT_CLASSIFICATION	 does not add back task here because back task should the same type the original task 
WITHOUT_CLASSIFICATION	 for equalpriority rules user rules come first because they are more specific then apps 
WITHOUT_CLASSIFICATION	 make sure the dpp sink has output for all the corresponding part columns 
WITHOUT_CLASSIFICATION	 now get nonexistant entry 
WITHOUT_CLASSIFICATION	 tests multimap structure 
WITHOUT_CLASSIFICATION	 rollback done for the benefit postgres which throws sqlstatep errorcode you attempt any stmt txn which had error 
WITHOUT_CLASSIFICATION	 the configuration 
WITHOUT_CLASSIFICATION	 not reducesink skip 
WITHOUT_CLASSIFICATION	 optimize whole decimal fits one binary word 
WITHOUT_CLASSIFICATION	 need subtract nwinext would the next write allocated but need highest allocated write 
WITHOUT_CLASSIFICATION	 more files for current bucket 
WITHOUT_CLASSIFICATION	 the time and number counters become available only after the 
WITHOUT_CLASSIFICATION	 add minmax and bloom filter aggregations 
WITHOUT_CLASSIFICATION	 some cfcq 
WITHOUT_CLASSIFICATION	 limit reached batchsize reduces whichever comes earlier 
WITHOUT_CLASSIFICATION	 move files one one because source subdirectory destination 
WITHOUT_CLASSIFICATION	 hadoop only expects username query param not form param post request for backwards compatibility this logic get username when its sent form parameter this added hive and should desupported 
WITHOUT_CLASSIFICATION	 scan 
WITHOUT_CLASSIFICATION	 the number partitions aggregated per cache node the number partitions requested this value well fetch directly from metastore 
WITHOUT_CLASSIFICATION	 fkcolumnname 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 ctas with nonacid target table 
WITHOUT_CLASSIFICATION	 cannot static because default unique perinstance 
WITHOUT_CLASSIFICATION	 fractional part powered too high 
WITHOUT_CLASSIFICATION	 there ptf between crs and prs cannot ignore the order direction 
WITHOUT_CLASSIFICATION	 buckets means turn off bucketing 
WITHOUT_CLASSIFICATION	 same thing might deleted other nodes just 
WITHOUT_CLASSIFICATION	 launch upto maxthreads tasks 
WITHOUT_CLASSIFICATION	 allownull 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 read the first row parquet data page this will only happened once for this instance 
WITHOUT_CLASSIFICATION	 return true for exprnodecolumndesc 
WITHOUT_CLASSIFICATION	 not nullable union 
WITHOUT_CLASSIFICATION	 call open read data split mockmocktable call split mockmocktable call open read data split mockmocktable call split mockmocktable call read footer split mockmocktable get offset since its original file 
WITHOUT_CLASSIFICATION	 check second most significant part 
WITHOUT_CLASSIFICATION	 always need call reset the codec 
WITHOUT_CLASSIFICATION	 find the list scripts execute for this upgrade 
WITHOUT_CLASSIFICATION	 for now allow only createview with select with grant 
WITHOUT_CLASSIFICATION	 add additional overhead each map entries 
WITHOUT_CLASSIFICATION	 and now wander straight into the swamp when instead adding subtract from utc midnight supposedly get local midnight the above case utc course given 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 are done reading batch send consumer for processing 
WITHOUT_CLASSIFICATION	 try genericudf translation 
WITHOUT_CLASSIFICATION	 correct version stored metastore during startup 
WITHOUT_CLASSIFICATION	 there should calls create partitions with each batch size 
WITHOUT_CLASSIFICATION	 should not happen 
WITHOUT_CLASSIFICATION	 dynamic allocation enabled numbers for memory and cores are meaningless dont try get 
WITHOUT_CLASSIFICATION	 when longer assume the caller will default with nulls etc 
WITHOUT_CLASSIFICATION	 for small tables only get the big table position first 
WITHOUT_CLASSIFICATION	 note the list expected few items its longer may want ihm 
WITHOUT_CLASSIFICATION	 inject behaviour where throws exception insert event found dynamically add partition through insert into cmd should just add addpartition event not insert event 
WITHOUT_CLASSIFICATION	 column aliases defined query for lateral view output are duplicated 
WITHOUT_CLASSIFICATION	 then serialize again using hrsd and compare results 
WITHOUT_CLASSIFICATION	 show table level privileges 
WITHOUT_CLASSIFICATION	 assert values retrieved 
WITHOUT_CLASSIFICATION	 inject behaviour where some events missing from notificationlog table 
WITHOUT_CLASSIFICATION	 simply need remember that weve seen event operator 
WITHOUT_CLASSIFICATION	 add window functions 
WITHOUT_CLASSIFICATION	 path must reused 
WITHOUT_CLASSIFICATION	 bytes necessary store extra bits the second timestamp storing timestamp 
WITHOUT_CLASSIFICATION	 update table level column stats 
WITHOUT_CLASSIFICATION	 current joinoperaotr will stop traverse the tree when any parent reducesinkoperaotr this joinoperator not considered correlated reducesinkoperator 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 sum input decimal and output decimal any mode partial partial final complete 
WITHOUT_CLASSIFICATION	 addpartition event partitioned table 
WITHOUT_CLASSIFICATION	 task failed 
WITHOUT_CLASSIFICATION	 called one more times the client and 
WITHOUT_CLASSIFICATION	 must deterministic order map for comparison across java versions 
WITHOUT_CLASSIFICATION	 patch the optimized query back into original ctas ast replacing the original query 
WITHOUT_CLASSIFICATION	 this works because logically need lock nonacidorctbl read and lock write but 
WITHOUT_CLASSIFICATION	 check the bucketing andor sorting columns were inferred 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlrowid 
WITHOUT_CLASSIFICATION	 mutator created 
WITHOUT_CLASSIFICATION	 try either yyyymmdd integer representing days since epoch 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 move the intermediate archived directory the original parent directory 
WITHOUT_CLASSIFICATION	 process the last byte 
WITHOUT_CLASSIFICATION	 either inittxnmgr from the sessionstate that order 
WITHOUT_CLASSIFICATION	 todo look repeating optimizations 
WITHOUT_CLASSIFICATION	 the partition spec not present 
WITHOUT_CLASSIFICATION	 non acid txn managers dont support txns but fwd lock requests lock managers acid txn manager requires all locks associated with txn end here open txn its because are processing something like use database which definition needs locks 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 columnreference node column the input row 
WITHOUT_CLASSIFICATION	 new threads 
WITHOUT_CLASSIFICATION	 operations involvingreturning yearmonth intervals 
WITHOUT_CLASSIFICATION	 estimate number reducers 
WITHOUT_CLASSIFICATION	 this type information specifies the data types the partition needs read 
WITHOUT_CLASSIFICATION	 read totalseconds nanos from datainput 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqldate 
WITHOUT_CLASSIFICATION	 filter will executed for constant 
WITHOUT_CLASSIFICATION	 handle repeated join key found 
WITHOUT_CLASSIFICATION	 stop the composite service 
WITHOUT_CLASSIFICATION	 holds the root the operator tree were currently processing this could table scan but also join ptf etc 
WITHOUT_CLASSIFICATION	 load the connection url properties from hivesitexml present the classpath 
WITHOUT_CLASSIFICATION	 extract drop privileges 
WITHOUT_CLASSIFICATION	 check the partitions exist the sourcetable 
WITHOUT_CLASSIFICATION	 not necessary here cause noone will looking these after set them for clarity 
WITHOUT_CLASSIFICATION	 ifexists 
WITHOUT_CLASSIFICATION	 need initialize those muxoperators first because first initialize other operators the states all parents those muxoperators are init including this demuxoperator but the inputinspector those muxoperators has not been set 
WITHOUT_CLASSIFICATION	 date part 
WITHOUT_CLASSIFICATION	 neginfinity end inclusive 
WITHOUT_CLASSIFICATION	 statement 
WITHOUT_CLASSIFICATION	 prevent instantiation 
WITHOUT_CLASSIFICATION	 set the table where were writing this data 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 stop 
WITHOUT_CLASSIFICATION	 now have probe the global hash and findorallocate 
WITHOUT_CLASSIFICATION	 populate reduce task 
WITHOUT_CLASSIFICATION	 for scale the minimum 
WITHOUT_CLASSIFICATION	 newcols are not specified use default ones 
WITHOUT_CLASSIFICATION	 find out the nullbytes 
WITHOUT_CLASSIFICATION	 its partial line continue collecting the pieces 
WITHOUT_CLASSIFICATION	 source 
WITHOUT_CLASSIFICATION	 tez can handle unpopulated buckets 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 assumption this point parse tree gen resolution will always true since started out that way 
WITHOUT_CLASSIFICATION	 instantiation 
WITHOUT_CLASSIFICATION	 populate stage 
WITHOUT_CLASSIFICATION	 this dummy assigner 
WITHOUT_CLASSIFICATION	 the trailing zeroes extend into the integer part only want eliminate the fractional zero digits 
WITHOUT_CLASSIFICATION	 for ctas runs late make table acid the initial write ends running nonacid 
WITHOUT_CLASSIFICATION	 semi join specific members 
WITHOUT_CLASSIFICATION	 join conditions 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 return true this one small set functions for which significantly easier use the old code path vectorized mode instead implementing new optimized vectorexpression depending performance requirements and frequency use these may implemented the future with optimized vectorexpression 
WITHOUT_CLASSIFICATION	 log exception this produces enough text force new logfile 
WITHOUT_CLASSIFICATION	 returncode 
WITHOUT_CLASSIFICATION	 get the power turn digits into the desired decimal with possible fractional part 
WITHOUT_CLASSIFICATION	 because use hives string type when avro calls for enum have expressly check for enumness 
WITHOUT_CLASSIFICATION	 the difference larger than then definitely larger than power increment 
WITHOUT_CLASSIFICATION	 once drop support for old hadoop versions change these getbytes and getlength fix the deprecation warnings not worth shim 
WITHOUT_CLASSIFICATION	 assert that and are not the same within epsilon tolerance 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 check immediately after reducer assigned cae the abort came during 
WITHOUT_CLASSIFICATION	 use the rowresolver from the input operator generate input objectinspector that can used initialize the udtf then the resulting output object inspector can used make the rowresolver 
WITHOUT_CLASSIFICATION	 exchange operator introduced later make sort operator create new stage for the moment 
WITHOUT_CLASSIFICATION	 update this information sparkworkmap 
WITHOUT_CLASSIFICATION	 build operator 
WITHOUT_CLASSIFICATION	 oldernode tasks proactively for now let the heartbeats fail them 
WITHOUT_CLASSIFICATION	 multitable inserts not supported 
WITHOUT_CLASSIFICATION	 then just look the other locks 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 zone zone should already have been checked for nulls 
WITHOUT_CLASSIFICATION	 sanity check 
WITHOUT_CLASSIFICATION	 use varchars text field directly 
WITHOUT_CLASSIFICATION	 modify table schema add columns 
WITHOUT_CLASSIFICATION	 this method does not depend setting 
WITHOUT_CLASSIFICATION	 typedesc 
WITHOUT_CLASSIFICATION	 partition spec node present set partition spec 
WITHOUT_CLASSIFICATION	 undone 
WITHOUT_CLASSIFICATION	 test for publish with missing partition key values 
WITHOUT_CLASSIFICATION	 more accurate information about the original ndv the column before any filtering 
WITHOUT_CLASSIFICATION	 have replication state record for the obj allow replacement 
WITHOUT_CLASSIFICATION	 read the type 
WITHOUT_CLASSIFICATION	 column stats for group column 
WITHOUT_CLASSIFICATION	 read the second flush and make sure see all rows 
WITHOUT_CLASSIFICATION	 not used the direct access client native vector map join 
WITHOUT_CLASSIFICATION	 this can happen only top most limit not while visiting limit operator since that can within subquery 
WITHOUT_CLASSIFICATION	 execdriver well have proper local properties 
WITHOUT_CLASSIFICATION	 need skip seek here index wont used anymore 
WITHOUT_CLASSIFICATION	 had the entire pool other list couldnt exist exhausted the entirepoolsized list 
WITHOUT_CLASSIFICATION	 plans 
WITHOUT_CLASSIFICATION	 memory hashmap stores small table keyvalue pairs stores big table rows 
WITHOUT_CLASSIFICATION	 row null for delete events 
WITHOUT_CLASSIFICATION	 missing database name the query 
WITHOUT_CLASSIFICATION	 for the generation the values expression just get the inputs 
WITHOUT_CLASSIFICATION	 resultexpression select list with the following variation the select keyword optional the parser checks the expression doesnt start with select not prefixes window clauses are not permitted expressions can operate the input columns plus the psuedo column path which array structs the shape the struct the same the input 
WITHOUT_CLASSIFICATION	 check can convert map join bucket scaling 
WITHOUT_CLASSIFICATION	 this implementation vectorized join delegating all the work the rowmode implementation hijacking the big table node evaluators and calling the rowmode join processop for each row the input batch since the join operator not fully vectorized anyway atm due the use rowmode smalltables this reasonable tradeoff 
WITHOUT_CLASSIFICATION	 followed call 
WITHOUT_CLASSIFICATION	 remove local copy hdfs location from resource map 
WITHOUT_CLASSIFICATION	 all together there only one security check 
WITHOUT_CLASSIFICATION	 get the application the spark app 
WITHOUT_CLASSIFICATION	 testtable 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlclob 
WITHOUT_CLASSIFICATION	 were replication scope its possible that were running the export long after the table was dropped the table not existing currently being different kind table not error simply means should noop and let future export capture the appropriate state 
WITHOUT_CLASSIFICATION	 involving local file system 
WITHOUT_CLASSIFICATION	 flip column references join condition specified reverse order join sources 
WITHOUT_CLASSIFICATION	 for simpler access make these members protected instead providing get methods 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 only support single expression when its udtf 
WITHOUT_CLASSIFICATION	 rehome location now that know the rest the partvals table table liststring partitioncols new arrayliststring 
WITHOUT_CLASSIFICATION	 create 
WITHOUT_CLASSIFICATION	 even table location specified table creation should fail 
WITHOUT_CLASSIFICATION	 verify table for key long hash table hashmultiset 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 only allow one single 
WITHOUT_CLASSIFICATION	 walk through udaf collect distinct info 
WITHOUT_CLASSIFICATION	 and round may cause exceed our precisionscale 
WITHOUT_CLASSIFICATION	 start with capacity make sure expand every put 
WITHOUT_CLASSIFICATION	 uri 
WITHOUT_CLASSIFICATION	 set value for the union type 
WITHOUT_CLASSIFICATION	 create the parent znodes recursively ignore the parent already exists 
WITHOUT_CLASSIFICATION	 myint 
WITHOUT_CLASSIFICATION	 uninitialized bucket 
WITHOUT_CLASSIFICATION	 for dynamic partitioned hash join assuming table split evenly among the reduce tasks 
WITHOUT_CLASSIFICATION	 note specify dynamic partitions desttab for writeentity 
WITHOUT_CLASSIFICATION	 see rowlength javadoc 
WITHOUT_CLASSIFICATION	 called the beginning the compile phase have another chance optimize the operator plan 
WITHOUT_CLASSIFICATION	 datapath 
WITHOUT_CLASSIFICATION	 multikey hash map based the 
WITHOUT_CLASSIFICATION	 last partial batch 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 all columns are dynamic nothing 
WITHOUT_CLASSIFICATION	 multiple children 
WITHOUT_CLASSIFICATION	 set the locations starting from startindex and wrapping around the sorted array 
WITHOUT_CLASSIFICATION	 determine the big table retained mapping first can optimize out with projection copying inner join big table keys the subsequent small table results section 
WITHOUT_CLASSIFICATION	 internal vars 
WITHOUT_CLASSIFICATION	 codahale just include the pool name the counter name 
WITHOUT_CLASSIFICATION	 probably cross product 
WITHOUT_CLASSIFICATION	 should noop since exists 
WITHOUT_CLASSIFICATION	 integer value was interpreted timestamp inconsistently milliseconds comparing floatdouble seconds since the issue exists for long time and some users may use such inconsistent way use the following flag keep backward compatible the flag set false integer value interpreted timestamp milliseconds otherwise its interpreted timestamp seconds 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 mouthful but safe guaranteed have atleast destination dont support multi insert picking the first dest 
WITHOUT_CLASSIFICATION	 adding oracle jdbc driver exists 
WITHOUT_CLASSIFICATION	 cvalue map rowvalues assertequals cvaluesize map mapval map assertequals mapvalsize mapval map assertequals mapvalsize 
WITHOUT_CLASSIFICATION	 ensure only one final event ever sent 
WITHOUT_CLASSIFICATION	 notify any queries waiting this cacheentry become valid 
WITHOUT_CLASSIFICATION	 looks like owner unsupported type 
WITHOUT_CLASSIFICATION	 verify drop partition nonexisting partition idempotent and just noop 
WITHOUT_CLASSIFICATION	 get here know that weve archived the partition files but they may the original partition location the intermediate original dir 
WITHOUT_CLASSIFICATION	 add not null constraints 
WITHOUT_CLASSIFICATION	 has use full name make sure does not conflict with 
WITHOUT_CLASSIFICATION	 use toolrunner files option could considered here 
WITHOUT_CLASSIFICATION	 build tokinsert tokdestination 
WITHOUT_CLASSIFICATION	 continue with next table 
WITHOUT_CLASSIFICATION	 return the maximum absolute decimal value for precision 
WITHOUT_CLASSIFICATION	 generate the columns according the column mapping provided note the generated column names are same the the qualifier name null each column familynamecoli where the index the column ranging from where the size the column mapping the filter function removes any special characters other than alphabets and numbers from the column family and qualifier name the only special character allowed column name which used separator between the column family and qualifier name 
WITHOUT_CLASSIFICATION	 set first argument boolean flag 
WITHOUT_CLASSIFICATION	 cli 
WITHOUT_CLASSIFICATION	 workers run concurrently 
WITHOUT_CLASSIFICATION	 now compact 
WITHOUT_CLASSIFICATION	 check the pruner only contains partition columns 
WITHOUT_CLASSIFICATION	 need for overflow checks assume selectivity always 
WITHOUT_CLASSIFICATION	 the union the first time seen set current task genmrunionctx 
WITHOUT_CLASSIFICATION	 requesting less partitions than allowed should work 
WITHOUT_CLASSIFICATION	 position the cursor line 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 need execute this stage 
WITHOUT_CLASSIFICATION	 can happen that although therere some partitions memory but their sizes are all 
WITHOUT_CLASSIFICATION	 the join key table column create the exprnodedesc based this column 
WITHOUT_CLASSIFICATION	 result 
WITHOUT_CLASSIFICATION	 the dispatcher fires the processor corresponding the closest matching 
WITHOUT_CLASSIFICATION	 create new sort operator the corresponding input 
WITHOUT_CLASSIFICATION	 they set ifnotexists check for existence first and bail exists this 
WITHOUT_CLASSIFICATION	 return table name for column name column has been specified 
WITHOUT_CLASSIFICATION	 hive renumber the position after remove such that getposition column always returns value between schemasize 
WITHOUT_CLASSIFICATION	 will create the folder does not exist 
WITHOUT_CLASSIFICATION	 cant push predicate delete delta were push delete delta wed filter out all rows since the row always null for delete events and wed produce data the delete never happened 
WITHOUT_CLASSIFICATION	 set the timezone the object mapper 
WITHOUT_CLASSIFICATION	 append the third group within pattern 
WITHOUT_CLASSIFICATION	 can just restart the session have received one 
WITHOUT_CLASSIFICATION	 bgenjjtree commaorsemicolon 
WITHOUT_CLASSIFICATION	 handle file format check for table level 
WITHOUT_CLASSIFICATION	 implicit conversion accepting the source type and putting the same target type columnvector type 
WITHOUT_CLASSIFICATION	 safety limit for potential list bugs 
WITHOUT_CLASSIFICATION	 look for tables but not find any 
WITHOUT_CLASSIFICATION	 operator can use the same 
WITHOUT_CLASSIFICATION	 end additional steps 
WITHOUT_CLASSIFICATION	 overwhelmingly executes once maybe twice replacing stale value 
WITHOUT_CLASSIFICATION	 store char type stripped pads 
WITHOUT_CLASSIFICATION	 skip this and rest cmds the line 
WITHOUT_CLASSIFICATION	 the algorithm looks all the mapjoins the operator pipeline until hits and for each mapjoin examines has paralllel semijoin edge dynamic partition pruning 
WITHOUT_CLASSIFICATION	 run the optimizations that use stats for optimization 
WITHOUT_CLASSIFICATION	 consolidation for outer joins 
WITHOUT_CLASSIFICATION	 date 
WITHOUT_CLASSIFICATION	 max time when waiting for write locks node list 
WITHOUT_CLASSIFICATION	 random batchsize unique ordered integers indices this could smarter 
WITHOUT_CLASSIFICATION	 give pass optionally have literaldelegate provide getliteralclass check 
WITHOUT_CLASSIFICATION	 this not the datanucleus but the assigned the sequence 
WITHOUT_CLASSIFICATION	 preserve interrupt status 
WITHOUT_CLASSIFICATION	 note columnids below makes additional changes for acid dont use this var directly 
WITHOUT_CLASSIFICATION	 basic case 
WITHOUT_CLASSIFICATION	 skip default directory only all value false 
WITHOUT_CLASSIFICATION	 check the metastore key set first 
WITHOUT_CLASSIFICATION	 inputs are not the same bail out 
WITHOUT_CLASSIFICATION	 time 
WITHOUT_CLASSIFICATION	 are only processing partition reset our evaluators 
WITHOUT_CLASSIFICATION	 the value before the value record offset make byte segment reference absolute 
WITHOUT_CLASSIFICATION	 for now exclude char until determine why there difference blank padding serializing with and the regular serde 
WITHOUT_CLASSIFICATION	 skewedcolnames 
WITHOUT_CLASSIFICATION	 run rule fix windowing issue when done over 
WITHOUT_CLASSIFICATION	 bail out not enabled for rewriting 
WITHOUT_CLASSIFICATION	 merging from targetinner nodeouter 
WITHOUT_CLASSIFICATION	 linkedhashmap provide the same iteration order when selecting random host 
WITHOUT_CLASSIFICATION	 found the class this would hadoop version newer see hadoop hadoop 
WITHOUT_CLASSIFICATION	 the object for storing row data 
WITHOUT_CLASSIFICATION	 the time has come 
WITHOUT_CLASSIFICATION	 required required required required required required required 
WITHOUT_CLASSIFICATION	 interface for the case where there explicit name for the function 
WITHOUT_CLASSIFICATION	 the object reused during evaluating make copy here 
WITHOUT_CLASSIFICATION	 this exception indicates that code record could not parsed and the caller can decide whether drop send dead letter queue rolling back the txn and retrying wont help since the tuple will exactly the same when its replayed 
WITHOUT_CLASSIFICATION	 nulls left nulls right 
WITHOUT_CLASSIFICATION	 drop first and then create 
WITHOUT_CLASSIFICATION	 set result the quotient 
WITHOUT_CLASSIFICATION	 multiple file sink descriptors are linked use the task created the first linked file descriptor 
WITHOUT_CLASSIFICATION	 the row consists some string columns some arrayarrayint columns 
WITHOUT_CLASSIFICATION	 the type info each column being assigned 
WITHOUT_CLASSIFICATION	 remove values key exprs schema for value already fixed 
WITHOUT_CLASSIFICATION	 when have decimal the input parameter then have see there special vector udaf for not will need convert the input parameter 
WITHOUT_CLASSIFICATION	 filterg stuff that way this method and byfilter would just merged 
WITHOUT_CLASSIFICATION	 there only one 
WITHOUT_CLASSIFICATION	 set will only allocate memory the buffer result smaller than 
WITHOUT_CLASSIFICATION	 skip since not selected query 
WITHOUT_CLASSIFICATION	 verification passed encode the reply 
WITHOUT_CLASSIFICATION	 nothing find for this type 
WITHOUT_CLASSIFICATION	 even reduction lets still test the original predicate see was already constant which case dont need any runtime decision 
WITHOUT_CLASSIFICATION	 zookeeper related configs 
WITHOUT_CLASSIFICATION	 get hive aggregate info 
WITHOUT_CLASSIFICATION	 for each entry collection retrieve skewed column retrieve skewed map 
WITHOUT_CLASSIFICATION	 compose file text 
WITHOUT_CLASSIFICATION	 determine the default encoding type specified the table the global default none was provided 
WITHOUT_CLASSIFICATION	 add constraints need not deep retrieval the table column descriptor while persisting the constraints since this transaction involving create table not yet committed 
WITHOUT_CLASSIFICATION	 status has not changed continue waiting 
WITHOUT_CLASSIFICATION	 since have done exact match tsselgbyrsgbyselfs 
WITHOUT_CLASSIFICATION	 table operations 
WITHOUT_CLASSIFICATION	 dont want that 
WITHOUT_CLASSIFICATION	 field look for the record identifier field inside recid look for row field inside recid look for original write field inside recid look for bucket for the original row for the record identifier struct for the long row inside the recordidentifier for the original write inside the record identifer 
WITHOUT_CLASSIFICATION	 push down current ppd context newly added filter 
WITHOUT_CLASSIFICATION	 data 
WITHOUT_CLASSIFICATION	 granularity 
WITHOUT_CLASSIFICATION	 string length should work after readfields 
WITHOUT_CLASSIFICATION	 set min possible value 
WITHOUT_CLASSIFICATION	 operator wants some work the end group 
WITHOUT_CLASSIFICATION	 move clock forward and request task 
WITHOUT_CLASSIFICATION	 get outputfieldois 
WITHOUT_CLASSIFICATION	 for column column only toss date and intervalyearmonth 
WITHOUT_CLASSIFICATION	 verify create table not called table but called for and 
WITHOUT_CLASSIFICATION	 not known estimate that based the number entries 
WITHOUT_CLASSIFICATION	 convert the stub from the configuration back into normal token 
WITHOUT_CLASSIFICATION	 add props from params set table schema 
WITHOUT_CLASSIFICATION	 this has called before initializing the instance rawstore 
WITHOUT_CLASSIFICATION	 should not use this optimization sorted dynamic partition optimizer used 
WITHOUT_CLASSIFICATION	 statementexecute after resultsetclose should fine too 
WITHOUT_CLASSIFICATION	 first get the columns named columns 
WITHOUT_CLASSIFICATION	 then update pool allocations 
WITHOUT_CLASSIFICATION	 format the row format statement 
WITHOUT_CLASSIFICATION	 there should original bucket files and plus one delta directory and one deletedelta directory when splitupdate enabled update event split into combination delete and insert that generates the deletedelta directory the delta directory should also have bucket files bucket and bucket 
WITHOUT_CLASSIFICATION	 bytes per stripe bytes per split 
WITHOUT_CLASSIFICATION	 the given filtercondn refers only table alias the qbjointree return that aliass position otherwise return 
WITHOUT_CLASSIFICATION	 check there are compactions requests left 
WITHOUT_CLASSIFICATION	 ival 
WITHOUT_CLASSIFICATION	 precalculated offset values for each alias 
WITHOUT_CLASSIFICATION	 used 
WITHOUT_CLASSIFICATION	 skip skipsize rows batch 
WITHOUT_CLASSIFICATION	 sparse map 
WITHOUT_CLASSIFICATION	 erase both headers the blocks merge 
WITHOUT_CLASSIFICATION	 given the previous range and the current range calculate the new sum from the previous sum and the difference save the computation 
WITHOUT_CLASSIFICATION	 literal bigint 
WITHOUT_CLASSIFICATION	 list alrady loaded containers number partitions each table should have the partition spilled next 
WITHOUT_CLASSIFICATION	 configure the authfilter with the kerberos params iff security 
WITHOUT_CLASSIFICATION	 converts amt days milliseconds 
WITHOUT_CLASSIFICATION	 lock are examining exclusive 
WITHOUT_CLASSIFICATION	 remove the proxy privilege and the auth should fail reality the proxy setting should not changed the fly 
WITHOUT_CLASSIFICATION	 current transaction 
WITHOUT_CLASSIFICATION	 create schema with serde then remap 
WITHOUT_CLASSIFICATION	 scalarscalar 
WITHOUT_CLASSIFICATION	 recompose filter possibly pulling out common elements from dnf 
WITHOUT_CLASSIFICATION	 parse out words the sentence 
WITHOUT_CLASSIFICATION	 constructing the default mapredwork 
WITHOUT_CLASSIFICATION	 teseted oracle database express edition release bit production 
WITHOUT_CLASSIFICATION	 escaping 
WITHOUT_CLASSIFICATION	 todo expose this operation client useful for streaming api abort all remaining trasnactions batch ioexceptions caller must rollback the transaction not all transactions were aborted since this will not attempt delete associated locks this case param dbconn active connection param txnids list transactions abort param maxheartbeat value used link performtimeouts ensure this doesnt abort txn which were hearbetated after performtimeouts select and this operation param isstrict true for strict mode false for besteffort mode strict mode all txns are not successfully aborted then the count updated ones will returned and the caller will roll back besteffort mode will ignore that fact and continue deleting the locks return number aborted transactions throws sqlexception 
WITHOUT_CLASSIFICATION	 retrieve enabled not null constraint from metastore 
WITHOUT_CLASSIFICATION	 digits digits 
WITHOUT_CLASSIFICATION	 describeexplainshow commands 
WITHOUT_CLASSIFICATION	 repeating null value 
WITHOUT_CLASSIFICATION	 add another column 
WITHOUT_CLASSIFICATION	 need update the status the creation signature 
WITHOUT_CLASSIFICATION	 apply partition pruning 
WITHOUT_CLASSIFICATION	 operations that require insertdelete privileges 
WITHOUT_CLASSIFICATION	 evaluate the value 
WITHOUT_CLASSIFICATION	 get structfields for bucketed cols 
WITHOUT_CLASSIFICATION	 see sessioninitcontext javadoc 
WITHOUT_CLASSIFICATION	 only user belonging admin role can create new roles 
WITHOUT_CLASSIFICATION	 for the case when the output can have null values follow the convention that the data values must for long and nan for double this prevent possible later zerodivide errors complex arithmetic expressions like col col the case when some col entries are null 
WITHOUT_CLASSIFICATION	 insert the new task between current task and its child 
WITHOUT_CLASSIFICATION	 tracks running and queued allocated tasks cleared after task completes 
WITHOUT_CLASSIFICATION	 matches only forwardoperators which are reducers and are followed groupbyoperators 
WITHOUT_CLASSIFICATION	 process the row batch that has less than defaultsize rows 
WITHOUT_CLASSIFICATION	 add self the end the queue 
WITHOUT_CLASSIFICATION	 for each task completion event get the associated task job 
WITHOUT_CLASSIFICATION	 vcontextenvironment 
WITHOUT_CLASSIFICATION	 just last one 
WITHOUT_CLASSIFICATION	 filtering for outer join just removes rows available for hash table matching 
WITHOUT_CLASSIFICATION	 hiveconf 
WITHOUT_CLASSIFICATION	 test basic assign vector 
WITHOUT_CLASSIFICATION	 all the other cases can not merge 
WITHOUT_CLASSIFICATION	 this will true node was examined the vectorizer class 
WITHOUT_CLASSIFICATION	 the only allowed nonoverlapping option extra bytes the end 
WITHOUT_CLASSIFICATION	 normal check stoptimer works 
WITHOUT_CLASSIFICATION	 unsupported type 
WITHOUT_CLASSIFICATION	 pick random avail port 
WITHOUT_CLASSIFICATION	 the jdoexception the nucleus exception may wrapped further metaexception 
WITHOUT_CLASSIFICATION	 processor creation 
WITHOUT_CLASSIFICATION	 gbyrsgbyrsgby top bottom 
WITHOUT_CLASSIFICATION	 not already part the groupby 
WITHOUT_CLASSIFICATION	 tests for the partition partition method 
WITHOUT_CLASSIFICATION	 update the subcache 
WITHOUT_CLASSIFICATION	 this get should fail because its variance way past maxvariance 
WITHOUT_CLASSIFICATION	 strict admin check allows only set true when set true checks true and the logged user via pam spnego kerberos list 
WITHOUT_CLASSIFICATION	 look for the nonescapedsemicolon the query text not the table name which comes the result 
WITHOUT_CLASSIFICATION	 process method call 
WITHOUT_CLASSIFICATION	 optional string user 
WITHOUT_CLASSIFICATION	 drop testdatabaseops via objectstore 
WITHOUT_CLASSIFICATION	 right side 
WITHOUT_CLASSIFICATION	 create the column expr map 
WITHOUT_CLASSIFICATION	 first separated substring will txnid and the rest are 
WITHOUT_CLASSIFICATION	 create the routes group 
WITHOUT_CLASSIFICATION	 ignorenot needed but useful for testing 
WITHOUT_CLASSIFICATION	 pkname 
WITHOUT_CLASSIFICATION	 scale down rounding clear fraction 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 exclusive 
WITHOUT_CLASSIFICATION	 set that the row the mutation 
WITHOUT_CLASSIFICATION	 the output arrays start index for output evaluator aggregations 
WITHOUT_CLASSIFICATION	 need pointer the hash map since this class must static support having 
WITHOUT_CLASSIFICATION	 base object inspector start column number number columns 
WITHOUT_CLASSIFICATION	 complete first request second pending request should through 
WITHOUT_CLASSIFICATION	 alter database set property 
WITHOUT_CLASSIFICATION	 add selectop match the schema 
WITHOUT_CLASSIFICATION	 get names all tables under this dbname 
WITHOUT_CLASSIFICATION	 populate semijoin select needed 
WITHOUT_CLASSIFICATION	 hack off the last word and try again 
WITHOUT_CLASSIFICATION	 move the next root node 
WITHOUT_CLASSIFICATION	 use linkedhashmapstring operator extends operatordesc getaliastowork should not apply this for nonnative table 
WITHOUT_CLASSIFICATION	 check see the directory already exists before calling mkdirs because the file system readonly mkdirs will throw exception even the directory already exists 
WITHOUT_CLASSIFICATION	 copy the source files cmroot the client will move the source files another location should make copy the files cmroot instead moving 
WITHOUT_CLASSIFICATION	 addoverride properties found from hivesite with userspecific properties 
WITHOUT_CLASSIFICATION	 should only used 
WITHOUT_CLASSIFICATION	 theres usually nothing escape will optimistic 
WITHOUT_CLASSIFICATION	 order fix hive 
WITHOUT_CLASSIFICATION	 test that timestamp arithmetic done utc and then converted back local timezone 
WITHOUT_CLASSIFICATION	 when have splitupdate and there are two kinds delta directories the deltaxy directory one which has only insert events and the deletedeltaxy directory which has only the delete events the clever thing about this kind splitting that everything the deltaxy directory can processed base files however this left out currently improvement for the future 
WITHOUT_CLASSIFICATION	 loginfodiscover ptns called 
WITHOUT_CLASSIFICATION	 case the property the conf will not set 
WITHOUT_CLASSIFICATION	 exit the loop and check next lock 
WITHOUT_CLASSIFICATION	 let yarn pick the queue name isnt provided hivesite via the commandline 
WITHOUT_CLASSIFICATION	 theres authentication then directly substitute the user 
WITHOUT_CLASSIFICATION	 the joins have been automatically converted mapjoins however the joins were converted sortmerge joins automatically they should also tried mapjoins 
WITHOUT_CLASSIFICATION	 create single insert delta with rows with rowids per original transaction 
WITHOUT_CLASSIFICATION	 because inverse scaled 
WITHOUT_CLASSIFICATION	 would still empty because stats are actually populated 
WITHOUT_CLASSIFICATION	 lazysimple seems throw away everything but and 
WITHOUT_CLASSIFICATION	 queries rejected from being cached because they exceeded the max cache entry size 
WITHOUT_CLASSIFICATION	 just retrieve value from conf 
WITHOUT_CLASSIFICATION	 hybrid strategy 
WITHOUT_CLASSIFICATION	 dont redisplay warnings have already seen 
WITHOUT_CLASSIFICATION	 line table not found tablename 
WITHOUT_CLASSIFICATION	 save usedcacheentry ensure reader released after query 
WITHOUT_CLASSIFICATION	 all the data comes from disk the reader may have split into multiple slices also possible theres data the file 
WITHOUT_CLASSIFICATION	 ensure that the table properties were copied 
WITHOUT_CLASSIFICATION	 should initialize the serde with the typeinfo when available 
WITHOUT_CLASSIFICATION	 there can error 
WITHOUT_CLASSIFICATION	 are unsecure mode 
WITHOUT_CLASSIFICATION	 sent shoulddietrue 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 all children are descendants 
WITHOUT_CLASSIFICATION	 check bucketsort cols 
WITHOUT_CLASSIFICATION	 the first time see big key this key not the last table the last table can always streamed define that get 
WITHOUT_CLASSIFICATION	 assumes value written after key 
WITHOUT_CLASSIFICATION	 partitionkeys added order 
WITHOUT_CLASSIFICATION	 result 
WITHOUT_CLASSIFICATION	 for map check for virtual columns 
WITHOUT_CLASSIFICATION	 trailing zeroes rounding 
WITHOUT_CLASSIFICATION	 concurrenthashmap does not allow null use substitute value 
WITHOUT_CLASSIFICATION	 exchange multiple partitions using partial partitionspec only one partition column 
WITHOUT_CLASSIFICATION	 get tag value from object last list 
WITHOUT_CLASSIFICATION	 reached here either and are both nulls and are both not nulls this internal error and should not let this happen throw exception 
WITHOUT_CLASSIFICATION	 create row file copy and empty copy 
WITHOUT_CLASSIFICATION	 case the statement insert 
WITHOUT_CLASSIFICATION	 keep track colnametoposmap for new 
WITHOUT_CLASSIFICATION	 column node 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this method looks locations specified above and returns the first location where the file exists the file does not exist any one the locations returns null 
WITHOUT_CLASSIFICATION	 lists are compatible and onlyif the elements are compatible 
WITHOUT_CLASSIFICATION	 all inputs this unionoperator are the same reducer not need break the operator tree 
WITHOUT_CLASSIFICATION	 weve seen this terminal before and have created union work object just need add this work there will children this one since weve passed this operator before 
WITHOUT_CLASSIFICATION	 noop 
WITHOUT_CLASSIFICATION	 see need kill some sessions because the pool was resized down while bunch sessions were outstanding see also deltaremaining javadoc 
WITHOUT_CLASSIFICATION	 internal variable 
WITHOUT_CLASSIFICATION	 test first argument repeating 
WITHOUT_CLASSIFICATION	 generate the signer with secret 
WITHOUT_CLASSIFICATION	 check keys are copied from token store when token loaded 
WITHOUT_CLASSIFICATION	 null for default partition 
WITHOUT_CLASSIFICATION	 subquery case tmp may from outside 
WITHOUT_CLASSIFICATION	 this position parent constant now propagate the constant from the parent the child 
WITHOUT_CLASSIFICATION	 multibyte truncation 
WITHOUT_CLASSIFICATION	 get record reader for the idxth chunk 
WITHOUT_CLASSIFICATION	 make looks like has been compacted but not cleaned 
WITHOUT_CLASSIFICATION	 hive conf option hiveconf 
WITHOUT_CLASSIFICATION	 global all pools inherit 
WITHOUT_CLASSIFICATION	 this file descriptor linked other file descriptors one use case that unionselect starfile sink broken down for consider query like select from subq union all subqx where subq subq involves mapreduce job broken into two independent queries involving subq and subq directly and the subqueries write subdirectories common directory the file sink 
WITHOUT_CLASSIFICATION	 call the appropriate hive authorizer function 
WITHOUT_CLASSIFICATION	 test content summary 
WITHOUT_CLASSIFICATION	 task metrics 
WITHOUT_CLASSIFICATION	 when sync called will return the value signaling the end the file this should result call sync the beginning the block was searching and should continue normally 
WITHOUT_CLASSIFICATION	 validate the update 
WITHOUT_CLASSIFICATION	 closing open scope should 
WITHOUT_CLASSIFICATION	 hive stuff 
WITHOUT_CLASSIFICATION	 when deal with big data the vectorizedrowbatch will used for the different file split cache the data here the situation the first split only have rows and vectorizedrowbatch cache them meanwhile the size vectorizedrowbatch will updated the following code simulate the size change but there will when process the next split which has more than rows 
WITHOUT_CLASSIFICATION	 ordering constant and column expression important correct range generation 
WITHOUT_CLASSIFICATION	 alter view select requires the view must exist 
WITHOUT_CLASSIFICATION	 instantiate which this will use lookup the serialization class and the actual class being deserialized 
WITHOUT_CLASSIFICATION	 grant priv required 
WITHOUT_CLASSIFICATION	 these members have information for data type conversion not defined there conversion 
WITHOUT_CLASSIFICATION	 format the serde statement 
WITHOUT_CLASSIFICATION	 create the filterjoin with the new condition 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 lets split bit hashcode into two bit hash codes and employ the technique mentioned the above paper 
WITHOUT_CLASSIFICATION	 replacing directly the pool should unblock get 
WITHOUT_CLASSIFICATION	 there hint and operator removed then throw error 
WITHOUT_CLASSIFICATION	 for select distinct queries dont move any aggregations 
WITHOUT_CLASSIFICATION	 trim blanks because oldhivedecimal did 
WITHOUT_CLASSIFICATION	 create table will start and coomit the transaction 
WITHOUT_CLASSIFICATION	 optional signablevertexspec vertex 
WITHOUT_CLASSIFICATION	 ballot box with bytes 
WITHOUT_CLASSIFICATION	 uber llap container 
WITHOUT_CLASSIFICATION	 create new fetchwork reference the new cache location 
WITHOUT_CLASSIFICATION	 required required required optional optional optional optional 
WITHOUT_CLASSIFICATION	 check the old values are still there 
WITHOUT_CLASSIFICATION	 hiveservice refers logical service name for now hiveserver hostname will used give service actions name this used kill query command can authorized specifically service necessary 
WITHOUT_CLASSIFICATION	 throw away extra more than decimal places 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 simulate show locks with different filter options 
WITHOUT_CLASSIFICATION	 answer must depending relative magnitude dividend and divisor 
WITHOUT_CLASSIFICATION	 inserted sort order hence explict sort 
WITHOUT_CLASSIFICATION	 reset the static variables hiveconf default values 
WITHOUT_CLASSIFICATION	 long 
WITHOUT_CLASSIFICATION	 for queries with windowing expressions the selexpr may have third child 
WITHOUT_CLASSIFICATION	 more bits should considered for finding longest zero runs set msb 
WITHOUT_CLASSIFICATION	 use input class read length 
WITHOUT_CLASSIFICATION	 sec months month days 
WITHOUT_CLASSIFICATION	 carefully check the children make sure they are decimal 
WITHOUT_CLASSIFICATION	 for unbucketed tables have exactly recordupdater until hive for each which ends writing file bucket see also link getbucketobject 
WITHOUT_CLASSIFICATION	 special handling grouping function 
WITHOUT_CLASSIFICATION	 fraction digits from lower longword 
WITHOUT_CLASSIFICATION	 add element listcolumnvector one one 
WITHOUT_CLASSIFICATION	 use final variable properly parameterize the closure 
WITHOUT_CLASSIFICATION	 null means dont return metadata wed need the array anyway for now 
WITHOUT_CLASSIFICATION	 the coltype not the known type long double etc then get 
WITHOUT_CLASSIFICATION	 the storage handler does not provide predicate decomposition support well implement the entire filter hive however still provide the full predicate the storage handler case wants any its own prefiltering 
WITHOUT_CLASSIFICATION	 clone thread local file system statistics 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 repeated iospecproto outputspecs 
WITHOUT_CLASSIFICATION	 for orc parquet all the following statements are the same analyze table partition compute statistics analyze table partition compute statistics noscan there will not any spark job above this task 
WITHOUT_CLASSIFICATION	 the byte comparison potentially expensive better branch null 
WITHOUT_CLASSIFICATION	 index the get requests make sure there are ordering artifacts when requeue 
WITHOUT_CLASSIFICATION	 load the options first can override the command line 
WITHOUT_CLASSIFICATION	 make batch test the trim functions 
WITHOUT_CLASSIFICATION	 kvtxt keys 
WITHOUT_CLASSIFICATION	 when mapping defined assumed that the hive column names are equivalent the column names the underlying table 
WITHOUT_CLASSIFICATION	 validwriteidlist 
WITHOUT_CLASSIFICATION	 this update need skip the first col since row 
WITHOUT_CLASSIFICATION	 marked skipped previously dont bother processing the rest the payload 
WITHOUT_CLASSIFICATION	 getpos should always return 
WITHOUT_CLASSIFICATION	 deserialize value into vector row columns 
WITHOUT_CLASSIFICATION	 get our own connection the database can get table and partition information 
WITHOUT_CLASSIFICATION	 store arraybyteend that can use the same formula compute the length 
WITHOUT_CLASSIFICATION	 various helper methods 
WITHOUT_CLASSIFICATION	 override the default delegation token lifetime for llap also set all the necessary settings defaults and llap configs not set 
WITHOUT_CLASSIFICATION	 column will removed from filter column will removed from filter 
WITHOUT_CLASSIFICATION	 the running class was loaded directly through eclipse rather than through 
WITHOUT_CLASSIFICATION	 init 
WITHOUT_CLASSIFICATION	 setting the comparison less the search should use the block 
WITHOUT_CLASSIFICATION	 hive states that should use run instead execute due hadoop known issue added hadoop 
WITHOUT_CLASSIFICATION	 test the idempotent behavior drop function 
WITHOUT_CLASSIFICATION	 note that may have two more duplicate partition names 
WITHOUT_CLASSIFICATION	 not clear replcopywork should inherit from copywork 
WITHOUT_CLASSIFICATION	 not supported 
WITHOUT_CLASSIFICATION	 vverbose 
WITHOUT_CLASSIFICATION	 find the constant origin certain column originated from constant 
WITHOUT_CLASSIFICATION	 select count from 
WITHOUT_CLASSIFICATION	 note here should use the new partition predicate pushdown api get list pruned list 
WITHOUT_CLASSIFICATION	 required check the original types manually primitivetypeequals does not care about 
WITHOUT_CLASSIFICATION	 ignore this expected 
WITHOUT_CLASSIFICATION	 that new one gets created for the next query the same 
WITHOUT_CLASSIFICATION	 element for key row and byte hash table hashmap 
WITHOUT_CLASSIFICATION	 converted table 
WITHOUT_CLASSIFICATION	 skip trailing blank characters 
WITHOUT_CLASSIFICATION	 merge numdistinctvalue estimators 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 its set parserewrittenquery 
WITHOUT_CLASSIFICATION	 assume one separator depth needed 
WITHOUT_CLASSIFICATION	 set optraits for dynamically partitioned hash join bucketcolnames reuse previous joinops bucketcolnames parent operators should reduce sink which should have bucket columns based the join keys numbuckets set number reducers sortcols this unsorted join sort cols 
WITHOUT_CLASSIFICATION	 finalize paths 
WITHOUT_CLASSIFICATION	 any file claim remain trash would granted 
WITHOUT_CLASSIFICATION	 need preserve currentuser 
WITHOUT_CLASSIFICATION	 the create time set 
WITHOUT_CLASSIFICATION	 create database and table that database because the being used each attempt create something should require two calls the retryinghmshandler 
WITHOUT_CLASSIFICATION	 undone fall through for these they dont appear supported yet 
WITHOUT_CLASSIFICATION	 whether theres any error occurred during query execution used for query lifetime hook 
WITHOUT_CLASSIFICATION	 deserialize group key into vector row columns 
WITHOUT_CLASSIFICATION	 none are null none are selected 
WITHOUT_CLASSIFICATION	 walk through all our inputs and set them note that this read part update 
WITHOUT_CLASSIFICATION	 check needed for single byte read 
WITHOUT_CLASSIFICATION	 even there are results move least check that have permission check the existence zerorowspath the read using the cache will fail 
WITHOUT_CLASSIFICATION	 try narrow type constant 
WITHOUT_CLASSIFICATION	 responsible for the flow rows through the ptf chain invocation wraps tablefunction the ptfop hands the chain each row through the processrow call also notifies the chain when partition startsfinishes there are several combinations depending whether the tablefunction and its successor support streaming batch mode combination streaming streaming start partition invoke startpartition tabfn process row invoke process row tabfn any output rows hand next tabfn chain forward next operator finish partition invoke finishpartition tabfn any output rows hand next tabfn chain forward next operator combination streaming batch same combination combination batch batch start partition create reset the input partition for the tabfn caveat prev also batch and not providing output iterator then can just use its output partition process row collect row input partition finish partition invoke evaluate tabfn input partition function gives output partition set next invocations input partition function gives output iterator iterate and call processrow next invocation for last invocation chain forward rows next operator combination batch stream similar combination except finish partition behavior slightly different finish partition invoke evaluate tabfn input partition iterate output rows hand next tabfn chain forward next operator 
WITHOUT_CLASSIFICATION	 allow for future ctor mutabulity design 
WITHOUT_CLASSIFICATION	 column 
WITHOUT_CLASSIFICATION	 inside job can pull out the actual properties 
WITHOUT_CLASSIFICATION	 were here want more events and either batchiter null batchiter 
WITHOUT_CLASSIFICATION	 ignore provides invalid url sometimes intentionally 
WITHOUT_CLASSIFICATION	 add new column with cascade option 
WITHOUT_CLASSIFICATION	 intermittent failure 
WITHOUT_CLASSIFICATION	 impersonation turned this called using the using sessionproxy the currentuser will the impersonated user here oozie cannot create proxy user which represents oozies client user here since cannot authenticate using kerberosdigest trust the user which opened session using kerberos this case impersonation turned off the current user hive which can open 
WITHOUT_CLASSIFICATION	 ideally wed want worker try for every slot there are workers want reread probability falling back random however make slightly more probable avoid too much rereading this handwavy 
WITHOUT_CLASSIFICATION	 queue for disabled nodes nodes make out this queue when their expiration timeout hit 
WITHOUT_CLASSIFICATION	 look for functions but not find any 
WITHOUT_CLASSIFICATION	 return null failed find 
WITHOUT_CLASSIFICATION	 the token already removed 
WITHOUT_CLASSIFICATION	 chooses representative alias index and order use the string the first used because set the constructor 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that inner join singlecolumn long using hash map 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 field within complex type 
WITHOUT_CLASSIFICATION	 never lose pool session should still able get 
WITHOUT_CLASSIFICATION	 need compute the input size runtime and select the biggest the big table 
WITHOUT_CLASSIFICATION	 create new type for handling precision conversions from decimal doublefloat the type only relevant boxliteral and all other functions treat identically 
WITHOUT_CLASSIFICATION	 are looking based the original fsop use the original path 
WITHOUT_CLASSIFICATION	 now have delta with delete events 
WITHOUT_CLASSIFICATION	 this sel sel for 
WITHOUT_CLASSIFICATION	 vectorization only works with struct object inspectors 
WITHOUT_CLASSIFICATION	 close them all 
WITHOUT_CLASSIFICATION	 base file 
WITHOUT_CLASSIFICATION	 for thrift connects 
WITHOUT_CLASSIFICATION	 init the list object inspector for handling partial aggregations 
WITHOUT_CLASSIFICATION	 numtrues 
WITHOUT_CLASSIFICATION	 real struct 
WITHOUT_CLASSIFICATION	 pretty print the values 
WITHOUT_CLASSIFICATION	 use modified portions dofastscaledown code here since not want allocate temporary fasthivedecimal object 
WITHOUT_CLASSIFICATION	 exponential backoff for ndvs descending order sort ndvs denominator ndv ndv ndv 
WITHOUT_CLASSIFICATION	 retrieve partitions 
WITHOUT_CLASSIFICATION	 this only used orc derive the structure most fields are unused 
WITHOUT_CLASSIFICATION	 passing for currenttxn means this validtxnlist not wrt any txn 
WITHOUT_CLASSIFICATION	 importing into existing transactional table will create new transactional table because export was done from transactional table need writeid explain plan doesnt open txn and hence need allocate write 
WITHOUT_CLASSIFICATION	 snapshot subset tez session info for printing events summary 
WITHOUT_CLASSIFICATION	 now hive meta store uses the same configuration hadoop sasl configuration 
WITHOUT_CLASSIFICATION	 test rounding 
WITHOUT_CLASSIFICATION	 stats were available try reduce 
WITHOUT_CLASSIFICATION	 for casting floating point types boolean 
WITHOUT_CLASSIFICATION	 for set role none clear all roles for current session 
WITHOUT_CLASSIFICATION	 read the data isnt null 
WITHOUT_CLASSIFICATION	 optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 test remainder carry indicates result therefore too 
WITHOUT_CLASSIFICATION	 all participating instances uses the same latch path and curator randomly chooses one instance leader which can verified via 
WITHOUT_CLASSIFICATION	 generate struct for each the given prefixes 
WITHOUT_CLASSIFICATION	 using the second table since table called testtable exists both databases 
WITHOUT_CLASSIFICATION	 note thrift returns ssl socket that already bound the specified hostport therefore open called this would noop later hence any ttransportexception related connecting with the peer are thrown here bubbling them the call hierarchy that retry can happen opentransport dynamic service discovery configured 
WITHOUT_CLASSIFICATION	 registry uses ephemeral sequential znodes that are never updated now 
WITHOUT_CLASSIFICATION	 hive 
WITHOUT_CLASSIFICATION	 move the setuppool code ctor for now least hasinitialsessions will false 
WITHOUT_CLASSIFICATION	 groupby not distinct like disabling 
WITHOUT_CLASSIFICATION	 first authorize the call 
WITHOUT_CLASSIFICATION	 the required outputlength 
WITHOUT_CLASSIFICATION	 there can atmost one element eligible converted metadata only 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 otherwise return the row 
WITHOUT_CLASSIFICATION	 the columns are same order 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 identifier ifnotexists 
WITHOUT_CLASSIFICATION	 sanity check the config 
WITHOUT_CLASSIFICATION	 the interval wake and check the queue 
WITHOUT_CLASSIFICATION	 its column 
WITHOUT_CLASSIFICATION	 userprivileges 
WITHOUT_CLASSIFICATION	 provide the type params the type cast 
WITHOUT_CLASSIFICATION	 after load from this dump all target tablespartitions will have initial set data but source will have latest data 
WITHOUT_CLASSIFICATION	 pretend this partition exists 
WITHOUT_CLASSIFICATION	 create writable object inspector for primitive type and return 
WITHOUT_CLASSIFICATION	 clienttool end singleton instance the job client 
WITHOUT_CLASSIFICATION	 column 
WITHOUT_CLASSIFICATION	 call increasebufferspace will ensure that buffer points byte with sufficient space for the specified size this will either point smallbuffer newly allocated byte array for larger values 
WITHOUT_CLASSIFICATION	 set the fetch operator for the new input file 
WITHOUT_CLASSIFICATION	 subtract 
WITHOUT_CLASSIFICATION	 reason poll untill the job initialized 
WITHOUT_CLASSIFICATION	 cause them localized for the sqoop job tasks 
WITHOUT_CLASSIFICATION	 the type information for all fields 
WITHOUT_CLASSIFICATION	 incoming events can ignored until the point when shuffle needs handled instead just scans 
WITHOUT_CLASSIFICATION	 rss schema key max min 
WITHOUT_CLASSIFICATION	 there single discardable operator tablescanoperator and means that have merged filter expressions for thus might need remove dpp predicates from the retainable tablescanoperator 
WITHOUT_CLASSIFICATION	 column 
WITHOUT_CLASSIFICATION	 already committed aborted 
WITHOUT_CLASSIFICATION	 select columns that actually not exist the file 
WITHOUT_CLASSIFICATION	 translate the udaf 
WITHOUT_CLASSIFICATION	 check right valid 
WITHOUT_CLASSIFICATION	 from orcinputformat 
WITHOUT_CLASSIFICATION	 order not necessary but sql require use fetch 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 drop role ignore error 
WITHOUT_CLASSIFICATION	 have readentity defaultacidtblpart writeentity defaultacidtblpart typetable writetypeinsert isdpfalse 
WITHOUT_CLASSIFICATION	 extract operator need rewrite 
WITHOUT_CLASSIFICATION	 trigger query hooks before query execution 
WITHOUT_CLASSIFICATION	 subscriber can get notification about drop database hcat listening topic named hcat and message selector string hcatevent hcatdropdatabase 
WITHOUT_CLASSIFICATION	 table creation should succeed even location specified 
WITHOUT_CLASSIFICATION	 this servlet based off the jmxproxyservlet from tomcat has been rewritten read only and output json format not really that close the original 
WITHOUT_CLASSIFICATION	 need get the partitions column names from the partition serde avro provides the table schema and ignores the partition schema 
WITHOUT_CLASSIFICATION	 apply join filters the row 
WITHOUT_CLASSIFICATION	 translate column names walking the ast 
WITHOUT_CLASSIFICATION	 dynamic partitions 
WITHOUT_CLASSIFICATION	 extract tables used the query which will turn used generate the corresponding txn write ids 
WITHOUT_CLASSIFICATION	 remove this cor var from output position mapping 
WITHOUT_CLASSIFICATION	 assumes the result set set valid row 
WITHOUT_CLASSIFICATION	 check this txn state already replicated for this given table yes then 
WITHOUT_CLASSIFICATION	 shouldnt really happen 
WITHOUT_CLASSIFICATION	 etl strategies will have start start first stripe 
WITHOUT_CLASSIFICATION	 analyzeexport creates tablespec which turn tries build public listpartition partitions looking the metastore find partitions matching the partition spec the export command these course dont exist yet since weve not ran the insert stmt yet 
WITHOUT_CLASSIFICATION	 tracks completed requests pre node 
WITHOUT_CLASSIFICATION	 successful task ids 
WITHOUT_CLASSIFICATION	 assumes the cache lock has already been taken 
WITHOUT_CLASSIFICATION	 session txnmanager that already use 
WITHOUT_CLASSIFICATION	 the histogram object 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 generate applicationid for the llap splits 
WITHOUT_CLASSIFICATION	 are going build the name 
WITHOUT_CLASSIFICATION	 spark task were currently processing 
WITHOUT_CLASSIFICATION	 did not generate anything for the new predicate bail out 
WITHOUT_CLASSIFICATION	 hadoop queue information 
WITHOUT_CLASSIFICATION	 table import statement specified location and the tableunpartitioned already exists ensure that the locations are the same partitioned tables not checked here since the location provided would need 
WITHOUT_CLASSIFICATION	 event alter marker stats event insert alter stats update event 
WITHOUT_CLASSIFICATION	 sorry too many ifs but this form looks optimal 
WITHOUT_CLASSIFICATION	 general when can have unlimited branches currently only handle either branch 
WITHOUT_CLASSIFICATION	 needed 
WITHOUT_CLASSIFICATION	 forward the overflow batch over and over reference new one big table rows values each time cross product current batch small table values todo this could further optimized copy big table equal keys once and only copy big table values each time and not set repeating every time 
WITHOUT_CLASSIFICATION	 adding the credentials from hadoop config hidden 
WITHOUT_CLASSIFICATION	 trim trailing zeroes and readjust scale 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 estimated number rows will product ndvs 
WITHOUT_CLASSIFICATION	 lazily create instances 
WITHOUT_CLASSIFICATION	 record current valid txn list that will used throughout the query compilation and processing only this transaction was already opened and the list has not been recorded yet explicit open transaction command 
WITHOUT_CLASSIFICATION	 first branch query second branch 
WITHOUT_CLASSIFICATION	 desc will null its create table like desc will contained createtablelikedesc currently hcat disallows ctlt prehook desc can never null 
WITHOUT_CLASSIFICATION	 well over allocate and then shrink the array for each type 
WITHOUT_CLASSIFICATION	 this includes the mandatory alias 
WITHOUT_CLASSIFICATION	 third group 
WITHOUT_CLASSIFICATION	 call out the real configure method 
WITHOUT_CLASSIFICATION	 second overflow error ansi sql numeric cast decimal throws overflow error 
WITHOUT_CLASSIFICATION	 some commands like show databases dont start implicit transactions 
WITHOUT_CLASSIFICATION	 the heartbeat has timeout double check whether can remove 
WITHOUT_CLASSIFICATION	 index for value from keys from values 
WITHOUT_CLASSIFICATION	 the udtf expects arguments object 
WITHOUT_CLASSIFICATION	 construct skewedvalue location map except default directory why query logic knows defaultdir structure and dont need get from map 
WITHOUT_CLASSIFICATION	 look thats the table for example drop table explicit txn not allowed some cases this requires looking more than just the operation for example hiveoperationload target table but not nonacid table 
WITHOUT_CLASSIFICATION	 this the original parent the currentrootoperator scan through the graph root operator might have multiple parents and just use this one remember where came from the current 
WITHOUT_CLASSIFICATION	 dont catch any execution exceptions here and let the caller catch 
WITHOUT_CLASSIFICATION	 sets permissions and group name partition dirs and files 
WITHOUT_CLASSIFICATION	 should have stats for all columns estimated actual 
WITHOUT_CLASSIFICATION	 new serde needs store fields metastore but the old serde doesnt save the fields that new serde could operate note that this may fail some fields from old serde are too long stored metastore but theres nothing can 
WITHOUT_CLASSIFICATION	 log warning row count missing 
WITHOUT_CLASSIFICATION	 use getperflogger get instance perflogger 
WITHOUT_CLASSIFICATION	 partition was found but was empty 
WITHOUT_CLASSIFICATION	 generate basic tez config 
WITHOUT_CLASSIFICATION	 set the hivedecimalwritable from bytes without converting string first for better performance 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 add new partition via objectstore 
WITHOUT_CLASSIFICATION	 query 
WITHOUT_CLASSIFICATION	 stored stats 
WITHOUT_CLASSIFICATION	 now new job requests should succeed all cancel threads would have completed 
WITHOUT_CLASSIFICATION	 nothing currently available for hash sets 
WITHOUT_CLASSIFICATION	 leaving some table from the list tables cached 
WITHOUT_CLASSIFICATION	 merge statements could have inserted cardinality violation branch need avoid that 
WITHOUT_CLASSIFICATION	 normalize prop name 
WITHOUT_CLASSIFICATION	 this can max never 
WITHOUT_CLASSIFICATION	 add default dir the end each list 
WITHOUT_CLASSIFICATION	 included will not null row options will fill the array with trues null 
WITHOUT_CLASSIFICATION	 setdays resets the isnulli false there parse exception 
WITHOUT_CLASSIFICATION	 fail silently 
WITHOUT_CLASSIFICATION	 test that existing exclusive partition with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 could null for default partition 
WITHOUT_CLASSIFICATION	 construct join rel node and rowresolver for the new join node 
WITHOUT_CLASSIFICATION	 use 
WITHOUT_CLASSIFICATION	 its parent sparkworks for the small tables 
WITHOUT_CLASSIFICATION	 the modelertype attribute was not found the class name used instead 
WITHOUT_CLASSIFICATION	 make vectorized operator 
WITHOUT_CLASSIFICATION	 java version system property formatted find second dot instead last dot safe 
WITHOUT_CLASSIFICATION	 set empty text property set null 
WITHOUT_CLASSIFICATION	 group operators select the key cols need find them the values 
WITHOUT_CLASSIFICATION	 only mark output null when input null 
WITHOUT_CLASSIFICATION	 create the directories filesinkoperators need 
WITHOUT_CLASSIFICATION	 need way know what thread interrupt since this blocking thread 
WITHOUT_CLASSIFICATION	 this may possible when srctype string but desttype integer 
WITHOUT_CLASSIFICATION	 since overlaps with long running still open does nothing 
WITHOUT_CLASSIFICATION	 creates the configuration object necessary run specific vertex from map work this includes input formats input processor etc 
WITHOUT_CLASSIFICATION	 the file may not exist and just ignore this 
WITHOUT_CLASSIFICATION	 show cannot create child null directory 
WITHOUT_CLASSIFICATION	 bloom filter for the new node that will eventually add the cache 
WITHOUT_CLASSIFICATION	 convert stringutf ascii bytes methods 
WITHOUT_CLASSIFICATION	 some bookkeeping 
WITHOUT_CLASSIFICATION	 forwardoperator that can add multiple filtergroup operators children 
WITHOUT_CLASSIFICATION	 event dump each subdir individual event dump need guarantee that the directory listing got order evid 
WITHOUT_CLASSIFICATION	 try another slot 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader long 
WITHOUT_CLASSIFICATION	 the schema for intersect all like this countc cnt minc create input project for udtf whose schema like this minc 
WITHOUT_CLASSIFICATION	 the simple validity check see the file size not other checks maybe added the future 
WITHOUT_CLASSIFICATION	 dont mess with the cached object 
WITHOUT_CLASSIFICATION	 pathtablelocation year 
WITHOUT_CLASSIFICATION	 can clean 
WITHOUT_CLASSIFICATION	 get our singlecolumn long hash multiset information for this specialized class 
WITHOUT_CLASSIFICATION	 clean above throws 
WITHOUT_CLASSIFICATION	 this rel references corvar and now needs rewritten must have been pulled above the correlator replace the input ref account for the lhs the correlator 
WITHOUT_CLASSIFICATION	 for explain plan txn wont opened and doesnt make sense allocate write 
WITHOUT_CLASSIFICATION	 register function 
WITHOUT_CLASSIFICATION	 power ten way beyond our precisionscale 
WITHOUT_CLASSIFICATION	 move all the children the front queue 
WITHOUT_CLASSIFICATION	 test outputformat with complex data type and with reduce 
WITHOUT_CLASSIFICATION	 parsing error invalid url string 
WITHOUT_CLASSIFICATION	 settable recursively all the nested fields are also settable 
WITHOUT_CLASSIFICATION	 with constant folding then the result will 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 test select rootcol from 
WITHOUT_CLASSIFICATION	 initialize transient fields called after deserialization other fields 
WITHOUT_CLASSIFICATION	 this the threshold that the user has specified fit mapjoin 
WITHOUT_CLASSIFICATION	 tests location returned when file present the first directory lookup order 
WITHOUT_CLASSIFICATION	 binaryvalue 
WITHOUT_CLASSIFICATION	 simplify vector bruteforce flattening nonulls isrepeating this can used reduce combinatorial explosion code paths vectorexpressions 
WITHOUT_CLASSIFICATION	 skip entire the data 
WITHOUT_CLASSIFICATION	 set nothing for prepared sql 
WITHOUT_CLASSIFICATION	 all files were copied successfully last try can break from here 
WITHOUT_CLASSIFICATION	 timestamps with second vint storing additional bits the seconds field 
WITHOUT_CLASSIFICATION	 remember the count positions 
WITHOUT_CLASSIFICATION	 initialize rownums have row 
WITHOUT_CLASSIFICATION	 read back 
WITHOUT_CLASSIFICATION	 heartbeater should running the background every second 
WITHOUT_CLASSIFICATION	 create and return clause 
WITHOUT_CLASSIFICATION	 for pattern find digits 
WITHOUT_CLASSIFICATION	 theory the below call isnt needed non thriftmode but lets not get too crazy 
WITHOUT_CLASSIFICATION	 class loading thread safe 
WITHOUT_CLASSIFICATION	 started from main and nolog should not output any logs turn the log please set dtestsilentfalse 
WITHOUT_CLASSIFICATION	 explicitly misset the catalog name 
WITHOUT_CLASSIFICATION	 second incremental load 
WITHOUT_CLASSIFICATION	 now make sure get the stats expect for partition are going add data later 
WITHOUT_CLASSIFICATION	 the client did not specify qop then just negotiate the one supported server 
WITHOUT_CLASSIFICATION	 round using the halfup method used hive 
WITHOUT_CLASSIFICATION	 will strain out the record for the underlying writer 
WITHOUT_CLASSIFICATION	 load using same dump with view should fail not empty 
WITHOUT_CLASSIFICATION	 collect keys 
WITHOUT_CLASSIFICATION	 assume dont need fetch the rest the skewed column data have columns 
WITHOUT_CLASSIFICATION	 add the new task child each the passed tasks 
WITHOUT_CLASSIFICATION	 throw new not sort order and unique 
WITHOUT_CLASSIFICATION	 checkoutputspecs mightve set some properties need have context reflect that 
WITHOUT_CLASSIFICATION	 tablestats 
WITHOUT_CLASSIFICATION	 now look any jars weve packaged using jarfinder returns null when 
WITHOUT_CLASSIFICATION	 the super method will reload hash table partition one the small tables currently for native vector map join will only one small table 
WITHOUT_CLASSIFICATION	 swap the first element the metastoreuris with random element from the rest the array rationale being that this method will generally called when the default connection has died and the default connection likely the first array element 
WITHOUT_CLASSIFICATION	 new exception 
WITHOUT_CLASSIFICATION	 start with dummy vector operator the parent the parallel vector operator tree are creating 
WITHOUT_CLASSIFICATION	 string utilities 
WITHOUT_CLASSIFICATION	 aggregate this batch 
WITHOUT_CLASSIFICATION	 used for testing 
WITHOUT_CLASSIFICATION	 there should reader event available coming soon okay blocking call 
WITHOUT_CLASSIFICATION	 decode the value necessary 
WITHOUT_CLASSIFICATION	 runinternal which defers the close the called that method 
WITHOUT_CLASSIFICATION	 getconvertedoi without any caching 
WITHOUT_CLASSIFICATION	 locate the where the branch starts this function works only for the following pattern fil fil sel sel gby gby join sparkpruningsink sparkpruningsink 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl call check side file for mockmocktbl call open mockmocktbl call check side file for mockmocktbl 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 make sure only mapjoins can performed 
WITHOUT_CLASSIFICATION	 safe assume else orc semantic analyzer will check for rcorc 
WITHOUT_CLASSIFICATION	 removed from make sure that can still get session from 
WITHOUT_CLASSIFICATION	 minutes 
WITHOUT_CLASSIFICATION	 for the failures the users have been notified just need clean theres session here its unused conflicts are possible just remove for successes the user has also been notified various requests are also possible however start wed just put the session into the sessions list and from there 
WITHOUT_CLASSIFICATION	 for xlint code will never reach here 
WITHOUT_CLASSIFICATION	 sets the job state completed and also sets the results value returns true completed status set otherwise returns false 
WITHOUT_CLASSIFICATION	 trigger kill threads and verify that get and expected message this should raise kill operations and ensure that retries keep the time out occupied for sec 
WITHOUT_CLASSIFICATION	 have reached the point where are transferring files across filesystems 
WITHOUT_CLASSIFICATION	 fill high long from middle long and middle long from lower long 
WITHOUT_CLASSIFICATION	 capture the cte definitions query 
WITHOUT_CLASSIFICATION	 equal key series checking 
WITHOUT_CLASSIFICATION	 optionally read current values big length big value len big value bytes 
WITHOUT_CLASSIFICATION	 long range bias for bit hashcodes 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 try find the class 
WITHOUT_CLASSIFICATION	 write the sync bytes flush header 
WITHOUT_CLASSIFICATION	 store the position the constant value for later use 
WITHOUT_CLASSIFICATION	 fast path first statement doesnt work rollback and the long version 
WITHOUT_CLASSIFICATION	 optional int withindagpriority 
WITHOUT_CLASSIFICATION	 using member variable the closure will not the right thing 
WITHOUT_CLASSIFICATION	 have scan the directory find min date greater than currentdir 
WITHOUT_CLASSIFICATION	 sets temp and task temp path 
WITHOUT_CLASSIFICATION	 join clause rewriting needed 
WITHOUT_CLASSIFICATION	 optional int amport 
WITHOUT_CLASSIFICATION	 this semijoin branch find this creating potential cycle with childjoin 
WITHOUT_CLASSIFICATION	 parameters for exporting metadata table drop requires the use the preevent listener 
WITHOUT_CLASSIFICATION	 temp tables just call underlying client 
WITHOUT_CLASSIFICATION	 now prunedcols are columns used child operators and columns 
WITHOUT_CLASSIFICATION	 first quickly check the two table scan operators can actually merged 
WITHOUT_CLASSIFICATION	 mybitint 
WITHOUT_CLASSIFICATION	 avro guarantees that the key will type string just need worry about deserializing the value here 
WITHOUT_CLASSIFICATION	 the entire seconds field stored the first bytes 
WITHOUT_CLASSIFICATION	 assuming that there transaction for read lock 
WITHOUT_CLASSIFICATION	 authorizer 
WITHOUT_CLASSIFICATION	 default outputtypeinfo long 
WITHOUT_CLASSIFICATION	 now create session specific dirs 
WITHOUT_CLASSIFICATION	 comma separated list classes batch 
WITHOUT_CLASSIFICATION	 replpolicy 
WITHOUT_CLASSIFICATION	 let writers release the memory for garbage collection 
WITHOUT_CLASSIFICATION	 reserve spaces for the byte size the struct which integer and takes four bytes 
WITHOUT_CLASSIFICATION	 the outputs intermediate map reduce jobs 
WITHOUT_CLASSIFICATION	 dont break this allocation failure was result localitydelay others could still allocated 
WITHOUT_CLASSIFICATION	 newer tasks first 
WITHOUT_CLASSIFICATION	 this just the directory need recurse and find the actual files but dont this until have determined there base this saves time plus possible that the cleaner running and removing these original files which case recursing through them could cause get error 
WITHOUT_CLASSIFICATION	 isreplace 
WITHOUT_CLASSIFICATION	 utf byte constants used stringutf bytes decimal and decimal stringutf byte conversion 
WITHOUT_CLASSIFICATION	 for better performance longdouble dont want the conditional statements inside the for loop 
WITHOUT_CLASSIFICATION	 directory where path resides 
WITHOUT_CLASSIFICATION	 theres more data 
WITHOUT_CLASSIFICATION	 higerbound when true 
WITHOUT_CLASSIFICATION	 hconf null unit testing 
WITHOUT_CLASSIFICATION	 either this arena being allocated already allocated next the 
WITHOUT_CLASSIFICATION	 these are the functions that have window fns window have already been processed 
WITHOUT_CLASSIFICATION	 push down filters 
WITHOUT_CLASSIFICATION	 keeps track the righthandside table name the leftsemijoin and 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 node marked failed node has capacity 
WITHOUT_CLASSIFICATION	 literal smallint 
WITHOUT_CLASSIFICATION	 can only used for kerberos user but not for the user logged via other authentications such ldap 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 calculate which writer use from the remaining values this needs done before delete cols 
WITHOUT_CLASSIFICATION	 this project will what the old input maps replacing any previous mapping from old input 
WITHOUT_CLASSIFICATION	 note this called under the epic lock 
WITHOUT_CLASSIFICATION	 reset for reading 
WITHOUT_CLASSIFICATION	 check this the best match far 
WITHOUT_CLASSIFICATION	 fall through 
WITHOUT_CLASSIFICATION	 always long 
WITHOUT_CLASSIFICATION	 such table the given database 
WITHOUT_CLASSIFICATION	 precompute normalization dont have deal with sqlexceptions later 
WITHOUT_CLASSIFICATION	 generate join operator 
WITHOUT_CLASSIFICATION	 besides the hiveshims jar which hadoop version dependent also always need include hive shims common jars 
WITHOUT_CLASSIFICATION	 locality delay 
WITHOUT_CLASSIFICATION	 need generate not null predicate for partitioning virtual column since those columns can never null 
WITHOUT_CLASSIFICATION	 take care view creation 
WITHOUT_CLASSIFICATION	 compare two strings from two byte arrays each with their own start position and length use lexicographic unsigned byte value order this whats used for utf sort order return negative value arg arg arg arg positive arg arg 
WITHOUT_CLASSIFICATION	 filesystem 
WITHOUT_CLASSIFICATION	 decimal columns use hivedecimalwritable 
WITHOUT_CLASSIFICATION	 delim 
WITHOUT_CLASSIFICATION	 not column need for the keys move 
WITHOUT_CLASSIFICATION	 walk through the ast replace all the toktabref adding additional masking and filter the table needs masked filtered for the replacement leverage the methods that are used for 
WITHOUT_CLASSIFICATION	 asserttrueve instanceof 
WITHOUT_CLASSIFICATION	 hivemetastore the deletedata flag indicates whether dfs data should removed drop 
WITHOUT_CLASSIFICATION	 values put above 
WITHOUT_CLASSIFICATION	 adding select operator top semijoin ensure projection only correct columns 
WITHOUT_CLASSIFICATION	 populate partition info 
WITHOUT_CLASSIFICATION	 the projection 
WITHOUT_CLASSIFICATION	 transform the table reference absolute reference dbtable 
WITHOUT_CLASSIFICATION	 cache extractobject 
WITHOUT_CLASSIFICATION	 case column stats grouping sets 
WITHOUT_CLASSIFICATION	 existing avro data 
WITHOUT_CLASSIFICATION	 key column name and the value the col stat object 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check that has priority overr 
WITHOUT_CLASSIFICATION	 optional string tokenidentifier 
WITHOUT_CLASSIFICATION	 this boolean true bypass the cache 
WITHOUT_CLASSIFICATION	 over the schema and convert type thrift type 
WITHOUT_CLASSIFICATION	 user providing config could null 
WITHOUT_CLASSIFICATION	 exact limit can taken care the fetch operator 
WITHOUT_CLASSIFICATION	 not care about the transformation rewriting ast which following statement does only care about the restriction checks they perform plan get rid these restrictions later 
WITHOUT_CLASSIFICATION	 this demuxoperator can appear multiple times muxoperators parentoperators 
WITHOUT_CLASSIFICATION	 only one reducer this configuration does not prevents 
WITHOUT_CLASSIFICATION	 grantoption 
WITHOUT_CLASSIFICATION	 room 
WITHOUT_CLASSIFICATION	 required required required required required required required required required 
WITHOUT_CLASSIFICATION	 reparse text passing null for context avoid clobbering the toplevel token stream 
WITHOUT_CLASSIFICATION	 the list bytes used for the separators the column nested struct such arrayarrayint will use multiple separators the list separators escapechar are the bytes required escaped 
WITHOUT_CLASSIFICATION	 the root interface for vector map join hash set 
WITHOUT_CLASSIFICATION	 should return null when there column 
WITHOUT_CLASSIFICATION	 insert data into both tables 
WITHOUT_CLASSIFICATION	 the complete list output columns these should added the vectorized row batch for processing the index the row batch equal the index this array plus initialoutputcol 
WITHOUT_CLASSIFICATION	 the query needs rowid maybe explicitly asked maybe its part updatedelete statement either way need decorate original rows with rowid 
WITHOUT_CLASSIFICATION	 check arrayidx argument category list 
WITHOUT_CLASSIFICATION	 interpolation needed because position does not have fraction 
WITHOUT_CLASSIFICATION	 ignore leading zeroes 
WITHOUT_CLASSIFICATION	 delete column statistics present 
WITHOUT_CLASSIFICATION	 use left alias mysql postgresql try widening conversion otherwise fail union 
WITHOUT_CLASSIFICATION	 this means there second vint present that specifies additional bits the timestamp the reversed nanoseconds value still encoded this vint 
WITHOUT_CLASSIFICATION	 check constraint fails only evaluates false nullunknown should evaluate true 
WITHOUT_CLASSIFICATION	 production byte 
WITHOUT_CLASSIFICATION	 default argless run simply runs and does not care about failure 
WITHOUT_CLASSIFICATION	 starting tez session pool start here let parent session state initialize cliservice state avoid sessionstateget return null during createtezdir 
WITHOUT_CLASSIFICATION	 some udfs may emit strings variable length like pattern matching udfs its hard find the length such udfs return the variable length from config 
WITHOUT_CLASSIFICATION	 check stats need recalculated 
WITHOUT_CLASSIFICATION	 first promote the next group the current group reached new group the previous fetch 
WITHOUT_CLASSIFICATION	 test alter table with rename 
WITHOUT_CLASSIFICATION	 any more input 
WITHOUT_CLASSIFICATION	 partitions added now should inherit tableschema properties etc 
WITHOUT_CLASSIFICATION	 initialize singlecolumn long members for this specialized class 
WITHOUT_CLASSIFICATION	 check groupingid needs projected out 
WITHOUT_CLASSIFICATION	 rewrite logic rewrite join condition map output positions and produce cor vars any 
WITHOUT_CLASSIFICATION	 root task 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 methods 
WITHOUT_CLASSIFICATION	 bootstrap constraint dump shouldnt fail the table droppedrenamed while dumping just log debug message and skip 
WITHOUT_CLASSIFICATION	 testing for equality doubles after math operation not always reliable use this tolerance 
WITHOUT_CLASSIFICATION	 lock database operation acquire the lock explicitly the operation itself doesnt need locked set the writeentity writetype ddlnolock here otherwise will conflict with hives transaction 
WITHOUT_CLASSIFICATION	 following suggestion from gopal quickly read the bytes from the stream consider have orc read the whole input stream into big byte array with one call the readbyte int off int len method and then let this method read from the big byte array 
WITHOUT_CLASSIFICATION	 keeps existing output column map etc 
WITHOUT_CLASSIFICATION	 same session 
WITHOUT_CLASSIFICATION	 nonempty java opts with xmx specified twice 
WITHOUT_CLASSIFICATION	 use table properties case unpartitioned tables and the union table properties and partition properties with partition taking precedence the case partitioned tables 
WITHOUT_CLASSIFICATION	 not expected further down the pipeline see jira for more details 
WITHOUT_CLASSIFICATION	 reset port setting 
WITHOUT_CLASSIFICATION	 table second read entity 
WITHOUT_CLASSIFICATION	 each http request must have authorization header 
WITHOUT_CLASSIFICATION	 note for deallocateevicted not release the memory memmanager may happen that the evictor tries use the allowance before the move finishes retryingmore defrag should take care that 
WITHOUT_CLASSIFICATION	 note important retain the same key the export 
WITHOUT_CLASSIFICATION	 operations require select priv 
WITHOUT_CLASSIFICATION	 the partition got dropped before went looking for 
WITHOUT_CLASSIFICATION	 hadoop default week 
WITHOUT_CLASSIFICATION	 when transforming not subquery need check for nulls the joining expressions the subquery there are nulls then the subquery always return false for more details see basically sql semantics say that not null always false not operator equivalent all since not equal check with null returns false not predicate against aset with null value always returns false for not subquery predicates join null count predicate and the joining condition that the null count query has count 
WITHOUT_CLASSIFICATION	 created demand 
WITHOUT_CLASSIFICATION	 the operator stack 
WITHOUT_CLASSIFICATION	 well set tez combine spits for iff the input format hiveinputformat 
WITHOUT_CLASSIFICATION	 ignore potentially incorrect values 
WITHOUT_CLASSIFICATION	 prevent view from referencing itself 
WITHOUT_CLASSIFICATION	 generate binary sortable key for current row vectorized row batch 
WITHOUT_CLASSIFICATION	 used replication copy files from source destination possible source file changedremoved during copy double check the checksum after copy 
WITHOUT_CLASSIFICATION	 means serializing another instance 
WITHOUT_CLASSIFICATION	 not print duplicate status while still middle print interval 
WITHOUT_CLASSIFICATION	 bail out 
WITHOUT_CLASSIFICATION	 use utc date ensure reader date same all timezones 
WITHOUT_CLASSIFICATION	 always set the null flag false when there value 
WITHOUT_CLASSIFICATION	 read the template into string 
WITHOUT_CLASSIFICATION	 bootstrap dump with empty 
WITHOUT_CLASSIFICATION	 case 
WITHOUT_CLASSIFICATION	 sleep this many seconds between each retry 
WITHOUT_CLASSIFICATION	 first look for this alias from cte and then from catalog 
WITHOUT_CLASSIFICATION	 the partition columns cant all found the values then the data not bucketed 
WITHOUT_CLASSIFICATION	 desttablename 
WITHOUT_CLASSIFICATION	 binary operator 
WITHOUT_CLASSIFICATION	 portion the join output 
WITHOUT_CLASSIFICATION	 only valid partitions should added 
WITHOUT_CLASSIFICATION	 create client instance 
WITHOUT_CLASSIFICATION	 all the setup done genmapredutils 
WITHOUT_CLASSIFICATION	 old version thrift client should have false but they not you add default value variable isset for that variable true regardless the where the message was created for object variables works correctly for boolean vars test mode upgrades are not tested client version and server version thrift always matches see unset here means something didnt set the appropriate value 
WITHOUT_CLASSIFICATION	 should allocate 
WITHOUT_CLASSIFICATION	 runtime objects 
WITHOUT_CLASSIFICATION	 nonjavadoc see for fns that are not give them the remaining rows rows whose span went beyond the end the partition for rest the functions invoke terminate while numoutputrows numinputrows for each that doesnt have enough invoke getnextobj there then flag this error 
WITHOUT_CLASSIFICATION	 this helper object deserializes lazybinary format small table values into columns row 
WITHOUT_CLASSIFICATION	 disable datasource case insert overwrite have disable the existing druid datasource 
WITHOUT_CLASSIFICATION	 need concurrent weakhashmap weakkeys that when underlying transport gets out scope still can gced since value map has ref key need weekvalues well 
WITHOUT_CLASSIFICATION	 raw socket connection nonsasl 
WITHOUT_CLASSIFICATION	 allocate free every other one allocate 
WITHOUT_CLASSIFICATION	 the right token 
WITHOUT_CLASSIFICATION	 move from the tmp dir intermediate directory the same level 
WITHOUT_CLASSIFICATION	 multiple instances such classes 
WITHOUT_CLASSIFICATION	 unused 
WITHOUT_CLASSIFICATION	 shellcmd binbash shellcmd 
WITHOUT_CLASSIFICATION	 special module for tests the rootdir 
WITHOUT_CLASSIFICATION	 udf rowid rowid 
WITHOUT_CLASSIFICATION	 hcat will always prune columns based what ask the 
WITHOUT_CLASSIFICATION	 okay were going need these originals recurse through them and figure out what really need 
WITHOUT_CLASSIFICATION	 existing lock for this partition 
WITHOUT_CLASSIFICATION	 func 
WITHOUT_CLASSIFICATION	 now pick server node randomly 
WITHOUT_CLASSIFICATION	 test bad field names 
WITHOUT_CLASSIFICATION	 checkh for and not the subquery must implicitly explicitly only contain one select item 
WITHOUT_CLASSIFICATION	 round each physical with row selection and logical with row selection 
WITHOUT_CLASSIFICATION	 build new join 
WITHOUT_CLASSIFICATION	 some join alias could changed alias newalias 
WITHOUT_CLASSIFICATION	 just return smaller than 
WITHOUT_CLASSIFICATION	 infer schema 
WITHOUT_CLASSIFICATION	 set memory available for operators 
WITHOUT_CLASSIFICATION	 does pool exist for this path already 
WITHOUT_CLASSIFICATION	 unused unknown reason 
WITHOUT_CLASSIFICATION	 end union 
WITHOUT_CLASSIFICATION	 invalid schemes 
WITHOUT_CLASSIFICATION	 suppress multispaces 
WITHOUT_CLASSIFICATION	 create the walker and the rules dispatcher 
WITHOUT_CLASSIFICATION	 then reopened session will use user specified queue name else default cluster queue names 
WITHOUT_CLASSIFICATION	 interrupted 
WITHOUT_CLASSIFICATION	 default block size set most cache line sizes are bytes and also avx friendly 
WITHOUT_CLASSIFICATION	 wrapper class write hsconnectionconfig file 
WITHOUT_CLASSIFICATION	 complete one task host 
WITHOUT_CLASSIFICATION	 compare next part 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 all keys matched 
WITHOUT_CLASSIFICATION	 check query results cache maskingfiltering required then can check the cache now before generating the operator tree and going through cbo 
WITHOUT_CLASSIFICATION	 should not have tried print any thing 
WITHOUT_CLASSIFICATION	 returns the singleton instance cache 
WITHOUT_CLASSIFICATION	 check for queryhint expressions ast 
WITHOUT_CLASSIFICATION	 reset the location and table name and compare the partitions 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 january 
WITHOUT_CLASSIFICATION	 when reach here must have some data already because size need see there are any data flushed into file system not can directly read from the current write block otherwise need read from the beginning the underlying file 
WITHOUT_CLASSIFICATION	 fetch existing ingestion spec from druid any 
WITHOUT_CLASSIFICATION	 set isnull before calls case they change their mind 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 setting defaults the same zookeeper 
WITHOUT_CLASSIFICATION	 for each the tables that are part the materialized view where the transaction had committed after the materialization was created 
WITHOUT_CLASSIFICATION	 create instance hive order create the tables 
WITHOUT_CLASSIFICATION	 initiate minor compaction request the table 
WITHOUT_CLASSIFICATION	 the function should support both short date and full timestamp format time part the timestamp should not skipped 
WITHOUT_CLASSIFICATION	 primitive column types ignore nulls and just copy all values 
WITHOUT_CLASSIFICATION	 only input side has nulls 
WITHOUT_CLASSIFICATION	 updating bucket column should move row from one file another not supported 
WITHOUT_CLASSIFICATION	 loginfowriting offset tailoffset lrptroffset 
WITHOUT_CLASSIFICATION	 make sure get all deltas within single transaction multistatement txn generate multiple delta files with the same txnid range course maxwriteid has already been minor compacted all per statement deltas are obsolete 
WITHOUT_CLASSIFICATION	 the query contains more than one join 
WITHOUT_CLASSIFICATION	 batch batchcounter joinresultname batchsize batchsize somerowsfilteredout somerowsfilteredout 
WITHOUT_CLASSIFICATION	 matches skewed values 
WITHOUT_CLASSIFICATION	 use vector parent get 
WITHOUT_CLASSIFICATION	 here portion the above explain the filterexpr the tablescan the pushed predicate ppd the line simply not there otherwise the plan the same map operator tree tablescan alias acidtbl filterexpr type boolean filter operator predicate type boolean select operator 
WITHOUT_CLASSIFICATION	 note kerberos user appid doesnt have access 
WITHOUT_CLASSIFICATION	 mixing down into the lower bits this produces worse hashcode purely numeric terms but leaving entropy the higher bits not useful for bucketing scheme see jsr concurrenthashmap released under public domain note concurrenthashmap has since reverted this retain entropy bits higher support the level hashing for segment which operates higher bitmask 
WITHOUT_CLASSIFICATION	 need check the properties are valid especially for stats they might changed via alter table update statistics alter table set tblproperties the property not rowcount rawdatasize could not changed through update statistics 
WITHOUT_CLASSIFICATION	 for table replication reach the max number tasks then for the next run will try reload the same table again this mainly for ease understanding the code then can avoid handling loading partitions for the table given that the creation table lead reaching max tasks loading next table since current one does not have partitions 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader long 
WITHOUT_CLASSIFICATION	 prints short events information that are safe for consistent testing 
WITHOUT_CLASSIFICATION	 findreadslot returning not found know never went that far when were inserting findreadslot key key slot slot pairindex pairindex 
WITHOUT_CLASSIFICATION	 create the output array 
WITHOUT_CLASSIFICATION	 expected for permanent udfs this point 
WITHOUT_CLASSIFICATION	 use the current return type when creating new call for operators with return type built into the operator definition and with type inference rules such cast function with less than operands 
WITHOUT_CLASSIFICATION	 create tablescandesc 
WITHOUT_CLASSIFICATION	 column value provided replace column name with value 
WITHOUT_CLASSIFICATION	 hive udtf only has single input 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 output will also repeating and null 
WITHOUT_CLASSIFICATION	 all are selected 
WITHOUT_CLASSIFICATION	 schemaversions 
WITHOUT_CLASSIFICATION	 returns path the partition created any else path table 
WITHOUT_CLASSIFICATION	 may not exist depending the version hadoop being used build hive use reflection the following lines allow the test compile regardless what version hadoop update credname entry the credential provider credentialprovider provider providerflush 
WITHOUT_CLASSIFICATION	 comes from cluster wide configured queue names explicitly set user session sets tezqueuename user has specified one use the one from cluster wide queue names there way differentiate how this was set user system unset this after opening the session that reopening session uses the correct queue names client has not died and the user has explicitly set queue name 
WITHOUT_CLASSIFICATION	 number digits number digits 
WITHOUT_CLASSIFICATION	 table files dont have footer rows 
WITHOUT_CLASSIFICATION	 reload tables from the metastore and create data files 
WITHOUT_CLASSIFICATION	 transport specific confs 
WITHOUT_CLASSIFICATION	 for now only alter name owner class name type 
WITHOUT_CLASSIFICATION	 logicalcolumnindex 
WITHOUT_CLASSIFICATION	 try get cluster information once avoid immediate clusterupdate event 
WITHOUT_CLASSIFICATION	 found exact bin match for value then just increment that bins count otherwise need insert new bin and trim the resulting histogram back size possible optimization here might set some threshold under which just assumed equal the closest bin fabsvbinsbinx threshold then just increment bin this not done now because dont want make any 
WITHOUT_CLASSIFICATION	 counters for debugging cannot use existing counters cntr and nextcntr operator since want individually track the number rows from 
WITHOUT_CLASSIFICATION	 there may multiple selects chose the one closest the table 
WITHOUT_CLASSIFICATION	 look for single column optimization 
WITHOUT_CLASSIFICATION	 test alter table 
WITHOUT_CLASSIFICATION	 however must end after the split end otherwise the next one would have been read 
WITHOUT_CLASSIFICATION	 get partition metadata partition specified 
WITHOUT_CLASSIFICATION	 restriction subquery isnot allowed lhs 
WITHOUT_CLASSIFICATION	 aggr distinct the parameter name constructed 
WITHOUT_CLASSIFICATION	 and transformation creates nodes andor thus not triggered 
WITHOUT_CLASSIFICATION	 the user has not returned and the kill has failed are going brute force kill the whatever user does now irrelevant 
WITHOUT_CLASSIFICATION	 create new local work and setup the dummy ops 
WITHOUT_CLASSIFICATION	 cannot merge 
WITHOUT_CLASSIFICATION	 found conflict 
WITHOUT_CLASSIFICATION	 turn speculative execution for reducers 
WITHOUT_CLASSIFICATION	 can read more than need the actualcount not multiple the bytebuffer size and file big enough that case cannot use flip method but need set buffer limit manually trans 
WITHOUT_CLASSIFICATION	 remove unused table scan operators 
WITHOUT_CLASSIFICATION	 compare hive version and metastore version 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 pass configs and precreate parse context 
WITHOUT_CLASSIFICATION	 got exception during doing the unlock rethrow here 
WITHOUT_CLASSIFICATION	 both the classname and the protocol name are table properties the only hardwired assumption that records are fixed per table basis 
WITHOUT_CLASSIFICATION	 colname 
WITHOUT_CLASSIFICATION	 get the parameter values 
WITHOUT_CLASSIFICATION	 subqueryast 
WITHOUT_CLASSIFICATION	 now this shouldnt find the path the 
WITHOUT_CLASSIFICATION	 were replacing the current big table with new one need count the current one map table then 
WITHOUT_CLASSIFICATION	 get our singlecolumn string hash map information for this specialized class 
WITHOUT_CLASSIFICATION	 for the current context for generating file sink operator either insert into insert overwrite 
WITHOUT_CLASSIFICATION	 default which created during automatic logging initialization static initialization block changing contextselector runtime requires creating new context factory which will internally create new context selector based system property 
WITHOUT_CLASSIFICATION	 poll the operation status till the operation complete 
WITHOUT_CLASSIFICATION	 second column exists 
WITHOUT_CLASSIFICATION	 start insert statement transaction and roll back this transaction 
WITHOUT_CLASSIFICATION	 set the table transactional for compaction work 
WITHOUT_CLASSIFICATION	 update min counter new value less than min seen far 
WITHOUT_CLASSIFICATION	 the absence uncompressedraw data size total file size will used for statistics annotation but the file may compressed encoded and serialized which may lesser size than the actual uncompressedraw data size this factor will multiplied file size estimate 
WITHOUT_CLASSIFICATION	 restore the interrupted status since not want catch 
WITHOUT_CLASSIFICATION	 set all properties specified via command line 
WITHOUT_CLASSIFICATION	 operand oneoperand another result unknown 
WITHOUT_CLASSIFICATION	 now output this timestamps millis value the equivalent totz 
WITHOUT_CLASSIFICATION	 avoid having huge bloomfilter need scale false positive probability 
WITHOUT_CLASSIFICATION	 setup deserializerobj inspectors for the incoming data source 
WITHOUT_CLASSIFICATION	 estimate row count 
WITHOUT_CLASSIFICATION	 just made existing table full acid which wasnt acid before and passed all checks initialize the write sequence that can handle assigning rowids original files already present the table 
WITHOUT_CLASSIFICATION	 catching exceptions here makes sure that the thread doesnt die case unexpected exceptions 
WITHOUT_CLASSIFICATION	 check only the partition that exists all should well 
WITHOUT_CLASSIFICATION	 constant varchar projection 
WITHOUT_CLASSIFICATION	 selectoperator should use only simple castcolumn access 
WITHOUT_CLASSIFICATION	 load same data again additive 
WITHOUT_CLASSIFICATION	 the pool empty queue the request 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 build nonpartpartvirtual column info for new rowschema 
WITHOUT_CLASSIFICATION	 histogram used for quantile approximation the quantiles requested 
WITHOUT_CLASSIFICATION	 because only fractional digits its not this much accurate 
WITHOUT_CLASSIFICATION	 this test collector operator for mapjoin rowmode 
WITHOUT_CLASSIFICATION	 should this also just ignored throw for now doas unlike queue often set admin 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 its filesink bucketed files use the bucket count the reducer number 
WITHOUT_CLASSIFICATION	 collectionssortkvs 
WITHOUT_CLASSIFICATION	 determine input vector expression using the 
WITHOUT_CLASSIFICATION	 initialize the resources from command line 
WITHOUT_CLASSIFICATION	 source cmd 
WITHOUT_CLASSIFICATION	 fall through and look for other options 
WITHOUT_CLASSIFICATION	 the union task empty the files created for all the inputs are assembled the union context and later used initialize the union plan 
WITHOUT_CLASSIFICATION	 see 
WITHOUT_CLASSIFICATION	 first project all groupby keys plus the transformed agg input 
WITHOUT_CLASSIFICATION	 weve filled the buffer write out 
WITHOUT_CLASSIFICATION	 unknown category 
WITHOUT_CLASSIFICATION	 before cleaner there should items 
WITHOUT_CLASSIFICATION	 abort proactively that dont wait for timeout perhaps should add version abort which 
WITHOUT_CLASSIFICATION	 restore data 
WITHOUT_CLASSIFICATION	 branch 
WITHOUT_CLASSIFICATION	 fail misconfigured 
WITHOUT_CLASSIFICATION	 not every partition uses bitvector for ndv just fall back the traditional extrapolation methods 
WITHOUT_CLASSIFICATION	 flip inclusion 
WITHOUT_CLASSIFICATION	 monday july 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream 
WITHOUT_CLASSIFICATION	 blank byte smiling face with open mouth and smiling eyes bytes 
WITHOUT_CLASSIFICATION	 database 
WITHOUT_CLASSIFICATION	 asserting other partition parameters can also changed but not the location 
WITHOUT_CLASSIFICATION	 message bus related properties 
WITHOUT_CLASSIFICATION	 this method used check whether the subtype sub type the grouptype for nested attribute need check its existence the root path recursive way 
WITHOUT_CLASSIFICATION	 numslotsavailable can negative the callback after the thread completes delayed 
WITHOUT_CLASSIFICATION	 this expected 
WITHOUT_CLASSIFICATION	 check the limit 
WITHOUT_CLASSIFICATION	 this for conditionaltask 
WITHOUT_CLASSIFICATION	 get the information from calcite 
WITHOUT_CLASSIFICATION	 with timeout 
WITHOUT_CLASSIFICATION	 per jdbc spec the driver does not support catalogs will silently ignore this request 
WITHOUT_CLASSIFICATION	 check entries beyond one 
WITHOUT_CLASSIFICATION	 todo assert ikey 
WITHOUT_CLASSIFICATION	 need different record handler for mergefilework 
WITHOUT_CLASSIFICATION	 set constraint name null before sending listener 
WITHOUT_CLASSIFICATION	 the common case far 
WITHOUT_CLASSIFICATION	 the keyhash missing the bloom filter then the value cannot exist any the spilled partition return nomatch 
WITHOUT_CLASSIFICATION	 right now are few cheap redundant update calls lets just the simple thing 
WITHOUT_CLASSIFICATION	 update messages 
WITHOUT_CLASSIFICATION	 hiveconf hivestatsndverror default produces 
WITHOUT_CLASSIFICATION	 column family becomes map 
WITHOUT_CLASSIFICATION	 make sure hadoop credentials objects not reuse the maps 
WITHOUT_CLASSIFICATION	 setup the necessary metadata originating from analyze rewrite 
WITHOUT_CLASSIFICATION	 ensure pig schema correct 
WITHOUT_CLASSIFICATION	 either variables will never null because default value returned case absence 
WITHOUT_CLASSIFICATION	 allocate writeid from test txn and then verify validwriteidlist write ids committed and self test txn should valid but writeid open txn should invalid 
WITHOUT_CLASSIFICATION	 see bucketnumreducersq bucketnumreducersq todo try using set 
WITHOUT_CLASSIFICATION	 not supported for the test case 
WITHOUT_CLASSIFICATION	 this task contains sortmergejoin can converted mapjoin task this operator present the mapper for sortmerge join operator present followed regular join cannot converted auto mapjoin 
WITHOUT_CLASSIFICATION	 link the work with the work associated with the reduce sink that triggered this rule 
WITHOUT_CLASSIFICATION	 first handle the condition where the first fetch was never done big table empty 
WITHOUT_CLASSIFICATION	 first try extract long value from the strings and compare them 
WITHOUT_CLASSIFICATION	 undone for now all 
WITHOUT_CLASSIFICATION	 user did not specify partition 
WITHOUT_CLASSIFICATION	 for windows need pass hivehadoopclasspath java parameter while starting 
WITHOUT_CLASSIFICATION	 test the same day month 
WITHOUT_CLASSIFICATION	 class 
WITHOUT_CLASSIFICATION	 skewed value 
WITHOUT_CLASSIFICATION	 build row type from field type name 
WITHOUT_CLASSIFICATION	 simple truncation 
WITHOUT_CLASSIFICATION	 iterate through all and locate the one introduce enforce bucketing 
WITHOUT_CLASSIFICATION	 fail transactional set true but the table not bucketed 
WITHOUT_CLASSIFICATION	 verify that have got correct set deletedeltas 
WITHOUT_CLASSIFICATION	 not calculated since high integer always for our decimals 
WITHOUT_CLASSIFICATION	 possible have file with same checksum cmpath but the content partially copied corrupted this case just overwrite the existing file with new one 
WITHOUT_CLASSIFICATION	 skip when already eof 
WITHOUT_CLASSIFICATION	 test parent references from preparedstatement 
WITHOUT_CLASSIFICATION	 pos driver alias 
WITHOUT_CLASSIFICATION	 use mapping object here can build the projection any order and get the ordered output columns the end also avoid copying big table key into the small table result area for inner joins reference with the projection there can duplicate output columns 
WITHOUT_CLASSIFICATION	 requires exact types both sides setop 
WITHOUT_CLASSIFICATION	 physical optimizers which follow this need careful not invalidate the inferences made this optimizer only optimizers which depend the results this one should 
WITHOUT_CLASSIFICATION	 can use the whole batch for output matches 
WITHOUT_CLASSIFICATION	 return length characters utf string byte array beginning start that len bytes long 
WITHOUT_CLASSIFICATION	 source code for the strtod library procedure copyright regents the university california permission use copy modify and distribute this software and its documentation for any purpose and without fee hereby granted provided that the above copyright notice appear all copies the university california makes representations about the suitability this software for any purpose provided without express implied warranty 
WITHOUT_CLASSIFICATION	 for caching aggregate column stats for all and all minus default partition key column name and the value list col stat objects 
WITHOUT_CLASSIFICATION	 figure out which kind bloom filter want for each column picks bloomfilterutf its available otherwise bloomfilter 
WITHOUT_CLASSIFICATION	 bgenjjtree unflagargs 
WITHOUT_CLASSIFICATION	 all selected nothing 
WITHOUT_CLASSIFICATION	 hive behavior where double decimal decimal gone 
WITHOUT_CLASSIFICATION	 vectorudfadaptor usage 
WITHOUT_CLASSIFICATION	 default 
WITHOUT_CLASSIFICATION	 add all children 
WITHOUT_CLASSIFICATION	 required required required required optional optional optional 
WITHOUT_CLASSIFICATION	 always just write mono 
WITHOUT_CLASSIFICATION	 check all parents are finished 
WITHOUT_CLASSIFICATION	 this the base class for the output parser output parser will parse the output pig hivehadoop other job and extract jobid note hadoop jobid extract rely the api hadoop application submitting the job different api will result different console output the jobid extraction logic not always working this case 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 gbinfo already has exprnode for gbkeys 
WITHOUT_CLASSIFICATION	 testing druid queries row filtering present 
WITHOUT_CLASSIFICATION	 default noop implementation 
WITHOUT_CLASSIFICATION	 just remove deletedelta there have been delete events 
WITHOUT_CLASSIFICATION	 tableonly fetch partitions regular export dont metadataonly 
WITHOUT_CLASSIFICATION	 ptf function must provide the external names the columns the transformed raw input 
WITHOUT_CLASSIFICATION	 mutate data 
WITHOUT_CLASSIFICATION	 nullindicator now different location the output 
WITHOUT_CLASSIFICATION	 make sure only return valdecompressor once 
WITHOUT_CLASSIFICATION	 means there transaction select statement which not part explicit transaction iud statement that not writing acid table 
WITHOUT_CLASSIFICATION	 use tez combine splits 
WITHOUT_CLASSIFICATION	 new plan absolutely better than old plan 
WITHOUT_CLASSIFICATION	 isenableandactivate 
WITHOUT_CLASSIFICATION	 return the caller since long polling timeout has expired 
WITHOUT_CLASSIFICATION	 see the above release the headers after unlocking 
WITHOUT_CLASSIFICATION	 note just like the move path only one level recursion 
WITHOUT_CLASSIFICATION	 test repeating nulls case 
WITHOUT_CLASSIFICATION	 allocate the source deserialization related arrays 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 rexliterals equal only consider value and type which isnt sufficient providing custom comparator which distinguishes objects irrespective valuetype 
WITHOUT_CLASSIFICATION	 this should never happen does its bug with the potential produce incorrect results 
WITHOUT_CLASSIFICATION	 get estimation parameters 
WITHOUT_CLASSIFICATION	 the ieee floating point spec specifies that signed and should treated equal 
WITHOUT_CLASSIFICATION	 temporary singlebatch context used for vectorization 
WITHOUT_CLASSIFICATION	 consider query like select from where the view defined select from the inputs will contain and parent will marked indirect entity using isdirect flag this will help distinguishing from the case where direct dependency 
WITHOUT_CLASSIFICATION	 violation etl queue 
WITHOUT_CLASSIFICATION	 table created hive catalog should have been automatically set transactional 
WITHOUT_CLASSIFICATION	 operators 
WITHOUT_CLASSIFICATION	 kill queries 
WITHOUT_CLASSIFICATION	 allocate output column and get column number 
WITHOUT_CLASSIFICATION	 null big table partitioned 
WITHOUT_CLASSIFICATION	 clear completed instances this case dont want provide information from the previous run 
WITHOUT_CLASSIFICATION	 determine the transaction manager use from the configuration 
WITHOUT_CLASSIFICATION	 drop the tables when were done this makes the test work inside ide 
WITHOUT_CLASSIFICATION	 history file stream 
WITHOUT_CLASSIFICATION	 the version that was included with the original magic which mapped 
WITHOUT_CLASSIFICATION	 datatype column 
WITHOUT_CLASSIFICATION	 prove invariant the compiler len len all array access between start startlen and start startlen are valid more oob exceptions are possible 
WITHOUT_CLASSIFICATION	 rowresolver helper methods 
WITHOUT_CLASSIFICATION	 execute generated plan 
WITHOUT_CLASSIFICATION	 may field name return the identifier and let the caller decide whether not 
WITHOUT_CLASSIFICATION	 set joins already processed 
WITHOUT_CLASSIFICATION	 establish context 
WITHOUT_CLASSIFICATION	 such partition 
WITHOUT_CLASSIFICATION	 check the stripped property the empty string 
WITHOUT_CLASSIFICATION	 all the contain bitvectors not need use uniform distribution assumption because can merge bitvectors get good estimation 
WITHOUT_CLASSIFICATION	 create data container 
WITHOUT_CLASSIFICATION	 should not happen 
WITHOUT_CLASSIFICATION	 test that exclusive blocks exclusive and write 
WITHOUT_CLASSIFICATION	 when run task after the jar has been added 
WITHOUT_CLASSIFICATION	 whether the statement boolean expression was repeating 
WITHOUT_CLASSIFICATION	 split the children into vertices that make the union and vertices that are 
WITHOUT_CLASSIFICATION	 figure out correlation and presence nonequi join predicate 
WITHOUT_CLASSIFICATION	 appends might exist after the root message strip tokens off until 
WITHOUT_CLASSIFICATION	 array strings type array arrays strings 
WITHOUT_CLASSIFICATION	 this constant was created while doing constant folding foldedfromcol holds the name original column from which was folded 
WITHOUT_CLASSIFICATION	 override hive specific operators 
WITHOUT_CLASSIFICATION	 not the right host 
WITHOUT_CLASSIFICATION	 only need username for ugi use for groups getgroups will fetch the groups based hadoop configuration documented 
WITHOUT_CLASSIFICATION	 make sure not loose 
WITHOUT_CLASSIFICATION	 when bias correction enabled 
WITHOUT_CLASSIFICATION	 release the unreleased stripelevel buffers see class comment about refcounts 
WITHOUT_CLASSIFICATION	 excluded via property already has stats only expect two updates 
WITHOUT_CLASSIFICATION	 this run initiator doesnt add any compactionqueue entry 
WITHOUT_CLASSIFICATION	 extract the collations indices 
WITHOUT_CLASSIFICATION	 test that existing sharedread table with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 run batch 
WITHOUT_CLASSIFICATION	 woe 
WITHOUT_CLASSIFICATION	 verify droptable recycle partition files 
WITHOUT_CLASSIFICATION	 avoid instantiation 
WITHOUT_CLASSIFICATION	 return just the single range 
WITHOUT_CLASSIFICATION	 only green qualifies and its entry 
WITHOUT_CLASSIFICATION	 add cast expression needed child expressions udf may return different data types and that would require converting their data types evaluate the udf for example decimal column added integer column would require integer column cast decimal 
WITHOUT_CLASSIFICATION	 note partcolsisnull only used for ptf which isnt supported yet 
WITHOUT_CLASSIFICATION	 possible reasons end here unable read version from manifestmf version string not the proper xxxxx format 
WITHOUT_CLASSIFICATION	 complex type constants currently not supported 
WITHOUT_CLASSIFICATION	 handle normal case 
WITHOUT_CLASSIFICATION	 current match may out order wrt the global name list add specific parts 
WITHOUT_CLASSIFICATION	 colnamespace 
WITHOUT_CLASSIFICATION	 need override assigns all assign ops will fail due size 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 for each row produced 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 insert junk middle file assumes file local disk 
WITHOUT_CLASSIFICATION	 rename partitioned table 
WITHOUT_CLASSIFICATION	 all values are filtered out 
WITHOUT_CLASSIFICATION	 final check find size alreadycalculated mapjoin operators same work sparkstage 
WITHOUT_CLASSIFICATION	 current state each selected column current run length etc 
WITHOUT_CLASSIFICATION	 make sure the proper transaction manager that supports acid being used 
WITHOUT_CLASSIFICATION	 find the same key 
WITHOUT_CLASSIFICATION	 invalid app name checked later 
WITHOUT_CLASSIFICATION	 cannot apply the reduction 
WITHOUT_CLASSIFICATION	 accept int writables and convert them 
WITHOUT_CLASSIFICATION	 row results should null 
WITHOUT_CLASSIFICATION	 allow date string casts note suspect this the reverse what actually want but matches the code cant see how users would altering date columns into string columns the other easily see since hive did not originally support datetime types also the comment the hive code says string date even though the code does the opposite but for now keeping 
WITHOUT_CLASSIFICATION	 number aborted txns found exceptions 
WITHOUT_CLASSIFICATION	 launch task 
WITHOUT_CLASSIFICATION	 test single highprecision multiply random inputs 
WITHOUT_CLASSIFICATION	 each branch 
WITHOUT_CLASSIFICATION	 just checked the user specified schema columns among regular table column and found some which are not regular now check they are dynamic partition columns for dynamic partitioning given create table multiparta int int partitioned int int for insert into multipart partitioncdda values expect parse tree look like this tokinsertinto toktab toktabname multipart tokpartspec tokpartval tokpartval toktabcolname 
WITHOUT_CLASSIFICATION	 have found valid cookie the client request 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 any the other fields need set 
WITHOUT_CLASSIFICATION	 open txns 
WITHOUT_CLASSIFICATION	 the inputoi the same the outputoi just return identityconverter 
WITHOUT_CLASSIFICATION	 tsfiltsfilfil 
WITHOUT_CLASSIFICATION	 pretend like has fractional digits can get the trailing zero count 
WITHOUT_CLASSIFICATION	 the output path used the subclasses used final destination same outpath for tables 
WITHOUT_CLASSIFICATION	 topnhashes are active proceed not already excluded order limit 
WITHOUT_CLASSIFICATION	 need check again exists 
WITHOUT_CLASSIFICATION	 normally import trying create table partition that does not yet exist error condition however the case repl load possible that are trying create tasks create table inside that asofnow does not exist but there precursor task waiting that will create before this encountered thus instantiate defaults and not error out that case 
WITHOUT_CLASSIFICATION	 already done iulrindex initialized before normalization 
WITHOUT_CLASSIFICATION	 empty list 
WITHOUT_CLASSIFICATION	 this indicates original query was either correlated exists 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 future this may examine config return appropriate hcatreader 
WITHOUT_CLASSIFICATION	 specific files they exist 
WITHOUT_CLASSIFICATION	 this class authenticates web via pam authenticate use httpget with header name authorization and header value basic authbcode where authbcode base string for loginpassword 
WITHOUT_CLASSIFICATION	 indices 
WITHOUT_CLASSIFICATION	 convert the work the smb plan regular join note that the operator tree not fixed only the pathalias mappings the plan are fixed the operator tree will still contain the smbjoinoperator 
WITHOUT_CLASSIFICATION	 create file well import later 
WITHOUT_CLASSIFICATION	 local mode command like dfs 
WITHOUT_CLASSIFICATION	 have rely hive implementation filesystem permission checks 
WITHOUT_CLASSIFICATION	 generate operator prune the tablenames only count the tablenames that are not empty strings empty string table aliases only allowed for virtual columns 
WITHOUT_CLASSIFICATION	 not using byref now since its unsafe for text readers might safe for others 
WITHOUT_CLASSIFICATION	 for compare will convert requisite children for between skip the first child the revert boolean 
WITHOUT_CLASSIFICATION	 create table 
WITHOUT_CLASSIFICATION	 handle repeating null 
WITHOUT_CLASSIFICATION	 indicate used the last rows bytes for large buffer 
WITHOUT_CLASSIFICATION	 msck called drop stale paritions from metastore and there are stale partitions 
WITHOUT_CLASSIFICATION	 add signature 
WITHOUT_CLASSIFICATION	 key key key key 
WITHOUT_CLASSIFICATION	 turn off passwords enable sasl and set keytab 
WITHOUT_CLASSIFICATION	 outer join hash map 
WITHOUT_CLASSIFICATION	 clear the previous values recorded 
WITHOUT_CLASSIFICATION	 classify partitions within the table directory into groups based shared properties 
WITHOUT_CLASSIFICATION	 initialize the metrics system 
WITHOUT_CLASSIFICATION	 use minimrcluster mapreduce 
WITHOUT_CLASSIFICATION	 handle repeating case 
WITHOUT_CLASSIFICATION	 numreducers newnumreducers newnumreducers numreducers will not consider reducesinkoperator with this newnumreducer correlated reducesinkoperator 
WITHOUT_CLASSIFICATION	 perform major compaction 
WITHOUT_CLASSIFICATION	 verify table for key byte hash table hashmap 
WITHOUT_CLASSIFICATION	 now new job requests should succeed list operation has cancel threads 
WITHOUT_CLASSIFICATION	 hive added files and jars 
WITHOUT_CLASSIFICATION	 next node consistent order died does not have free slots rollover next 
WITHOUT_CLASSIFICATION	 note power can positive negative note power effectively multiply 
WITHOUT_CLASSIFICATION	 add the privs for roles curroles new roletopriv map 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 get our vector map join fast hash table variation from the vector map join table container 
WITHOUT_CLASSIFICATION	 load keycountadj keycountadj load threshold threshold load loadfactor loadfactor load wbsize wbsize 
WITHOUT_CLASSIFICATION	 insert rows both tables 
WITHOUT_CLASSIFICATION	 retrieve token and store the cache 
WITHOUT_CLASSIFICATION	 the antlr grammar looks like kwconstraint idfridentifier kwforeign kwkey kwreferences tabnametablename tokforeignkey idfr fkcols tabname parcols relyspec enablespec validatespec when the user specifies the constraint name childgetchildcount kwforeign kwkey kwreferences tabnametablename tokforeignkey fkcols tabname parcols relyspec enablespec validatespec when the user does not specify the constraint name childgetchildcount 
WITHOUT_CLASSIFICATION	 order keys 
WITHOUT_CLASSIFICATION	 replace bucketing columns with hashcode numbuckets 
WITHOUT_CLASSIFICATION	 create clauses 
WITHOUT_CLASSIFICATION	 this test verifies that the alter table set owner command will change the owner metadata the table hms 
WITHOUT_CLASSIFICATION	 release the copies made directly the cleaner 
WITHOUT_CLASSIFICATION	 convert our source object just read into the target object and store that the vectorizedrowbatch 
WITHOUT_CLASSIFICATION	 prefixing with mask ensure conflict with named columns the file schema 
WITHOUT_CLASSIFICATION	 loginforeturning 
WITHOUT_CLASSIFICATION	 simple join from relations denom maxv 
WITHOUT_CLASSIFICATION	 look for any valid versions this will also throw the schema itself doesnt exist which what want 
WITHOUT_CLASSIFICATION	 hive will reuse the configuration object that passes initialized therefore need make sure dont look for any 
WITHOUT_CLASSIFICATION	 this testfilter 
WITHOUT_CLASSIFICATION	 protected for testing and can pass conf for testing 
WITHOUT_CLASSIFICATION	 grantor 
WITHOUT_CLASSIFICATION	 get the list dynamic partition paths 
WITHOUT_CLASSIFICATION	 the start 
WITHOUT_CLASSIFICATION	 starting from the given rowidx scan the given direction until rows expr evaluates amt that crosses the amt threshold specified the boundarydef 
WITHOUT_CLASSIFICATION	 try read the dropped partition via cachedstore 
WITHOUT_CLASSIFICATION	 string 
WITHOUT_CLASSIFICATION	 partitioned table 
WITHOUT_CLASSIFICATION	 nothing add 
WITHOUT_CLASSIFICATION	 task has completed 
WITHOUT_CLASSIFICATION	 srcpositionstart cant accept negative numbers 
WITHOUT_CLASSIFICATION	 need three params differentiate between this and param method auto generated since some calls the autogenerated code use null param for param and thus 
WITHOUT_CLASSIFICATION	 note dont add added jars reloadable jars etc that design there are too many ways add jars hive some which are sessionetc specific env conf arg should enough 
WITHOUT_CLASSIFICATION	 create keyvalue tabledesc when the operator plan split into tasks the reduce operator will initialize extract operator with information 
WITHOUT_CLASSIFICATION	 weve opened transaction need commit rollback rather than explicitly 
WITHOUT_CLASSIFICATION	 char strategic blanks string length beyond max 
WITHOUT_CLASSIFICATION	 cmapintmapintint 
WITHOUT_CLASSIFICATION	 fallback regular logic 
WITHOUT_CLASSIFICATION	 extract all the columns 
WITHOUT_CLASSIFICATION	 know they are not equal because the one with the larger scale has nonzero digits below the others scale since the scale does not include trailing zeroes 
WITHOUT_CLASSIFICATION	 add supported protocols 
WITHOUT_CLASSIFICATION	 todo verify any escaping needed for values 
WITHOUT_CLASSIFICATION	 use method adddelegationtokens instead getdelegationtoken get all the tokens including kms 
WITHOUT_CLASSIFICATION	 the biggest output from parent 
WITHOUT_CLASSIFICATION	 note hive convention doesnt pushdown non deterministic expressions 
WITHOUT_CLASSIFICATION	 local plan not null want merge into smbmapjoinoperators local work 
WITHOUT_CLASSIFICATION	 there are elements the list 
WITHOUT_CLASSIFICATION	 get partial aggregation results and store reducevalues 
WITHOUT_CLASSIFICATION	 comes from tblproperties compact ttp 
WITHOUT_CLASSIFICATION	 create the lineage context 
WITHOUT_CLASSIFICATION	 source path subdirectory the destination path the other way around insert overwrite directory select srcvalue where srckey where the staging directory subdirectory the destination directory not delete the dest dir before doing the move operation assumed that subdir and dir are same encryption zone move individual files from scr dir dest dir 
WITHOUT_CLASSIFICATION	 find the move task 
WITHOUT_CLASSIFICATION	 serialize the hcatpartitionspec 
WITHOUT_CLASSIFICATION	 multikey specific lookup key 
WITHOUT_CLASSIFICATION	 make sure buffers are eligible for discard 
WITHOUT_CLASSIFICATION	 clear rounding portion middle longword and add right scale roundmultiplyfactor lower longword result 
WITHOUT_CLASSIFICATION	 the other overload should have been used 
WITHOUT_CLASSIFICATION	 factory method for serde 
WITHOUT_CLASSIFICATION	 middle word bits need multiplier longmaxvalue digit commad 
WITHOUT_CLASSIFICATION	 the table alias information map already contains the current table 
WITHOUT_CLASSIFICATION	 reached here have match for generated cookie 
WITHOUT_CLASSIFICATION	 deregister versionnumber 
WITHOUT_CLASSIFICATION	 are running using the same umbilical 
WITHOUT_CLASSIFICATION	 set the security for plugin endpoint will create the token and publish the registry note this application bogus and only needed for 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this the start containerannotated logging 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 the antlr grammar looks like kwconstraint idfridentifier kwprimary kwkey tokprimarykey pkcols idfr when the user specifies the constraint name kwprimary kwkey tokprimarykey when the user does not specify the constraint name 
WITHOUT_CLASSIFICATION	 tests for partition tablename string dbname string name method 
WITHOUT_CLASSIFICATION	 currently all tez work the cluster 
WITHOUT_CLASSIFICATION	 verify preemption requests since everything the same priority 
WITHOUT_CLASSIFICATION	 hashcodes had conversion positive done different ways the past abs now obsolete and all inserts now use integermaxvalue the compat mode assumes that old data couldve been loaded using the other conversion 
WITHOUT_CLASSIFICATION	 init 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 while processing bucket columns atleast one bucket column missed this results different bucketing scheme add empty list 
WITHOUT_CLASSIFICATION	 add sortingbucketing needed 
WITHOUT_CLASSIFICATION	 longsize tablerowsize longsize 
WITHOUT_CLASSIFICATION	 skip class loading the class name didnt change 
WITHOUT_CLASSIFICATION	 can fully qualified name use default database 
WITHOUT_CLASSIFICATION	 the outputops this vertex 
WITHOUT_CLASSIFICATION	 for bucketed tables optimization 
WITHOUT_CLASSIFICATION	 not display vectorization objects 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 abort all the allocated txns that the mapped write ids are referred aborted ones 
WITHOUT_CLASSIFICATION	 bogus warnings despite closequietly 
WITHOUT_CLASSIFICATION	 fallback for unexpected relnode type 
WITHOUT_CLASSIFICATION	 this means that nothing was set for default stripe size previously should unset 
WITHOUT_CLASSIFICATION	 start the cache threads cache also serves buffer manager 
WITHOUT_CLASSIFICATION	 the data must type string 
WITHOUT_CLASSIFICATION	 this means the process will exit without waiting for this thread 
WITHOUT_CLASSIFICATION	 this shouldnt throw exception 
WITHOUT_CLASSIFICATION	 the serialized nonnull series keys these members represent the value 
WITHOUT_CLASSIFICATION	 streaming ingest writer with single transaction batch size which case the transaction either committed aborted either cases dont need flush length file but need flush intermediate footer reduce memory pressure also with hive streaming writer does automatic memory management which would require flush open files without actually closing 
WITHOUT_CLASSIFICATION	 now were use stale value would use 
WITHOUT_CLASSIFICATION	 query logresutsr explain logical 
WITHOUT_CLASSIFICATION	 logdebugclassname repeated key key 
WITHOUT_CLASSIFICATION	 for nonmm tables directory structure 
WITHOUT_CLASSIFICATION	 test greater 
WITHOUT_CLASSIFICATION	 init without the knowledge llap usage lack thereof the cluster 
WITHOUT_CLASSIFICATION	 handles cases where the query has predicate columnnameconstant 
WITHOUT_CLASSIFICATION	 have singleton and just return the child 
WITHOUT_CLASSIFICATION	 there bitvector but not adjacent the previous ones 
WITHOUT_CLASSIFICATION	 log warn given how unfortunate this 
WITHOUT_CLASSIFICATION	 load row 
WITHOUT_CLASSIFICATION	 handle string types properly 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 gets list job ids and calls getjobstatus get status for each job 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 override properties from tblproperties applicable 
WITHOUT_CLASSIFICATION	 before reparsing they are known semanticanalyzer logic 
WITHOUT_CLASSIFICATION	 both inputs the join are unique there nothing gained this rule fact this aggregatejoin may the result previous invocation this rule continue might loop forever 
WITHOUT_CLASSIFICATION	 make copy 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 number elements sum elements sumxavg this actually times the variance 
WITHOUT_CLASSIFICATION	 returns true the specified path matches the prefix stored 
WITHOUT_CLASSIFICATION	 set entry null 
WITHOUT_CLASSIFICATION	 substitute for any carriage return line feed characters line with the escaped character sequences param line the string for the crlf substitution return there were replacements then just return line otherwise new string with escaped crlf 
WITHOUT_CLASSIFICATION	 this stream can separated using index lets that 
WITHOUT_CLASSIFICATION	 since rowcount used later instantiate 
WITHOUT_CLASSIFICATION	 connect via kerberos with trusted proxy user 
WITHOUT_CLASSIFICATION	 add test parameters from storage formats specified table 
WITHOUT_CLASSIFICATION	 src scratch directory need trim the part key value pairs from path 
WITHOUT_CLASSIFICATION	 configure the broker 
WITHOUT_CLASSIFICATION	 when type config set hive 
WITHOUT_CLASSIFICATION	 testing multibyte string substring 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject int int 
WITHOUT_CLASSIFICATION	 drop the materialized view but not its data 
WITHOUT_CLASSIFICATION	 should also not able add fields different types with same name 
WITHOUT_CLASSIFICATION	 especially since llap prone turn off the mapjoindesc later 
WITHOUT_CLASSIFICATION	 async progress the callback will take care this 
WITHOUT_CLASSIFICATION	 get next batch table names this list 
WITHOUT_CLASSIFICATION	 failed set job status completed which mean the main thread would have exited and not waiting for the result kill the submitted job 
WITHOUT_CLASSIFICATION	 taskwrapper used structures well for ordering using comparators 
WITHOUT_CLASSIFICATION	 represents windowframe applied partitioning window can refer isourcei window name the source window provides the basis for this window definition this window specification extendsoverrides the isourcei window definition our the select expression sumpretailprice over translated into windowfunction instance that has window specification that refers the global window specification the functions specification has content but inherits all its attributes from during subsequent phases translation 
WITHOUT_CLASSIFICATION	 when are doing vector deserialization these are the fast deserializer and the vector row deserializer 
WITHOUT_CLASSIFICATION	 objectstore also stores name lowercase 
WITHOUT_CLASSIFICATION	 loglevel args are parsed the python processor 
WITHOUT_CLASSIFICATION	 the user asking the token same the owner then dont any proxy authorization checks for cases like oozie where gets delegation token for another user need make sure oozie authorized get delegation token 
WITHOUT_CLASSIFICATION	 generate the list bucketing pruning predicate 
WITHOUT_CLASSIFICATION	 set transitive true default 
WITHOUT_CLASSIFICATION	 just forward the row 
WITHOUT_CLASSIFICATION	 havent added anything should return all 
WITHOUT_CLASSIFICATION	 classification 
WITHOUT_CLASSIFICATION	 now apply sarg any sarg this will just initialize stripergs 
WITHOUT_CLASSIFICATION	 add columns from inprr 
WITHOUT_CLASSIFICATION	 through all map joins and find out all which have enabled bucket map 
WITHOUT_CLASSIFICATION	 clone the original join operator and replace with the 
WITHOUT_CLASSIFICATION	 restore input and output streams 
WITHOUT_CLASSIFICATION	 not updated yet 
WITHOUT_CLASSIFICATION	 connect jersey 
WITHOUT_CLASSIFICATION	 note that valid value corresponding nanosecond timestamp because the second vint present use the value reversednanoseconds the second vint 
WITHOUT_CLASSIFICATION	 operator file sink reduce sink something that forces new vertex 
WITHOUT_CLASSIFICATION	 want the driver try print the header 
WITHOUT_CLASSIFICATION	 leftright skip filtered valid skip filtered valid right alias has any pair for left alias continue for continue has pair but not this turn for inner join join and continue 
WITHOUT_CLASSIFICATION	 its all good create new entry the map update existing one 
WITHOUT_CLASSIFICATION	 case the job context not yet lets wait since this supposed synchronous rpc 
WITHOUT_CLASSIFICATION	 scratch objects 
WITHOUT_CLASSIFICATION	 got using clause previous join need generate select list per standard for will have joining columns first nonrepeated followed other columns 
WITHOUT_CLASSIFICATION	 has this filesink already been processed 
WITHOUT_CLASSIFICATION	 this constant for whole series 
WITHOUT_CLASSIFICATION	 first make sure through complete iteration the loop before resetting 
WITHOUT_CLASSIFICATION	 when running unit test mode pass this property hcat which will turn pass hive make sure that hive tries write directory that exists 
WITHOUT_CLASSIFICATION	 new table always created with new column descriptor 
WITHOUT_CLASSIFICATION	 year month day 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 set the bit element not null 
WITHOUT_CLASSIFICATION	 check the file format the file matches that the table 
WITHOUT_CLASSIFICATION	 use this map map the position arglist the position grouping set 
WITHOUT_CLASSIFICATION	 were interested specific partitions dont check for any others 
WITHOUT_CLASSIFICATION	 number rows not matching the regex 
WITHOUT_CLASSIFICATION	 attempting fix valid should not result new file 
WITHOUT_CLASSIFICATION	 loop through each the lists exprs looking for match 
WITHOUT_CLASSIFICATION	 check should turn into streaming mode 
WITHOUT_CLASSIFICATION	 test negative integers result 
WITHOUT_CLASSIFICATION	 attempt extended acl operations only its enabled but dont fail the operation regardless 
WITHOUT_CLASSIFICATION	 read each expression and save the value registry 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 skewed value has the same length 
WITHOUT_CLASSIFICATION	 the value read last time 
WITHOUT_CLASSIFICATION	 stop once see dynamic partition 
WITHOUT_CLASSIFICATION	 test when the session opened user 
WITHOUT_CLASSIFICATION	 then assume from its own vertex 
WITHOUT_CLASSIFICATION	 add rounding 
WITHOUT_CLASSIFICATION	 schema 
WITHOUT_CLASSIFICATION	 currently testproccedures always returns empty resultset for hive 
WITHOUT_CLASSIFICATION	 possible concurrent modification issues try remove cache entries while traversing the cache structures save the entries remove separate list 
WITHOUT_CLASSIFICATION	 otherwise fall back return null use local tmp dir only 
WITHOUT_CLASSIFICATION	 make sure nothing escapes this run method and kills the metastore large wrap big catch throwable statement 
WITHOUT_CLASSIFICATION	 these members have information for extracting row column objects from vectorizedrowbatch columns 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject 
WITHOUT_CLASSIFICATION	 compute statistics for columns viewtime 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 generate test data 
WITHOUT_CLASSIFICATION	 take away the only session was expiring 
WITHOUT_CLASSIFICATION	 standardstruct uses arraylist store the row 
WITHOUT_CLASSIFICATION	 noarg ctor required for json deserialization 
WITHOUT_CLASSIFICATION	 create rules registry not trigger rule more than once 
WITHOUT_CLASSIFICATION	 wasnt create dbtable 
WITHOUT_CLASSIFICATION	 user provided fully specified partition spec but doesnt exist fail 
WITHOUT_CLASSIFICATION	 join occurs before the sortmerge join not useful convert the the sortmerge join mapjoin might simpler perform the join and then sortmerge join join converting the sortmerge join mapjoin the job will executed mapjoins the best case the number inputs for the join more than would difficult figure out the big table for the mapjoin 
WITHOUT_CLASSIFICATION	 construct output object inspector 
WITHOUT_CLASSIFICATION	 left input positions are not changed 
WITHOUT_CLASSIFICATION	 first pass identify and break tree necessary 
WITHOUT_CLASSIFICATION	 hcat will allow these operations performed 
WITHOUT_CLASSIFICATION	 insert two rows partitioned table 
WITHOUT_CLASSIFICATION	 identical strings 
WITHOUT_CLASSIFICATION	 this case are assuming that there single distinct function 
WITHOUT_CLASSIFICATION	 create and drop partition times events 
WITHOUT_CLASSIFICATION	 invalid character present return null 
WITHOUT_CLASSIFICATION	 directory 
WITHOUT_CLASSIFICATION	 location will vary system 
WITHOUT_CLASSIFICATION	 the form 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 integer part 
WITHOUT_CLASSIFICATION	 only have file done 
WITHOUT_CLASSIFICATION	 writable constant null then return size 
WITHOUT_CLASSIFICATION	 its map 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 same priority for all tasks 
WITHOUT_CLASSIFICATION	 create table target 
WITHOUT_CLASSIFICATION	 make left child right 
WITHOUT_CLASSIFICATION	 modifying filter condition the incremental rewriting rule generated clause where first disjunct contains the condition for the update branch tokwhere and disjunct for update toktableorcol hdt toktableorcol hdt toktableorcol hdt toktableorcol hdt and disjunct for insert tokfunction isnull toktableorcol hdt tokfunction isnull toktableorcol hdt 
WITHOUT_CLASSIFICATION	 exceptions the range 
WITHOUT_CLASSIFICATION	 assume the varchar maximum length was enforced when the object was created 
WITHOUT_CLASSIFICATION	 stub out the serializer 
WITHOUT_CLASSIFICATION	 test repeating logic 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 return immediately entries found for pruning verified via the timeout 
WITHOUT_CLASSIFICATION	 and 
WITHOUT_CLASSIFICATION	 allow multiple mappings the same columninfo when columninfo mapped multiple times only the first inverse mapping captured 
WITHOUT_CLASSIFICATION	 steps extract the archive temporary folder move the archive dir intermediate dir that the same dir originallocation call the new dir rename the original partitions dir intermediate dir call the renamed dir intermediatearchive rename the original partitions dir change the metadata delete the archived partitions files intermediatearchive 
WITHOUT_CLASSIFICATION	 skip mode msck should ignore invalid partitions instead throwing exception 
WITHOUT_CLASSIFICATION	 asserttrueexc instanceof hcatexception hcatexception excgeterrortype with dynamic partitioning this isnt error that the keyvalues specified didnt values 
WITHOUT_CLASSIFICATION	 class fspaths 
WITHOUT_CLASSIFICATION	 unnecessary dataputintboffset dataputintboffset 
WITHOUT_CLASSIFICATION	 for other tasks just return its children tasks 
WITHOUT_CLASSIFICATION	 lastheartbeat 
WITHOUT_CLASSIFICATION	 its update metrics for two tasks parallel but not for the same one 
WITHOUT_CLASSIFICATION	 todo cat for now always use the default catalog eventually will want see the user specified catalog 
WITHOUT_CLASSIFICATION	 the returned value hivedecimal assume maximum precisionscale 
WITHOUT_CLASSIFICATION	 class connectionimpl 
WITHOUT_CLASSIFICATION	 now inplace reversal resultgetbytes first reverse every 
WITHOUT_CLASSIFICATION	 binarystats 
WITHOUT_CLASSIFICATION	 set ddl time now not specified 
WITHOUT_CLASSIFICATION	 all tables partitions are bucketed and their bucket number stored bucketnumbers need check the number buckets 
WITHOUT_CLASSIFICATION	 scheme use default file system uri 
WITHOUT_CLASSIFICATION	 these arent column types they are info for how things are stored thrift 
WITHOUT_CLASSIFICATION	 any the type isnt exact double chosen 
WITHOUT_CLASSIFICATION	 hcat wants intercept following tokens and specialhandle them 
WITHOUT_CLASSIFICATION	 use result with some all fractional digits stripped 
WITHOUT_CLASSIFICATION	 for auth 
WITHOUT_CLASSIFICATION	 converting tofrom external table 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 nothing this should generate proper index 
WITHOUT_CLASSIFICATION	 need specify the reserved memory for each work that contains map join 
WITHOUT_CLASSIFICATION	 fixup parent and child relations 
WITHOUT_CLASSIFICATION	 look them make sure they are all there 
WITHOUT_CLASSIFICATION	 statementid from directory name there none 
WITHOUT_CLASSIFICATION	 using biggest small table calculate number partitions create for each small table 
WITHOUT_CLASSIFICATION	 need use new filesystem object here have the correct ugi 
WITHOUT_CLASSIFICATION	 position last sync random bytes 
WITHOUT_CLASSIFICATION	 set the staging directory use 
WITHOUT_CLASSIFICATION	 groupprivileges 
WITHOUT_CLASSIFICATION	 cache 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 determine the direction order 
WITHOUT_CLASSIFICATION	 different columns means different commands have run 
WITHOUT_CLASSIFICATION	 verify that the partitions specified are continuous subpartition value specified without specifying partitions value 
WITHOUT_CLASSIFICATION	 merge them together 
WITHOUT_CLASSIFICATION	 conditional node constructed its condition true all the nodes that have been pushed since the node was opened are made children the conditional node which then pushed the stack the condition false the node not constructed and they are left the stack 
WITHOUT_CLASSIFICATION	 the following two keys should ideally used control connect timeouts however 
WITHOUT_CLASSIFICATION	 after constant folding child expression the return type udfwhen might have changed recreate the expression 
WITHOUT_CLASSIFICATION	 uris are checked for string equivalence even spaces make them different 
WITHOUT_CLASSIFICATION	 check orc and not sorted 
WITHOUT_CLASSIFICATION	 only print the first line the stack trace contains the error message and other lines may contain line numbers which are volatile also only take the string after the first two spaces because the prefix date and and time stamp 
WITHOUT_CLASSIFICATION	 release the hms connection for this service thread 
WITHOUT_CLASSIFICATION	 add more variables required 
WITHOUT_CLASSIFICATION	 since count has return type big int need make literal type big int relbuilders literal doesnt allow this 
WITHOUT_CLASSIFICATION	 this table dynamic partition 
WITHOUT_CLASSIFICATION	 these functions only work the string type 
WITHOUT_CLASSIFICATION	 schedule cmclearer thread will invoked metastore 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl call check existence side file for mockmocktbl call open mockmocktbl call check existence side file for mockmocktbl 
WITHOUT_CLASSIFICATION	 there are some txns the list which has write allocated and hence ahead and get the next write for the given table and update with new next write 
WITHOUT_CLASSIFICATION	 scheme specified but not authority then use the default authority 
WITHOUT_CLASSIFICATION	 stores result cache 
WITHOUT_CLASSIFICATION	 set output names reducesink 
WITHOUT_CLASSIFICATION	 llap cache purge requires admin privilege mutates state cache the cluster 
WITHOUT_CLASSIFICATION	 default put row into partition memory 
WITHOUT_CLASSIFICATION	 there should delta directories the new one the aborted one 
WITHOUT_CLASSIFICATION	 this method helps reuse session case there has been change the configuration session this will happen only the case nonhiveserver sessions for when cli session started the cli session could reuse the same tez session eliminating the latencies new and containers 
WITHOUT_CLASSIFICATION	 the caller has already stopped the session 
WITHOUT_CLASSIFICATION	 might want setxattr for the new location the future 
WITHOUT_CLASSIFICATION	 first argument the column transformed 
WITHOUT_CLASSIFICATION	 using property defined hiveconfconfvars test system property overriding 
WITHOUT_CLASSIFICATION	 indicates the initial capacity the cache 
WITHOUT_CLASSIFICATION	 the separator for the hive row would using the separator for this struct would 
WITHOUT_CLASSIFICATION	 grantor 
WITHOUT_CLASSIFICATION	 based the plan outputs find out the target table name and column names 
WITHOUT_CLASSIFICATION	 step check mapjointask has single child 
WITHOUT_CLASSIFICATION	 these are closurebound for all the walkers context 
WITHOUT_CLASSIFICATION	 should only downcast within valid range 
WITHOUT_CLASSIFICATION	 compile time 
WITHOUT_CLASSIFICATION	 delegationtoken 
WITHOUT_CLASSIFICATION	 list was recreated while were exhausting 
WITHOUT_CLASSIFICATION	 classname 
WITHOUT_CLASSIFICATION	 kerberos connections hms required 
WITHOUT_CLASSIFICATION	 had cache range already expect single matching disk slice given that theres cached data expect there some disk data 
WITHOUT_CLASSIFICATION	 now tasks would have passed verify that new job requests should succeed with issues 
WITHOUT_CLASSIFICATION	 close and release resources within running query process since runs under driver state compiling executing interrupt would not have race condition 
WITHOUT_CLASSIFICATION	 get the sel branch 
WITHOUT_CLASSIFICATION	 get the context info and set the shared tmp uri 
WITHOUT_CLASSIFICATION	 logdebugclassname currentkey 
WITHOUT_CLASSIFICATION	 leave this one for the next round 
WITHOUT_CLASSIFICATION	 writeid for data from nonacid table and writeidhwm would ensure those data are readable any txns 
WITHOUT_CLASSIFICATION	 case column stats hash aggregation grouping sets 
WITHOUT_CLASSIFICATION	 check whether have enough memory allocate for another hash partition need get the size the first hash partition get idea 
WITHOUT_CLASSIFICATION	 just act passthru between the session and allocation manager dont change the allocation target only thread can that therefore can this directly and actualstatebased sync will take care multiple potential message senders 
WITHOUT_CLASSIFICATION	 from this point the update motion someone changes the state again that 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 cleaner static object use static synchronized make sure its threadsafe 
WITHOUT_CLASSIFICATION	 zero out the bits above bitstowrite 
WITHOUT_CLASSIFICATION	 setting key same value should not trigger configchange event during shutdown 
WITHOUT_CLASSIFICATION	 bloom filter merge input and output are bytes just modes partial final 
WITHOUT_CLASSIFICATION	 dont worry about this likely just means its already been created 
WITHOUT_CLASSIFICATION	 remove subquery 
WITHOUT_CLASSIFICATION	 createtable insert truncate insert the result just one record 
WITHOUT_CLASSIFICATION	 this the new number rows after joining with 
WITHOUT_CLASSIFICATION	 switch the database 
WITHOUT_CLASSIFICATION	 not based arp and cannot assume uniform distribution bail 
WITHOUT_CLASSIFICATION	 restore index 
WITHOUT_CLASSIFICATION	 the hive type this column 
WITHOUT_CLASSIFICATION	 for nonprefix maps 
WITHOUT_CLASSIFICATION	 otherwise all null afterwards 
WITHOUT_CLASSIFICATION	 for primitive types use lazybinarys object for complex types make standard java object from lazybinarys object 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 restart sensitive instance 
WITHOUT_CLASSIFICATION	 other nodes may trying delete this the same time just log errors and skip them 
WITHOUT_CLASSIFICATION	 override this with noop subclass doesnt need treat nan null 
WITHOUT_CLASSIFICATION	 any expression child referencing parent column which result function 
WITHOUT_CLASSIFICATION	 abstract class for hash map result for reading the values onebyone 
WITHOUT_CLASSIFICATION	 prscrs 
WITHOUT_CLASSIFICATION	 cleanup the entries less than minuncommittedtxnid from the txntowriteid table 
WITHOUT_CLASSIFICATION	 create table associated with the import 
WITHOUT_CLASSIFICATION	 this way the only way the recursive stack fetchnextbatch returns got nonempty result and can consume reached the end the queue and there are more events when return from the fetchnextbatch stack have more results batch were done 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this will cause next txn marked aborted but the data still written disk 
WITHOUT_CLASSIFICATION	 got match return the value 
WITHOUT_CLASSIFICATION	 getdelay means the task will evicted from the queue 
WITHOUT_CLASSIFICATION	 events can start coming the moment the inputinitializer created the pruner must setup and initialized here that sets its structures start accepting events setting initialize leads window where events may come before the pruner 
WITHOUT_CLASSIFICATION	 write sql statements file 
WITHOUT_CLASSIFICATION	 even the service hasnt started its make this invocation since this will only happen after the atomicreference address has been populated not adding additional check 
WITHOUT_CLASSIFICATION	 for now because subquery only supported filter 
WITHOUT_CLASSIFICATION	 put empty list null 
WITHOUT_CLASSIFICATION	 scope close 
WITHOUT_CLASSIFICATION	 evaluate will return text object 
WITHOUT_CLASSIFICATION	 drain the buffer 
WITHOUT_CLASSIFICATION	 row count using the classic formula 
WITHOUT_CLASSIFICATION	 can update all partitions with single analyze query 
WITHOUT_CLASSIFICATION	 end relshuttleimpljava 
WITHOUT_CLASSIFICATION	 dont restrict child expressions for projection always use loose filter mode 
WITHOUT_CLASSIFICATION	 the parent reduce sink the big table side has the same emit key cols its parent can create bucket map join eliminating the reduce sink 
WITHOUT_CLASSIFICATION	 job request execution time out seconds then request will not timed out 
WITHOUT_CLASSIFICATION	 alter the via cachedstore can only alter owner parameters 
WITHOUT_CLASSIFICATION	 test for merging configs 
WITHOUT_CLASSIFICATION	 start getting the work part the task and call the output plan for the work 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 null values and values zero length are not added the cachedmap 
WITHOUT_CLASSIFICATION	 the rest ops are fks 
WITHOUT_CLASSIFICATION	 the values from 
WITHOUT_CLASSIFICATION	 along the way for the columns that the group uses keys 
WITHOUT_CLASSIFICATION	 testing substring starting from index 
WITHOUT_CLASSIFICATION	 the mathexpr class contains helper functions for cases when existing library 
WITHOUT_CLASSIFICATION	 initialize result 
WITHOUT_CLASSIFICATION	 copy partitions that will split into batches 
WITHOUT_CLASSIFICATION	 get the mode the lock encoded the path 
WITHOUT_CLASSIFICATION	 different result after clearing 
WITHOUT_CLASSIFICATION	 open new connection with these conf vars 
WITHOUT_CLASSIFICATION	 lockrequestbuilder dedups locks the same entity only keep the highest level lock requested 
WITHOUT_CLASSIFICATION	 compute the fixed size overhead for the keys 
WITHOUT_CLASSIFICATION	 precondition make sure this done after the rest the serde initialization done 
WITHOUT_CLASSIFICATION	 get arguments 
WITHOUT_CLASSIFICATION	 recreate cluster that picks the additional traitdef 
WITHOUT_CLASSIFICATION	 update the byte size the map 
WITHOUT_CLASSIFICATION	 any partition updated then update repl state partition object 
WITHOUT_CLASSIFICATION	 write the value 
WITHOUT_CLASSIFICATION	 and return false 
WITHOUT_CLASSIFICATION	 bucketized keys note that the order need not the same 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 decrease then increase sessions should not killed return 
WITHOUT_CLASSIFICATION	 here the global state confined just this process 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 level create all keys sumc sumvcolc for 
WITHOUT_CLASSIFICATION	 preserve the original view definition specified the user 
WITHOUT_CLASSIFICATION	 using binary search 
WITHOUT_CLASSIFICATION	 end relshuttlejava 
WITHOUT_CLASSIFICATION	 property specified file not found local file system use default setting 
WITHOUT_CLASSIFICATION	 reusable output for serialization 
WITHOUT_CLASSIFICATION	 get all user jars from work input format stuff 
WITHOUT_CLASSIFICATION	 failure handling import command and repl load commands are different import will set the last repl before copying data files and hence need allow replacement loaded from same dump twice after failing copy previous attempt but repl load will set the last repl only after the successful copy data files and 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 all other tables are small and are cached the hash table 
WITHOUT_CLASSIFICATION	 get all the tasks nodes from root task 
WITHOUT_CLASSIFICATION	 txnid 
WITHOUT_CLASSIFICATION	 handle repeating case 
WITHOUT_CLASSIFICATION	 production setfieldtype 
WITHOUT_CLASSIFICATION	 try eat dot now since could the end remember saw dot can 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 the login context name not set are the client and dont need auth 
WITHOUT_CLASSIFICATION	 when stopping the process are redirecting from the streams might closed during reading should not log the related exceptions visible level they might mislead the user 
WITHOUT_CLASSIFICATION	 columnar splits unknown size estimate worstcase 
WITHOUT_CLASSIFICATION	 remove from the too was pushed 
WITHOUT_CLASSIFICATION	 try infer the type the constant only there are two 
WITHOUT_CLASSIFICATION	 disable memory estimation for this test class 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create split for the previous unfinished stripe 
WITHOUT_CLASSIFICATION	 test that existing sharedread table with new exclusive coalesces 
WITHOUT_CLASSIFICATION	 use linkedhashmap make sure the iteration order 
WITHOUT_CLASSIFICATION	 decimal division remainder 
WITHOUT_CLASSIFICATION	 how run this test you can run this test via the command line mvn clean install java jar targetbenchmarksjar prof perf linux java jar targetbenchmarksjar prof perfnorm linux java jar targetbenchmarksjar prof perfasm linux java jar targetbenchmarksjar prof allocation counting via java jar targetbenchmarksjar hasnullstrue isrepeatingfalse processmodehash evalmodepartial java jar targetbenchmarksjar 
WITHOUT_CLASSIFICATION	 addition with overflow check overflow produces null output 
WITHOUT_CLASSIFICATION	 create the demuxoperaotr 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 how get around that 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 set one the roles user belongs 
WITHOUT_CLASSIFICATION	 compute the reducers run time statistics for the job 
WITHOUT_CLASSIFICATION	 modify conf using set commands 
WITHOUT_CLASSIFICATION	 base case its leaf 
WITHOUT_CLASSIFICATION	 map key separator 
WITHOUT_CLASSIFICATION	 error heuristic could have generated different errorandsolution for each task attempt but most likely they are the same plus one those probably good enough for debugging 
WITHOUT_CLASSIFICATION	 update null counter null value seen 
WITHOUT_CLASSIFICATION	 batch rows emit per processnextrecord call 
WITHOUT_CLASSIFICATION	 this mimic previous behavior where was thrown through this method 
WITHOUT_CLASSIFICATION	 version annotation 
WITHOUT_CLASSIFICATION	 fill the all the vector entries with provided value 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 case the job empty there wont jobstartjobend events the only way 
WITHOUT_CLASSIFICATION	 regenerate the valuetabledesc 
WITHOUT_CLASSIFICATION	 test udf considers the difference time components date and date 
WITHOUT_CLASSIFICATION	 create the objectinspectors for the fields 
WITHOUT_CLASSIFICATION	 operands 
WITHOUT_CLASSIFICATION	 sign stays the same 
WITHOUT_CLASSIFICATION	 the caller this method should guarantee this 
WITHOUT_CLASSIFICATION	 add sign byte since high bit off 
WITHOUT_CLASSIFICATION	 results 
WITHOUT_CLASSIFICATION	 oterwise may later release permit acquired someone else 
WITHOUT_CLASSIFICATION	 either the token should passed here ctor 
WITHOUT_CLASSIFICATION	 the table has property external set update table type accordingly 
WITHOUT_CLASSIFICATION	 not allowed 
WITHOUT_CLASSIFICATION	 write another large value this should use different byte buffer 
WITHOUT_CLASSIFICATION	 get detailed tableinfo from query desc extended tablename 
WITHOUT_CLASSIFICATION	 check ownership for all partitions 
WITHOUT_CLASSIFICATION	 pktablename 
WITHOUT_CLASSIFICATION	 this always replaced atomically dont care about concurrency here 
WITHOUT_CLASSIFICATION	 dont write first empty value get offset reduce the relative offset later there are more than value 
WITHOUT_CLASSIFICATION	 pull apart the kids the expression 
WITHOUT_CLASSIFICATION	 add value numdistinctvalue estimator 
WITHOUT_CLASSIFICATION	 get the path expression for the row only 
WITHOUT_CLASSIFICATION	 first scale with check overflow 
WITHOUT_CLASSIFICATION	 are not running this mapred task via child jvm 
WITHOUT_CLASSIFICATION	 boring scenario two concurrent increases 
WITHOUT_CLASSIFICATION	 cache administration 
WITHOUT_CLASSIFICATION	 add the test 
WITHOUT_CLASSIFICATION	 add another partition the source 
WITHOUT_CLASSIFICATION	 are the last initialize 
WITHOUT_CLASSIFICATION	 deep copy case downstream changes 
WITHOUT_CLASSIFICATION	 isetlongarg exprsetarg 
WITHOUT_CLASSIFICATION	 row with columns 
WITHOUT_CLASSIFICATION	 purpose 
WITHOUT_CLASSIFICATION	 alter table for perform schema evolution 
WITHOUT_CLASSIFICATION	 copy critical columns 
WITHOUT_CLASSIFICATION	 only the killed case requires message sent out the 
WITHOUT_CLASSIFICATION	 dont throw new exception for this just keep going with the next one 
WITHOUT_CLASSIFICATION	 hivedecimal float number 
WITHOUT_CLASSIFICATION	 helper method 
WITHOUT_CLASSIFICATION	 hadoop and hadoop hadoophome gone and replaced with hadoopprefix 
WITHOUT_CLASSIFICATION	 add another partitioning key based floorrand 
WITHOUT_CLASSIFICATION	 try ignoring the transaction and make sure works still 
WITHOUT_CLASSIFICATION	 serde for fetchtask 
WITHOUT_CLASSIFICATION	 requesttype 
WITHOUT_CLASSIFICATION	 project the relevant key column 
WITHOUT_CLASSIFICATION	 attempt locate existing jar for the class 
WITHOUT_CLASSIFICATION	 when compacting each split needs process the whole logical bucket 
WITHOUT_CLASSIFICATION	 everything now base 
WITHOUT_CLASSIFICATION	 return less than because rights digits below lefts scale 
WITHOUT_CLASSIFICATION	 build the locations predictable order simplify testing 
WITHOUT_CLASSIFICATION	 check that hook disable transforms has not been added 
WITHOUT_CLASSIFICATION	 columns are keys column the aggregate value input 
WITHOUT_CLASSIFICATION	 false not cache yet 
WITHOUT_CLASSIFICATION	 replace virtual columns with nulls see javadoc for details 
WITHOUT_CLASSIFICATION	 all well 
WITHOUT_CLASSIFICATION	 all are selected nothing 
WITHOUT_CLASSIFICATION	 write out from hive rcfile table and orc table 
WITHOUT_CLASSIFICATION	 currently support only raw data size stat 
WITHOUT_CLASSIFICATION	 this helper method copies the group keys from one vectorized row batch another but does not increment the outputbatchsize the next output position was designed for sorted reduce group batch processing mode copy the group keys startgroup 
WITHOUT_CLASSIFICATION	 this pluggable policy choose the candidate mapjoin table for converting join sort merge join the largest table chosen based the size the tables 
WITHOUT_CLASSIFICATION	 instantiating the hmshandler with will cause initialize instance the 
WITHOUT_CLASSIFICATION	 duplicate 
WITHOUT_CLASSIFICATION	 set partition and order columns overflowbatch can set ref since our last batch held 
WITHOUT_CLASSIFICATION	 the partitons are also the same check the fieldschema 
WITHOUT_CLASSIFICATION	 test both with field comments and without 
WITHOUT_CLASSIFICATION	 statuses can null ddl etc 
WITHOUT_CLASSIFICATION	 repeated otherinfo 
WITHOUT_CLASSIFICATION	 remove from list live operations 
WITHOUT_CLASSIFICATION	 insert the current table alias entry into the map not already present tablealiastoinfo 
WITHOUT_CLASSIFICATION	 evaluation the decimal constant vector expression after the vector 
WITHOUT_CLASSIFICATION	 make sure weve built the lock manager 
WITHOUT_CLASSIFICATION	 continue with next database 
WITHOUT_CLASSIFICATION	 was null the new authorization plugin must specified config 
WITHOUT_CLASSIFICATION	 see below the simple newtons equation 
WITHOUT_CLASSIFICATION	 handle aborted deltas currently this can only happen for tables 
WITHOUT_CLASSIFICATION	 the set object containing the list use hashset hivedecimalwritable objects instead hivedecimal objects can lookup decimalcolumnvector hivedecimalwritable quickly without creating hivedecimal lookup object 
WITHOUT_CLASSIFICATION	 the process choosing new blank works 
WITHOUT_CLASSIFICATION	 make sure dont collide with the source 
WITHOUT_CLASSIFICATION	 already existing semijoin branch reuse 
WITHOUT_CLASSIFICATION	 correlation optimizer will not try optimize this query 
WITHOUT_CLASSIFICATION	 dynamic partition insert case 
WITHOUT_CLASSIFICATION	 statementclose after resultsetclose should close the statement 
WITHOUT_CLASSIFICATION	 time based log retrieval may not fetch the above log line logging stderr for debugging purpose 
WITHOUT_CLASSIFICATION	 following two config keys are required fileoutputformat work correctly usual case hadoop jobtracker will set these before launching tasks since there jobtracker here set ourself 
WITHOUT_CLASSIFICATION	 normalize positive 
WITHOUT_CLASSIFICATION	 topn query 
WITHOUT_CLASSIFICATION	 bround with digits 
WITHOUT_CLASSIFICATION	 add bigint values 
WITHOUT_CLASSIFICATION	 since nulls can provide values for all rows 
WITHOUT_CLASSIFICATION	 need lookup the table and get the select statement and then parse 
WITHOUT_CLASSIFICATION	 does not make sense have any the metastore config variables 
WITHOUT_CLASSIFICATION	 this makes sure has the same downstream operator plan the original join 
WITHOUT_CLASSIFICATION	 compare the results fetched last time 
WITHOUT_CLASSIFICATION	 there may not base dir the partition was empty before inserts this partition just now being converted acid 
WITHOUT_CLASSIFICATION	 ever created for taskattempt 
WITHOUT_CLASSIFICATION	 listener parameters arent expected have many values far only will add parameter lets set low initial capacity for now find out many parameters are added then can adjust remove this initial capacity 
WITHOUT_CLASSIFICATION	 base class for mocking job operations with concurrent requests 
WITHOUT_CLASSIFICATION	 the letter sequence foo 
WITHOUT_CLASSIFICATION	 deferclose indicates the closedestroy should deferred when the process has been interrupted should set true the compile called within another method like 
WITHOUT_CLASSIFICATION	 could also check writeset but that seems overkill 
WITHOUT_CLASSIFICATION	 sign mark 
WITHOUT_CLASSIFICATION	 have data from this point could unneeded skip 
WITHOUT_CLASSIFICATION	 lets first test for default permissions this the case when user specified nothing 
WITHOUT_CLASSIFICATION	 oncreatetable alters the table add the topic name since this class generating that alter dont want notify that alter take quick look and see 
WITHOUT_CLASSIFICATION	 indicate that weve replaced the value 
WITHOUT_CLASSIFICATION	 ensure the table online 
WITHOUT_CLASSIFICATION	 write null element element field omitted 
WITHOUT_CLASSIFICATION	 get partitionlist from source 
WITHOUT_CLASSIFICATION	 defaultpoolpath 
WITHOUT_CLASSIFICATION	 make sure dont compact dont need compact 
WITHOUT_CLASSIFICATION	 cbo did not optimize the query might need replace grouping function special handling grouping function 
WITHOUT_CLASSIFICATION	 redact the sensitive information from the configuration values 
WITHOUT_CLASSIFICATION	 this noop return successfully 
WITHOUT_CLASSIFICATION	 check the conditions apply this transformation not meet them bail out 
WITHOUT_CLASSIFICATION	 todo delink from sessionstate tezsession can linked different hive sessions via the pool 
WITHOUT_CLASSIFICATION	 the element list 
WITHOUT_CLASSIFICATION	 expandandrehash new resizethreshold resizethreshold metricexpands metricexpands 
WITHOUT_CLASSIFICATION	 converts partnames into partname string partname string 
WITHOUT_CLASSIFICATION	 useexternalbuffer 
WITHOUT_CLASSIFICATION	 restart even theres internal error 
WITHOUT_CLASSIFICATION	 initialize the function localizer 
WITHOUT_CLASSIFICATION	 add another session 
WITHOUT_CLASSIFICATION	 check has sqcountcheck 
WITHOUT_CLASSIFICATION	 thread executing the query 
WITHOUT_CLASSIFICATION	 convert the field java class string because objects string type 
WITHOUT_CLASSIFICATION	 its retrying first regenerate the path list 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 for entry should set null 
WITHOUT_CLASSIFICATION	 delete jar and its dependencies added using query 
WITHOUT_CLASSIFICATION	 the vertex name longer than column width trim down 
WITHOUT_CLASSIFICATION	 common outer join result processing 
WITHOUT_CLASSIFICATION	 coordinator running overlord well 
WITHOUT_CLASSIFICATION	 finally remove the partition columns from the end derivedschema clearing the sublist writes through the underlying 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 need update the exprnode currently they refer columns the output the join they should refer the columns output the 
WITHOUT_CLASSIFICATION	 since left integer always some products here are not included 
WITHOUT_CLASSIFICATION	 also add the last vcol 
WITHOUT_CLASSIFICATION	 info from 
WITHOUT_CLASSIFICATION	 the number duplicates for each series key null nonnull 
WITHOUT_CLASSIFICATION	 dont check compatibility two object inspectors but directly pass them into users this class should make sure doesnt throw exceptions and returns correct results 
WITHOUT_CLASSIFICATION	 will try pushdown first make the filter this will also validate the expression 
WITHOUT_CLASSIFICATION	 used initialize streaming evaluator 
WITHOUT_CLASSIFICATION	 create table 
WITHOUT_CLASSIFICATION	 for the optimization that reduce number input file limit number files allowed more than specific number files have selected skip this optimization since having too many files inputs can cause unpredictable latency its not necessarily cheaper 
WITHOUT_CLASSIFICATION	 only permanent functions need authorized builtin function access allowed all users user can create temp function they should able use without additional authorization 
WITHOUT_CLASSIFICATION	 complete txn 
WITHOUT_CLASSIFICATION	 the result the swapping operation either project 
WITHOUT_CLASSIFICATION	 preempt only theres pending preemptions avoid preempting twice for task 
WITHOUT_CLASSIFICATION	 send these potentially large objects longer intervals avoid overloading the 
WITHOUT_CLASSIFICATION	 the foo nonexistent 
WITHOUT_CLASSIFICATION	 process singlecolumn string inner bigonly join vectorized row batch 
WITHOUT_CLASSIFICATION	 decimal types can specified with different precision and scales decimal opposed other data types which can represented constant strings the regex matches only the decimal prefix the type 
WITHOUT_CLASSIFICATION	 all the catalogs should cached 
WITHOUT_CLASSIFICATION	 finally get all the stuff for serdes just the params 
WITHOUT_CLASSIFICATION	 set config that will produce multiple queries 
WITHOUT_CLASSIFICATION	 key the database name value map from the qualified name the view object 
WITHOUT_CLASSIFICATION	 create fake root for local 
WITHOUT_CLASSIFICATION	 lefts signum wins dont need anything 
WITHOUT_CLASSIFICATION	 check see this table level request partitioned table 
WITHOUT_CLASSIFICATION	 for one element the variance always 
WITHOUT_CLASSIFICATION	 mapping from hadoop job the stack traces collected from the map reduce task logs 
WITHOUT_CLASSIFICATION	 create colinfo and then row resolver 
WITHOUT_CLASSIFICATION	 generic function node case operator udf node 
WITHOUT_CLASSIFICATION	 single split 
WITHOUT_CLASSIFICATION	 inmemory hdfs 
WITHOUT_CLASSIFICATION	 turn escape 
WITHOUT_CLASSIFICATION	 nodetype 
WITHOUT_CLASSIFICATION	 assume cache chunks would always match the way read check and skip 
WITHOUT_CLASSIFICATION	 this can never null empty 
WITHOUT_CLASSIFICATION	 case topn for windowing need distinguish between rows with null partition keys and rows with value for partition keys 
WITHOUT_CLASSIFICATION	 count all values seen far 
WITHOUT_CLASSIFICATION	 aggregation classes 
WITHOUT_CLASSIFICATION	 over all the input paths and calculate known total size known 
WITHOUT_CLASSIFICATION	 get the key 
WITHOUT_CLASSIFICATION	 extract column from the given exprnodedesc 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 testing negative substring index 
WITHOUT_CLASSIFICATION	 then partition number any 
WITHOUT_CLASSIFICATION	 there are nonnumeric arguments that dont match from one udf another give this point 
WITHOUT_CLASSIFICATION	 tracks pending preemptions per host using the hostname always accessed inside lock 
WITHOUT_CLASSIFICATION	 set columnaccessinfo for view column authorization 
WITHOUT_CLASSIFICATION	 map integer string 
WITHOUT_CLASSIFICATION	 remove entry for operator 
WITHOUT_CLASSIFICATION	 merge task could after dynamic partition insert 
WITHOUT_CLASSIFICATION	 use type promotion 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 equal sum small tables size 
WITHOUT_CLASSIFICATION	 there are aggregations order need remember them 
WITHOUT_CLASSIFICATION	 need run this get consistent filterop conditionsfor operator tree matching 
WITHOUT_CLASSIFICATION	 replace view 
WITHOUT_CLASSIFICATION	 for numeric well minimum necessary cast cast the type expression bad things will happen 
WITHOUT_CLASSIFICATION	 nothing but count 
WITHOUT_CLASSIFICATION	 get the transaction 
WITHOUT_CLASSIFICATION	 boolean that says whether slow start not 
WITHOUT_CLASSIFICATION	 need consolidate more buffers into one decompress 
WITHOUT_CLASSIFICATION	 initialize stats publisher necessary 
WITHOUT_CLASSIFICATION	 create client manage our transaction 
WITHOUT_CLASSIFICATION	 index rank function 
WITHOUT_CLASSIFICATION	 preempt any host 
WITHOUT_CLASSIFICATION	 determin which task has been preempted normally task would preempted based starting later however both may have the same start time either could picked 
WITHOUT_CLASSIFICATION	 directly serialize with the caller writing fieldbyfield serialization format the caller responsible for calling the write method for the right type each field calling writenull the field null 
WITHOUT_CLASSIFICATION	 first table union query with view parent 
WITHOUT_CLASSIFICATION	 test the validation incorrect null values the tables 
WITHOUT_CLASSIFICATION	 trace error not exists 
WITHOUT_CLASSIFICATION	 track cleaner metrics 
WITHOUT_CLASSIFICATION	 have exhausted our current batch read the next batch 
WITHOUT_CLASSIFICATION	 jump out the loop need input from the big table 
WITHOUT_CLASSIFICATION	 set fake input and output streams 
WITHOUT_CLASSIFICATION	 synchronous event processing loop wont return until all events have 
WITHOUT_CLASSIFICATION	 throw exception 
WITHOUT_CLASSIFICATION	 swallow the exception since wont affect the final result 
WITHOUT_CLASSIFICATION	 the session has delegation token obtained from the metastore then cancel 
WITHOUT_CLASSIFICATION	 append asis 
WITHOUT_CLASSIFICATION	 try authenticating with the httphost principal 
WITHOUT_CLASSIFICATION	 columncolumn 
WITHOUT_CLASSIFICATION	 get the names out 
WITHOUT_CLASSIFICATION	 since not used further the tree 
WITHOUT_CLASSIFICATION	 last row last batch determines isgroupresultnull and long lastvalue 
WITHOUT_CLASSIFICATION	 empty parameters are sent columnmapping 
WITHOUT_CLASSIFICATION	 external client currently cannot use guaranteed 
WITHOUT_CLASSIFICATION	 executes the callable task with help execute call and gets the result the task also sets job status completed state not already set failed and returns result future 
WITHOUT_CLASSIFICATION	 have initialization here because the supers constructor calls next and thus need initialize before our constructor 
WITHOUT_CLASSIFICATION	 install the configuration the runtime 
WITHOUT_CLASSIFICATION	 extract columns and values 
WITHOUT_CLASSIFICATION	 index entries table usage 
WITHOUT_CLASSIFICATION	 the field that passed not primitive and either the field not declared schema was given initialization the field declared primitive initialization serialize the data json string otherwise serialize the data the delimited way 
WITHOUT_CLASSIFICATION	 the parameters are checked manually not check them 
WITHOUT_CLASSIFICATION	 batch size and decaying factor 
WITHOUT_CLASSIFICATION	 werent able check 
WITHOUT_CLASSIFICATION	 there should delta directories 
WITHOUT_CLASSIFICATION	 the aggregation type sum scaleup 
WITHOUT_CLASSIFICATION	 should generate 
WITHOUT_CLASSIFICATION	 this called from hcat always allow embedded metastore was the default 
WITHOUT_CLASSIFICATION	 core logic load hash table using hashtableloader 
WITHOUT_CLASSIFICATION	 disabled for acid path 
WITHOUT_CLASSIFICATION	 the was called hmshandlershutdown would have already cleaned thread local rawstore otherwise now 
WITHOUT_CLASSIFICATION	 add shutdown hook for cleanup there are elements remaining the cache which were not cleaned this the best effort approach ignore any error while doing notice that most the clients would get cleaned via either the removallistener the close call only the active clients that are the cache expired but being used other threads wont get cleaned the following code will only clean the active cache ones the ones expired from cache but being hold other threads are the mercy finalize being called 
WITHOUT_CLASSIFICATION	 check the easy cases first 
WITHOUT_CLASSIFICATION	 make sure nothing really moved 
WITHOUT_CLASSIFICATION	 simulate emitting records processnextrecord with large memory usage limit 
WITHOUT_CLASSIFICATION	 previously when path empty null and default path specified was the return value for escapepathname 
WITHOUT_CLASSIFICATION	 practice dont really care about the data any these tables except far creates partitions the sql being test not actually executed and results the wrt acid metadata supplied manually via but having data makes easier follow the intent 
WITHOUT_CLASSIFICATION	 export case 
WITHOUT_CLASSIFICATION	 use multiple lines for statements not terminated the delimiter 
WITHOUT_CLASSIFICATION	 resultset output formatting classes 
WITHOUT_CLASSIFICATION	 primitiveentry 
WITHOUT_CLASSIFICATION	 for serialization 
WITHOUT_CLASSIFICATION	 its table alias 
WITHOUT_CLASSIFICATION	 our reading positioned the value 
WITHOUT_CLASSIFICATION	 the table has sample specified bail from calcite path 
WITHOUT_CLASSIFICATION	 try alternate config param 
WITHOUT_CLASSIFICATION	 serializescale fastscale 
WITHOUT_CLASSIFICATION	 record partitions that were written 
WITHOUT_CLASSIFICATION	 optimize the scenario when there are grouping keys only reducer 
WITHOUT_CLASSIFICATION	 other columns provided nonnull values can return repeated output 
WITHOUT_CLASSIFICATION	 partitionview does not have and not need update its column stats 
WITHOUT_CLASSIFICATION	 this will used the outputcommitter pass the metastore client which turn will pass the tokenselector that can select 
WITHOUT_CLASSIFICATION	 map container succeeded 
WITHOUT_CLASSIFICATION	 dont fail this besteffort 
WITHOUT_CLASSIFICATION	 skip the directory that have found 
WITHOUT_CLASSIFICATION	 arguments 
WITHOUT_CLASSIFICATION	 simple map join the whole relation goes memory 
WITHOUT_CLASSIFICATION	 the assign method will overridden for char and varchar 
WITHOUT_CLASSIFICATION	 have found map systematically deserialize the values the map and return back the map 
WITHOUT_CLASSIFICATION	 thread pool execute job requests 
WITHOUT_CLASSIFICATION	 the join does fetching next row groups itself 
WITHOUT_CLASSIFICATION	 authorization done just call super 
WITHOUT_CLASSIFICATION	 push the feed its subscribers 
WITHOUT_CLASSIFICATION	 start cleanup 
WITHOUT_CLASSIFICATION	 this point weve set all the tables and ptns were going test drops across replicate first and then well drop the source 
WITHOUT_CLASSIFICATION	 rowid always the first field 
WITHOUT_CLASSIFICATION	 note assume reuse only possible for the same user and config 
WITHOUT_CLASSIFICATION	 use ranges and duplicate multipliers reduce the size the display 
WITHOUT_CLASSIFICATION	 coldouble 
WITHOUT_CLASSIFICATION	 nonjavadoc see set null only because carryforwardnames true 
WITHOUT_CLASSIFICATION	 get aliastopath and pass the heuristic 
WITHOUT_CLASSIFICATION	 try insert times rehash that fails 
WITHOUT_CLASSIFICATION	 initialize fetchtask right here 
WITHOUT_CLASSIFICATION	 implementation note implement hivedecimal with the mutable fasthivedecimal class that class uses protected all its methods they will not visible the hivedecimal class even one casts fasthivedecimal you shouldnt able violate the immutability hivedecimal class 
WITHOUT_CLASSIFICATION	 not create identity project does not rename any fields 
WITHOUT_CLASSIFICATION	 change file modification time and look for cache misses 
WITHOUT_CLASSIFICATION	 keep open txn which refers the aborted txn 
WITHOUT_CLASSIFICATION	 update the attrs 
WITHOUT_CLASSIFICATION	 table deleted 
WITHOUT_CLASSIFICATION	 the hash code for each nonnull key 
WITHOUT_CLASSIFICATION	 pooltriggers 
WITHOUT_CLASSIFICATION	 muxoperator should only have single child 
WITHOUT_CLASSIFICATION	 determine the name our map reduce task for debug tracing 
WITHOUT_CLASSIFICATION	 group aggregate minmax and bloom filter 
WITHOUT_CLASSIFICATION	 password file contents are trimmed trailing whitespaces and newlines 
WITHOUT_CLASSIFICATION	 dont zerodivide one comes random 
WITHOUT_CLASSIFICATION	 only one result column verify the column name verify the column name 
WITHOUT_CLASSIFICATION	 extract stages 
WITHOUT_CLASSIFICATION	 not allow temp table rename the new name already exists temp table 
WITHOUT_CLASSIFICATION	 these types are not supported yet should define complex type date thrift that contains single int member and dynamicserde should convert date type runtime 
WITHOUT_CLASSIFICATION	 number digits retain from the end 
WITHOUT_CLASSIFICATION	 tablescan only available during compile 
WITHOUT_CLASSIFICATION	 check two arguments were passed 
WITHOUT_CLASSIFICATION	 and again one last time 
WITHOUT_CLASSIFICATION	 note may make sizeetc configurable later 
WITHOUT_CLASSIFICATION	 note dont pass the config reopen the session was already open would have kept running with its current config preserve that behavior 
WITHOUT_CLASSIFICATION	 calcite creates null literal with null type here but 
WITHOUT_CLASSIFICATION	 right full outer join need iterate through the row container that contains all the right records that did not produce results then for each those records replace the left side with null values and produce the records observe that only enter this block when have finished iterating through all the left and right records aliasnum numaliases and thus have tried evaluate the postfilter condition every possible combination 
WITHOUT_CLASSIFICATION	 firstname null firstname sue 
WITHOUT_CLASSIFICATION	 push down projections columnar store works for rcfile and orcfile 
WITHOUT_CLASSIFICATION	 uses level parallel implementation bfs recursive dfs implementations have issue where the number threads can run out the number nested subdirectories more than the pool size using two queue implementation simpler than one queue since then will have add the complex mechanisms let the free worker threads know when new levels are discovered using notifywait mechanisms which can potentially lead bugs not done right 
WITHOUT_CLASSIFICATION	 stop requested and handled inside 
WITHOUT_CLASSIFICATION	 load the properties from config file 
WITHOUT_CLASSIFICATION	 the fields that uses give information about plugin endpoint some these will removed when registry implemented will generate and publish them 
WITHOUT_CLASSIFICATION	 first try nonrandom values 
WITHOUT_CLASSIFICATION	 note inserts into new part this wont fail 
WITHOUT_CLASSIFICATION	 throw exception simulate issue with cleaner thread 
WITHOUT_CLASSIFICATION	 the file missing getting modified then refer path 
WITHOUT_CLASSIFICATION	 get the other one examining join 
WITHOUT_CLASSIFICATION	 make sure the columns does not already exist 
WITHOUT_CLASSIFICATION	 final reduction case column stats numrows case column stats grouping sets minnumrows ndvproduct sizeofgroupingset case column stats grouping sets minnumrows ndvproduct 
WITHOUT_CLASSIFICATION	 enable createasacid 
WITHOUT_CLASSIFICATION	 hadoopproxyuser set create delegationtoken using real user 
WITHOUT_CLASSIFICATION	 check specificfilterset for qtest specific ones 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 when all inputs are accounted for the output forwarded appropriately 
WITHOUT_CLASSIFICATION	 directory creation otherwise within the writers 
WITHOUT_CLASSIFICATION	 binary decimal conversion 
WITHOUT_CLASSIFICATION	 originally copied from but seemed have bug 
WITHOUT_CLASSIFICATION	 list compactions clean 
WITHOUT_CLASSIFICATION	 get the output location the order partition keys are defined for the table 
WITHOUT_CLASSIFICATION	 start metrics 
WITHOUT_CLASSIFICATION	 storage information 
WITHOUT_CLASSIFICATION	 and condition needs computed 
WITHOUT_CLASSIFICATION	 nothing handled below types will mismatch 
WITHOUT_CLASSIFICATION	 bucketed sorted tablepartition they cannot merged 
WITHOUT_CLASSIFICATION	 blocks more llap queries can submitted 
WITHOUT_CLASSIFICATION	 cached successfully add policy 
WITHOUT_CLASSIFICATION	 note assume here the session before resolve killquery result here still use that because all the user ops above like return reopen etc dont actually returnreopen when kill query progress 
WITHOUT_CLASSIFICATION	 apply overlay query specific settings any 
WITHOUT_CLASSIFICATION	 get the collection separator and map key separator 
WITHOUT_CLASSIFICATION	 evaluate the aggregation input argument expression 
WITHOUT_CLASSIFICATION	 the initialcapacity which cannot provide reasonable positive number here 
WITHOUT_CLASSIFICATION	 col 
WITHOUT_CLASSIFICATION	 add the jar file 
WITHOUT_CLASSIFICATION	 ignoresee 
WITHOUT_CLASSIFICATION	 test different format types 
WITHOUT_CLASSIFICATION	 generate the groupbyoperator for the query block parseinfogetxxxdest the new groupbyoperator will child the param mode the mode the aggregation partial complete param not null this function will store the mapping from aggregation stringtree the this parameter can used the nextstage groupby aggregations return the new groupbyoperator 
WITHOUT_CLASSIFICATION	 boolval 
WITHOUT_CLASSIFICATION	 all partitions with should have columns 
WITHOUT_CLASSIFICATION	 expected the operation takes time continue the loop and wait for completion 
WITHOUT_CLASSIFICATION	 null args 
WITHOUT_CLASSIFICATION	 note the sinks and ddl cannot coexist this time but they could would 
WITHOUT_CLASSIFICATION	 map requires levels key separator and keypair separator 
WITHOUT_CLASSIFICATION	 close off the buffer with normal tag 
WITHOUT_CLASSIFICATION	 now check the table folder and see find anything that isnt the metastore 
WITHOUT_CLASSIFICATION	 logger jobs 
WITHOUT_CLASSIFICATION	 get ready for another round small table values 
WITHOUT_CLASSIFICATION	 get the list table scan operators for this join interface has been provided currently only enabled for simple filters and selects 
WITHOUT_CLASSIFICATION	 col 
WITHOUT_CLASSIFICATION	 need least server for testing 
WITHOUT_CLASSIFICATION	 here are now doing acid read must have orcsplit 
WITHOUT_CLASSIFICATION	 this helper object serializes lazybinary format reducer values from columns row 
WITHOUT_CLASSIFICATION	 check for the line how check the line invalid for any reason the job will fail 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create cost metadata provider 
WITHOUT_CLASSIFICATION	 parse until pair separator currentlevel 
WITHOUT_CLASSIFICATION	 need state store eventually for current state measure backoffs 
WITHOUT_CLASSIFICATION	 construct the setfacl command 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 cols 
WITHOUT_CLASSIFICATION	 found the plan already connected which means this derived from the cache 
WITHOUT_CLASSIFICATION	 element for key byte hash table hashmultiset 
WITHOUT_CLASSIFICATION	 hcat requires alterdata privileges for alter table location statements for the old tablepartition location and the new location 
WITHOUT_CLASSIFICATION	 add for tag 
WITHOUT_CLASSIFICATION	 were continuing existing command 
WITHOUT_CLASSIFICATION	 wrapping for exception handling 
WITHOUT_CLASSIFICATION	 leadership state changes and sending out notifications listener happens inside synchronous method curator only lightweight actions mainevent handler thread time consuming operations are handled via separate executor service registered via 
WITHOUT_CLASSIFICATION	 this call fills the column names types and partition column count 
WITHOUT_CLASSIFICATION	 leading for month and day should work 
WITHOUT_CLASSIFICATION	 grant option 
WITHOUT_CLASSIFICATION	 handle the isnull array first tight loops 
WITHOUT_CLASSIFICATION	 extract stats keys from statstask 
WITHOUT_CLASSIFICATION	 should not used 
WITHOUT_CLASSIFICATION	 both branches are null 
WITHOUT_CLASSIFICATION	 col 
WITHOUT_CLASSIFICATION	 generation 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 writeentity defaultacidtblpart typetable writetypeinsert isdpfalse 
WITHOUT_CLASSIFICATION	 this collector just row counter 
WITHOUT_CLASSIFICATION	 targetpath path xyz here xyz present the file system create the structure till xyz work rename for multilevel directory and rename fails delete the path xyz targetpath have multilevel directories like xyz xyz the renaming the directories are not atomic the execution will happen one one 
WITHOUT_CLASSIFICATION	 assume the full acid table 
WITHOUT_CLASSIFICATION	 cant happen 
WITHOUT_CLASSIFICATION	 split exclusively serves alias which needs sampled add the split list the alias 
WITHOUT_CLASSIFICATION	 task requested host got host since host dead and host full 
WITHOUT_CLASSIFICATION	 create syntax tree for function call myisnullcol unknown 
WITHOUT_CLASSIFICATION	 map each aborted write with each allocated txn 
WITHOUT_CLASSIFICATION	 there should still four directories the location 
WITHOUT_CLASSIFICATION	 bucket mapjoin llap make sure the caches are populated get the subcache 
WITHOUT_CLASSIFICATION	 enablejobreconnect param was not passed user use cluster wide default 
WITHOUT_CLASSIFICATION	 optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 this also provides completion information and possible notification when task actually starts running first heartbeat 
WITHOUT_CLASSIFICATION	 rows emitted with separate thread per processnextrecord call 
WITHOUT_CLASSIFICATION	 add grpset col 
WITHOUT_CLASSIFICATION	 serde property for how the hive column maps accumulo 
WITHOUT_CLASSIFICATION	 only allow constant field name for now 
WITHOUT_CLASSIFICATION	 this function checks whether all bucketing columns are also join keys and are same order 
WITHOUT_CLASSIFICATION	 test gettables 
WITHOUT_CLASSIFICATION	 test that separate partitions dont coalesce 
WITHOUT_CLASSIFICATION	 header 
WITHOUT_CLASSIFICATION	 copy bytes into scratch buffer 
WITHOUT_CLASSIFICATION	 block make sure move happened successfully 
WITHOUT_CLASSIFICATION	 make sure the cleanup doesnt leave the pool without session 
WITHOUT_CLASSIFICATION	 release attempt again succeed 
WITHOUT_CLASSIFICATION	 key columns 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 placeholder minimum value enforced ngramestimator 
WITHOUT_CLASSIFICATION	 there should only one columnstatistics 
WITHOUT_CLASSIFICATION	 sanity check restrictedconfig always set setup 
WITHOUT_CLASSIFICATION	 new tai lue letter tha bytes 
WITHOUT_CLASSIFICATION	 round towards negative infinity 
WITHOUT_CLASSIFICATION	 partitionvalues 
WITHOUT_CLASSIFICATION	 verify that columnstatsaccurate removed from params 
WITHOUT_CLASSIFICATION	 resultsignum 
WITHOUT_CLASSIFICATION	 get the field objectinspector and the field object 
WITHOUT_CLASSIFICATION	 constant can ignore 
WITHOUT_CLASSIFICATION	 note other parent readers init everything ctor but union does startstripe 
WITHOUT_CLASSIFICATION	 move pending and intervening discard the allocator can release 
WITHOUT_CLASSIFICATION	 hadoop missing public api check for snapshotable directories check with the directory name until more appropriate api provided hdfs 
WITHOUT_CLASSIFICATION	 the first child set the mode variable value otherwise the mode are working different bail out 
WITHOUT_CLASSIFICATION	 this keep track null literal which already has been visited 
WITHOUT_CLASSIFICATION	 reset the selection vector 
WITHOUT_CLASSIFICATION	 strip the leading provided this the assumption with which were going start configuring sessionconfext 
WITHOUT_CLASSIFICATION	 important note for multior the class will catch cases with more parameters 
WITHOUT_CLASSIFICATION	 create the serialized string for type 
WITHOUT_CLASSIFICATION	 decimals are stored biginteger convert and compare 
WITHOUT_CLASSIFICATION	 only write the record file are writing line otherwise might get garbage from backspaces and such 
WITHOUT_CLASSIFICATION	 map filter the new filter over join 
WITHOUT_CLASSIFICATION	 since expect files minor compacted job produce delta minor compacted job file obsolete make delta skipped and then the requested minor compaction combine delta delta and delta make delta major compaction create base 
WITHOUT_CLASSIFICATION	 this uses 
WITHOUT_CLASSIFICATION	 are going use cache change the path depend file for extra consistency 
WITHOUT_CLASSIFICATION	 accumulate the counts 
WITHOUT_CLASSIFICATION	 available copy state registry for optimization rules 
WITHOUT_CLASSIFICATION	 task just created 
WITHOUT_CLASSIFICATION	 write header 
WITHOUT_CLASSIFICATION	 least one task would have been added update the repl state 
WITHOUT_CLASSIFICATION	 assertassertequals locksgetlockid 
WITHOUT_CLASSIFICATION	 standard object 
WITHOUT_CLASSIFICATION	 false prune 
WITHOUT_CLASSIFICATION	 neither side repeating 
WITHOUT_CLASSIFICATION	 truncate the table missing either due droprename which follows the truncate the existing table newer than our update 
WITHOUT_CLASSIFICATION	 see javadoc 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 next check this table has partitions and get the list partition names well allocate 
WITHOUT_CLASSIFICATION	 leading spaces are significant 
WITHOUT_CLASSIFICATION	 these are null when evaluate called for the first time 
WITHOUT_CLASSIFICATION	 since the loop left txntowriteidssize 
WITHOUT_CLASSIFICATION	 acquire different locks different levels 
WITHOUT_CLASSIFICATION	 firstfetchhappened true reality almost always calls joinonegroup fix 
WITHOUT_CLASSIFICATION	 more matches expected 
WITHOUT_CLASSIFICATION	 follow 
WITHOUT_CLASSIFICATION	 also need delete partdate make consistent 
WITHOUT_CLASSIFICATION	 now have vector aggregation buffer sets use for each row can start computing the aggregates the number distinct keys the batch can use the optimized code path aggregateinput 
WITHOUT_CLASSIFICATION	 could not get status for some reason log and send empty status back with just the that caller knows even look the log file 
WITHOUT_CLASSIFICATION	 sample data 
WITHOUT_CLASSIFICATION	 assuming that valid ugi with kerberos cred created llap 
WITHOUT_CLASSIFICATION	 translate args 
WITHOUT_CLASSIFICATION	 possible the table deleted during fetching tables the database that case continue with the next table 
WITHOUT_CLASSIFICATION	 this happens when the code inside the jmx bean threw exception log and dont output the bean 
WITHOUT_CLASSIFICATION	 its hard distinguish union with null from null union 
WITHOUT_CLASSIFICATION	 use nullindicator decide whether project null nothing the literal null 
WITHOUT_CLASSIFICATION	 rip apart the object inspector making sure got what expect 
WITHOUT_CLASSIFICATION	 gather memory threshold 
WITHOUT_CLASSIFICATION	 the udtf 
WITHOUT_CLASSIFICATION	 instances are running 
WITHOUT_CLASSIFICATION	 make this client wait job tracker not behaving well 
WITHOUT_CLASSIFICATION	 were here then socketgetlocalport was the port exclude since both sockets were open together point time were guaranteed that socketgetlocalport not the same 
WITHOUT_CLASSIFICATION	 dont lose this bit weird dance here 
WITHOUT_CLASSIFICATION	 the end 
WITHOUT_CLASSIFICATION	 phase hold onto any cte definitions aliastocte cte definitions are global the query 
WITHOUT_CLASSIFICATION	 create cmroot with permission not exist 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 test query where timeout does not kick set 
WITHOUT_CLASSIFICATION	 nonjavadoc this provides lazyfloat like class which can initialized from data stored binary format see int int 
WITHOUT_CLASSIFICATION	 wildcard bind 
WITHOUT_CLASSIFICATION	 fetch the tablescan operator 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl 
WITHOUT_CLASSIFICATION	 bail muxoperator because mux operator masks the emit keys the constituent reduce sinks 
WITHOUT_CLASSIFICATION	 key expiration create already expired key 
WITHOUT_CLASSIFICATION	 visiblefortesting 
WITHOUT_CLASSIFICATION	 the following union operation returns union which traverses over the first set once and then then over each element second set order that not contained first this means doesnt replace anything first set and would preserve the writetype writeentity first set case outputs list 
WITHOUT_CLASSIFICATION	 weve not seen this terminal before need check rootunionworkmap which contains the information mapping the root operator union work union work 
WITHOUT_CLASSIFICATION	 someone allocating arena 
WITHOUT_CLASSIFICATION	 reenable timeouts 
WITHOUT_CLASSIFICATION	 this the set entities that the statement represented extlockid wants update 
WITHOUT_CLASSIFICATION	 session should not lost however the fraction should discarded 
WITHOUT_CLASSIFICATION	 batchindex classname nomatch currentkey currentkey 
WITHOUT_CLASSIFICATION	 check select expr constant current logic used look for there none then the expression constant 
WITHOUT_CLASSIFICATION	 not fetching from table directly but from temp location 
WITHOUT_CLASSIFICATION	 constructors various flavors follow 
WITHOUT_CLASSIFICATION	 old schemas within 
WITHOUT_CLASSIFICATION	 serialize json based field annotations only 
WITHOUT_CLASSIFICATION	 check implementation class 
WITHOUT_CLASSIFICATION	 list softreferences 
WITHOUT_CLASSIFICATION	 negotiation complete remove this handler from the pipeline and register with the kryo instance handle encryption needed 
WITHOUT_CLASSIFICATION	 this really just 
WITHOUT_CLASSIFICATION	 case column stats hash aggregation grouping sets 
WITHOUT_CLASSIFICATION	 lookup long the hash set param key the long key param hashsetresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 hex input also supported 
WITHOUT_CLASSIFICATION	 base 
WITHOUT_CLASSIFICATION	 decimal 
WITHOUT_CLASSIFICATION	 middle pattern 
WITHOUT_CLASSIFICATION	 join operator 
WITHOUT_CLASSIFICATION	 initialize map local work 
WITHOUT_CLASSIFICATION	 add all nonvirtual columns from the tablescan operator 
WITHOUT_CLASSIFICATION	 process multikey inner bigonly join vectorized row batch 
WITHOUT_CLASSIFICATION	 lazily create pathchildrencache 
WITHOUT_CLASSIFICATION	 group requires arraylist dont ask 
WITHOUT_CLASSIFICATION	 priority 
WITHOUT_CLASSIFICATION	 typespecific handling 
WITHOUT_CLASSIFICATION	 its not clear this rewrite always performant since extra map phase introduced for job may offset gains this multistage aggregation need cost model for enable this 
WITHOUT_CLASSIFICATION	 get table metadata 
WITHOUT_CLASSIFICATION	 key reducesinkkeyint reducesinkkeyint value colint 
WITHOUT_CLASSIFICATION	 sparsesparse merge 
WITHOUT_CLASSIFICATION	 table ownership for createdropalter index 
WITHOUT_CLASSIFICATION	 get the tables for the desired patten populate the output stream 
WITHOUT_CLASSIFICATION	 keep separate from the creating events case the send blocks 
WITHOUT_CLASSIFICATION	 the following tests will verify the deprecation variable still usable 
WITHOUT_CLASSIFICATION	 assume the worst 
WITHOUT_CLASSIFICATION	 tell the operator the status the next keygrouped vectorizedrowbatch that will delivered the process method reduceshuffle these semantics are needed ptf can 
WITHOUT_CLASSIFICATION	 add the actual source input 
WITHOUT_CLASSIFICATION	 move the pointer the next byte since have written 
WITHOUT_CLASSIFICATION	 will generate results for all matching and nonmatching rows 
WITHOUT_CLASSIFICATION	 get the group keys columninfo 
WITHOUT_CLASSIFICATION	 failed because object doesnt exist 
WITHOUT_CLASSIFICATION	 thread simulating user session hiveserver 
WITHOUT_CLASSIFICATION	 keeps track regular timed heartbeats primarily used timing mechanism send log counters 
WITHOUT_CLASSIFICATION	 verify the connection fails after canceling the token 
WITHOUT_CLASSIFICATION	 decode utf 
WITHOUT_CLASSIFICATION	 same object 
WITHOUT_CLASSIFICATION	 aware that result could the same object 
WITHOUT_CLASSIFICATION	 next should always return false 
WITHOUT_CLASSIFICATION	 optional string executionmode 
WITHOUT_CLASSIFICATION	 null plan means disabled via command could still reenabled 
WITHOUT_CLASSIFICATION	 set that parent initialization done and call initialize children 
WITHOUT_CLASSIFICATION	 note this thing should know nothing about acid schema reads physical columns index schema evolutionacid schema considerations should higher level 
WITHOUT_CLASSIFICATION	 get aggregation from calcite given name ret type and input arg 
WITHOUT_CLASSIFICATION	 are currently walking the big table side the merge join need create hook 
WITHOUT_CLASSIFICATION	 partition values are specified nonpartitioned table 
WITHOUT_CLASSIFICATION	 sort and pick partition keys 
WITHOUT_CLASSIFICATION	 remove requested quantiles from the head the list 
WITHOUT_CLASSIFICATION	 lock few blocks without telling the policy 
WITHOUT_CLASSIFICATION	 need add cast datetime family 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 row count and where there are nulls means index disabled and dont have stats 
WITHOUT_CLASSIFICATION	 some cases were converted before calling emulate those cases first 
WITHOUT_CLASSIFICATION	 return aggregate collations 
WITHOUT_CLASSIFICATION	 this the last byte leave the high bit off 
WITHOUT_CLASSIFICATION	 add partitions located the tabledirectory default 
WITHOUT_CLASSIFICATION	 fileheader resulttype argtype argtype 
WITHOUT_CLASSIFICATION	 testing matched and with case statement using targeta breaks this 
WITHOUT_CLASSIFICATION	 old table the cache but the new table cannot cached 
WITHOUT_CLASSIFICATION	 null implies empty column qualifier 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 only log the first wait and check after wait the last iteration 
WITHOUT_CLASSIFICATION	 return sec difference 
WITHOUT_CLASSIFICATION	 use runlength encoding only record run length same prevvaluelen occurs more than one time and negative the run length distinguish runlength and normal value length for example the values lengths are record and for value lengths record 
WITHOUT_CLASSIFICATION	 the key the next lowest reader 
WITHOUT_CLASSIFICATION	 expected result entry the recordidentifier data entry file before compact 
WITHOUT_CLASSIFICATION	 tasks started after the addfile call completes 
WITHOUT_CLASSIFICATION	 the key found mapcolumnvector set the value 
WITHOUT_CLASSIFICATION	 this means there are tables something the database 
WITHOUT_CLASSIFICATION	 save the evaluator that can used the nextstage 
WITHOUT_CLASSIFICATION	 this plan projecta all gby keys rewritten nullable projexpr aggregate groupbyall left input refs aggrewritten expression agg projectb rewriten original projected exprs joinreplace corvar input ref from leftinputrel leftinputrel rightinputrel 
WITHOUT_CLASSIFICATION	 update the aggregations 
WITHOUT_CLASSIFICATION	 this arbitrary note that metadata may come from big scan and nuke all the data from some small frequently accessed tables because gets such large priority boost start with think the multiplier the number accesses after which the data becomes more important than some random readonce metadata purelfu scheme 
WITHOUT_CLASSIFICATION	 cancel 
WITHOUT_CLASSIFICATION	 negative numbers indicate column deserialize read from the small tables lazybinary value row 
WITHOUT_CLASSIFICATION	 string string equivalent considered numeric when used arithmetic operator 
WITHOUT_CLASSIFICATION	 string can used write char and varchar when the caller takes responsibility for truncationpadding issues 
WITHOUT_CLASSIFICATION	 lower word bits the lower bit used the sign and removed need multiplier digit commad 
WITHOUT_CLASSIFICATION	 the objectinspector for the row 
WITHOUT_CLASSIFICATION	 numreducers and newnumreducers 
WITHOUT_CLASSIFICATION	 continue null out the field 
WITHOUT_CLASSIFICATION	 those tables directory 
WITHOUT_CLASSIFICATION	 the serializer then requires that noopfetchformatter used but when isnt then either the thriftformatter the should used 
WITHOUT_CLASSIFICATION	 decimal rounding 
WITHOUT_CLASSIFICATION	 update the joinkeys appropriately 
WITHOUT_CLASSIFICATION	 perform major compaction 
WITHOUT_CLASSIFICATION	 add column change serde file formats 
WITHOUT_CLASSIFICATION	 store the changes 
WITHOUT_CLASSIFICATION	 set method this requires calcite change 
WITHOUT_CLASSIFICATION	 the combinations below definitely result overflow 
WITHOUT_CLASSIFICATION	 remove currtask from parenttasks 
WITHOUT_CLASSIFICATION	 dates are also valid dates the dates are within 
WITHOUT_CLASSIFICATION	 will store all the new changed properties the job the udf context the the method need not 
WITHOUT_CLASSIFICATION	 for now olderclass has version and newerclass versions 
WITHOUT_CLASSIFICATION	 allow add any counters they have collected 
WITHOUT_CLASSIFICATION	 from 
WITHOUT_CLASSIFICATION	 partition level column stats merging 
WITHOUT_CLASSIFICATION	 create new operator hashtable dummyoperator which share the table desc 
WITHOUT_CLASSIFICATION	 create few read locks all the same resource 
WITHOUT_CLASSIFICATION	 check whether current operators are equal 
WITHOUT_CLASSIFICATION	 lets create new list and copy dont have linked list 
WITHOUT_CLASSIFICATION	 validate and setup symbolinfo 
WITHOUT_CLASSIFICATION	 possible because querycomplete message from the can come first kill successful before the fragmentcomplete reported 
WITHOUT_CLASSIFICATION	 heartbeat 
WITHOUT_CLASSIFICATION	 given the keyindex these arrays return the columnvectortype the type specific index into longindices doubleindices etc 
WITHOUT_CLASSIFICATION	 must just column name 
WITHOUT_CLASSIFICATION	 varchar 
WITHOUT_CLASSIFICATION	 child can expr alias expr 
WITHOUT_CLASSIFICATION	 deletes for the bucket being taken into consideration for this split processing 
WITHOUT_CLASSIFICATION	 prevent instantiation 
WITHOUT_CLASSIFICATION	 for dynamic partitioned writes without all keyvalues specified create temp dir for the associated write job 
WITHOUT_CLASSIFICATION	 since ifexists was not set true trying create the same table again will result exception 
WITHOUT_CLASSIFICATION	 set values look for include the original blank value longminvalue make sure 
WITHOUT_CLASSIFICATION	 relative positions the blocks dont change over time priorities expire can only decrease only have one block that could have broken heap rule and always move down therefore can update priorities other blocks for part the heap correct any discrepancy wthe parent after expiring priority and any block expire the priority for already has lower priority than that its children 
WITHOUT_CLASSIFICATION	 table current event has partition flag different from existing table means some the previous events same batch have drop and create table events with same same but different partition flag this case should with current events table type and create the dummy table object for adding repl tasks 
WITHOUT_CLASSIFICATION	 verify udf reflect allowed exception will thrown 
WITHOUT_CLASSIFICATION	 bigtablecandidates can never null 
WITHOUT_CLASSIFICATION	 find tables which name contains tofind hidden the default database 
WITHOUT_CLASSIFICATION	 subscriber can get notification about addition table hcat listening topic named hcat and message selector string hcatevent hcataddtable 
WITHOUT_CLASSIFICATION	 independent hashtable and can modified need copy 
WITHOUT_CLASSIFICATION	 this integer because derby converts boolean char breaking sysdb 
WITHOUT_CLASSIFICATION	 open the log file and read the lines parse out stack traces 
WITHOUT_CLASSIFICATION	 file checksum not implemented for local filesystem rawlocalfilesystem 
WITHOUT_CLASSIFICATION	 handle initialization results 
WITHOUT_CLASSIFICATION	 create another table for incremental repl verification 
WITHOUT_CLASSIFICATION	 cancel the query 
WITHOUT_CLASSIFICATION	 have regular single rows from the input file format reader that will need deserialize 
WITHOUT_CLASSIFICATION	 determine which rows are left 
WITHOUT_CLASSIFICATION	 newschema 
WITHOUT_CLASSIFICATION	 incase this corresponds subquery then modify its point subquery alias 
WITHOUT_CLASSIFICATION	 remove valid columns 
WITHOUT_CLASSIFICATION	 find file instead dir dont change inputpath 
WITHOUT_CLASSIFICATION	 done processing the task 
WITHOUT_CLASSIFICATION	 the subqueries are maponly jobs 
WITHOUT_CLASSIFICATION	 there correlation condition anywhere the filter dont push this filter past project since some cases can prevent correlate from being decorrelated 
WITHOUT_CLASSIFICATION	 accessing order join cols bucket cols should same 
WITHOUT_CLASSIFICATION	 task the new task 
WITHOUT_CLASSIFICATION	 note this partitionspec has ordered map 
WITHOUT_CLASSIFICATION	 credential provider has entry for our credential then should used 
WITHOUT_CLASSIFICATION	 first value written without next relative offset next value length next value last flag next value length small flag etc 
WITHOUT_CLASSIFICATION	 need drop the primary metastore shared both primaryreplica constraints 
WITHOUT_CLASSIFICATION	 make power backing down the 
WITHOUT_CLASSIFICATION	 where product the carry from 
WITHOUT_CLASSIFICATION	 did not find the column 
WITHOUT_CLASSIFICATION	 store the bucket path bucket number mapping the table scan operator although one mapper per file used possible that any mapper can pick any file depending the size the files the bucket number 
WITHOUT_CLASSIFICATION	 rename the table missing either due droprename which follows the current rename the existing table newer than our update 
WITHOUT_CLASSIFICATION	 avro considers bytes primitive hive doesnt make them list tinyint 
WITHOUT_CLASSIFICATION	 set hiveconf statics default values 
WITHOUT_CLASSIFICATION	 updatecurrentkey needs called initialize the master key there should null check added the future rollmasterkey updatecurrentkey 
WITHOUT_CLASSIFICATION	 todo setup ssl shuffle 
WITHOUT_CLASSIFICATION	 vectors 
WITHOUT_CLASSIFICATION	 collection usage threshold not support worst case set memory threshold memory usage before 
WITHOUT_CLASSIFICATION	 helper determine what java options use for the containers falls back mapreduces map java opts tez specific options are set 
WITHOUT_CLASSIFICATION	 one work only one map join operator can bucketed 
WITHOUT_CLASSIFICATION	 check singleaggrel singlevalue agg 
WITHOUT_CLASSIFICATION	 nonjavadoc this provides lazybinary like class which can initialized from data stored binary format see int int 
WITHOUT_CLASSIFICATION	 table for which show locks being executed 
WITHOUT_CLASSIFICATION	 set pass property operation and check its set the query config 
WITHOUT_CLASSIFICATION	 timestamp between 
WITHOUT_CLASSIFICATION	 bypass only outerrr not null otherwise need look for expressions outerrr for subqueries select minbvalue from table group bkey 
WITHOUT_CLASSIFICATION	 ascending null first default for ascending order 
WITHOUT_CLASSIFICATION	 interface into the registry service 
WITHOUT_CLASSIFICATION	 this stage mapreduce processing the groupby tha mapside aggregates was just used reduce output data case distincts partial results are not used and iterate again invoked the reducer case nondistincts partial results are used and merge invoked the reducer 
WITHOUT_CLASSIFICATION	 see also 
WITHOUT_CLASSIFICATION	 try empty rows query 
WITHOUT_CLASSIFICATION	 returns addr 
WITHOUT_CLASSIFICATION	 wait for another iteration make sure event gets processed for receive allocation 
WITHOUT_CLASSIFICATION	 check that are cleaning the empty aborted transactions 
WITHOUT_CLASSIFICATION	 the rowids are the same after compaction 
WITHOUT_CLASSIFICATION	 these constants are also imported 
WITHOUT_CLASSIFICATION	 for run length byte encoding record the number bits within current byte consume 
WITHOUT_CLASSIFICATION	 remove reducer 
WITHOUT_CLASSIFICATION	 this still could dpp 
WITHOUT_CLASSIFICATION	 whole repeated key batch was filtered out 
WITHOUT_CLASSIFICATION	 this spawns separate thread walk through the cache and removes expired nodes only one cleaner thread should running any point 
WITHOUT_CLASSIFICATION	 step move the file destination 
WITHOUT_CLASSIFICATION	 handle the rest the aggregation that the bottom aggregate hasnt handled 
WITHOUT_CLASSIFICATION	 returns rows from possibly multiple bucket files small table ascending order utilizing primary queue borrowed from hadoop 
WITHOUT_CLASSIFICATION	 need preserve loggedinuser 
WITHOUT_CLASSIFICATION	 may have already connected work with childwork case for example lateral view lvf sel sel lvjudtf sel here can reached from via two different paths there any child work after dont want connect them with the work associated with more than once 
WITHOUT_CLASSIFICATION	 degree parallelism 
WITHOUT_CLASSIFICATION	 make power 
WITHOUT_CLASSIFICATION	 partition value cant end this suffix 
WITHOUT_CLASSIFICATION	 the planner gives subset virtual columns available for this table scan and only support some virtual columns vectorization create the intersection note these are available vectorizable virtual columns later remember which virtual columns were actually used the query just those will included the map that has the information for creating the map vectorizedrowbatch 
WITHOUT_CLASSIFICATION	 this instance will not added back since its services are not yet 
WITHOUT_CLASSIFICATION	 match 
WITHOUT_CLASSIFICATION	 compare cell value with constant value filter they match and cell value isnt other return true they dont match but cell other and value filter not skewed value return unknown why not true true not enough since not true false but not unknown unknown for example skewed column skewed value clause where not cell other evaluate notc other ture notc will false but wrong skip default dir but unknown notc will unknown will choose default dir all others return false 
WITHOUT_CLASSIFICATION	 default false 
WITHOUT_CLASSIFICATION	 setting true ensures that performs the query operation the connected user instead the user running 
WITHOUT_CLASSIFICATION	 found something before ran out components use 
WITHOUT_CLASSIFICATION	 verify the actual locations being correct should different location splits are supposed consistent across jvms the test setup verify different host make sure not hash the same host osos the test were fail because the host the same the assumption about consistent across jvm 
WITHOUT_CLASSIFICATION	 kills templeton job with multiple retries job exists returns true kill job attempt success otherwise returns false 
WITHOUT_CLASSIFICATION	 needrequirelock false the release here will nothing because there lock 
WITHOUT_CLASSIFICATION	 since equiv should get back first container 
WITHOUT_CLASSIFICATION	 queryplan here 
WITHOUT_CLASSIFICATION	 for spark nonlocal mode any added dependencies are stored which the executors working directory local mode need manually point the processs working directory 
WITHOUT_CLASSIFICATION	 return empty result since only constant desc exists 
WITHOUT_CLASSIFICATION	 before closing the operator check statistics gathering requested and provided record writer this different from the statistics gathering done processop processop for each row added serde statistics about the row gathered and accumulated hashmap this adds more overhead the actual processing row but the record writer already gathers the statistics can simply return the accumulated statistics which will aggregated case spray writers 
WITHOUT_CLASSIFICATION	 calculate unique skewed elements for each skewed column 
WITHOUT_CLASSIFICATION	 one serialized key for more rows for the duplicate keys reduceskiptag tag tag reducetagbyte int reducetagbyte keylength loginfoprocess offset length 
WITHOUT_CLASSIFICATION	 trailing space should ignored for char comparisons write stripped values for this serde 
WITHOUT_CLASSIFICATION	 setting these props match lazysimpleserde 
WITHOUT_CLASSIFICATION	 check column types 
WITHOUT_CLASSIFICATION	 check task started 
WITHOUT_CLASSIFICATION	 internal representation integer representing day offset from our epoch value 
WITHOUT_CLASSIFICATION	 not public since must have the serialize write object 
WITHOUT_CLASSIFICATION	 classloader invokes this static block when its first loaded lazy initialization 
WITHOUT_CLASSIFICATION	 constant null expr just return 
WITHOUT_CLASSIFICATION	 apply the transformation 
WITHOUT_CLASSIFICATION	 create dummy vertex for mergejoin branch for self join this 
WITHOUT_CLASSIFICATION	 future add intervalyearmonth etc desired 
WITHOUT_CLASSIFICATION	 advance the primary reader the next record 
WITHOUT_CLASSIFICATION	 walk through udaf collect udaf info 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 array was passed parameter make sure its array primitives 
WITHOUT_CLASSIFICATION	 number arguments this udf external name 
WITHOUT_CLASSIFICATION	 valid paths 
WITHOUT_CLASSIFICATION	 surrogate pair case 
WITHOUT_CLASSIFICATION	 fetch rows from splits 
WITHOUT_CLASSIFICATION	 not need anything bail out 
WITHOUT_CLASSIFICATION	 the numbers input columns and output columns should match for regular query 
WITHOUT_CLASSIFICATION	 raise custom exception like ioexception and verify expected message 
WITHOUT_CLASSIFICATION	 old test moved msckrepairq 
WITHOUT_CLASSIFICATION	 walk operator tree create expression tree for filter buckets 
WITHOUT_CLASSIFICATION	 the columns the group expressions should not intersect with the columns the distinct expressions 
WITHOUT_CLASSIFICATION	 have when not matched and boolean expr then insert 
WITHOUT_CLASSIFICATION	 the plan knows are reading this locks security 
WITHOUT_CLASSIFICATION	 note that this keyinterval may adjusted later due copyn files 
WITHOUT_CLASSIFICATION	 check the existing partition values can type casted the new column type 
WITHOUT_CLASSIFICATION	 table matcher 
WITHOUT_CLASSIFICATION	 nexttxnidntxnnext could minuncommittedtxnid 
WITHOUT_CLASSIFICATION	 right now assume only and hll are available 
WITHOUT_CLASSIFICATION	 since there collision index will used for the next value have the map point back original index 
WITHOUT_CLASSIFICATION	 add the auth filter 
WITHOUT_CLASSIFICATION	 fill the buffer with key value pairs 
WITHOUT_CLASSIFICATION	 this field null 
WITHOUT_CLASSIFICATION	 edge case 
WITHOUT_CLASSIFICATION	 this case abd join will executed first and abdc join will executed next 
WITHOUT_CLASSIFICATION	 generate absolute path relative current directory hdfs home directory 
WITHOUT_CLASSIFICATION	 reads the index file for each requested mapid and figures out the overall length the response which populated into the response header 
WITHOUT_CLASSIFICATION	 the actual deserialization may involve nested records which require recursion 
WITHOUT_CLASSIFICATION	 different queries the session may using the same lock manager 
WITHOUT_CLASSIFICATION	 lock operations themselves dont require the lock 
WITHOUT_CLASSIFICATION	 partition columns partition values 
WITHOUT_CLASSIFICATION	 update the leaf place 
WITHOUT_CLASSIFICATION	 create conditional work list and task list 
WITHOUT_CLASSIFICATION	 custom root specified update the parent path 
WITHOUT_CLASSIFICATION	 nope look see our home dir has been explicitly set 
WITHOUT_CLASSIFICATION	 object inspectors corresponding the struct returned terminatepartial and the long field within the struct count 
WITHOUT_CLASSIFICATION	 singlethreaded init case with this the ordering sessions the queue will with sessions queues there ensuring uniform distribution the sessions across queues least begin with then sessions get freed the list may change this ordering multi threaded init case its free for all 
WITHOUT_CLASSIFICATION	 because this file will fetched fetch operator 
WITHOUT_CLASSIFICATION	 reduce filter with stats information 
WITHOUT_CLASSIFICATION	 summary for column statistics 
WITHOUT_CLASSIFICATION	 clear out any rows may have processed rowmode for the current partition 
WITHOUT_CLASSIFICATION	 added project need produce new keys than the original input fields 
WITHOUT_CLASSIFICATION	 look for getfieldname isfieldname 
WITHOUT_CLASSIFICATION	 only used for dynamic partitioned hash joins mapjoin operator the reducer 
WITHOUT_CLASSIFICATION	 choose first full batch with selection 
WITHOUT_CLASSIFICATION	 implementation row container 
WITHOUT_CLASSIFICATION	 writeidhwm known query all writeids under the writeid hwm any writeid under hwm allocated txn txnid hwm belongs openaborted txns then will added invalid list the results should sorted ascending order based write the sorting needed exceptions list validwriteidlist would lookedup 
WITHOUT_CLASSIFICATION	 value will null 
WITHOUT_CLASSIFICATION	 test when first argument has nulls 
WITHOUT_CLASSIFICATION	 record move events need cluster fraction updates that happens step 
WITHOUT_CLASSIFICATION	 using the hook startup ensures that the hook always has priority over settings xml the thread local conf needs used because this point has already been initialized using conf 
WITHOUT_CLASSIFICATION	 iterate over the rest the children 
WITHOUT_CLASSIFICATION	 cbucketcols pbucketcols have constant node expressions avoid the merge 
WITHOUT_CLASSIFICATION	 insert clause 
WITHOUT_CLASSIFICATION	 ket partition values and the value wrapper around the partition object 
WITHOUT_CLASSIFICATION	 need collect statistics index columns 
WITHOUT_CLASSIFICATION	 put uncompressed data cache 
WITHOUT_CLASSIFICATION	 handle the single block case 
WITHOUT_CLASSIFICATION	 parent reduce sinks 
WITHOUT_CLASSIFICATION	 clear all inmemory partitions first 
WITHOUT_CLASSIFICATION	 notnullconstraints 
WITHOUT_CLASSIFICATION	 hcat doesnt support transactional tables 
WITHOUT_CLASSIFICATION	 round power here required writebuffers 
WITHOUT_CLASSIFICATION	 delta with files raw format are result load data opposed compaction streaming ingest must have interval length 
WITHOUT_CLASSIFICATION	 linear interpolation get the exact percentile 
WITHOUT_CLASSIFICATION	 drop databases created other test cases 
WITHOUT_CLASSIFICATION	 loginfofirst tail offset 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 preinstall the database all the tables are there 
WITHOUT_CLASSIFICATION	 check that dropping database from wrong catalog fails 
WITHOUT_CLASSIFICATION	 first create expression from defaultvalueast 
WITHOUT_CLASSIFICATION	 writeid recently committed txn which was open when get validtxnlist snapshot should invalid well 
WITHOUT_CLASSIFICATION	 all columns 
WITHOUT_CLASSIFICATION	 since may calculation and produce scratch column need map the right batch column 
WITHOUT_CLASSIFICATION	 accepting object means accepting everything but there conversion cost 
WITHOUT_CLASSIFICATION	 different level the drop command specified 
WITHOUT_CLASSIFICATION	 linkedhashmap have repeatable iteration order 
WITHOUT_CLASSIFICATION	 cleanup the mapwork path 
WITHOUT_CLASSIFICATION	 jobs stages per job tasks per stage 
WITHOUT_CLASSIFICATION	 reset the resolver 
WITHOUT_CLASSIFICATION	 this the overwhelmingly common case 
WITHOUT_CLASSIFICATION	 first child should rowid 
WITHOUT_CLASSIFICATION	 link backtrack selectop filesinkop 
WITHOUT_CLASSIFICATION	 initialize reduce operator tree 
WITHOUT_CLASSIFICATION	 for example original max dist min 
WITHOUT_CLASSIFICATION	 constant constant expressions shouldnt getting this 
WITHOUT_CLASSIFICATION	 arbitrary column names used internally for serializing spill table 
WITHOUT_CLASSIFICATION	 groupingsets cube rollup 
WITHOUT_CLASSIFICATION	 the whole column vector has nulls this true otherwise false 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 generate the intermediate aggregate the one the bottom that converts distinct call group call bottom aggregate the same the original aggregate except that 
WITHOUT_CLASSIFICATION	 operations read committed sufficient 
WITHOUT_CLASSIFICATION	 else fallback hlloriginal algorithm 
WITHOUT_CLASSIFICATION	 filesadded 
WITHOUT_CLASSIFICATION	 create the filter for the queryid appender 
WITHOUT_CLASSIFICATION	 with hive the dictionaries will disabled after writing the first stripe there are too many distinct values hence only stripes compared stripes version above test case 
WITHOUT_CLASSIFICATION	 the perbatch setup for outer join 
WITHOUT_CLASSIFICATION	 get the table from metastore 
WITHOUT_CLASSIFICATION	 already check rowcnt null and rowcnt means table empty 
WITHOUT_CLASSIFICATION	 reset the conf variable values that changed for this test 
WITHOUT_CLASSIFICATION	 should copy only those table parameters that are specified the config 
WITHOUT_CLASSIFICATION	 skip semijoin branch 
WITHOUT_CLASSIFICATION	 copy new slot table 
WITHOUT_CLASSIFICATION	 counts are cached avoid repeated complex computation register value 
WITHOUT_CLASSIFICATION	 stores the tablescan operators processed avoid redoing them 
WITHOUT_CLASSIFICATION	 number registers 
WITHOUT_CLASSIFICATION	 test submission concurrent job requests with the controlled number concurrent requests verify that get busy exception and appropriate message 
WITHOUT_CLASSIFICATION	 the child has different schema create project operator between them both cannot prune the columns the groupby operator 
WITHOUT_CLASSIFICATION	 one serialized key for more rows for the duplicate keys 
WITHOUT_CLASSIFICATION	 for backwards compatibility since some threads used hard coded but only run frequency was 
WITHOUT_CLASSIFICATION	 sum all nonnull double column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 uses nontransactional table cannot considered 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 verify the validwriteidlist with one open txn this table write open txn should invalid 
WITHOUT_CLASSIFICATION	 hash aggregations for group 
WITHOUT_CLASSIFICATION	 its set set our own conf value 
WITHOUT_CLASSIFICATION	 equal 
WITHOUT_CLASSIFICATION	 checking against the partition question instead 
WITHOUT_CLASSIFICATION	 registrations and unregistrations will happen and when tasks are submitted are removed reference counting likely required connection needs established each app master ignore exceptions when communicating with the later point report back saying the dead that tasks can removed from the running queue race when task completes sends out its message via the regular taskreporter the after this may run another dag may die this may need consolidated with the llaptaskreporter try ensuring theres race between the two single thread which sends heartbeats appmasters events drain off queue 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 groupingc equivalent 
WITHOUT_CLASSIFICATION	 todo hive sampling task running 
WITHOUT_CLASSIFICATION	 stagecounters 
WITHOUT_CLASSIFICATION	 perform any bucket expressions results will into scratch columns 
WITHOUT_CLASSIFICATION	 all returned type will text 
WITHOUT_CLASSIFICATION	 index the last seen delimiter the given line 
WITHOUT_CLASSIFICATION	 over the expressions the project operator and separate the windowing nodes that are result 
WITHOUT_CLASSIFICATION	 get nanos between epoch tozone and local time tozone 
WITHOUT_CLASSIFICATION	 update the pending state for now release this lock take both 
WITHOUT_CLASSIFICATION	 this means the name null 
WITHOUT_CLASSIFICATION	 class hiveendpoint 
WITHOUT_CLASSIFICATION	 the setchildren method initializes the object inspector needed the operators based path and partition information 
WITHOUT_CLASSIFICATION	 knuth the art computer programming edition volii algd normalize high digit base that guarantee 
WITHOUT_CLASSIFICATION	 probably expression cant handle that 
WITHOUT_CLASSIFICATION	 same with the termination after the failed update should maintain the correct count 
WITHOUT_CLASSIFICATION	 start the session fireandforget manner when the asynchronously initialized parts the session are needed the corresponding getters and other methods will wait needed 
WITHOUT_CLASSIFICATION	 use the big table row output 
WITHOUT_CLASSIFICATION	 also try creating ugi object for the spnego principal 
WITHOUT_CLASSIFICATION	 hcatalog specific configurations that can put hivesitexml 
WITHOUT_CLASSIFICATION	 order predciates based ndv reverse order ndvcrossproduct ndvpe ndvpe ndvpe ndvpe 
WITHOUT_CLASSIFICATION	 remove the current task from its original parent tasks dependent task 
WITHOUT_CLASSIFICATION	 walk over the input schema and copy the output 
WITHOUT_CLASSIFICATION	 filter enabled injection enabled exception not expected 
WITHOUT_CLASSIFICATION	 the table not bucketed add dummy filter rand 
WITHOUT_CLASSIFICATION	 order does not matter below wide 
WITHOUT_CLASSIFICATION	 set the bucketing version 
WITHOUT_CLASSIFICATION	 use linkedhashmap provide deterministic order 
WITHOUT_CLASSIFICATION	 this intentionally does not include interval types 
WITHOUT_CLASSIFICATION	 test invalid case without version 
WITHOUT_CLASSIFICATION	 reescape any backtick characters the identifier 
WITHOUT_CLASSIFICATION	 merge the names from the imposed schema into the types from the derived schema 
WITHOUT_CLASSIFICATION	 start after group keys 
WITHOUT_CLASSIFICATION	 defining bunch configs here instead hiveconf these are experimental and mainly for use when retry handling fixed yarnhadoop 
WITHOUT_CLASSIFICATION	 couldnt sql filter pushdown get names via normal means 
WITHOUT_CLASSIFICATION	 copy whole value for strings 
WITHOUT_CLASSIFICATION	 state 
WITHOUT_CLASSIFICATION	 drop all partitions from tbl tbl tbl and add new partitions tbl and tbl 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 test booleanvalued nonfilter expression for strings 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 default drivers txn manager txn manager specified 
WITHOUT_CLASSIFICATION	 its ddl query 
WITHOUT_CLASSIFICATION	 static fieldsinitializers 
WITHOUT_CLASSIFICATION	 longer term should always have txn and then wont need track locks here all 
WITHOUT_CLASSIFICATION	 new entry the hash table 
WITHOUT_CLASSIFICATION	 the routing appender which manages underlying appenders 
WITHOUT_CLASSIFICATION	 give preference tblproperties over serdeproperties really should only use tblproperties this just for backwards compatibility with the original specs 
WITHOUT_CLASSIFICATION	 flush partially full deserializerbatch return return true the operator tree not done yet 
WITHOUT_CLASSIFICATION	 since the log length the sql operation may vary during hive dev calculate proper maxrows 
WITHOUT_CLASSIFICATION	 use inspector get byte out lazybinary 
WITHOUT_CLASSIFICATION	 add the request interceptor the client builder 
WITHOUT_CLASSIFICATION	 explode 
WITHOUT_CLASSIFICATION	 add alias table name and partitions hadoop conf that their children will inherit these 
WITHOUT_CLASSIFICATION	 overflow batch 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that left semi join multikey using hash set 
WITHOUT_CLASSIFICATION	 from the prunedcols list filter out columns that refer windowfns windowexprs the returned list set the prunedlist needed the ptfop 
WITHOUT_CLASSIFICATION	 the expired nodes did not result cache being cleanuntil size 
WITHOUT_CLASSIFICATION	 through these hijinxes because java considers systemgetenv readonly and offers way set env var from within process only for processes that subspawn 
WITHOUT_CLASSIFICATION	 monday august 
WITHOUT_CLASSIFICATION	 any aggregate functions not support splitting bail out 
WITHOUT_CLASSIFICATION	 build the new predicate and return 
WITHOUT_CLASSIFICATION	 float family timestamp are handled via descriptor based lookup int family needs 
WITHOUT_CLASSIFICATION	 directory empty doesnt have any that could have been produced load data 
WITHOUT_CLASSIFICATION	 execution stuff 
WITHOUT_CLASSIFICATION	 partitions updated and other entries 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 todo relying everywhere the magical constants and columns being together means acid columns are going super hard change backward compat manner can foresee someone cursing while refactoring all the magic for prefix schema changes exclude the row column 
WITHOUT_CLASSIFICATION	 expect that colid will the same for all many sds 
WITHOUT_CLASSIFICATION	 concurrent increase and revocation increase fails revocation needed 
WITHOUT_CLASSIFICATION	 represents udaf invocation the context window frame explained above sometimes udafs will handled window functions even explicit window specification this support queries that have group clause window function invocation captures the astnode that represents this invocation its name whether stardistinct invocation its alias and optional window specification 
WITHOUT_CLASSIFICATION	 make sure capture the same metrics hadoop metrics system via annotations 
WITHOUT_CLASSIFICATION	 relies the fact that cache does not actually store these 
WITHOUT_CLASSIFICATION	 points the last txn which dont want heartbeat 
WITHOUT_CLASSIFICATION	 only trigger major compaction for ttp deltapctthreshold because the newly inserted row actual pct 
WITHOUT_CLASSIFICATION	 tamil ubd bytes 
WITHOUT_CLASSIFICATION	 create temporary scratch dir 
WITHOUT_CLASSIFICATION	 all this completely bogus and mostly captures the following function foutput ieyeballedtheoutput theylookok its pretty much golden file 
WITHOUT_CLASSIFICATION	 next column has similar name previous but different casing this allowed druid but should fail hive 
WITHOUT_CLASSIFICATION	 need final pass inner class 
WITHOUT_CLASSIFICATION	 blockingdeque methods 
WITHOUT_CLASSIFICATION	 flush necessary 
WITHOUT_CLASSIFICATION	 support 
WITHOUT_CLASSIFICATION	 read database table via cachedstore 
WITHOUT_CLASSIFICATION	 update the aggs 
WITHOUT_CLASSIFICATION	 flag indicate there data parquet data page 
WITHOUT_CLASSIFICATION	 create standard copy the object 
WITHOUT_CLASSIFICATION	 preallocated members for storing information single and matches valuecounts number empty small table values allmatchindices logical indices into allmatchs the first row match possible series duplicate keys duplicatecounts the duplicate count for each matched key 
WITHOUT_CLASSIFICATION	 functionname 
WITHOUT_CLASSIFICATION	 next command should produce error 
WITHOUT_CLASSIFICATION	 merge with its right child 
WITHOUT_CLASSIFICATION	 primary key pkfk relationship unique constraint not null constraint 
WITHOUT_CLASSIFICATION	 multiple values 
WITHOUT_CLASSIFICATION	 ptf input represents the input ptf function input can hive subquery table another ptf function input instance captures the astnode that this instance was created from 
WITHOUT_CLASSIFICATION	 this method can replaced with filescopysource target replaceexisting once hive uses java 
WITHOUT_CLASSIFICATION	 store text the original query 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 otherwise return null 
WITHOUT_CLASSIFICATION	 this not called when building hashtable dont expect called ever 
WITHOUT_CLASSIFICATION	 methods that need data object 
WITHOUT_CLASSIFICATION	 the end this function the stream should pointing the last token that corresponds the value being skipped this way the next call nexttoken will advance the next field name 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 for load you only add does exist you might loading outdated 
WITHOUT_CLASSIFICATION	 check behavior while change the order columns 
WITHOUT_CLASSIFICATION	 alter rebuild 
WITHOUT_CLASSIFICATION	 corvar change input ref 
WITHOUT_CLASSIFICATION	 throw hiveexception for nonrcfile 
WITHOUT_CLASSIFICATION	 the simd optimized form 
WITHOUT_CLASSIFICATION	 run sql operations 
WITHOUT_CLASSIFICATION	 revalidated the new version after upgrade 
WITHOUT_CLASSIFICATION	 sparsedense merge 
WITHOUT_CLASSIFICATION	 its possible that the job doesnt have the token its credentials this case 
WITHOUT_CLASSIFICATION	 either weve found multiple big table branches the current branch cannot big table branch disable mapjoin for these cases 
WITHOUT_CLASSIFICATION	 cols 
WITHOUT_CLASSIFICATION	 also need update the expr that the index query can generated 
WITHOUT_CLASSIFICATION	 unique the load func and input file name table our case 
WITHOUT_CLASSIFICATION	 the tasks from now are more important than the candidate 
WITHOUT_CLASSIFICATION	 defining partition names unsorted order 
WITHOUT_CLASSIFICATION	 format single cluster sort statement 
WITHOUT_CLASSIFICATION	 try combine next level works recursively 
WITHOUT_CLASSIFICATION	 set the start and stop rows only asked 
WITHOUT_CLASSIFICATION	 could just remove here and handle the missing tail during read but that can 
WITHOUT_CLASSIFICATION	 mapping 
WITHOUT_CLASSIFICATION	 loginfocreating list record copying lengthslength lrptroffset lrptroffset 
WITHOUT_CLASSIFICATION	 recursively extract fields from exprnodedesc deeply nested structs can have multiple levels fields them 
WITHOUT_CLASSIFICATION	 truncatepartition event partitioned table 
WITHOUT_CLASSIFICATION	 not setting ranges scans the entire table 
WITHOUT_CLASSIFICATION	 capture stdout and stderr 
WITHOUT_CLASSIFICATION	 add destination pool 
WITHOUT_CLASSIFICATION	 get list indexes for which the columns the schema are the same 
WITHOUT_CLASSIFICATION	 multiple parents find the right one based the table alias the parentexpr 
WITHOUT_CLASSIFICATION	 check that exception from getmetadata reported correctly 
WITHOUT_CLASSIFICATION	 class partitioniterator 
WITHOUT_CLASSIFICATION	 formatted 
WITHOUT_CLASSIFICATION	 there only table alias return 
WITHOUT_CLASSIFICATION	 this column coming from right input only then update num nulls 
WITHOUT_CLASSIFICATION	 not qualify this optimization 
WITHOUT_CLASSIFICATION	 check for required fields 
WITHOUT_CLASSIFICATION	 wrap the transport exception rte since ugidoas then goes and unwraps this for out the doas block then unwrap one more time our catch clause get back the tte ugh 
WITHOUT_CLASSIFICATION	 threadunsafe position used write time 
WITHOUT_CLASSIFICATION	 higher compute cost 
WITHOUT_CLASSIFICATION	 nulls case 
WITHOUT_CLASSIFICATION	 this query will give runtime error 
WITHOUT_CLASSIFICATION	 drop primary key 
WITHOUT_CLASSIFICATION	 getprogressupdate 
WITHOUT_CLASSIFICATION	 get aggregation evaluators 
WITHOUT_CLASSIFICATION	 remove column 
WITHOUT_CLASSIFICATION	 set the local work all the operator can get this context 
WITHOUT_CLASSIFICATION	 secret 
WITHOUT_CLASSIFICATION	 create dispatcher and graph walker 
WITHOUT_CLASSIFICATION	 mergesum 
WITHOUT_CLASSIFICATION	 note fsdelete will fail windows the reason outputcommitter hadoop still writing logshistory linux dont care file still open and remove the directory anyway but windows refuse remove directory containing open files windows will leave output directory behind when job fail user needs remove the output directory manually 
WITHOUT_CLASSIFICATION	 new part 
WITHOUT_CLASSIFICATION	 generate reducesinkoperator 
WITHOUT_CLASSIFICATION	 finally check serializable 
WITHOUT_CLASSIFICATION	 hadoop gets and hadoop gets sigh 
WITHOUT_CLASSIFICATION	 schema 
WITHOUT_CLASSIFICATION	 maximum table size 
WITHOUT_CLASSIFICATION	 run initiator execute 
WITHOUT_CLASSIFICATION	 helper methods 
WITHOUT_CLASSIFICATION	 the function being added under database namespace then add entity representing the database only applicable permanentmetastore functions also add second entity representing the function name the authorization api implementation can decide which entities wants use authorize the createdrop function call 
WITHOUT_CLASSIFICATION	 rightinputrel filter and contains correlated reference make sure the correlated keys the filter condition forms unique key the rhs 
WITHOUT_CLASSIFICATION	 round 
WITHOUT_CLASSIFICATION	 now recompute state since weve done minor compactions and have different best set deltas 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 given these lines dont need double check later 
WITHOUT_CLASSIFICATION	 load partition that doesnt exist there some parallelism going you load more than partition which dont understand thats reasonable since each partition loaded parallel why happens here beyond the file name changes from run run between and and the data correct but this causes rowidbucketidfile names change 
WITHOUT_CLASSIFICATION	 copy the group key output batch now well copy the aggregates the end the group 
WITHOUT_CLASSIFICATION	 start 
WITHOUT_CLASSIFICATION	 this does the testing using remote metastore that finds more issues thrift 
WITHOUT_CLASSIFICATION	 this string constant used indicate alterhandler that 
WITHOUT_CLASSIFICATION	 build col details used scan 
WITHOUT_CLASSIFICATION	 view column authorization again even triggered again 
WITHOUT_CLASSIFICATION	 literal tinyint 
WITHOUT_CLASSIFICATION	 timer shared across entire factory and must released separately 
WITHOUT_CLASSIFICATION	 char columns should have correct display sizeprecision 
WITHOUT_CLASSIFICATION	 rest tests just picked above 
WITHOUT_CLASSIFICATION	 ensure there partition dir 
WITHOUT_CLASSIFICATION	 case replication idempotent taken care gettargettxnid 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 test that existing exclusive partition with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 outer join cannot performed table which being cached 
WITHOUT_CLASSIFICATION	 this project has correlated reference create value generator 
WITHOUT_CLASSIFICATION	 return the type string the first argument argument 
WITHOUT_CLASSIFICATION	 this sel cols udtf cols 
WITHOUT_CLASSIFICATION	 path being passed table dump location ahead and load needed tblname null then default the table name specified metadata which good are both specified which case thats what are intended create the new table 
WITHOUT_CLASSIFICATION	 test lazyhcatrecord init and read 
WITHOUT_CLASSIFICATION	 map 
WITHOUT_CLASSIFICATION	 need read bucket number which the last column value after partition columns 
WITHOUT_CLASSIFICATION	 codahale artifacts are lazilycreated 
WITHOUT_CLASSIFICATION	 must consistent with uncompressed stream seek orc see call site comments 
WITHOUT_CLASSIFICATION	 the case altering table for its partitions dont need lock the table itself just the partitions but the table will have readentity mark that readentity lock 
WITHOUT_CLASSIFICATION	 get exception resolving partition could describe table key return null continue processing for describe table key 
WITHOUT_CLASSIFICATION	 expect the dags not super large store full dependency set for each vertex 
WITHOUT_CLASSIFICATION	 make stripes with rows each 
WITHOUT_CLASSIFICATION	 add udaf args deduped reduce values 
WITHOUT_CLASSIFICATION	 not comparing hashctx irrelevant 
WITHOUT_CLASSIFICATION	 implies netty default number available processors 
WITHOUT_CLASSIFICATION	 boolean value match for extended char field 
WITHOUT_CLASSIFICATION	 test when two jars are added with shared dependencies and one jar deleted the shared dependencies should not deleted 
WITHOUT_CLASSIFICATION	 caller must remember small value length 
WITHOUT_CLASSIFICATION	 must have one those this point 
WITHOUT_CLASSIFICATION	 write large value this should use different byte buffer 
WITHOUT_CLASSIFICATION	 rows will filtered 
WITHOUT_CLASSIFICATION	 add parents for the newly created operator 
WITHOUT_CLASSIFICATION	 called when the value the partition has changed update the currentrank 
WITHOUT_CLASSIFICATION	 get unionoperator right now only handle when can find correlated reducesinkoperators from all inputs 
WITHOUT_CLASSIFICATION	 for select count from where tds 
WITHOUT_CLASSIFICATION	 original files delta directories deletedelta directory and base directories 
WITHOUT_CLASSIFICATION	 get the partition object already exists 
WITHOUT_CLASSIFICATION	 errored 
WITHOUT_CLASSIFICATION	 this weirdness setting our conf and then reading back does two things one handles the conversion the timeunit two keeps the value around for later case need again 
WITHOUT_CLASSIFICATION	 test that read can acquire after write 
WITHOUT_CLASSIFICATION	 initialize pathtoaliases 
WITHOUT_CLASSIFICATION	 for uncompressed case need some special processing before read 
WITHOUT_CLASSIFICATION	 appended since there could multiple scalar subqueries and filter 
WITHOUT_CLASSIFICATION	 the positions rscolinfolst are follows grpkeydistkeyvalues but distudaf may beforeafter some nondistudaf their positions can mixed for all udaf first check see groupby key not distinct key 
WITHOUT_CLASSIFICATION	 main memory hashmap 
WITHOUT_CLASSIFICATION	 basic algorithm determine rounding part nonzero for rounding scale away fractional digits present rounding clear integer rounding portion and add 
WITHOUT_CLASSIFICATION	 version 
WITHOUT_CLASSIFICATION	 scalesignum 
WITHOUT_CLASSIFICATION	 init aggregationclasses 
WITHOUT_CLASSIFICATION	 whether skippruning depends the payload from event which may signal skip the event payload too large 
WITHOUT_CLASSIFICATION	 this wont have decimal part because the hasdecimalmask bit not set 
WITHOUT_CLASSIFICATION	 combining acute acent bytes 
WITHOUT_CLASSIFICATION	 may not true with correlation operators muxdemux 
WITHOUT_CLASSIFICATION	 get the function documentation 
WITHOUT_CLASSIFICATION	 foo 
WITHOUT_CLASSIFICATION	 bucket centered already exists this must checked the next step 
WITHOUT_CLASSIFICATION	 verify that there now only new directory basexxxxxxx and the rest have have been cleaned 
WITHOUT_CLASSIFICATION	 use this via command line arg decimal use this via command line arg decimal 
WITHOUT_CLASSIFICATION	 row count exists stats arent estimated return 
WITHOUT_CLASSIFICATION	 remember for additional processing later 
WITHOUT_CLASSIFICATION	 timestamp values are pst timezone for tests set pst default 
WITHOUT_CLASSIFICATION	 delete the data the database 
WITHOUT_CLASSIFICATION	 handle select distinct gby there exist windowing functions 
WITHOUT_CLASSIFICATION	 means 
WITHOUT_CLASSIFICATION	 now delete the rest tables 
WITHOUT_CLASSIFICATION	 table missing then partitions are also wouldve been dropped just noop 
WITHOUT_CLASSIFICATION	 captures how the input ptf function should partitioned and ordered refers partition and order instance 
WITHOUT_CLASSIFICATION	 need check paths and partition desc for mapworks 
WITHOUT_CLASSIFICATION	 return false just noop 
WITHOUT_CLASSIFICATION	 all must selected otherwise size would zero repeating property will not change 
WITHOUT_CLASSIFICATION	 have some disk buffers see have entire part etc 
WITHOUT_CLASSIFICATION	 will invoked anyway teztask doing early initialize triggers for nonpool tez session 
WITHOUT_CLASSIFICATION	 expecting this exception 
WITHOUT_CLASSIFICATION	 get the number columns the users rows 
WITHOUT_CLASSIFICATION	 verify that task kill went out for all nodes running the specified host 
WITHOUT_CLASSIFICATION	 order use order from the block above relnode has pointer parent hence need top down but each block really belong its srcfrom hence the need pass sort for each block from its parent limit 
WITHOUT_CLASSIFICATION	 rawlocalfilesystem seems not able get the right permissions for local file always returns hdfs default permission can not overwrite directory deleting and recreating the directory and restoring its permissions should delete all its files and subdirectories instead 
WITHOUT_CLASSIFICATION	 this smb join 
WITHOUT_CLASSIFICATION	 listcoord coord holds two doubles 
WITHOUT_CLASSIFICATION	 optional string user 
WITHOUT_CLASSIFICATION	 value out range other unknown cases 
WITHOUT_CLASSIFICATION	 dummy create table command mark proper last repl after dump 
WITHOUT_CLASSIFICATION	 nonjavadoc order update decimal fast allocation need expose access the internal storage bytes and scale return 
WITHOUT_CLASSIFICATION	 add the udtfoperator the operator dag 
WITHOUT_CLASSIFICATION	 validate input formats all the partitions can vectorized 
WITHOUT_CLASSIFICATION	 this may happen were able establish connection once but its longer valid 
WITHOUT_CLASSIFICATION	 ckpt property not set empty means bootstrap not run this object 
WITHOUT_CLASSIFICATION	 also set the connection between each parent work and child work 
WITHOUT_CLASSIFICATION	 alter table add column change the metadata 
WITHOUT_CLASSIFICATION	 add another partition without stats 
WITHOUT_CLASSIFICATION	 figure out table acid not 
WITHOUT_CLASSIFICATION	 close resultset ignore exception any 
WITHOUT_CLASSIFICATION	 set alias work and put into smalltablealiaslist 
WITHOUT_CLASSIFICATION	 validate that some progress being made 
WITHOUT_CLASSIFICATION	 this means there existing partition 
WITHOUT_CLASSIFICATION	 skip authorization skip checking inside view skip checking authorization flag not enabled skip checking 
WITHOUT_CLASSIFICATION	 round with digits 
WITHOUT_CLASSIFICATION	 now set the output for the history 
WITHOUT_CLASSIFICATION	 wait until rjiscomplete 
WITHOUT_CLASSIFICATION	 newaggrel 
WITHOUT_CLASSIFICATION	 open the session closed 
WITHOUT_CLASSIFICATION	 these errors happen the jni lib not available for your platform 
WITHOUT_CLASSIFICATION	 now add again add this before our own config files that the 
WITHOUT_CLASSIFICATION	 need carry the insideview information from calcite into the ast 
WITHOUT_CLASSIFICATION	 prefix for separate row keys 
WITHOUT_CLASSIFICATION	 the children after not might need cast get common types for the two comparisons casting for between handled here special case because the first child for not and doesnt need 
WITHOUT_CLASSIFICATION	 check for part log message well part progress information 
WITHOUT_CLASSIFICATION	 the following code for mapjoin 
WITHOUT_CLASSIFICATION	 null error message here means the user has access 
WITHOUT_CLASSIFICATION	 intervalyearmonth 
WITHOUT_CLASSIFICATION	 disable resource monitoring although should off default 
WITHOUT_CLASSIFICATION	 add the number partitions given the current batchsize 
WITHOUT_CLASSIFICATION	 rightinputrel has this shape filter references corvar filterinputrel 
WITHOUT_CLASSIFICATION	 configure web application contexts for the web server 
WITHOUT_CLASSIFICATION	 counters for task execution side 
WITHOUT_CLASSIFICATION	 lower case null used within json objects 
WITHOUT_CLASSIFICATION	 nulls are considered not matching for equality comparison add the position the most recently inserted key 
WITHOUT_CLASSIFICATION	 all rowids are unique read after conversion acid rowids are exactly the same before and after compaction also check the file name only after compaction for completeness 
WITHOUT_CLASSIFICATION	 base type name primitivetypeentry map 
WITHOUT_CLASSIFICATION	 this means the key didnt exist the insertion point negative minus 
WITHOUT_CLASSIFICATION	 the aggregation buffer not estimable then get all the declared fields and compute the sizes from field types 
WITHOUT_CLASSIFICATION	 allocate the buffers prepare cache keys this point have read all the cbs need read cachebuffers contains some cache data and some unallocated membufs for decompression todecompress contains all the work need and each item points one the membufs cachebuffers target the iter 
WITHOUT_CLASSIFICATION	 header row transactions rows 
WITHOUT_CLASSIFICATION	 this key will put the conf file when planning acid operation 
WITHOUT_CLASSIFICATION	 for fast check possible existence will checked again 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 must support offsets able split 
WITHOUT_CLASSIFICATION	 only applicable nway hybrid grace hash join 
WITHOUT_CLASSIFICATION	 despite having fixed schema from hive have sparse columns accumulo 
WITHOUT_CLASSIFICATION	 lookup type infos for our input types and expected return type 
WITHOUT_CLASSIFICATION	 make one have nonstandard location 
WITHOUT_CLASSIFICATION	 case any other exception retry this also fails report original error and exit 
WITHOUT_CLASSIFICATION	 set the perms readonly access and create acl entries allowing write access 
WITHOUT_CLASSIFICATION	 are skipping the cds table here seems totally useless 
WITHOUT_CLASSIFICATION	 long not between 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 check that the columns referenced rightjoinkeys form 
WITHOUT_CLASSIFICATION	 analyze each side and let the left and right exprs the conjunct object return conjunct contains details the left and right side the conjunct expression 
WITHOUT_CLASSIFICATION	 log with double base 
WITHOUT_CLASSIFICATION	 currently this sole field affecting mergee task 
WITHOUT_CLASSIFICATION	 unit test for github howl issue 
WITHOUT_CLASSIFICATION	 get all valid partition paths and existing partitions for them any 
WITHOUT_CLASSIFICATION	 the consumer joinoutputprojrel nullindicator located 
WITHOUT_CLASSIFICATION	 far weve read the ones complement add turn into twos complement 
WITHOUT_CLASSIFICATION	 the output has some extra fields set them null convert 
WITHOUT_CLASSIFICATION	 input contains header skip header 
WITHOUT_CLASSIFICATION	 selectobjs hold the row from the select until receiving row from 
WITHOUT_CLASSIFICATION	 create task aliases mapping and alias input file mapping for resolver 
WITHOUT_CLASSIFICATION	 not equal convert all double and compare 
WITHOUT_CLASSIFICATION	 get the column names the aggregations for reduce sink 
WITHOUT_CLASSIFICATION	 for auto reduce parallelism minimum reducers requested 
WITHOUT_CLASSIFICATION	 this will create delta and deletedelta see mockrawreader 
WITHOUT_CLASSIFICATION	 provide facility set current timestamp during tests 
WITHOUT_CLASSIFICATION	 try get consistent view can make copy the headers 
WITHOUT_CLASSIFICATION	 recreate the partition existed before 
WITHOUT_CLASSIFICATION	 char 
WITHOUT_CLASSIFICATION	 the same applies files added with addfile theyre only guaranteed available 
WITHOUT_CLASSIFICATION	 even though the stats were estimated need warn user that stats are not available 
WITHOUT_CLASSIFICATION	 encoding must have data 
WITHOUT_CLASSIFICATION	 with data 
WITHOUT_CLASSIFICATION	 add fields used the condition 
WITHOUT_CLASSIFICATION	 describe the table populate the output stream 
WITHOUT_CLASSIFICATION	 recalculate the hdfs stats auto gather stats set 
WITHOUT_CLASSIFICATION	 the join key 
WITHOUT_CLASSIFICATION	 skip rowid 
WITHOUT_CLASSIFICATION	 all the integer types float double string char varchar 
WITHOUT_CLASSIFICATION	 order make the dependencies accessible 
WITHOUT_CLASSIFICATION	 gen would have prevented 
WITHOUT_CLASSIFICATION	 nothing here 
WITHOUT_CLASSIFICATION	 conversion avro primitive types hive primitive types avro hive null boolean boolean check int int check long bigint check float double check double double check bytes binary check fixed binary check string string check tinyint smallint 
WITHOUT_CLASSIFICATION	 constant means filter ignore when null 
WITHOUT_CLASSIFICATION	 might have connect parent work with this work later 
WITHOUT_CLASSIFICATION	 true true true false false false false true 
WITHOUT_CLASSIFICATION	 create file with all the job properties read sparksubmit change the files permissions that only the owner can read this avoid having the 
WITHOUT_CLASSIFICATION	 get the list tracking jobs these can used determine which jobs have expired 
WITHOUT_CLASSIFICATION	 implement reloptrule override the rule order union all branch elimination 
WITHOUT_CLASSIFICATION	 create parameter converters 
WITHOUT_CLASSIFICATION	 check there only one immediate child task and stats task 
WITHOUT_CLASSIFICATION	 ukname 
WITHOUT_CLASSIFICATION	 use the multichar delimiter parse the lazy struct 
WITHOUT_CLASSIFICATION	 here recursively check whether there are exact one limit the query whether there aggregation groupby distinct sort distributed table sampling any the subquery the query only qualifies both conditions are satisfied example qualified queries create table select col col from tbl limit insert overwrite table select col hashcol splitcol from limit select from select col col select from limit 
WITHOUT_CLASSIFICATION	 check semantic conditions 
WITHOUT_CLASSIFICATION	 tasks including all dependencies 
WITHOUT_CLASSIFICATION	 ptf node form tokptblfunction name alias partitioningspec expression ptf node guaranteed have alias here 
WITHOUT_CLASSIFICATION	 get bigkeysdirtotaskmap 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqltimestamp 
WITHOUT_CLASSIFICATION	 has already been initialized using hiveconf 
WITHOUT_CLASSIFICATION	 gets status job form job maximum concurrent job status requests are configured then status request will executed thread from thread pool job status request time out configured then request execution thread will interrupted thread times out and does action 
WITHOUT_CLASSIFICATION	 from load data into acid converted table 
WITHOUT_CLASSIFICATION	 thought had the entire part cache but dont convert start noncached since are the first gap the previous stuff must contiguous 
WITHOUT_CLASSIFICATION	 myenumstringmap 
WITHOUT_CLASSIFICATION	 clean tables default 
WITHOUT_CLASSIFICATION	 always send secure cookies for ssl mode 
WITHOUT_CLASSIFICATION	 default argumentcompletor strict mode meaning token only autocompleted all prior tokens match dont want that since there are valid tokens 
WITHOUT_CLASSIFICATION	 clear set capture new set functions 
WITHOUT_CLASSIFICATION	 make char and varchar type info parsable 
WITHOUT_CLASSIFICATION	 set context for 
WITHOUT_CLASSIFICATION	 return false only occurred error when execution the sql and the sql should follow the rules 
WITHOUT_CLASSIFICATION	 for now 
WITHOUT_CLASSIFICATION	 create split for the given partition 
WITHOUT_CLASSIFICATION	 for each dynamically created directory construct full partition spec and load the partition based that 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 return value not checked due concurrent access 
WITHOUT_CLASSIFICATION	 output debug info 
WITHOUT_CLASSIFICATION	 these tasks should have come from the same job 
WITHOUT_CLASSIFICATION	 create view with name already exist just verify failure flow clears the added createtable event 
WITHOUT_CLASSIFICATION	 list status jobs request maximum concurrent job list requests are configured then list request will executed thread from thread pool job list request time out configured then request execution thread will interrupted thread times out and does action 
WITHOUT_CLASSIFICATION	 for all parents other than the big table insert dummy store operator 
WITHOUT_CLASSIFICATION	 clean the databases 
WITHOUT_CLASSIFICATION	 server args 
WITHOUT_CLASSIFICATION	 objecttype 
WITHOUT_CLASSIFICATION	 events 
WITHOUT_CLASSIFICATION	 create the project after for those repeated values select 
WITHOUT_CLASSIFICATION	 tracing down the operator tree from the table scan operator 
WITHOUT_CLASSIFICATION	 note the enum names match field names the struct 
WITHOUT_CLASSIFICATION	 the remaining parameters are starting parameter name 
WITHOUT_CLASSIFICATION	 specificationtitle 
WITHOUT_CLASSIFICATION	 how many levels ancestors keep the stack during dispatching 
WITHOUT_CLASSIFICATION	 first underflow not error ansi sql numeric cast decimal without error 
WITHOUT_CLASSIFICATION	 need run spark job make sure the jar added the class loader monitoring sparkcontextaddjar doesnt mean much can only sure jars have been distributed 
WITHOUT_CLASSIFICATION	 test when third argument has nulls and repeats 
WITHOUT_CLASSIFICATION	 this import being done for replication then this will managed table and replacements are allowed irrespective what the table currently looks like more checks are necessary 
WITHOUT_CLASSIFICATION	 flag indicate whether cancel the task when exception timeoutexception raised the default cancel thread 
WITHOUT_CLASSIFICATION	 the index 
WITHOUT_CLASSIFICATION	 insert mapside 
WITHOUT_CLASSIFICATION	 the test table has rows total query time should 
WITHOUT_CLASSIFICATION	 retrieve the mbean server 
WITHOUT_CLASSIFICATION	 query will try add more partitions already existing partitions but will get cancelled for violation 
WITHOUT_CLASSIFICATION	 running all the servers 
WITHOUT_CLASSIFICATION	 note could call for tables the recursive call usually needed for nonmm tables because the path management not strict and the code does whatever that should not happen for tables keep like this for now may need replacement find valid use case 
WITHOUT_CLASSIFICATION	 here means currently committing txn performed updatedelete and should check conflict 
WITHOUT_CLASSIFICATION	 readlock not updating any stats the moment 
WITHOUT_CLASSIFICATION	 seconds 
WITHOUT_CLASSIFICATION	 read the whole file 
WITHOUT_CLASSIFICATION	 publish configs for this instance the data the node 
WITHOUT_CLASSIFICATION	 checkh for and not the subquery must implicitly explicitly only contain one select item 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this should not happen unless are evicting lot once buffers are large 
WITHOUT_CLASSIFICATION	 when introduce discrepancy the state give the task updater unless was already given one the updater already doing stuff would handle the changed state when its done with whatever its doing the updater not going give until the discrepancies are eliminated 
WITHOUT_CLASSIFICATION	 find functions which name contains tofind the dummy database 
WITHOUT_CLASSIFICATION	 events can start coming the moment the inputinitializer created the pruner must setup and initialized here that sets its structures start accepting events setting initialize leads window where events may come before the pruner initialized which may cause drop events 
WITHOUT_CLASSIFICATION	 statistics for the column already exist use 
WITHOUT_CLASSIFICATION	 note that need call getresults for simple fetch optimization however need skip all the results 
WITHOUT_CLASSIFICATION	 compute distance and store sorted map 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the sort order ascendingdescending for each field set true when descending invert 
WITHOUT_CLASSIFICATION	 make sure dont collide with the source files tables dont support concat dont expect the merge merged files 
WITHOUT_CLASSIFICATION	 create vertexgroup 
WITHOUT_CLASSIFICATION	 case substring from index the end 
WITHOUT_CLASSIFICATION	 sleep for expiry time and then fetch again sleep twice the ttl interval things should have been cleaned then 
WITHOUT_CLASSIFICATION	 walk through the ast 
WITHOUT_CLASSIFICATION	 the process has not logged using keytab this should test mode cant use keytab authenticate with zookeeper 
WITHOUT_CLASSIFICATION	 range starts here 
WITHOUT_CLASSIFICATION	 check job config for overrides otherwise use the default server value 
WITHOUT_CLASSIFICATION	 fastbitset rather than using integers 
WITHOUT_CLASSIFICATION	 the user tries actually use this session and fails proceeding returndestroy 
WITHOUT_CLASSIFICATION	 insert into newtablename select from where partition spec 
WITHOUT_CLASSIFICATION	 these are things that goes through singleton initialization most queries 
WITHOUT_CLASSIFICATION	 one last check 
WITHOUT_CLASSIFICATION	 istbllevel 
WITHOUT_CLASSIFICATION	 verify that returns zero events there are more notifications available 
WITHOUT_CLASSIFICATION	 nulls 
WITHOUT_CLASSIFICATION	 then the current node with the previous one 
WITHOUT_CLASSIFICATION	 years months years years month years month 
WITHOUT_CLASSIFICATION	 create map source file system destination path list files copy 
WITHOUT_CLASSIFICATION	 remove begining 
WITHOUT_CLASSIFICATION	 qualified column access for which table was not found 
WITHOUT_CLASSIFICATION	 write out the plan local file 
WITHOUT_CLASSIFICATION	 change the resource plan that the session gets killed 
WITHOUT_CLASSIFICATION	 get the updated path list 
WITHOUT_CLASSIFICATION	 set really low batch size ensure batching 
WITHOUT_CLASSIFICATION	 unpartitioned table 
WITHOUT_CLASSIFICATION	 verify that partition was added correctly and properties were inherited from the hcattable 
WITHOUT_CLASSIFICATION	 some application depends the original value being set 
WITHOUT_CLASSIFICATION	 note that when serializing row the logical mapping using selected use has already been performed batchindex the actual index the row 
WITHOUT_CLASSIFICATION	 get task detail link from the jobtask page 
WITHOUT_CLASSIFICATION	 project the column corresponding the distinct aggregate project asis all the nondistinct aggregates 
WITHOUT_CLASSIFICATION	 since the start and the length the first call sync should with the value return that for getpos 
WITHOUT_CLASSIFICATION	 relative start position the windowing can negative 
WITHOUT_CLASSIFICATION	 initialize hcatinputformat 
WITHOUT_CLASSIFICATION	 infix operator 
WITHOUT_CLASSIFICATION	 rounding numbers that increase int digits 
WITHOUT_CLASSIFICATION	 impl note hive provides authorization with its own model and calls the defined from however hcat has additional calls the auth provider implement expected behavior for this means that the defined auth provider called both hive and hcat the following are missing from hives implementation and when they are fixed hive can remove the hcatspecific auth checks create databasetable add partition statements does not call with the candidate objects which means that cannot checks against defined location hiveoperation does not define sufficient privileges for most the operations especially database operations for some the operations hive semanticanalyzer does not add the changed object writeentity readentity see see 
WITHOUT_CLASSIFICATION	 need kill anything 
WITHOUT_CLASSIFICATION	 currtask roottasks remove and add its children the roottasks which currtask its only parent task 
WITHOUT_CLASSIFICATION	 size query will fail but least wed get see the query debug output 
WITHOUT_CLASSIFICATION	 not merge not know how connect two operator trees 
WITHOUT_CLASSIFICATION	 all arguments are known length then can keep track the max length the return type however the return length exceeds the 
WITHOUT_CLASSIFICATION	 create operator info list return 
WITHOUT_CLASSIFICATION	 the source column names for orc serde that will used the schema 
WITHOUT_CLASSIFICATION	 value null the type should also void 
WITHOUT_CLASSIFICATION	 setup the compression codec 
WITHOUT_CLASSIFICATION	 nothing output null 
WITHOUT_CLASSIFICATION	 construct outer struct 
WITHOUT_CLASSIFICATION	 create two tables one user foo and other user bar 
WITHOUT_CLASSIFICATION	 now both batches have committed but not closed for each primary file expect side file exist and indicate the true length primary file 
WITHOUT_CLASSIFICATION	 sql std authorization managing privileges the tableview levels only ignore partitions 
WITHOUT_CLASSIFICATION	 remove any semijoin branch associated with hashjoins parents operator 
WITHOUT_CLASSIFICATION	 assumptions about the range numeric data being analyzed 
WITHOUT_CLASSIFICATION	 preceeding work must set the newly generated map 
WITHOUT_CLASSIFICATION	 parameterstotalsize parametersnumfiles 
WITHOUT_CLASSIFICATION	 skip first child since struct 
WITHOUT_CLASSIFICATION	 second split delta 
WITHOUT_CLASSIFICATION	 reloadable jars 
WITHOUT_CLASSIFICATION	 cleanup the synthetic predicate the tablescan operator replacing with true 
WITHOUT_CLASSIFICATION	 the storage arrays for this column vector corresponds the storage hiveintervaldaytime 
WITHOUT_CLASSIFICATION	 double min and max 
WITHOUT_CLASSIFICATION	 allocate temporary output dir the location the table 
WITHOUT_CLASSIFICATION	 batches will sized 
WITHOUT_CLASSIFICATION	 checking mapjoin can converted bucket mapjoin 
WITHOUT_CLASSIFICATION	 used because that the correct move task for the merge and move use case 
WITHOUT_CLASSIFICATION	 the input file has changed every operator can invoke specific action 
WITHOUT_CLASSIFICATION	 round the specified number decimal places using the standard hive round function 
WITHOUT_CLASSIFICATION	 base encoded and stringified token for server 
WITHOUT_CLASSIFICATION	 rel offset word big value len next value small len value bytes and beyond records have relative offset word the beginning 
WITHOUT_CLASSIFICATION	 access the fields 
WITHOUT_CLASSIFICATION	 hbase input formats are not thread safe today see hive 
WITHOUT_CLASSIFICATION	 guaranteed there only list within listbucketcols 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream int 
WITHOUT_CLASSIFICATION	 largest size allowed smallbuffer 
WITHOUT_CLASSIFICATION	 disable auth the call should succeed 
WITHOUT_CLASSIFICATION	 nothing this was fixed mapreduce 
WITHOUT_CLASSIFICATION	 insert five rows nonacid table 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 fulltablenames 
WITHOUT_CLASSIFICATION	 cap factrowcount because numerical artifacts can cause 
WITHOUT_CLASSIFICATION	 send the bucket ids associated with the tasks must happen after parallelism set 
WITHOUT_CLASSIFICATION	 there are elements the union 
WITHOUT_CLASSIFICATION	 once the source node reached stop traversal for this 
WITHOUT_CLASSIFICATION	 there should one base dir the location 
WITHOUT_CLASSIFICATION	 print the stack from all threads for debugging purposes 
WITHOUT_CLASSIFICATION	 type job request 
WITHOUT_CLASSIFICATION	 network cost map side join 
WITHOUT_CLASSIFICATION	 spill the big table rows into appropriate partition when the joinresult spill means the corresponding small table row may have been spilled disk least the partition that holds this row disk need 
WITHOUT_CLASSIFICATION	 build map not convert multiple times further remove already included predicates 
WITHOUT_CLASSIFICATION	 set the location the storagedescriptor 
WITHOUT_CLASSIFICATION	 hiveserver output consumed jdbcodbc clients 
WITHOUT_CLASSIFICATION	 need know count since this specialized for innot corr subqueries 
WITHOUT_CLASSIFICATION	 remove the value every key found matching 
WITHOUT_CLASSIFICATION	 convert each keyvaluemap appropriate expression 
WITHOUT_CLASSIFICATION	 this should not schedule new compaction due prior failures but will create attempted entry 
WITHOUT_CLASSIFICATION	 add tblid and empty bitvector 
WITHOUT_CLASSIFICATION	 means user specified table not partition 
WITHOUT_CLASSIFICATION	 convert requiredprivileges 
WITHOUT_CLASSIFICATION	 reserve bytes for the hash dont just reserve there may junk there 
WITHOUT_CLASSIFICATION	 otherwise gbevaluator and expr nodes may get shared among multiple ops 
WITHOUT_CLASSIFICATION	 minihs will continue leader 
WITHOUT_CLASSIFICATION	 lastanalyzed 
WITHOUT_CLASSIFICATION	 emulate serializationutils deserialization used orc 
WITHOUT_CLASSIFICATION	 translate the double into sign exponent and significand according 
WITHOUT_CLASSIFICATION	 return true both are null false one null and the other isnt 
WITHOUT_CLASSIFICATION	 replicate the remaining insert overwrite operations the table 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 add the select operator 
WITHOUT_CLASSIFICATION	 for each basework with operator build sparkwork for its small table baseworks 
WITHOUT_CLASSIFICATION	 each partition may have different format have check them all before deciding make full crud table run batches prevent oom 
WITHOUT_CLASSIFICATION	 decimal math 
WITHOUT_CLASSIFICATION	 else common code 
WITHOUT_CLASSIFICATION	 skip the driver and directly loadable tables 
WITHOUT_CLASSIFICATION	 wait time milliseconds before another cancel request made 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 find the parsed deltas some them containing only the insert delta events 
WITHOUT_CLASSIFICATION	 the strict mode have provide partition pruner for each table 
WITHOUT_CLASSIFICATION	 test gtltltegtelike for strings 
WITHOUT_CLASSIFICATION	 perform logical optimization 
WITHOUT_CLASSIFICATION	 data array and masks array are then traversed together and checked for corresponding set bits 
WITHOUT_CLASSIFICATION	 update the maps note output for sortrel considered same its input may end not using that present sort rel also note that rowtype sortrel the type child child happens synthetic project that introduced then that projectrel would 
WITHOUT_CLASSIFICATION	 eliminate stripes that doesnt satisfy the predicate condition 
WITHOUT_CLASSIFICATION	 destination disk 
WITHOUT_CLASSIFICATION	 tables need custom handling for union suffix tables use parent too 
WITHOUT_CLASSIFICATION	 the same umbilical used multiple tasks problematic the case where multiple tasks 
WITHOUT_CLASSIFICATION	 return new operator 
WITHOUT_CLASSIFICATION	 third parameter has been specified should integer that specifies the number 
WITHOUT_CLASSIFICATION	 parse string select list this allows table functions passed expression strings that are translated the context they define invocation time currently used npath allow users specify what output they want npath allows expressions tpath column that represents the matched set rows this column doesnt exist the input schema and hence the result expression cannot analyzed the regular hive translation process 
WITHOUT_CLASSIFICATION	 takes instead taskattemptcontext cant use that here 
WITHOUT_CLASSIFICATION	 reset everything for the next arena assume everything has been cleaned 
WITHOUT_CLASSIFICATION	 multikey specific save key 
WITHOUT_CLASSIFICATION	 create deeply nested table which has more partition keys than the pool size 
WITHOUT_CLASSIFICATION	 create rowid column select clause from left input for the right outer join this needed for the update clause hence find the following node tokquery tokfrom tokrightouterjoin toksubquery tokquery tokinsert tokselect and then create the following child node tokselexpr toktableorcol cmvmatview 
WITHOUT_CLASSIFICATION	 create empty file which not valid rcfile 
WITHOUT_CLASSIFICATION	 smile mapper used read query results that are serilized binary instead json 
WITHOUT_CLASSIFICATION	 can still fold since here null equivalent false 
WITHOUT_CLASSIFICATION	 second 
WITHOUT_CLASSIFICATION	 nesting level limits 
WITHOUT_CLASSIFICATION	 interceptor for adding username pwd 
WITHOUT_CLASSIFICATION	 test varchar literal string column comparison 
WITHOUT_CLASSIFICATION	 all left data small tables are less than and equal the left data big table lets them catch 
WITHOUT_CLASSIFICATION	 has nulls not repeating 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 static fieldsinitializers 
WITHOUT_CLASSIFICATION	 vectorization only supports primitive data types assert the same 
WITHOUT_CLASSIFICATION	 multikey check for repeating 
WITHOUT_CLASSIFICATION	 read permissionno permissions the expected user 
WITHOUT_CLASSIFICATION	 build mapside graph graph template either input ptfmap reducesink input reducesink here the exprnodedescriptors the querydef are based the input operators 
WITHOUT_CLASSIFICATION	 hive assumes that user names the files per the corresponding bucket for file names should follow the format etc here the file will belong bucket and bucket and 
WITHOUT_CLASSIFICATION	 flatten and 
WITHOUT_CLASSIFICATION	 reuse existing available column the same required type 
WITHOUT_CLASSIFICATION	 test capability for tests 
WITHOUT_CLASSIFICATION	 this code with branches and all not executed there are string keys 
WITHOUT_CLASSIFICATION	 modifiable 
WITHOUT_CLASSIFICATION	 test with nulls input 
WITHOUT_CLASSIFICATION	 there were grp and perms begin with 
WITHOUT_CLASSIFICATION	 start 
WITHOUT_CLASSIFICATION	 actualbatchsize half batchsize when exception expected 
WITHOUT_CLASSIFICATION	 set columnaccessinfo for 
WITHOUT_CLASSIFICATION	 partarchivelevel 
WITHOUT_CLASSIFICATION	 optional vertexorbinary workspec 
WITHOUT_CLASSIFICATION	 create new open session request object 
WITHOUT_CLASSIFICATION	 zeroes 
WITHOUT_CLASSIFICATION	 use try finally cleanup temp file something goes wrong 
WITHOUT_CLASSIFICATION	 all alters done now replicate them over 
WITHOUT_CLASSIFICATION	 delete the original node 
WITHOUT_CLASSIFICATION	 per acid write test nonacidacid conversion mixed with load data 
WITHOUT_CLASSIFICATION	 sort columns are not allowed for full acid table change insertonly table 
WITHOUT_CLASSIFICATION	 current value 
WITHOUT_CLASSIFICATION	 vector serde can disabled both client and server side 
WITHOUT_CLASSIFICATION	 more than operator 
WITHOUT_CLASSIFICATION	 length greater than max key prefix 
WITHOUT_CLASSIFICATION	 create row file copy 
WITHOUT_CLASSIFICATION	 first delete any mvs avoid race conditions 
WITHOUT_CLASSIFICATION	 determine the temp table path 
WITHOUT_CLASSIFICATION	 update for 
WITHOUT_CLASSIFICATION	 use the cache rather than full query execution this point the caller should return from semantic analysis 
WITHOUT_CLASSIFICATION	 the specified operator class 
WITHOUT_CLASSIFICATION	 reuse the partition specs from dest partition since they should the same 
WITHOUT_CLASSIFICATION	 optional string applicationidstring 
WITHOUT_CLASSIFICATION	 check see have seen this request before ignore not add our queue 
WITHOUT_CLASSIFICATION	 that can test old files 
WITHOUT_CLASSIFICATION	 enter 
WITHOUT_CLASSIFICATION	 future this may examine context return appropriate hcatwriter 
WITHOUT_CLASSIFICATION	 driver driver new driverconf clisessionstateconf 
WITHOUT_CLASSIFICATION	 case test might not want remove the log directory 
WITHOUT_CLASSIFICATION	 inputs cannot use with input formats 
WITHOUT_CLASSIFICATION	 make more inserts that have copy copy files export 
WITHOUT_CLASSIFICATION	 fill starting with highest digit highest longword and move down end will will shift everything down necessary 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 multicolumn also covers table nondefault database 
WITHOUT_CLASSIFICATION	 ududed okhotsk atka mackerel kanji 
WITHOUT_CLASSIFICATION	 multikey outer null detection 
WITHOUT_CLASSIFICATION	 input array used fill the entire size the vector row batch 
WITHOUT_CLASSIFICATION	 drop any constraints the table 
WITHOUT_CLASSIFICATION	 format sorted statement 
WITHOUT_CLASSIFICATION	 descriptor 
WITHOUT_CLASSIFICATION	 restore the context 
WITHOUT_CLASSIFICATION	 fetch operator 
WITHOUT_CLASSIFICATION	 scriptoperator echo the output the select 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 have that the null does not interfere with the current equal key series there one not set savejoinresult let current match equal key series keep going let current spill equal key series keep going let current nomatch keep not matching 
WITHOUT_CLASSIFICATION	 table was created user specified location using the ddl like create table tbl location should treated like external table the table rename its data location should not changed can check the table directory was created directly under its database directory tell such table 
WITHOUT_CLASSIFICATION	 sets the field and tag the union returns the union 
WITHOUT_CLASSIFICATION	 sorted 
WITHOUT_CLASSIFICATION	 maximum number times cancel request sent job request execution thread futurecancel may not able interrupt the thread blocked network calls 
WITHOUT_CLASSIFICATION	 the acid state probably absent warning logged the get method 
WITHOUT_CLASSIFICATION	 pushed the predicate into the table scan need remove the 
WITHOUT_CLASSIFICATION	 use that task 
WITHOUT_CLASSIFICATION	 druid timestamp column 
WITHOUT_CLASSIFICATION	 logj configuration file not set could not found use default setting 
WITHOUT_CLASSIFICATION	 http today but might not 
WITHOUT_CLASSIFICATION	 use defaults partitions are put the table directory 
WITHOUT_CLASSIFICATION	 restrictionm allow only subquery expression per query 
WITHOUT_CLASSIFICATION	 only needed when grouping sets are present 
WITHOUT_CLASSIFICATION	 test that existing sharedread with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 glorified cast from iterabletbase iterablepartition 
WITHOUT_CLASSIFICATION	 all tables good destination drop source 
WITHOUT_CLASSIFICATION	 mintxnid would nonzero txnid 
WITHOUT_CLASSIFICATION	 get all join columns from join keys 
WITHOUT_CLASSIFICATION	 make sure only return keydecompressor once 
WITHOUT_CLASSIFICATION	 filters dont change the column names need anything for them 
WITHOUT_CLASSIFICATION	 can this mapjoin converted bucketed mapjoin the following checks are performed the join columns contains all the bucket columns the join keys are not transformed the subquery all partitions contain the expected number files number buckets the number buckets the big table can divided buckets small tables 
WITHOUT_CLASSIFICATION	 location expected with 
WITHOUT_CLASSIFICATION	 fall through 
WITHOUT_CLASSIFICATION	 check vectorized orc reader against orc row reader 
WITHOUT_CLASSIFICATION	 value becomes null for rounding beyond 
WITHOUT_CLASSIFICATION	 open txn tested for validwriteidlist get the validtxnlist during open itself verify the validwriteidlist with openaborted write txns this table 
WITHOUT_CLASSIFICATION	 now are handling exact types base implementation handles type promotion 
WITHOUT_CLASSIFICATION	 required required required optional optional required optional optional 
WITHOUT_CLASSIFICATION	 dont support maskingfiltering against acid query the moment 
WITHOUT_CLASSIFICATION	 update file sink descriptor 
WITHOUT_CLASSIFICATION	 move process the next parseddelta 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 default threshold for using main memory based hashmap 
WITHOUT_CLASSIFICATION	 prefix for primary row keys 
WITHOUT_CLASSIFICATION	 for mapwork getallrootoperators not suitable since checks getpathtoaliases and will return null this empty here are 
WITHOUT_CLASSIFICATION	 and finally cache policy uses cache notify eviction the cycle complete 
WITHOUT_CLASSIFICATION	 create new projectsortproject sequence 
WITHOUT_CLASSIFICATION	 this request lower priority should not affect anything 
WITHOUT_CLASSIFICATION	 interface for vector map join hash table which could hash map hash multiset hash set for single long 
WITHOUT_CLASSIFICATION	 add aux jars 
WITHOUT_CLASSIFICATION	 delete something but make sure txn rolled back 
WITHOUT_CLASSIFICATION	 set the config value catalogs other than hive 
WITHOUT_CLASSIFICATION	 singlecolumn string specific repeated lookup 
WITHOUT_CLASSIFICATION	 create the struct needed 
WITHOUT_CLASSIFICATION	 null filtering does not work here currently doing filter thrifthttpservlet 
WITHOUT_CLASSIFICATION	 initialization fails and does the retry resource plan change 
WITHOUT_CLASSIFICATION	 not needed anymore 
WITHOUT_CLASSIFICATION	 zero length key not allowed block compress writer use byte writable 
WITHOUT_CLASSIFICATION	 the size flush the string buffer 
WITHOUT_CLASSIFICATION	 todo fix currently lpde can only carry single partitionspec and that defeats the purpose 
WITHOUT_CLASSIFICATION	 outside the inner loop results npeoutofbounds errors 
WITHOUT_CLASSIFICATION	 release locks from select from unblock hte drop partition retest the the drop partiton lock 
WITHOUT_CLASSIFICATION	 skipped over leading and xffs 
WITHOUT_CLASSIFICATION	 the expr column const will try cast the const string according the data type the column 
WITHOUT_CLASSIFICATION	 need check finishable here was set would already the queue 
WITHOUT_CLASSIFICATION	 perform some checks whether the node will become available not 
WITHOUT_CLASSIFICATION	 since were only creating view not executing dont need optimize translate the plan and fact those procedures can 
WITHOUT_CLASSIFICATION	 this will throw expected exception since clientserver modes are incompatible 
WITHOUT_CLASSIFICATION	 didnt find case when udf 
WITHOUT_CLASSIFICATION	 delimited way 
WITHOUT_CLASSIFICATION	 set this encounter condition were not expecting 
WITHOUT_CLASSIFICATION	 binarycolumns 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 note that this will invoked cases directsql was disabled start with directsql threw and was disabled 
WITHOUT_CLASSIFICATION	 note location check here the buffer always locked for move 
WITHOUT_CLASSIFICATION	 hive these decimal values should trimmed trailing zeros 
WITHOUT_CLASSIFICATION	 null last default for descending order 
WITHOUT_CLASSIFICATION	 calcite always needs the else clause defined explicitly 
WITHOUT_CLASSIFICATION	 change all the linked file sink descriptors 
WITHOUT_CLASSIFICATION	 also prefer missed heartbeat over stuck query case discrepancy 
WITHOUT_CLASSIFICATION	 create directory with permissions 
WITHOUT_CLASSIFICATION	 will ultimately instantiate the accumuloserde with null configuration have accept this and just fail late data attempted pulled from the configuration 
WITHOUT_CLASSIFICATION	 new hivesitexml file 
WITHOUT_CLASSIFICATION	 empty java opts 
WITHOUT_CLASSIFICATION	 remove any virtual cols 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 scaling down may have opened trailing zeroes 
WITHOUT_CLASSIFICATION	 vectorized implementation roundcol function 
WITHOUT_CLASSIFICATION	 first field always starts from even when missing 
WITHOUT_CLASSIFICATION	 ascending 
WITHOUT_CLASSIFICATION	 new operators 
WITHOUT_CLASSIFICATION	 have the dag now proceed get the splits 
WITHOUT_CLASSIFICATION	 need capture the timing 
WITHOUT_CLASSIFICATION	 begin walk through the task tree 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 verify mergeonlytask not optimized merge task writes directly finaldirname then movetask executed 
WITHOUT_CLASSIFICATION	 remove operator and combine 
WITHOUT_CLASSIFICATION	 each headersi virtual byte minallocation 
WITHOUT_CLASSIFICATION	 note here should use the new partition predicate pushdown api get list pruned list 
WITHOUT_CLASSIFICATION	 cms parallel ggc other vendors like ibm azul etc use different names 
WITHOUT_CLASSIFICATION	 hive queryid not always unique 
WITHOUT_CLASSIFICATION	 make sure the user has not requested insane amount txns 
WITHOUT_CLASSIFICATION	 execute one instruction terminate executing script there error silent mode prevent the query and prompt being echoed back terminal 
WITHOUT_CLASSIFICATION	 create deltabucket with row and close the file 
WITHOUT_CLASSIFICATION	 gen optimized ast 
WITHOUT_CLASSIFICATION	 web port cannot obtained 
WITHOUT_CLASSIFICATION	 refs these comparisons are anded together 
WITHOUT_CLASSIFICATION	 uses generic jdbc escape functions add limit clause query string 
WITHOUT_CLASSIFICATION	 dont need recordshuffleinfo since the out sync unregister will not remove the credentials 
WITHOUT_CLASSIFICATION	 demuxoperator forwards row exactly one child its children list based the tag and newtagtochildindex processop method need not anything here 
WITHOUT_CLASSIFICATION	 bgenjjtree struct 
WITHOUT_CLASSIFICATION	 compact ttp running the worker explicitly order get the reference the compactor job 
WITHOUT_CLASSIFICATION	 can there acls theres some access get acls assume means free for all 
WITHOUT_CLASSIFICATION	 block make sure kill happened successfully 
WITHOUT_CLASSIFICATION	 map join dump file name 
WITHOUT_CLASSIFICATION	 same instance driver which can run multiple queries 
WITHOUT_CLASSIFICATION	 increase check that the pool grows 
WITHOUT_CLASSIFICATION	 may need linear interpolation get the exact percentile 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 this only your own peril and never the production code 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 marker track the previous character was escape character 
WITHOUT_CLASSIFICATION	 the default setting throw assume dovalidate doskip means throw 
WITHOUT_CLASSIFICATION	 not doing any check 
WITHOUT_CLASSIFICATION	 execute extended optimization for each check whether other same work could merge into this one they are merged operators the resulting work will considered 
WITHOUT_CLASSIFICATION	 signature for wrapped storer see comments 
WITHOUT_CLASSIFICATION	 constant node 
WITHOUT_CLASSIFICATION	 subclass must provide this method this method invoked during translation and also when the operator initialized during runtime subclass must use this call setup the shape its output subsequent this call call getoutputoi call the link must return the the output this function 
WITHOUT_CLASSIFICATION	 remove the path not present 
WITHOUT_CLASSIFICATION	 case multitable insert the path alias mapping needed for all the sources since there reducer treat plan with null reducer 
WITHOUT_CLASSIFICATION	 can optimized later that operator operator initclose performed only after that operation has been performed all the parents this will require initializing the whole tree all the mappers which might required for mappers spanning multiple files anyway future 
WITHOUT_CLASSIFICATION	 nonjavadoc this provides lazyinteger like class which can initialized from data stored binary format see int int 
WITHOUT_CLASSIFICATION	 calculate complete collection 
WITHOUT_CLASSIFICATION	 convert filternode 
WITHOUT_CLASSIFICATION	 already handled all delete deltas above and there should not any other deltas for any table type this was acid code path 
WITHOUT_CLASSIFICATION	 this vectorexpression special case dont return descriptor 
WITHOUT_CLASSIFICATION	 find the biggest reduce sink 
WITHOUT_CLASSIFICATION	 inputts the the local timezone for this udf want the timezone represented fromtz 
WITHOUT_CLASSIFICATION	 for each component this lock request add entry the txncomponents table 
WITHOUT_CLASSIFICATION	 setup exprnode 
WITHOUT_CLASSIFICATION	 would either return fqdn 
WITHOUT_CLASSIFICATION	 http server 
WITHOUT_CLASSIFICATION	 top level query 
WITHOUT_CLASSIFICATION	 used group dependent tasks for multi table inserts 
WITHOUT_CLASSIFICATION	 select from tab txn 
WITHOUT_CLASSIFICATION	 bootstrap 
WITHOUT_CLASSIFICATION	 unfortunately making prunedpartitions immutable not possible here with semijoins not all tables are costed cbo their 
WITHOUT_CLASSIFICATION	 use milliseconds parser pattern matches our specialcase millis pattern string 
WITHOUT_CLASSIFICATION	 perf this function called for every row setting the selectedprojected columns the first call and dont that for the following calls ideally this should done the constructor where dont need branch the function for each row 
WITHOUT_CLASSIFICATION	 dont send the parseddbname this method will parse itself 
WITHOUT_CLASSIFICATION	 close 
WITHOUT_CLASSIFICATION	 operator wants some work the beginning group 
WITHOUT_CLASSIFICATION	 modifier letter small bytes 
WITHOUT_CLASSIFICATION	 too many sessions are outstanding due expiration restarts should not happen with inuse sessions because already kills the extras will kill 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 try fold otherwise return the expression itself 
WITHOUT_CLASSIFICATION	 the mapping from the index child operator its corresponding 
WITHOUT_CLASSIFICATION	 for instance this the case when are creating the table 
WITHOUT_CLASSIFICATION	 create currjobcontext the latest gets all the config changes 
WITHOUT_CLASSIFICATION	 only stored update based the original fixtmppath only stored update based the original fixtmppath 
WITHOUT_CLASSIFICATION	 execute prepared statement 
WITHOUT_CLASSIFICATION	 column family 
WITHOUT_CLASSIFICATION	 multiple level folder are there fsrename failing first create the targetpathgetparent not exist 
WITHOUT_CLASSIFICATION	 test class read series values the designated input stream 
WITHOUT_CLASSIFICATION	 this implementation vectorized join delegating all the work the rowmode implementation hijacking the big table node evaluators and calling the rowmode join processop for each row the input batch since the join operator not fully vectorized anyway the moment due the use rowmode smalltables this reasonable tradeoff 
WITHOUT_CLASSIFICATION	 generates grouping set position grouping set generally the last keys declared grouping set values bitsets acquired from grouping set values 
WITHOUT_CLASSIFICATION	 have field and are positioned read 
WITHOUT_CLASSIFICATION	 now create the filter with the transactions information particular each table the materialization will only have contents such that rowidwriteid highwatermark and rowidwriteid not openinvalidids hence add that condition top the source table the rewriting will then have the possibility create partial rewritings that read the materialization and the source tables and hence produce incremental 
WITHOUT_CLASSIFICATION	 since cannot merge operators into single job from here should remove reducesinkoperators added into walked exploitjfc 
WITHOUT_CLASSIFICATION	 where the log files wll written 
WITHOUT_CLASSIFICATION	 original bucket files and delta directory should have been cleaned 
WITHOUT_CLASSIFICATION	 finally add the fixed acid key index 
WITHOUT_CLASSIFICATION	 this method finds any columns the right side set statement thus rcols and puts them 
WITHOUT_CLASSIFICATION	 there select following clone the select also useful for followon optimization where the union 
WITHOUT_CLASSIFICATION	 getdatasize tries estimate stats doesnt exist using file size would like avoid file system calls too expensive 
WITHOUT_CLASSIFICATION	 cache settings will need setup llapdaemonsitexml since the daemons dont read hivesitexml 
WITHOUT_CLASSIFICATION	 floor integer argument noop but less code handle this way 
WITHOUT_CLASSIFICATION	 perform upgrade 
WITHOUT_CLASSIFICATION	 lock associated with txn can only unlock its waiting state which really means that the caller wants give waiting for the lock 
WITHOUT_CLASSIFICATION	 sum all nonnull decimal column values for avg maintain isgroupresultnull after last row last group batch compute the group avg when sum nonnull 
WITHOUT_CLASSIFICATION	 get some nonliterals need punt 
WITHOUT_CLASSIFICATION	 try again with some different data values and divisor 
WITHOUT_CLASSIFICATION	 slow remainder with biginteger 
WITHOUT_CLASSIFICATION	 will load into directory and hide previous directories needed 
WITHOUT_CLASSIFICATION	 currently only handles one input input 
WITHOUT_CLASSIFICATION	 call open mockmocktbl 
WITHOUT_CLASSIFICATION	 value conversion methods 
WITHOUT_CLASSIFICATION	 these are mostly copied from the root pomxml 
WITHOUT_CLASSIFICATION	 will returning text object 
WITHOUT_CLASSIFICATION	 reset 
WITHOUT_CLASSIFICATION	 for grouping sets add dummy grouping key 
WITHOUT_CLASSIFICATION	 verify that some dummy param can set 
WITHOUT_CLASSIFICATION	 process singlecolumn long outer join vectorized row batch 
WITHOUT_CLASSIFICATION	 cannot merge would end with cycle the dag 
WITHOUT_CLASSIFICATION	 exception from the rpc layer communication failure consider killed service down 
WITHOUT_CLASSIFICATION	 end string 
WITHOUT_CLASSIFICATION	 maxposition the percentile 
WITHOUT_CLASSIFICATION	 create tables user 
WITHOUT_CLASSIFICATION	 need make sure dont get two write ids for the same table 
WITHOUT_CLASSIFICATION	 another small write smallbuffer should reused for this write 
WITHOUT_CLASSIFICATION	 dynamic partition pruning enabled only for map join false and true 
WITHOUT_CLASSIFICATION	 the offset bigger than our current number fields grow 
WITHOUT_CLASSIFICATION	 step connect the operator trees two mapredtasks 
WITHOUT_CLASSIFICATION	 already found variable this isnt sarg 
WITHOUT_CLASSIFICATION	 this will replace the old value there one overwrite the existing file 
WITHOUT_CLASSIFICATION	 vertex waiting for inputslots complete 
WITHOUT_CLASSIFICATION	 todo pause fetching 
WITHOUT_CLASSIFICATION	 verify the scenario when maxprobesize very small value doesnt fail 
WITHOUT_CLASSIFICATION	 rename unmanaged files conform hives naming standard example warehousetablepartm will get renamed staging directory already contains the file taskidcopyn naming will used 
WITHOUT_CLASSIFICATION	 register with the amreporter when the callable setup unregister once starts running 
WITHOUT_CLASSIFICATION	 exists want this error condition repl load not intended replace 
WITHOUT_CLASSIFICATION	 for entry should work but generate nan 
WITHOUT_CLASSIFICATION	 not equals 
WITHOUT_CLASSIFICATION	 this method used traverse the dag created tasks list and add the dependent task 
WITHOUT_CLASSIFICATION	 given candidate mapjoin can this join converted the candidate mapjoin was derived from the pluggable sort merge join big 
WITHOUT_CLASSIFICATION	 set the updated fetch size from the server into the configuration map for the client 
WITHOUT_CLASSIFICATION	 first see have sessions that were planning restartkill get rid those 
WITHOUT_CLASSIFICATION	 were generating the splits the just need set 
WITHOUT_CLASSIFICATION	 when the next value small was not recorded with the old next value and 
WITHOUT_CLASSIFICATION	 synchronized besteffort display the queue order 
WITHOUT_CLASSIFICATION	 emulate serializationutils serialization used orc 
WITHOUT_CLASSIFICATION	 cannot hold all map tables memory cannot convert 
WITHOUT_CLASSIFICATION	 then optimization should merge them 
WITHOUT_CLASSIFICATION	 else recurse the children 
WITHOUT_CLASSIFICATION	 return the wrapper the root node 
WITHOUT_CLASSIFICATION	 deserialize data column values and populate the row record 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 mark agg produces count which needs reference the 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 old containers which are likely shutting down new containers which launched between yarn service statusdiagnostics skip for this iteration 
WITHOUT_CLASSIFICATION	 create the tez tmp dir and directory for hive resources 
WITHOUT_CLASSIFICATION	 tbltypes 
WITHOUT_CLASSIFICATION	 string not between 
WITHOUT_CLASSIFICATION	 given that are trying reuse this session must some poolsessions kills that could have removed must have cleared sessiontoreuse 
WITHOUT_CLASSIFICATION	 rcfile write 
WITHOUT_CLASSIFICATION	 thread pool for taking entities off the wait queue 
WITHOUT_CLASSIFICATION	 test string double 
WITHOUT_CLASSIFICATION	 expected error should throw 
WITHOUT_CLASSIFICATION	 address size check check for something better than non zero 
WITHOUT_CLASSIFICATION	 return true when the child type and the conversion target type the 
WITHOUT_CLASSIFICATION	 get valid txn list 
WITHOUT_CLASSIFICATION	 split not include itself 
WITHOUT_CLASSIFICATION	 ignore exceptions from stop 
WITHOUT_CLASSIFICATION	 onetime setup make query able run with tez 
WITHOUT_CLASSIFICATION	 set high worker count get longer queue 
WITHOUT_CLASSIFICATION	 should get all partitions for partitioned table 
WITHOUT_CLASSIFICATION	 hiveconf has changed new object should returned 
WITHOUT_CLASSIFICATION	 default need the results from droppartitions 
WITHOUT_CLASSIFICATION	 setval with the same function signature righttrim lefttrim truncate etc below 
WITHOUT_CLASSIFICATION	 this vertex has multiple reduce operators 
WITHOUT_CLASSIFICATION	 verify invalid column error 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 create client 
WITHOUT_CLASSIFICATION	 use array instead only one object case future hive does not the byte copy 
WITHOUT_CLASSIFICATION	 special case for unions these items translate vertexgroups 
WITHOUT_CLASSIFICATION	 todo hive cleanup may required exiting early 
WITHOUT_CLASSIFICATION	 note this recursive struct 
WITHOUT_CLASSIFICATION	 this should only true for copy tasks created from functions otherwise there should never 
WITHOUT_CLASSIFICATION	 when have partial partitions specification must assume partitions lie standard place they were custom locations putting them into one archive would involve mass amount copying full partition specification case allow custom locations 
WITHOUT_CLASSIFICATION	 someone not done both user and the kill have returned 
WITHOUT_CLASSIFICATION	 map from primitivetypeinfo 
WITHOUT_CLASSIFICATION	 schedule task should get the only duck the one the same pri doesnt get one when the first one finishes the duck goes the and then becomes unused 
WITHOUT_CLASSIFICATION	 mutate operations 
WITHOUT_CLASSIFICATION	 get table metadata 
WITHOUT_CLASSIFICATION	 adding them restricted list 
WITHOUT_CLASSIFICATION	 set this read because cant overwrite any existing partitions 
WITHOUT_CLASSIFICATION	 case max list members max query string length and exact members single clause 
WITHOUT_CLASSIFICATION	 make the union operator 
WITHOUT_CLASSIFICATION	 statsindexes 
WITHOUT_CLASSIFICATION	 field length difference between positions hence one extra 
WITHOUT_CLASSIFICATION	 source table scan 
WITHOUT_CLASSIFICATION	 note use instead 
WITHOUT_CLASSIFICATION	 for consistency with tables 
WITHOUT_CLASSIFICATION	 these values come from setvalueresult when finds key these values allow this 
WITHOUT_CLASSIFICATION	 called runtime initialize the custom edge 
WITHOUT_CLASSIFICATION	 want isolate any potential issue may introduce 
WITHOUT_CLASSIFICATION	 look for unlikely database name and see either metaexception texception thrown 
WITHOUT_CLASSIFICATION	 weve reached our limit throw the last one 
WITHOUT_CLASSIFICATION	 since this test runs local file system which does not have api tell files open not are testing for negative case even though the bucket files are still open 
WITHOUT_CLASSIFICATION	 the same reported 
WITHOUT_CLASSIFICATION	 months produces type date via calendar calculation 
WITHOUT_CLASSIFICATION	 create delta cant push predicate 
WITHOUT_CLASSIFICATION	 dont want cancel the delegation token think the callback going retried for example because the job not complete yet 
WITHOUT_CLASSIFICATION	 config name used find the number concurrent requests 
WITHOUT_CLASSIFICATION	 need add connection status listener what will that 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this should the case only this create partition the privilege needed the table should alterdata and not create 
WITHOUT_CLASSIFICATION	 use tcompactprotocol read serialized tcolumns 
WITHOUT_CLASSIFICATION	 trivial case nothing read 
WITHOUT_CLASSIFICATION	 different table name 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 gather the operators that will used for next iteration extended optimization 
WITHOUT_CLASSIFICATION	 series equal keys 
WITHOUT_CLASSIFICATION	 register session first for backward compatibility 
WITHOUT_CLASSIFICATION	 europefrance 
WITHOUT_CLASSIFICATION	 most recent instance the pmf 
WITHOUT_CLASSIFICATION	 figure out the partition spec from the input this only done once for the first row when stat null since all rows the same mapper should from the same partition 
WITHOUT_CLASSIFICATION	 regardless the above should have the key weve signed with 
WITHOUT_CLASSIFICATION	 fallthrough throw exception its not expected for execution reach here 
WITHOUT_CLASSIFICATION	 all partition column type should string partition column virtual column 
WITHOUT_CLASSIFICATION	 all nonprimitive ois are writable need only check this case 
WITHOUT_CLASSIFICATION	 used only for explain 
WITHOUT_CLASSIFICATION	 cant have relative path there schemeauthority 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 definition here copy the limit the buffer 
WITHOUT_CLASSIFICATION	 temporary typesafe casting 
WITHOUT_CLASSIFICATION	 truncate slice byte array maximum number characters and place the result into element vector 
WITHOUT_CLASSIFICATION	 for split sampling shrinkedlength checked against which from recordreadergetpos some inputformats which does not support getpos 
WITHOUT_CLASSIFICATION	 previously this was handled filtertablenames but cant anymore because can longer depend mapping between table name and entry the list 
WITHOUT_CLASSIFICATION	 nanos 
WITHOUT_CLASSIFICATION	 check the new configs are added 
WITHOUT_CLASSIFICATION	 try dropping table user should succeed 
WITHOUT_CLASSIFICATION	 the big table has reached new key group try let the small tables 
WITHOUT_CLASSIFICATION	 enable dynamic partitioning 
WITHOUT_CLASSIFICATION	 integer and boolean types require conversion use noop 
WITHOUT_CLASSIFICATION	 use self alias 
WITHOUT_CLASSIFICATION	 double quote seen and the index not inside single quoted string and the previous character was not escape then update the flag 
WITHOUT_CLASSIFICATION	 localizing files for submitting dag 
WITHOUT_CLASSIFICATION	 set later with setoutput methods 
WITHOUT_CLASSIFICATION	 this used the future make sure disable grouping the payload isnt already disabled 
WITHOUT_CLASSIFICATION	 get the string value and convert interval value 
WITHOUT_CLASSIFICATION	 try single stripe 
WITHOUT_CLASSIFICATION	 event methods 
WITHOUT_CLASSIFICATION	 add the transformation that computes the lineage information 
WITHOUT_CLASSIFICATION	 assumption this will run last after col pruning the prejoinorder optimizations projectrel not synthetic then ppd would have already pushed relevant pieces down and hence point running ppd again for synthetic projects dont care about non deterministic udfs 
WITHOUT_CLASSIFICATION	 subscriber can get notification about addition database hcat listening topic named hcat and message selector string hcatevent hcatadddatabase 
WITHOUT_CLASSIFICATION	 returns code true such base file can used materialize the snapshot represented this code validwriteidlist param writeid highest write given basexxxx file return true the base file can used 
WITHOUT_CLASSIFICATION	 return 
WITHOUT_CLASSIFICATION	 update the vectorptfdesc with data that used during validation and that doesnt rely lookup column names etc 
WITHOUT_CLASSIFICATION	 verify that new sessionstate has default 
WITHOUT_CLASSIFICATION	 add new vector child the vector parents children list 
WITHOUT_CLASSIFICATION	 rare case 
WITHOUT_CLASSIFICATION	 subclasses can override this step for example for temporary tables 
WITHOUT_CLASSIFICATION	 sort trivial 
WITHOUT_CLASSIFICATION	 drop table idempotent 
WITHOUT_CLASSIFICATION	 make sure compare them entity that its the same table partition etc 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 boolean that tells the hivemetastore remote server being used can used determine the calls metastore api hmshandler are being made with embedded metastore remote one 
WITHOUT_CLASSIFICATION	 verify that objectstore fetches the latest notification event 
WITHOUT_CLASSIFICATION	 list bucketing then bail out 
WITHOUT_CLASSIFICATION	 create columnstatistics obj 
WITHOUT_CLASSIFICATION	 test that there lead second adjustment 
WITHOUT_CLASSIFICATION	 store column name mapwork 
WITHOUT_CLASSIFICATION	 get partition metadata 
WITHOUT_CLASSIFICATION	 should come back null 
WITHOUT_CLASSIFICATION	 format the properties statement 
WITHOUT_CLASSIFICATION	 test one random highprecision subtract 
WITHOUT_CLASSIFICATION	 calling close unopened session probably harmless 
WITHOUT_CLASSIFICATION	 get the needed columns and name 
WITHOUT_CLASSIFICATION	 finally check the filter for nonbuiltin udfs these are present cannot 
WITHOUT_CLASSIFICATION	 the output the lateral view join will the columns from the select parent followed the column from the udtf parent 
WITHOUT_CLASSIFICATION	 compaction doesnt filter deltas but may have reader for base 
WITHOUT_CLASSIFICATION	 testing substring index starting with and zero length 
WITHOUT_CLASSIFICATION	 setop rewrite 
WITHOUT_CLASSIFICATION	 everything the batch has already been filtered out 
WITHOUT_CLASSIFICATION	 inherent most properties from table level schema and overwrite some properties the following code this mainly for saving cpu and memory reuse the column names types and 
WITHOUT_CLASSIFICATION	 source replication not set 
WITHOUT_CLASSIFICATION	 bounds check for oob exception 
WITHOUT_CLASSIFICATION	 count nonnull column rows 
WITHOUT_CLASSIFICATION	 logged info multiple other places 
WITHOUT_CLASSIFICATION	 require admin privilege 
WITHOUT_CLASSIFICATION	 pos outer join aliaspos other aliasnum filters outer join aliasxn for example left outer join akbk and full outer join akck and and that means apos there are overlapped filters associated with bpos and cpos has one filter and also has one filter making filter map for 
WITHOUT_CLASSIFICATION	 change file length and look for cache misses 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 check that there one datasource with the published segment 
WITHOUT_CLASSIFICATION	 special case rare the segment buffer boundary 
WITHOUT_CLASSIFICATION	 test translation both filters and booleanvalued expressions nonfilters 
WITHOUT_CLASSIFICATION	 this isnt one track just return whatever matches the string 
WITHOUT_CLASSIFICATION	 represents the total memory that this join operator will use mapjoin operator 
WITHOUT_CLASSIFICATION	 one last test are enabling the rewrite need check that query 
WITHOUT_CLASSIFICATION	 try invalid alter table with partition key name 
WITHOUT_CLASSIFICATION	 udtf not handled yet the parent selectop udtf should just assume all columns 
WITHOUT_CLASSIFICATION	 check delegation token job conf any 
WITHOUT_CLASSIFICATION	 test upper case 
WITHOUT_CLASSIFICATION	 rollup and cubes are syntactic sugar top grouping sets 
WITHOUT_CLASSIFICATION	 create the groupby operator 
WITHOUT_CLASSIFICATION	 cbo related 
WITHOUT_CLASSIFICATION	 need plugins handle llap and uber mode 
WITHOUT_CLASSIFICATION	 number the test rows with collection order 
WITHOUT_CLASSIFICATION	 avoid long overflow will divide the max row count denominator and use that factor multiply with other row counts 
WITHOUT_CLASSIFICATION	 for the char and varchar data types the maximum character length the columns otherwise 
WITHOUT_CLASSIFICATION	 none the columns need cast theres need for additional select operator 
WITHOUT_CLASSIFICATION	 limit compensation arrays for keyvaluehashcodes 
WITHOUT_CLASSIFICATION	 junk after exponent 
WITHOUT_CLASSIFICATION	 super hack city notice the mod plus only happens after firstfield hit right 
WITHOUT_CLASSIFICATION	 will estimate map object only its field 
WITHOUT_CLASSIFICATION	 after merge the sparse switching threshold exceeded then change dense encoding 
WITHOUT_CLASSIFICATION	 objectinspector for input data 
WITHOUT_CLASSIFICATION	 same for char 
WITHOUT_CLASSIFICATION	 determine minimum all nonnull double column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 smith lastname 
WITHOUT_CLASSIFICATION	 the reducer 
WITHOUT_CLASSIFICATION	 undone trim trailing zeroes 
WITHOUT_CLASSIFICATION	 errormessage 
WITHOUT_CLASSIFICATION	 tablenames filter short assertequals tablenamessize 
WITHOUT_CLASSIFICATION	 all small aliases are staged need full bucket context 
WITHOUT_CLASSIFICATION	 set can verify they are reset operation 
WITHOUT_CLASSIFICATION	 should need for child vector expressions which would imply castingconversion 
WITHOUT_CLASSIFICATION	 eltindex string returns the string columnexpression value the specified index expression the first argument expression indicates the index the string retrieved from remaining arguments return null when the index number less than index number greater than the number the string arguments 
WITHOUT_CLASSIFICATION	 there should expectedcallcount calls drop partitions with each batch size 
WITHOUT_CLASSIFICATION	 first child should operand 
WITHOUT_CLASSIFICATION	 insert the additional http headers 
WITHOUT_CLASSIFICATION	 jar found 
WITHOUT_CLASSIFICATION	 row 
WITHOUT_CLASSIFICATION	 logj 
WITHOUT_CLASSIFICATION	 case verify the difference 
WITHOUT_CLASSIFICATION	 process the grouping sets 
WITHOUT_CLASSIFICATION	 not need apply the optimization 
WITHOUT_CLASSIFICATION	 optional required required 
WITHOUT_CLASSIFICATION	 noop are process sending have the correct value 
WITHOUT_CLASSIFICATION	 last param complete 
WITHOUT_CLASSIFICATION	 sparse registers are delta and variable length encoded 
WITHOUT_CLASSIFICATION	 when are running current query 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 seems that loadtabledesc has operationinsert only for ctas 
WITHOUT_CLASSIFICATION	 unexpected 
WITHOUT_CLASSIFICATION	 this position constant 
WITHOUT_CLASSIFICATION	 use the default field delimiter replace the multiplechar field delimiter but cannot use parse the row since column data can contain well 
WITHOUT_CLASSIFICATION	 one call per root input 
WITHOUT_CLASSIFICATION	 check the inspectors 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 only columns can selected for both sorted and bucketed positions 
WITHOUT_CLASSIFICATION	 entry point aliasnum 
WITHOUT_CLASSIFICATION	 need the expr that generated the key the reduce sink 
WITHOUT_CLASSIFICATION	 add cache same group tsop 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 nodeid can null the task gets unregistered due failure being killed the daemon itself 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 col 
WITHOUT_CLASSIFICATION	 both old and new params are not null merge them 
WITHOUT_CLASSIFICATION	 reserve space for the int length 
WITHOUT_CLASSIFICATION	 executequery should always throw sqlexception 
WITHOUT_CLASSIFICATION	 intentionally overwrites anything the user may have put here 
WITHOUT_CLASSIFICATION	 false for continue has pair but not this turn 
WITHOUT_CLASSIFICATION	 replace original stddevpopx with sqrt sumx sumx sumx countx countx 
WITHOUT_CLASSIFICATION	 not confused with vectorizer level which represents the value configuration 
WITHOUT_CLASSIFICATION	 try the different getters 
WITHOUT_CLASSIFICATION	 convert such 
WITHOUT_CLASSIFICATION	 physical optimizer stages 
WITHOUT_CLASSIFICATION	 now that exceptions aka abortedtxnlist sorted 
WITHOUT_CLASSIFICATION	 blockedbyintid 
WITHOUT_CLASSIFICATION	 first try reuse from the same pool should just work 
WITHOUT_CLASSIFICATION	 for those stmthandle passed from instead statement 
WITHOUT_CLASSIFICATION	 bail out there nothing push 
WITHOUT_CLASSIFICATION	 test dropping tables and trash behavior 
WITHOUT_CLASSIFICATION	 insert some data new schema 
WITHOUT_CLASSIFICATION	 methods that does not need data object 
WITHOUT_CLASSIFICATION	 row 
WITHOUT_CLASSIFICATION	 col 
WITHOUT_CLASSIFICATION	 perform any key expressions results will into scratch columns 
WITHOUT_CLASSIFICATION	 initialize wfns 
WITHOUT_CLASSIFICATION	 uses noop proxy 
WITHOUT_CLASSIFICATION	 orc creates batch size make memory check align with instead 
WITHOUT_CLASSIFICATION	 initialize using projection the column range fieldssize 
WITHOUT_CLASSIFICATION	 private void out throws ioexception 
WITHOUT_CLASSIFICATION	 create file with blocks spread around the cluster 
WITHOUT_CLASSIFICATION	 possible that the row got absorbed the operator tree 
WITHOUT_CLASSIFICATION	 supports acidinputformat which not use the key pass rowid info 
WITHOUT_CLASSIFICATION	 check that right rows are selected 
WITHOUT_CLASSIFICATION	 cluster state changes will notify and wed update the queries again 
WITHOUT_CLASSIFICATION	 request two messages 
WITHOUT_CLASSIFICATION	 coming from smalltable side some bookkeeping and skip traversal 
WITHOUT_CLASSIFICATION	 serializationformat property has the default value will not included serde properties 
WITHOUT_CLASSIFICATION	 first try full match 
WITHOUT_CLASSIFICATION	 join key origin has been traced table column check the table external 
WITHOUT_CLASSIFICATION	 write the remaining part the array 
WITHOUT_CLASSIFICATION	 for outer joins contains the potential nulls for the concerned aliases 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 not lock the list for this and use volatile lastaccesstime instead 
WITHOUT_CLASSIFICATION	 created case the mapjoin failed 
WITHOUT_CLASSIFICATION	 table should not null 
WITHOUT_CLASSIFICATION	 constant from accumulos 
WITHOUT_CLASSIFICATION	 one produce these will the same using any other 
WITHOUT_CLASSIFICATION	 drop the table first case some previous test created 
WITHOUT_CLASSIFICATION	 pass 
WITHOUT_CLASSIFICATION	 the base compositeservice already stopped dont anything again 
WITHOUT_CLASSIFICATION	 run execdriver another jvm 
WITHOUT_CLASSIFICATION	 now start concurrent select from tab txn 
WITHOUT_CLASSIFICATION	 check there log file without the suffix 
WITHOUT_CLASSIFICATION	 update col stats map with col stats for columns from right side 
WITHOUT_CLASSIFICATION	 will only overwritten close errors out 
WITHOUT_CLASSIFICATION	 cred provider has entry and conf does not cred provider used 
WITHOUT_CLASSIFICATION	 sre lock are examining exclusive 
WITHOUT_CLASSIFICATION	 this entry output not present the output schema first check the table schema see part col 
WITHOUT_CLASSIFICATION	 fetch the column expression there should atleast one 
WITHOUT_CLASSIFICATION	 this should block behind the lock 
WITHOUT_CLASSIFICATION	 default all users authorizations when configuration provided 
WITHOUT_CLASSIFICATION	 publish the new partitions 
WITHOUT_CLASSIFICATION	 incrementalrows constructor should buffer the first rows 
WITHOUT_CLASSIFICATION	 todo constraintcache 
WITHOUT_CLASSIFICATION	 job hash map 
WITHOUT_CLASSIFICATION	 the object was added later for the same class see addtoprocessing 
WITHOUT_CLASSIFICATION	 now the add with java bigdecimal 
WITHOUT_CLASSIFICATION	 parts the partition 
WITHOUT_CLASSIFICATION	 lazysimple seems work better with row object array instead java object 
WITHOUT_CLASSIFICATION	 timeseries query results records 
WITHOUT_CLASSIFICATION	 create remote dirs once 
WITHOUT_CLASSIFICATION	 change body overridden methods use file settings file templates 
WITHOUT_CLASSIFICATION	 spot check null propagation 
WITHOUT_CLASSIFICATION	 have base work from 
WITHOUT_CLASSIFICATION	 add list saved historic operations 
WITHOUT_CLASSIFICATION	 basic sanity check other cases are not skipped because similar the case for long 
WITHOUT_CLASSIFICATION	 number spilled partitions only one last one partition left memory how often rows apart check memory full configuration for nway join write buffer size for 
WITHOUT_CLASSIFICATION	 reader will check for the event queue upon the end the input stream need interrupt 
WITHOUT_CLASSIFICATION	 populate vectormapjoininfo 
WITHOUT_CLASSIFICATION	 constructor with the individual addinitialcolumn method 
WITHOUT_CLASSIFICATION	 ignore index tables those will dropped with parent tables 
WITHOUT_CLASSIFICATION	 helper object that efficiently copies the big table key columns input key expressions 
WITHOUT_CLASSIFICATION	 remember the event operators weve seen 
WITHOUT_CLASSIFICATION	 first find the select closest the top 
WITHOUT_CLASSIFICATION	 join different keys different tables can longer apply multijoin conversion this longer valid star join bail out this the case 
WITHOUT_CLASSIFICATION	 for spark job with empty source data its not submitted actually would never receive jobstartjobend event jobstatelistener use javafutureaction get current job state 
WITHOUT_CLASSIFICATION	 hive 
WITHOUT_CLASSIFICATION	 read the altered tbl via cachedstore 
WITHOUT_CLASSIFICATION	 now add the keywords from the current connection 
WITHOUT_CLASSIFICATION	 owner information unchanged then properties wouldve changed 
WITHOUT_CLASSIFICATION	 currently smb broken cannot check its compatible with elevator dont use the below code that would get the correct mapwork see hive 
WITHOUT_CLASSIFICATION	 are compacting and its acid schema create reader for the bucket file that not empty 
WITHOUT_CLASSIFICATION	 not materialized view not rewrite 
WITHOUT_CLASSIFICATION	 tailing zeroes difference 
WITHOUT_CLASSIFICATION	 check columns 
WITHOUT_CLASSIFICATION	 closing the operator can sometimes yield more rows hive 
WITHOUT_CLASSIFICATION	 extract information from reference word from slot table 
WITHOUT_CLASSIFICATION	 optional int deletedelay default 
WITHOUT_CLASSIFICATION	 note that the pool per edc within edc cvbs are expected have the same schema 
WITHOUT_CLASSIFICATION	 extract type for the arguments 
WITHOUT_CLASSIFICATION	 this expected fail 
WITHOUT_CLASSIFICATION	 restrict with any filters found from where predicates 
WITHOUT_CLASSIFICATION	 after this optimization the tree should like fil skewed rows join fil skewed rows union fil skewed rows join fil skewed rows 
WITHOUT_CLASSIFICATION	 for windows paths 
WITHOUT_CLASSIFICATION	 user running the test belongs 
WITHOUT_CLASSIFICATION	 set the collection fields some code might not check presence before accessing them 
WITHOUT_CLASSIFICATION	 replace each the position alias groupby with the actual column name 
WITHOUT_CLASSIFICATION	 old table the cache and the new table can also cached 
WITHOUT_CLASSIFICATION	 multiple threads could try initialize the same time 
WITHOUT_CLASSIFICATION	 number elements list cannot determined this value will used 
WITHOUT_CLASSIFICATION	 default any additional data columns null once for the file they are present 
WITHOUT_CLASSIFICATION	 bgenjjtree constmap 
WITHOUT_CLASSIFICATION	 common analysis the statement boolean expression the following protected members can examined afterwards boolean boolean int thenselectedcount int thenselected int elseselectedcount int elseselected 
WITHOUT_CLASSIFICATION	 create 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 add the following strings column name table name tablenamecolumnname 
WITHOUT_CLASSIFICATION	 prscrscgby 
WITHOUT_CLASSIFICATION	 first remove the input operators the expression that 
WITHOUT_CLASSIFICATION	 row column information 
WITHOUT_CLASSIFICATION	 for now always convert double cant find common type 
WITHOUT_CLASSIFICATION	 create default route 
WITHOUT_CLASSIFICATION	 validate skewed information 
WITHOUT_CLASSIFICATION	 insert the value corresponding the current expression 
WITHOUT_CLASSIFICATION	 should technically update memory usage updating the old object but dont for now there mechanism properly notify the cache policyetc wrt parallel evicts 
WITHOUT_CLASSIFICATION	 the data not escaped reference the data directly 
WITHOUT_CLASSIFICATION	 some point these should inserted 
WITHOUT_CLASSIFICATION	 replace the original selectop the parents with selectunionop 
WITHOUT_CLASSIFICATION	 create new conf object bypass metastore authorization need retrieve all materialized views from all databases 
WITHOUT_CLASSIFICATION	 asc nulls first 
WITHOUT_CLASSIFICATION	 authorize this call the schema objects 
WITHOUT_CLASSIFICATION	 data structures 
WITHOUT_CLASSIFICATION	 first time registration new register comes before the previous unregister 
WITHOUT_CLASSIFICATION	 rely the caller supply reasonable total could log warning this doesnt match the allocation the last session beyond some threshold 
WITHOUT_CLASSIFICATION	 shouldnt hit digits until year 
WITHOUT_CLASSIFICATION	 the column has been read from disk 
WITHOUT_CLASSIFICATION	 reset value case any date fields are missing from the date pattern 
WITHOUT_CLASSIFICATION	 handle dual nature 
WITHOUT_CLASSIFICATION	 test nulls propagation 
WITHOUT_CLASSIFICATION	 clone the table 
WITHOUT_CLASSIFICATION	 check metrics during semantic analysis 
WITHOUT_CLASSIFICATION	 print out last part buffer 
WITHOUT_CLASSIFICATION	 find the extra table 
WITHOUT_CLASSIFICATION	 offset relative the beginning the stream where this ends 
WITHOUT_CLASSIFICATION	 required optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 based update above 
WITHOUT_CLASSIFICATION	 this the overwrite case not care about the accuracy 
WITHOUT_CLASSIFICATION	 the only allowed flag newalloc and that only are not discarding 
WITHOUT_CLASSIFICATION	 for leftsemi join generate additional selection groupby operator before reducesink 
WITHOUT_CLASSIFICATION	 attempt match oracle semantics for timestamp arithmetic where timestamp arithmetic done utc then converted back local timezone 
WITHOUT_CLASSIFICATION	 the tables location currently unset left unset allowing the metastore fill the tables location note that the previous logic for some reason would make special case the was the default database and actually attempt generate location this seems incorrect and uncessary since the metastore just able fill the default table location the case the default for nondefault dbs 
WITHOUT_CLASSIFICATION	 end listiterator 
WITHOUT_CLASSIFICATION	 since metastore connections dont require the url this allowable 
WITHOUT_CLASSIFICATION	 ugi information not available connection setup time will set later via setugi rpc 
WITHOUT_CLASSIFICATION	 sleep until all threads with clean tasks are completed seconds completing task and sec grace period 
WITHOUT_CLASSIFICATION	 makes the message more informative helps find bugs client code 
WITHOUT_CLASSIFICATION	 get objinspectors for entire record and bucketed cols 
WITHOUT_CLASSIFICATION	 anything else fail 
WITHOUT_CLASSIFICATION	 note copywork supports copying multiple files but replcopywork doesnt 
WITHOUT_CLASSIFICATION	 split since mutatetransaction txn just does deletes 
WITHOUT_CLASSIFICATION	 for the case when the output can have null values follow the convention that the data values must for long and nan for double this prevent possible later zerodivide errors complex arithmetic expressions like col col the case when some col entries are null 
WITHOUT_CLASSIFICATION	 set needed columns for this dummy tablescanoperator 
WITHOUT_CLASSIFICATION	 undone provide isrepeated selected isnull 
WITHOUT_CLASSIFICATION	 catch the exception caused missing jpamso which otherwise would crashes the thread and causes the client hanging rather than notifying the client nicely 
WITHOUT_CLASSIFICATION	 now localtask the parent task the current task 
WITHOUT_CLASSIFICATION	 confirm grouping 
WITHOUT_CLASSIFICATION	 one scheduler pass from the nodes that are added startup 
WITHOUT_CLASSIFICATION	 normalize the columns sizes 
WITHOUT_CLASSIFICATION	 validate the third parameter which should integer represent 
WITHOUT_CLASSIFICATION	 move the last bytes the prefix area 
WITHOUT_CLASSIFICATION	 waitonprecursor determines whether not nonexistence dependent object error for regular imports for now the only thing this affects whether not the exists 
WITHOUT_CLASSIFICATION	 avgdecimal 
WITHOUT_CLASSIFICATION	 this the min number reducers for the bottom layer reducesinkoperators avoid query 
WITHOUT_CLASSIFICATION	 run load primary itself 
WITHOUT_CLASSIFICATION	 verify hit error while connecting 
WITHOUT_CLASSIFICATION	 flag for bucket map join one usage set 
WITHOUT_CLASSIFICATION	 are here when the left and right are nonzero and have the same sign 
WITHOUT_CLASSIFICATION	 bit packing 
WITHOUT_CLASSIFICATION	 replace with actual dir existed only want the absolute path remove the header such hdfslocalhost 
WITHOUT_CLASSIFICATION	 forgive error 
WITHOUT_CLASSIFICATION	 mapping once established not dependent upon the file channel that was used create delete file and hold onto the map 
WITHOUT_CLASSIFICATION	 for each dir get all files under the dir getsplits each individual file and then create 
WITHOUT_CLASSIFICATION	 validwriteidlist with hwmmaxlong include the data for aborted txn 
WITHOUT_CLASSIFICATION	 insert reduceside 
WITHOUT_CLASSIFICATION	 this selstarnocompute then this select operator treated like default operator just call the super classes process method 
WITHOUT_CLASSIFICATION	 array bitvectors where each entry denotes whether the element used not whether null not the size the bitvector same the number inputsaliases under consideration currently 
WITHOUT_CLASSIFICATION	 hadoop ipc wraps grrr 
WITHOUT_CLASSIFICATION	 intended predicate removed 
WITHOUT_CLASSIFICATION	 adds tables only for create view ppd filter can appended outer query 
WITHOUT_CLASSIFICATION	 only vectorized orc input cached theres reason 
WITHOUT_CLASSIFICATION	 use the same mechanism copy filesotherfiles and libdir but only want put contents libdir sqooplib thus pass the list names here 
WITHOUT_CLASSIFICATION	 this must leadlag udaf 
WITHOUT_CLASSIFICATION	 allocated 
WITHOUT_CLASSIFICATION	 trim trailing zeroes but only below the decimal point 
WITHOUT_CLASSIFICATION	 unlikely thrown 
WITHOUT_CLASSIFICATION	 need make sure that the list element type settable 
WITHOUT_CLASSIFICATION	 create database without location clause 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the query has enforced that sortmerge join should performed for more details look filesinkdescjava 
WITHOUT_CLASSIFICATION	 that always true now but wasnt some day the below would throw getcolumndata 
WITHOUT_CLASSIFICATION	 allow debugging disabling column reuse input cols are never reused design only 
WITHOUT_CLASSIFICATION	 field because cannot multiinherit 
WITHOUT_CLASSIFICATION	 kill previously launched child jobs started this launcher prevent having 
WITHOUT_CLASSIFICATION	 test delete column stats col name passed all column stats associated with the 
WITHOUT_CLASSIFICATION	 these need based the target 
WITHOUT_CLASSIFICATION	 first kill any running jobs 
WITHOUT_CLASSIFICATION	 private 
WITHOUT_CLASSIFICATION	 the return type the genericudf boolean and all partitions agree result update the state the node true false 
WITHOUT_CLASSIFICATION	 synchronize the cache entry that one else can invalidate this entry 
WITHOUT_CLASSIFICATION	 fraction digit parsing move next lower longword 
WITHOUT_CLASSIFICATION	 one the children left right union identity projection followed union merge with 
WITHOUT_CLASSIFICATION	 now that the primary reader has advanced need see 
WITHOUT_CLASSIFICATION	 this cache range prefix the requested one the above also applies the cache may still contain the rest the requested range dont set gotalldata 
WITHOUT_CLASSIFICATION	 setup stdout and stderr 
WITHOUT_CLASSIFICATION	 note scratchdir reused implicitly because the sessionid the same 
WITHOUT_CLASSIFICATION	 the value has schema and not fieldschema 
WITHOUT_CLASSIFICATION	 validate sort columns and bucket columns 
WITHOUT_CLASSIFICATION	 llap cache can disabled via config isplancache 
WITHOUT_CLASSIFICATION	 does not contain limit operation bail out 
WITHOUT_CLASSIFICATION	 outside range 
WITHOUT_CLASSIFICATION	 correlate does not have clause for left correlate predicate must evaluated first for inner can defer 
WITHOUT_CLASSIFICATION	 partcount not equally divided into batches last batch size will less than batch size 
WITHOUT_CLASSIFICATION	 suppress leading zeroes 
WITHOUT_CLASSIFICATION	 extra element make sure have the same formula compute the length each element the array 
WITHOUT_CLASSIFICATION	 the other list doesnt exist create the first index our 
WITHOUT_CLASSIFICATION	 have statistics for the table size appropriately 
WITHOUT_CLASSIFICATION	 add original direcotries obsolete list any 
WITHOUT_CLASSIFICATION	 the context for creating the vectorizedrowbatch for this map node that the vectorizer class determined 
WITHOUT_CLASSIFICATION	 enabled get the final output writers and prepare the real output row 
WITHOUT_CLASSIFICATION	 register jvm metrics 
WITHOUT_CLASSIFICATION	 lock was outdated and was removed then maybe another transaction picked changed its state 
WITHOUT_CLASSIFICATION	 skip combine for all paths 
WITHOUT_CLASSIFICATION	 test decimal column decimal scalar division this used cover all the cases used the source code template the template used for division and modulo 
WITHOUT_CLASSIFICATION	 normal case variablelength arguments 
WITHOUT_CLASSIFICATION	 account for potential partial chunks 
WITHOUT_CLASSIFICATION	 groupingid groupingidoi 
WITHOUT_CLASSIFICATION	 register the pending events sent for this spec 
WITHOUT_CLASSIFICATION	 get the for the next entry the queue 
WITHOUT_CLASSIFICATION	 this method takes object accepts whatever types that are passed 
WITHOUT_CLASSIFICATION	 just copy the payload link has already been populated 
WITHOUT_CLASSIFICATION	 matching oracle behavior 
WITHOUT_CLASSIFICATION	 for orc case send the boundaries the stripes dont have send the footer 
WITHOUT_CLASSIFICATION	 convert skewdata contain exprnodedesc the keys 
WITHOUT_CLASSIFICATION	 the cost the result 
WITHOUT_CLASSIFICATION	 when people forget quote string opop null for example select from sometable where and 
WITHOUT_CLASSIFICATION	 fetch the table marked the message and compare 
WITHOUT_CLASSIFICATION	 for now this only used determine the bucketingsorting outputs the future this can removed optimize the query plan based the bucketingsorting properties 
WITHOUT_CLASSIFICATION	 realign the positionmap for the columns appearing after hcatfieldschema 
WITHOUT_CLASSIFICATION	 very simple counter keep track number rows processed operator dumps every million times and quickly before that 
WITHOUT_CLASSIFICATION	 for non partitioned table this will represent the last tablename replicated else its the name the 
WITHOUT_CLASSIFICATION	 logging inside 
WITHOUT_CLASSIFICATION	 cache key 
WITHOUT_CLASSIFICATION	 first try known drivers 
WITHOUT_CLASSIFICATION	 cache size should now 
WITHOUT_CLASSIFICATION	 test feb leap year 
WITHOUT_CLASSIFICATION	 submits the request and returns 
WITHOUT_CLASSIFICATION	 crttbldesc 
WITHOUT_CLASSIFICATION	 mapping from constraint name list default constraints 
WITHOUT_CLASSIFICATION	 some keys need left null corresponding that grouping set 
WITHOUT_CLASSIFICATION	 write cost 
WITHOUT_CLASSIFICATION	 die 
WITHOUT_CLASSIFICATION	 remove this branch 
WITHOUT_CLASSIFICATION	 synonym some places the code 
WITHOUT_CLASSIFICATION	 padding needed 
WITHOUT_CLASSIFICATION	 the operation atomic 
WITHOUT_CLASSIFICATION	 need check the total size local tables under the limit here are using strong condition which the total size local tables used all input paths actually can relax this condition check the total size local tables for every input path example unionall mapjoin mapjoin big big this case have two mapjoins mapjoin and mapjoin big and big are two big tables and and are four small tables hash tables and will only used map tasks processing big hash tables and will only used map tasks processing big bigbig should only check the size under the limit and the size under the limit but right now are checking the size under the limit bigbig will only scan path once mapjoin and mapjoin will executed the same map task this case need make sure the size 
WITHOUT_CLASSIFICATION	 initialize the keys and values 
WITHOUT_CLASSIFICATION	 enabled accept quoting all character backslash qooting mechanism 
WITHOUT_CLASSIFICATION	 user has fully specified partition validate that partition exists 
WITHOUT_CLASSIFICATION	 the user trying insert into external tables 
WITHOUT_CLASSIFICATION	 however the fastbigintegerbytes can take trailing zeroes make larger 
WITHOUT_CLASSIFICATION	 create table 
WITHOUT_CLASSIFICATION	 stub out the zki mock 
WITHOUT_CLASSIFICATION	 check based the hive integer type need test with isbyte isshort isint islong not use corrupted truncated values for the hive integer type 
WITHOUT_CLASSIFICATION	 reopen the hms connection 
WITHOUT_CLASSIFICATION	 whether this for the columnlevel schema opposed nested column fields 
WITHOUT_CLASSIFICATION	 remember the event operators weve abandoned 
WITHOUT_CLASSIFICATION	 base tables set lets replicate them over 
WITHOUT_CLASSIFICATION	 convert from bucket map join sort merge bucket map join enabled 
WITHOUT_CLASSIFICATION	 doesnt require additional jobs 
WITHOUT_CLASSIFICATION	 input produces correlated variables move them the front right after any existing group fields 
WITHOUT_CLASSIFICATION	 initialize args 
WITHOUT_CLASSIFICATION	 must use raw local because the checksummer doesnt honor flushes 
WITHOUT_CLASSIFICATION	 and shouldnt even and are even then none the values will set bit thus introducing errors the estimate both and can even the times and result the bit vectors could inaccurate avoid this always pick odd values for and 
WITHOUT_CLASSIFICATION	 validate the materialized view statement 
WITHOUT_CLASSIFICATION	 update jobconf using mrinput info like filename comes via this 
WITHOUT_CLASSIFICATION	 best effort attempt write all output from the script before marking the operator 
WITHOUT_CLASSIFICATION	 assumes row schema stringintstring 
WITHOUT_CLASSIFICATION	 get max split size for 
WITHOUT_CLASSIFICATION	 map work should start with our 
WITHOUT_CLASSIFICATION	 because scaling down this could happen 
WITHOUT_CLASSIFICATION	 first multiple elements 
WITHOUT_CLASSIFICATION	 check the hints see the user has specified mapside join this will removed later once the costbased 
WITHOUT_CLASSIFICATION	 show tables dbname invalid show tables syntax hive does not return any tables this case 
WITHOUT_CLASSIFICATION	 assume stream list sorted column and that nondata streams not interleave data streams for the same column 
WITHOUT_CLASSIFICATION	 hive this creates deltabucket 
WITHOUT_CLASSIFICATION	 currently only used during reoptimization related parts 
WITHOUT_CLASSIFICATION	 add mapreduce job tag placeholder 
WITHOUT_CLASSIFICATION	 there credential provider configured for hadoop jobconf should not contain credstore password and provider path even env set 
WITHOUT_CLASSIFICATION	 bit before restart the loop 
WITHOUT_CLASSIFICATION	 bytesfieldbyteend separator 
WITHOUT_CLASSIFICATION	 lazysimpleserde doesnt support projection 
WITHOUT_CLASSIFICATION	 for given work descriptor extracts information about the reducesinkops the work for tez you can restrict reducesinks for particular output vertex 
WITHOUT_CLASSIFICATION	 get the input and prepare the output 
WITHOUT_CLASSIFICATION	 are skipping the skewedstringlist table here seems totally useless 
WITHOUT_CLASSIFICATION	 experiment 
WITHOUT_CLASSIFICATION	 few checks determine eligibility optimization look select list see its min max count etc connect metastore and get the stats compose rows and add fetchwork 
WITHOUT_CLASSIFICATION	 dont try log anything when appender stopped 
WITHOUT_CLASSIFICATION	 and then see what happens based the provided schema 
WITHOUT_CLASSIFICATION	 test between 
WITHOUT_CLASSIFICATION	 validate the third parameter which should also integer 
WITHOUT_CLASSIFICATION	 the hive config values 
WITHOUT_CLASSIFICATION	 use smallint outputtypeinfo 
WITHOUT_CLASSIFICATION	 inputformat 
WITHOUT_CLASSIFICATION	 the vertex that should inlined operator list vertex that 
WITHOUT_CLASSIFICATION	 start hiveserver with given config fail server doesnt start 
WITHOUT_CLASSIFICATION	 should not update the following values serdeinfo contains these this keep backward compatible with getschema where these keys are updated after serdeinfo properties got copied 
WITHOUT_CLASSIFICATION	 retrieve skewed columns 
WITHOUT_CLASSIFICATION	 tests concurrent modification and that results are the same per input across threads but different between inputs 
WITHOUT_CLASSIFICATION	 rule requires that aggregate key the same the join key the way neither superset nor subset would work 
WITHOUT_CLASSIFICATION	 either were interrupted one handleevent which case there reader error event waiting for the queue some other unrelated cause which interrupted which case there may not reader event coming either way should not try block trying read the reader events queue 
WITHOUT_CLASSIFICATION	 split pairs delimiter 
WITHOUT_CLASSIFICATION	 arbitrary tokens the renewer should the principal the jobtracker 
WITHOUT_CLASSIFICATION	 add test parameters from official storage formats registered with hive via 
WITHOUT_CLASSIFICATION	 static partition and list bucketing 
WITHOUT_CLASSIFICATION	 ignore the char after escapechar 
WITHOUT_CLASSIFICATION	 data and can just ignore them 
WITHOUT_CLASSIFICATION	 have already updated the metrics for the failure change the state 
WITHOUT_CLASSIFICATION	 this needs return live connection used operation that follows thus only closes connection failureretry 
WITHOUT_CLASSIFICATION	 otherwise compare with power and half 
WITHOUT_CLASSIFICATION	 return buildtestdata dfs 
WITHOUT_CLASSIFICATION	 acidop flag has checked use java hash which works like identity function for integers necessary read recordidentifier incase acid updatesdeletes 
WITHOUT_CLASSIFICATION	 try fold the expression remove cast constant 
WITHOUT_CLASSIFICATION	 the other list will exhausted when commits create new one pending that commit 
WITHOUT_CLASSIFICATION	 astring 
WITHOUT_CLASSIFICATION	 this multiply produces more than digits overflow 
WITHOUT_CLASSIFICATION	 eventually want this richer description having table role etc scope for now have trivial impl having only and table scopes determined whether not the tablename null 
WITHOUT_CLASSIFICATION	 wrap thrift connection with sasl for secure connection 
WITHOUT_CLASSIFICATION	 dont pass the parsed name will parse itself 
WITHOUT_CLASSIFICATION	 this not transactional 
WITHOUT_CLASSIFICATION	 flip the sign bit and unused bits the highorder byte the sevenbyte long back 
WITHOUT_CLASSIFICATION	 this truncate column command 
WITHOUT_CLASSIFICATION	 these are the vectorized batch expressions for filtering key expressions and value 
WITHOUT_CLASSIFICATION	 number tokens drop between sleep intervals 
WITHOUT_CLASSIFICATION	 verify table has been the target replication and check hiveconf were allowed override not fail 
WITHOUT_CLASSIFICATION	 are going add splits for these directories with recursive false ignore any subdirectories deltas original directories and only read the original files the fact that theres loop calling addsplitsforgroup already implies its the real input format multiple times however some split concurrencyetc configs that are applied separately each call will effectively ignored for such splits 
WITHOUT_CLASSIFICATION	 removed from heap without evicting 
WITHOUT_CLASSIFICATION	 ignored 
WITHOUT_CLASSIFICATION	 limit nothing propagate just bail out 
WITHOUT_CLASSIFICATION	 for now dont support joins using decimal 
WITHOUT_CLASSIFICATION	 not conversion function 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 set proxy user privilege and initialize the global state proxyusers 
WITHOUT_CLASSIFICATION	 partial time time part will skipped 
WITHOUT_CLASSIFICATION	 need extrapolate this partition based the other partitions 
WITHOUT_CLASSIFICATION	 dummy handle for thriftcliservice 
WITHOUT_CLASSIFICATION	 there are some required privileges missing create error message sort the privileges that error message deterministic for tests 
WITHOUT_CLASSIFICATION	 walk the list and acquire the locks any lock cant acquired release all locks sleep 
WITHOUT_CLASSIFICATION	 remove small table ailias from aliastoworkavoid concurrent modification 
WITHOUT_CLASSIFICATION	 initialize output vector buffer receive data 
WITHOUT_CLASSIFICATION	 not sure need this exec context but all the operators the work will pass this context throught 
WITHOUT_CLASSIFICATION	 replace stderr and run command 
WITHOUT_CLASSIFICATION	 case dynamic queries possible have incomplete dummy partitions 
WITHOUT_CLASSIFICATION	 the cases that have multistage insert skew join can happen that want multiple commits into the same directory from different tasks not just task instances nonmm case ensures unique names could the same here but this will still cause the old file deleted because has not been committed this fsop are going fail safe potentially could implement some partial commit between stages this 
WITHOUT_CLASSIFICATION	 all row indexes are null then indexes are disabled 
WITHOUT_CLASSIFICATION	 this should now throw some useful exception 
WITHOUT_CLASSIFICATION	 execute query ignore exception any 
WITHOUT_CLASSIFICATION	 print the column names 
WITHOUT_CLASSIFICATION	 not expected encountered for hive fail 
WITHOUT_CLASSIFICATION	 clears the dest dir when src subdir dest 
WITHOUT_CLASSIFICATION	 word size choose bits stay below the bit sign bit need multiplier digit commad 
WITHOUT_CLASSIFICATION	 int 
WITHOUT_CLASSIFICATION	 partitioning spec 
WITHOUT_CLASSIFICATION	 virtual relation generated the reduce sync 
WITHOUT_CLASSIFICATION	 set value minvalue that minvalue overflows and gets set minvalue again 
WITHOUT_CLASSIFICATION	 test getalltables 
WITHOUT_CLASSIFICATION	 hive servers session input stream not used open persession file autoflush mode for writing temp results and tmp error output 
WITHOUT_CLASSIFICATION	 write the size the map which vint 
WITHOUT_CLASSIFICATION	 stagelist 
WITHOUT_CLASSIFICATION	 the first bounds check requires least one more byte beyond for int hence 
WITHOUT_CLASSIFICATION	 there should original bucket files and base directory plus two new delta directories and one deletedelta directory that would created due 
WITHOUT_CLASSIFICATION	 normalize prop name 
WITHOUT_CLASSIFICATION	 the total size local tables localworki unknown 
WITHOUT_CLASSIFICATION	 need get new one see the comment wrt threadlocals 
WITHOUT_CLASSIFICATION	 first incremental 
WITHOUT_CLASSIFICATION	 check the stlevel children and simple semantic checks ctlt and ctas should not coexists ctlt ctas should not coexists with column list target table schema ctas does not support partitioning for now 
WITHOUT_CLASSIFICATION	 datestats 
WITHOUT_CLASSIFICATION	 execute the test query 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 combine two lists 
WITHOUT_CLASSIFICATION	 for each alias add object inspector for filter tag the last element 
WITHOUT_CLASSIFICATION	 may contain duplicates remove duplicates 
WITHOUT_CLASSIFICATION	 use separate metastore client for heartbeat calls ensure heartbeat rpc calls are isolated from the other transaction related rpc calls 
WITHOUT_CLASSIFICATION	 this could brittle 
WITHOUT_CLASSIFICATION	 assume ranges ranges are nonoverlapping thus will save next advance 
WITHOUT_CLASSIFICATION	 the virtual columns available under vectorization they may not actually used this query unused columns will null just like unused data and partition columns are 
WITHOUT_CLASSIFICATION	 traverse through all the source files and see any file not copied partially copied yes then add the retry list source file missing then retry with path path 
WITHOUT_CLASSIFICATION	 initialize fastsetfrom 
WITHOUT_CLASSIFICATION	 production typedef definitiontype thisname 
WITHOUT_CLASSIFICATION	 check that all calls were recorded 
WITHOUT_CLASSIFICATION	 operations involvingreturning daytime intervals 
WITHOUT_CLASSIFICATION	 project 
WITHOUT_CLASSIFICATION	 insert overwrite unpartitioned table 
WITHOUT_CLASSIFICATION	 mstringstring 
WITHOUT_CLASSIFICATION	 tests for int partitionspec method 
WITHOUT_CLASSIFICATION	 for stripelevel streams dont need the extra refcount the block 
WITHOUT_CLASSIFICATION	 object inspector for serializing input tuples 
WITHOUT_CLASSIFICATION	 handle reducesinkoperator here can safely ignore table alias and the current comparator implementation does not can ignore table alias since when compare reducesinkoperator all its ancestors need match down table scan thus make sure that both plans are the same 
WITHOUT_CLASSIFICATION	 distcp currently does not copy single file distributed manner hence dont care about the size file there only file dont want launch distcp 
WITHOUT_CLASSIFICATION	 additional static classes defined after this point 
WITHOUT_CLASSIFICATION	 initialize second mocked filesystem implement only necessary stuff 
WITHOUT_CLASSIFICATION	 error was expected 
WITHOUT_CLASSIFICATION	 arithmetic specializations are done convoluted manner mark them builtin 
WITHOUT_CLASSIFICATION	 clear any existing databases 
WITHOUT_CLASSIFICATION	 set that can leverage columnpruner 
WITHOUT_CLASSIFICATION	 full constant propagation only perform expression shortcutting remove unnecessary andor operators 
WITHOUT_CLASSIFICATION	 write value element 
WITHOUT_CLASSIFICATION	 used keep positionlength for complex type fields note the top level uses startpositions instead 
WITHOUT_CLASSIFICATION	 jobid job new jobid 
WITHOUT_CLASSIFICATION	 linear search since this wont take much time from the total execution anyway lower has the range total 
WITHOUT_CLASSIFICATION	 make sure the path normalized expect validation pass since just created 
WITHOUT_CLASSIFICATION	 propagate input format necessary 
WITHOUT_CLASSIFICATION	 bean methods 
WITHOUT_CLASSIFICATION	 this for backward compatibility the user did not specify the output column list assume that there are columns key and value however the script outputs col col col seperated tab the requirement key col and value col tab col 
WITHOUT_CLASSIFICATION	 dont create new separate filter most cases there will only one 
WITHOUT_CLASSIFICATION	 same primitive category but different qualifiers 
WITHOUT_CLASSIFICATION	 toss all other exceptions related reflection failure 
WITHOUT_CLASSIFICATION	 the original may have settable info that needs added the new copy 
WITHOUT_CLASSIFICATION	 note this code very similar the code 
WITHOUT_CLASSIFICATION	 before cleaner there should items 
WITHOUT_CLASSIFICATION	 has failed because the query was killed from under 
WITHOUT_CLASSIFICATION	 planner rule that creates code semijoinrule from link top link todo remove this rule and use calcites semijoinrule not possible currently since calcite doesnt use relbuilder for this rule and want generate hivesemijoin rel here 
WITHOUT_CLASSIFICATION	 table giving binary powers entry 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader 
WITHOUT_CLASSIFICATION	 process the third child nodeif exists get partition specs 
WITHOUT_CLASSIFICATION	 optional string srcname 
WITHOUT_CLASSIFICATION	 copy the test files into hadoop required 
WITHOUT_CLASSIFICATION	 test submission concurrent job requests 
WITHOUT_CLASSIFICATION	 supports keeping timestampwritable object without having import that definition 
WITHOUT_CLASSIFICATION	 process the normal splits 
WITHOUT_CLASSIFICATION	 check that the columns referenced these comparisons form 
WITHOUT_CLASSIFICATION	 position where wed write position where wed read unsafely write time 
WITHOUT_CLASSIFICATION	 only need apply negation all words when there are words etc 
WITHOUT_CLASSIFICATION	 thrown when the table altered does not exist 
WITHOUT_CLASSIFICATION	 set mock warehouse 
WITHOUT_CLASSIFICATION	 for partial and partial 
WITHOUT_CLASSIFICATION	 value correct 
WITHOUT_CLASSIFICATION	 import external table partition partcolumnvalue from sourcepath location importtargetpath 
WITHOUT_CLASSIFICATION	 hive server mode are not able retry the fetchtask case when calling fetch queries since execute has returned 
WITHOUT_CLASSIFICATION	 inner join 
WITHOUT_CLASSIFICATION	 throw new hiveexceptionremove not sort order and unique 
WITHOUT_CLASSIFICATION	 does not need unique just nonzero distinct value test against 
WITHOUT_CLASSIFICATION	 whether are using acid compliant transaction manager has already been caught are updating deleting and getting nonacid here means the table itself doesnt support 
WITHOUT_CLASSIFICATION	 just making sure datevalueof works 
WITHOUT_CLASSIFICATION	 these are used 
WITHOUT_CLASSIFICATION	 how many columns 
WITHOUT_CLASSIFICATION	 adding constant memory for the rabit hole deep implement memoryestimate interface also constant overhead 
WITHOUT_CLASSIFICATION	 set the fetch operator for the new input file 
WITHOUT_CLASSIFICATION	 insert overwrite 
WITHOUT_CLASSIFICATION	 project with the output our operator 
WITHOUT_CLASSIFICATION	 reallocate larger multiple defaultsize 
WITHOUT_CLASSIFICATION	 there should calls create partitions with batch sizes 
WITHOUT_CLASSIFICATION	 init output object inspectors the return type for partial aggregation still list strings the return type for final and complete full aggregation result which 
WITHOUT_CLASSIFICATION	 create temp table with current connection 
WITHOUT_CLASSIFICATION	 drain unused sessions the close sync delegate the caller 
WITHOUT_CLASSIFICATION	 variables for llap hash table loading memory monitor 
WITHOUT_CLASSIFICATION	 iterate and update masks array 
WITHOUT_CLASSIFICATION	 are the system registry and this feature enabled try get from metastore 
WITHOUT_CLASSIFICATION	 try push the full filter predicate iff the filter top tablescan the filter top ptf between ptf and filter there might select operators otherwise push only the synthetic join predicates note pushing filter top ptf necessary the for rank functions gets enabled 
WITHOUT_CLASSIFICATION	 corresponding branch since only that branch will factor the reduction 
WITHOUT_CLASSIFICATION	 after all the perf changes that might was well hardcode them separately 
WITHOUT_CLASSIFICATION	 this fast path for query optimizations can find this info quickly using directsql point failing back slow path here 
WITHOUT_CLASSIFICATION	 with non null value before trying alter the partition column type 
WITHOUT_CLASSIFICATION	 root the start the operator pipeline were currently 
WITHOUT_CLASSIFICATION	 extract date special handling since function hive does include timeunit observe that timeunit information implicit the function name thus translation will proceed correctly just ignore the timeunit 
WITHOUT_CLASSIFICATION	 this known incomplete caused orc end boundaries being estimates 
WITHOUT_CLASSIFICATION	 see comment next the field 
WITHOUT_CLASSIFICATION	 the input file has changed load the correct hash bucket 
WITHOUT_CLASSIFICATION	 user specified the memory for local mode hadoop run 
WITHOUT_CLASSIFICATION	 restore the local job tracker back original 
WITHOUT_CLASSIFICATION	 dont log the stack this normal 
WITHOUT_CLASSIFICATION	 hadoopclientopts appended hadoopopts hadoopsh should remove the old hadoopclientopts which might have the main debug options from current hadoopopts new hadoopclientopts created with child jvm debug options and will appended hadoopopts agina when hadoopsh executed for the child process 
WITHOUT_CLASSIFICATION	 now new job requests should succeed status operation has cancel threads 
WITHOUT_CLASSIFICATION	 take all the driver run hooks and postexecute them 
WITHOUT_CLASSIFICATION	 else cast newinput end 
WITHOUT_CLASSIFICATION	 now hook the children 
WITHOUT_CLASSIFICATION	 not repartition take number splits from children 
WITHOUT_CLASSIFICATION	 construct the inner struct 
WITHOUT_CLASSIFICATION	 cvalue map rowvalues assertequals cvaluesize assertequalsv cvaluegetk 
WITHOUT_CLASSIFICATION	 avoid copy when newtmpjars null empty 
WITHOUT_CLASSIFICATION	 either the user the kill not done yet 
WITHOUT_CLASSIFICATION	 find first virtual column and clip them off 
WITHOUT_CLASSIFICATION	 this correlated need make and type and type should retrieved from outerrr 
WITHOUT_CLASSIFICATION	 final return type that goes back hive list structs with ngrams and their estimated frequencies 
WITHOUT_CLASSIFICATION	 update path iocontext 
WITHOUT_CLASSIFICATION	 splits 
WITHOUT_CLASSIFICATION	 collect newer entry superset existing entry 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 must fill high word from both middle and lower longs 
WITHOUT_CLASSIFICATION	 next translate the tezwork tez dag 
WITHOUT_CLASSIFICATION	 assume only parent for operator 
WITHOUT_CLASSIFICATION	 add smallint values 
WITHOUT_CLASSIFICATION	 theres extensive need for this have its own type mirrors the intent copy enough this might change later though 
WITHOUT_CLASSIFICATION	 the getsession call should also fail 
WITHOUT_CLASSIFICATION	 verify standard case 
WITHOUT_CLASSIFICATION	 the length compactor only keep the rest 
WITHOUT_CLASSIFICATION	 required optional optional required 
WITHOUT_CLASSIFICATION	 one time initialization 
WITHOUT_CLASSIFICATION	 top level 
WITHOUT_CLASSIFICATION	 assert that the source and target partitions are equivalent 
WITHOUT_CLASSIFICATION	 for backward compatibility let the above parameter used 
WITHOUT_CLASSIFICATION	 verify that getoperationstatus returned only after the long polling timeout 
WITHOUT_CLASSIFICATION	 trying connect with nonexistent user should still fail with login failure 
WITHOUT_CLASSIFICATION	 this also gets around the enum issue since just take the value 
WITHOUT_CLASSIFICATION	 table will queried directly llap acquire locks necessary they will released during session cleanup the read will have readcommitted level semantics 
WITHOUT_CLASSIFICATION	 return itself should noop the pool went from with session the pool 
WITHOUT_CLASSIFICATION	 prepare the field objectinspectors 
WITHOUT_CLASSIFICATION	 only process equivalences found the join conditions processing equivalences from the left right side infer predicates that are already present the tree below the join 
WITHOUT_CLASSIFICATION	 set unique constraint name null before sending listener 
WITHOUT_CLASSIFICATION	 process grouping set for the reduce sink operator 
WITHOUT_CLASSIFICATION	 operations other than table rename 
WITHOUT_CLASSIFICATION	 ignore nulls 
WITHOUT_CLASSIFICATION	 would expect entries txntowriteid each insert would have allocated writeid including aborted one 
WITHOUT_CLASSIFICATION	 system environment variables 
WITHOUT_CLASSIFICATION	 return the value 
WITHOUT_CLASSIFICATION	 should look take the parent fsops task the current task 
WITHOUT_CLASSIFICATION	 using commontreeadaptor because the adaptor parsedriver doesnt carry the token indexes when duplicating tree 
WITHOUT_CLASSIFICATION	 round towards positive infinity 
WITHOUT_CLASSIFICATION	 check have visited this operator 
WITHOUT_CLASSIFICATION	 test submission concurrent job requests with the controlled number concurrent requests and job request execution time outs verify that get appropriate exceptions and exception message 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 variables for metrics 
WITHOUT_CLASSIFICATION	 all resources including hdfs are session based 
WITHOUT_CLASSIFICATION	 use instead 
WITHOUT_CLASSIFICATION	 when set 
WITHOUT_CLASSIFICATION	 compaction can only done the whole table the table nonpartitioned 
WITHOUT_CLASSIFICATION	 commit 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 newtbl 
WITHOUT_CLASSIFICATION	 since our probing method totally bogus give after some time 
WITHOUT_CLASSIFICATION	 combo both set none 
WITHOUT_CLASSIFICATION	 the only time this condition should false the case dynamic partitioning where the data bucketed dynamic partitioning column and the filesinkoperator being processed this case the dynamic partition column will not appear colinfos and due the limitations dynamic partitioning they will appear the end the input schema since the order the columns hasnt changed and new columns have been addedremoved safe assume that these will have indexes greater than equal colinfossize 
WITHOUT_CLASSIFICATION	 the join columns should contain all the sort columns the sort columns all the tables should the same order 
WITHOUT_CLASSIFICATION	 add spark job the hive history 
WITHOUT_CLASSIFICATION	 set the usernamepasswd for the accumulo connection 
WITHOUT_CLASSIFICATION	 map values may serialized binary format when they are primitive and binary 
WITHOUT_CLASSIFICATION	 oracle cannot have over expressions inlist 
WITHOUT_CLASSIFICATION	 the child selectoperator has the columnexprmap need update the columnexprmap the parent selectoperator 
WITHOUT_CLASSIFICATION	 partition names are url encoded decode the names unless hive configured use the encoded names 
WITHOUT_CLASSIFICATION	 all are filtered out 
WITHOUT_CLASSIFICATION	 and and and and and 
WITHOUT_CLASSIFICATION	 these should have viable defaults 
WITHOUT_CLASSIFICATION	 this point show compactions should have failed initiated explicitly user 
WITHOUT_CLASSIFICATION	 convert udaf params exprnodedesc 
WITHOUT_CLASSIFICATION	 make create table statement 
WITHOUT_CLASSIFICATION	 compiles the given pattern with proper algorithm 
WITHOUT_CLASSIFICATION	 execute the given sql statement 
WITHOUT_CLASSIFICATION	 add more failed compactions that the total exactly 
WITHOUT_CLASSIFICATION	 now remove all baseworks all the childsparkworks that created 
WITHOUT_CLASSIFICATION	 serialize each field 
WITHOUT_CLASSIFICATION	 cookie passes the validation return null the caller 
WITHOUT_CLASSIFICATION	 first allocation write hwm should add the table the nextwriteid meta table 
WITHOUT_CLASSIFICATION	 all correlation variables are now satisfied skip creating value 
WITHOUT_CLASSIFICATION	 set second argument 
WITHOUT_CLASSIFICATION	 tolerance for long range bias and for short range bias 
WITHOUT_CLASSIFICATION	 stripped down version this adapted for hive but should eventually deleted from hive and make use above 
WITHOUT_CLASSIFICATION	 get the input output file formats 
WITHOUT_CLASSIFICATION	 work boundary stop exploring 
WITHOUT_CLASSIFICATION	 force reread the configuration file this done because 
WITHOUT_CLASSIFICATION	 the reason use comparecommand rather than simply getting the serialized output and comparing for partitionbased commands that the partition specification order can different different serializations but still effectively the same ababc should the same babca 
WITHOUT_CLASSIFICATION	 view entity 
WITHOUT_CLASSIFICATION	 longdouble arithmetic 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 close the stripe reader are done reading 
WITHOUT_CLASSIFICATION	 commit 
WITHOUT_CLASSIFICATION	 make clone existing hive conf make clone existing hive conf 
WITHOUT_CLASSIFICATION	 choose random sign 
WITHOUT_CLASSIFICATION	 todo hive potential blocking call mrinput handles this correctly even interrupt swallowed 
WITHOUT_CLASSIFICATION	 not support grouping set right now 
WITHOUT_CLASSIFICATION	 checkconstraintcols 
WITHOUT_CLASSIFICATION	 the storage arrays for this column vector corresponds the storage timestamp 
WITHOUT_CLASSIFICATION	 different parts the code rely this being set 
WITHOUT_CLASSIFICATION	 failed init remove from cache 
WITHOUT_CLASSIFICATION	 mysql parser 
WITHOUT_CLASSIFICATION	 return provider 
WITHOUT_CLASSIFICATION	 string type affinity 
WITHOUT_CLASSIFICATION	 the init method hmshandler throws exception all the times should retried until reached before giving 
WITHOUT_CLASSIFICATION	 dynamic partition 
WITHOUT_CLASSIFICATION	 remember the output name the reduce sink 
WITHOUT_CLASSIFICATION	 state has not changed already registered for notifications 
WITHOUT_CLASSIFICATION	 the types the expressions for the lateral view generated rows 
WITHOUT_CLASSIFICATION	 the methods 
WITHOUT_CLASSIFICATION	 for hybrid grace hash join need see there any spilled data processed next 
WITHOUT_CLASSIFICATION	 should not happen this should not get called before thisstart called 
WITHOUT_CLASSIFICATION	 rewriting was produced will check whether was part incremental rebuild try replace insert overwrite insert 
WITHOUT_CLASSIFICATION	 acquire lock 
WITHOUT_CLASSIFICATION	 add hbase related configuration spark because security mode spark needs generate hbase delegation token for spark this temp solution deal with spark problem 
WITHOUT_CLASSIFICATION	 nonbean 
WITHOUT_CLASSIFICATION	 the fourth could combined again 
WITHOUT_CLASSIFICATION	 field name field type 
WITHOUT_CLASSIFICATION	 try merge join tree from inner most source was merged from outer most inner which could invalid join tree abcd where not mergeable with can merged with into single join and only and has same join type 
WITHOUT_CLASSIFICATION	 note grouping sets are not allowed with map side aggregation set false dont have worry about 
WITHOUT_CLASSIFICATION	 descriptors for subq and subq are linked 
WITHOUT_CLASSIFICATION	 close one connection verify still two left 
WITHOUT_CLASSIFICATION	 writable 
WITHOUT_CLASSIFICATION	 incremental repl with alters dbtablepartition 
WITHOUT_CLASSIFICATION	 something else wrong 
WITHOUT_CLASSIFICATION	 run some queries 
WITHOUT_CLASSIFICATION	 perform any partition expressions results will into scratch columns 
WITHOUT_CLASSIFICATION	 hhelp 
WITHOUT_CLASSIFICATION	 from precision minps max scale maxs 
WITHOUT_CLASSIFICATION	 thread local configuration needed many threads could make changes 
WITHOUT_CLASSIFICATION	 bailout select involves transform 
WITHOUT_CLASSIFICATION	 verify that new job requests should succeed with issues 
WITHOUT_CLASSIFICATION	 test that existing sharedread partition with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 perform major compaction 
WITHOUT_CLASSIFICATION	 adding headerlogix getsecondintlogix before logix 
WITHOUT_CLASSIFICATION	 add the function name writeentity 
WITHOUT_CLASSIFICATION	 check the given sparktask has child sparktask that contains the target mapwork does not then remove the target from dpp 
WITHOUT_CLASSIFICATION	 this valid executable command then add the buffer 
WITHOUT_CLASSIFICATION	 this helper object deserializes known deserialization input file format combination into columns row vectorized row batch 
WITHOUT_CLASSIFICATION	 construct new decimalformat only new dvalue 
WITHOUT_CLASSIFICATION	 rows with rank ranklimit are output only the first row with rank ranklimit output 
WITHOUT_CLASSIFICATION	 the cinfo for astnode this function returns the astnode that for 
WITHOUT_CLASSIFICATION	 dpp indeed set parallel edges true 
WITHOUT_CLASSIFICATION	 this noop there column assign and val expected null 
WITHOUT_CLASSIFICATION	 for the moment pretend all matched are selected can evaluate the value expressions since may use the overflow batch when generating results will assign the selected and real batch size later 
WITHOUT_CLASSIFICATION	 update delete merge there need cardinality check 
WITHOUT_CLASSIFICATION	 create dummy key for searching the owidbucket the compressed owid ranges 
WITHOUT_CLASSIFICATION	 immediate retry 
WITHOUT_CLASSIFICATION	 create matcher for custom path 
WITHOUT_CLASSIFICATION	 add aggregate see the reference example above the top aggregate 
WITHOUT_CLASSIFICATION	 reread length 
WITHOUT_CLASSIFICATION	 create new batch with one char column for input and one long column for output 
WITHOUT_CLASSIFICATION	 causing each thread get different client even the conf same 
WITHOUT_CLASSIFICATION	 additional rows corresponding grouping sets need created here 
WITHOUT_CLASSIFICATION	 todo test changes mark cleaned clean txns and txncomponents 
WITHOUT_CLASSIFICATION	 write the size the list vint 
WITHOUT_CLASSIFICATION	 can put multiple group bys single reducer determine suitable groups expressions otherwise treat all the expressions single group 
WITHOUT_CLASSIFICATION	 this assumes ranges passed cache fetch have data beforehand 
WITHOUT_CLASSIFICATION	 save join type 
WITHOUT_CLASSIFICATION	 the sorting order the child more specific than that the parent assign the sorting order the child the parent 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 when last txn finished the currenttxnindex pointing that txn need start from next one any also batch was created but 
WITHOUT_CLASSIFICATION	 not overwrite 
WITHOUT_CLASSIFICATION	 with the values generated and propagated from the right input 
WITHOUT_CLASSIFICATION	 skip the colinfos which are not for this particular alias 
WITHOUT_CLASSIFICATION	 for mapreduce job 
WITHOUT_CLASSIFICATION	 create hivesessionimpl object 
WITHOUT_CLASSIFICATION	 pipeline which can cause cycle after hashjoin optimization 
WITHOUT_CLASSIFICATION	 revokegrantoption 
WITHOUT_CLASSIFICATION	 return true current min has next row 
WITHOUT_CLASSIFICATION	 translate work vertex 
WITHOUT_CLASSIFICATION	 get the positions for partition bucket and sort columns 
WITHOUT_CLASSIFICATION	 initialize the config 
WITHOUT_CLASSIFICATION	 traversing operator tree 
WITHOUT_CLASSIFICATION	 the rpc server will take care timeouts here 
WITHOUT_CLASSIFICATION	 handle child tasks here could add them directly whereever need 
WITHOUT_CLASSIFICATION	 positive unix time 
WITHOUT_CLASSIFICATION	 bucketcols 
WITHOUT_CLASSIFICATION	 available and grammar check there the language itself 
WITHOUT_CLASSIFICATION	 intwritable can just sum the reduce 
WITHOUT_CLASSIFICATION	 full paths are replaced with base filenames 
WITHOUT_CLASSIFICATION	 test the user session specific conf overlaying global init conf 
WITHOUT_CLASSIFICATION	 due the limitation that can only have one instance persistence manager factory jvm are not able create multiple embedded derby instances for two different metastore instances 
WITHOUT_CLASSIFICATION	 couldnt find reference expression 
WITHOUT_CLASSIFICATION	 consider query like select from subq has filter join subq has filter some key let assume that subq the small table either specified the user inferred automatically the following operator tree will created tablescan subq select filter dummystore smbjoin tablescan subq select filter 
WITHOUT_CLASSIFICATION	 non partitioned table 
WITHOUT_CLASSIFICATION	 using signatures and encryption 
WITHOUT_CLASSIFICATION	 try valid alter table 
WITHOUT_CLASSIFICATION	 use table properties case unpartitioned tables and the union table properties and partition properties with partition 
WITHOUT_CLASSIFICATION	 find first nonsign xff byte input 
WITHOUT_CLASSIFICATION	 insert into partitions and get the last repl 
WITHOUT_CLASSIFICATION	 retain this digit 
WITHOUT_CLASSIFICATION	 for use from within 
WITHOUT_CLASSIFICATION	 particular use size data number uses 
WITHOUT_CLASSIFICATION	 singleton using dcl 
WITHOUT_CLASSIFICATION	 used groupby 
WITHOUT_CLASSIFICATION	 insert this map into the stats 
WITHOUT_CLASSIFICATION	 over the subqueries and getmetadata for these 
WITHOUT_CLASSIFICATION	 with grant also implies without grant privilege add without privilege well 
WITHOUT_CLASSIFICATION	 get the existing table 
WITHOUT_CLASSIFICATION	 case views the underlying views tables are not direct dependencies and are not used for authorization checks this readentity represents one the underlying tablesviews skip see description the isdirect readentity 
WITHOUT_CLASSIFICATION	 fill much the overflow batch possible with small table values 
WITHOUT_CLASSIFICATION	 regular create table create table like ctlt create table select ctas 
WITHOUT_CLASSIFICATION	 set ssl 
WITHOUT_CLASSIFICATION	 now trim the overstuffed histogram down the correct number bins 
WITHOUT_CLASSIFICATION	 indicates that are test mode 
WITHOUT_CLASSIFICATION	 drop table event 
WITHOUT_CLASSIFICATION	 tablescan nonhive table dont support for materializations 
WITHOUT_CLASSIFICATION	 returning true does not guarantee that the task will run considering other queries may running the system also depends upon the capacity usage configuration 
WITHOUT_CLASSIFICATION	 tests not setting maxrows 
WITHOUT_CLASSIFICATION	 dont have column stats just assume hash aggregation disabled 
WITHOUT_CLASSIFICATION	 sparkwork 
WITHOUT_CLASSIFICATION	 increase target list pos target list being drained set delta and refcount 
WITHOUT_CLASSIFICATION	 look for hint not run test some hadoop versions 
WITHOUT_CLASSIFICATION	 helper function create jobconf for specific reducework 
WITHOUT_CLASSIFICATION	 methods that create group keys and aggregate calls 
WITHOUT_CLASSIFICATION	 required required required required optional optional optional optional 
WITHOUT_CLASSIFICATION	 run cleaner this run doesnt anything for the above aborted transaction since the current compaction request entry the compaction queue updated have highestwriteid when the worker run before the aborted transaction specifically the for the entry but the aborted transaction has writeid this run does transition the entry 
WITHOUT_CLASSIFICATION	 contains aliases from subquery are just converting common merge join operator the shuffle join mapreduce case 
WITHOUT_CLASSIFICATION	 the overwhelming majority cases will here read bytes tada 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the entire current slice part the split note that split end equals lastend the split would also read the next row need look the next slice any although wed probably find cannot use note also that not treat endoffile differently here cause not know any such thing the caller must handle lastend end split end file match correctly terms how lrr handles them see above for startoffile 
WITHOUT_CLASSIFICATION	 the return values are capped return and 
WITHOUT_CLASSIFICATION	 launchable task one that hasnt been queued hasnt been initialized and runnable 
WITHOUT_CLASSIFICATION	 has the user enabled merging files for maponly jobs for all jobs 
WITHOUT_CLASSIFICATION	 partitioned table stats not updated 
WITHOUT_CLASSIFICATION	 look for inputfileformat serde combinations can deserialize more efficiently using and deserialize class with the deserializeread interface the vectorized rowbyrow deserialization into vectorizedrowbatch the 
WITHOUT_CLASSIFICATION	 this the last file for this bucket maxkey null means the split the tail the file want leave blank make sure any insert events delta files are included conversely its not the last file set the maxkey that events from deltas that dont modify anything the current split are excluded 
WITHOUT_CLASSIFICATION	 case max list members max query string length 
WITHOUT_CLASSIFICATION	 all bits are set expected should 
WITHOUT_CLASSIFICATION	 rxrx and and rand 
WITHOUT_CLASSIFICATION	 creator 
WITHOUT_CLASSIFICATION	 minus here otherwise driver also counted executor 
WITHOUT_CLASSIFICATION	 let uval the value the unsigned long with the same bits twos complement uval max uval max now use the fact abc acbcc 
WITHOUT_CLASSIFICATION	 batchindex classname nomatch duplicate 
WITHOUT_CLASSIFICATION	 schema info 
WITHOUT_CLASSIFICATION	 prepare the list databases 
WITHOUT_CLASSIFICATION	 batching not enabled try drop all the partitions one call 
WITHOUT_CLASSIFICATION	 move any incompatible files final path 
WITHOUT_CLASSIFICATION	 check external table property being removed 
WITHOUT_CLASSIFICATION	 remove values key exprs for value table schema value expression for hashsink will modified 
WITHOUT_CLASSIFICATION	 this check recursively all the parent roles currole 
WITHOUT_CLASSIFICATION	 deserializer null case vectormapoperator 
WITHOUT_CLASSIFICATION	 todo when will the queue ever null pass queue and default constructor parameters and make them final 
WITHOUT_CLASSIFICATION	 set memory usage for the operator 
WITHOUT_CLASSIFICATION	 the names the columns the input matchpath used setup the tpath struct column 
WITHOUT_CLASSIFICATION	 create proxy for the local filesystem the schemeauthority serving the proxy derived from the supplied uri 
WITHOUT_CLASSIFICATION	 get the sort positions and sort order for the table 
WITHOUT_CLASSIFICATION	 adjust the aggregator argument positions note aggregator does not change input ordering the input output position mapping can used derive the new positions 
WITHOUT_CLASSIFICATION	 skewed keys which intersect with join keys 
WITHOUT_CLASSIFICATION	 leaf 
WITHOUT_CLASSIFICATION	 sort does not change input ordering 
WITHOUT_CLASSIFICATION	 false for the following cases name list which matches the spec name bag which indicates existing hive pig data 
WITHOUT_CLASSIFICATION	 the fromindexinclusive and toindexexclusive for each unique owid 
WITHOUT_CLASSIFICATION	 create syntax tree for simple function call longudfcol 
WITHOUT_CLASSIFICATION	 tenscale can make this long comparison 
WITHOUT_CLASSIFICATION	 look for matches time based counters 
WITHOUT_CLASSIFICATION	 both are numeric make sure the new type larger than the old 
WITHOUT_CLASSIFICATION	 the database newer than the create event then noop 
WITHOUT_CLASSIFICATION	 noop 
WITHOUT_CLASSIFICATION	 use the maximum parallelism from all parent reduce sinks 
WITHOUT_CLASSIFICATION	 the groupbyoperator not initialized which means there data since initialize the operators when see the first record just nothing here 
WITHOUT_CLASSIFICATION	 use the sign the reversednanoseconds field indicate that there second vint present 
WITHOUT_CLASSIFICATION	 also collect table stats while collecting column stats 
WITHOUT_CLASSIFICATION	 findwriteslot slot slot tripleindex tripleindex existing 
WITHOUT_CLASSIFICATION	 simple one long key map join benchmarks build with mvn clean install dskiptests pdistitests main hive directory from itestshivejmh directory run java jar targetbenchmarksjar inner innerbigonly leftsemi outer rowmodehashmap rowmodeoptimized vectorpassthrough nativevectorfast 
WITHOUT_CLASSIFICATION	 nonjavadoc see byte 
WITHOUT_CLASSIFICATION	 object inspectors corresponding the struct returned terminatepartial and the fields within the struct maxlength sumlength count countnulls 
WITHOUT_CLASSIFICATION	 only one belows notnull total length sample prunes splits exceeded percent total input prunes splits exceeded row count per split not prune splits 
WITHOUT_CLASSIFICATION	 hive acidorc requires hiveinputformat 
WITHOUT_CLASSIFICATION	 retrieve all partitions generated from partition pruner and partition column pruner 
WITHOUT_CLASSIFICATION	 the type the hive column cannot store the actual typeinfo because that would require 
WITHOUT_CLASSIFICATION	 check this udf has been provided with type params for the output char type 
WITHOUT_CLASSIFICATION	 alter table add columns 
WITHOUT_CLASSIFICATION	 right larger 
WITHOUT_CLASSIFICATION	 bacabc 
WITHOUT_CLASSIFICATION	 test variations callbacks increases revokes not update the same task again then increase decrease and decrease increase the call coming after the message sent the message callback should undo the change 
WITHOUT_CLASSIFICATION	 append this container the loaded list 
WITHOUT_CLASSIFICATION	 captures the window processing specified query query may contain udaf invocations window leadlag function invocations that can only evaluated partition for queries that dont have group all udaf invocations are treated window function invocations for queries that dont have group the having condition handled post processing the rows output windowing processing windowing container all the select expressions that are handled windowing these are held lists the functions list holds windowfunction invocations the expressions list holds select expressions having leadlag function calls may also contain astnode representing the post filter apply the output window functions windowing also contains all the windows defined the query one the windows designated the default window the query has distribute bycluster clause then the information these clauses captured partitioning and used the default window for the query otherwise the first window specified treated the default finally windowing maintains map from alias the astnode that represents the select expression that was translated window function invocation window expression this used when building rowresolvers 
WITHOUT_CLASSIFICATION	 cannot use integermaxvalue which 
WITHOUT_CLASSIFICATION	 because pairs give java the vapors 
WITHOUT_CLASSIFICATION	 returns null always 
WITHOUT_CLASSIFICATION	 level the return all filesdirectories under the specified path 
WITHOUT_CLASSIFICATION	 its not this property wont any others 
WITHOUT_CLASSIFICATION	 equivalent acidsinks but for ddl operations that change data 
WITHOUT_CLASSIFICATION	 rewritten grouping function 
WITHOUT_CLASSIFICATION	 remove currtask from childtasks 
WITHOUT_CLASSIFICATION	 and integermaxvalue the check for varchar precision done hive 
WITHOUT_CLASSIFICATION	 maximum reasonable defragmentation headroom mostly kicks very small caches 
WITHOUT_CLASSIFICATION	 storing nanosecond interval longs produces timestamp 
WITHOUT_CLASSIFICATION	 the the bucket care about here 
WITHOUT_CLASSIFICATION	 start inclusive infinity inclusive 
WITHOUT_CLASSIFICATION	 table comment 
WITHOUT_CLASSIFICATION	 make sure cross some buffer boundaries 
WITHOUT_CLASSIFICATION	 xmx speficied 
WITHOUT_CLASSIFICATION	 have override with the new conf since this where prewarm gets the conf object 
WITHOUT_CLASSIFICATION	 scale down again 
WITHOUT_CLASSIFICATION	 need new object obey our immutable behavior 
WITHOUT_CLASSIFICATION	 while initializing this need done here instead constructor 
WITHOUT_CLASSIFICATION	 release but keep the lock present 
WITHOUT_CLASSIFICATION	 case column stats 
WITHOUT_CLASSIFICATION	 first call createpartitions should throw exception 
WITHOUT_CLASSIFICATION	 haruri used access the partitions files which are the archive the format the something like 
WITHOUT_CLASSIFICATION	 are creating filter here should not returning null not sure why calcite return null 
WITHOUT_CLASSIFICATION	 check edge case throw exception can not build single query for not clause cases mentioned the method comments 
WITHOUT_CLASSIFICATION	 were sure this part smaller than memory limit 
WITHOUT_CLASSIFICATION	 replace essentially renaming plan the name existing plan with backup 
WITHOUT_CLASSIFICATION	 successful 
WITHOUT_CLASSIFICATION	 todo create this centrally case 
WITHOUT_CLASSIFICATION	 decimal note the scale parameter for text serialization that creates trailing zeroes output decimals 
WITHOUT_CLASSIFICATION	 now make exporttask from temp table 
WITHOUT_CLASSIFICATION	 pending pending pending 
WITHOUT_CLASSIFICATION	 for missing wdw frames for frames with only start boundary completely 
WITHOUT_CLASSIFICATION	 not able acquire the lock within that time period 
WITHOUT_CLASSIFICATION	 dictionary encoding 
WITHOUT_CLASSIFICATION	 the output columns for the destination table should match with the join keys this handle queries the form insert overwrite table select tkey tkey udftvalue tvalue from join tkey tkey and tkey tkey where and are bucketizedsorted key and key assuming the table which the mapper run the following true the number buckets for and should same the bucketingsorting columns for and should same the sort order should match with the sort order for partitioned only single partition can selected the select list should contain with tkey tkey tkey tkey 
WITHOUT_CLASSIFICATION	 consolereader will the substitution and only there exactly one valid completion ignore other cases 
WITHOUT_CLASSIFICATION	 table ddl 
WITHOUT_CLASSIFICATION	 the current expression node column see the column alias already part the return set not and already have entry set this invalid expression 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 validate the first parameter which the expression compute over this should 
WITHOUT_CLASSIFICATION	 default false 
WITHOUT_CLASSIFICATION	 open txns are already sorted ascending order this list may may not include hwm but guaranteed that list wont have txn hwm but overwrite the hwm with currenttxn 
WITHOUT_CLASSIFICATION	 get the nulls 
WITHOUT_CLASSIFICATION	 utility get patterns from url every array element match for one 
WITHOUT_CLASSIFICATION	 the stack the table scan operator 
WITHOUT_CLASSIFICATION	 should have rolled over next transaction batch 
WITHOUT_CLASSIFICATION	 first quickly check the two operators can actually merged already know that these two operators have the same parent but need check whether both are actually equal further check whether their child also equal any these conditions are not 
WITHOUT_CLASSIFICATION	 format 
WITHOUT_CLASSIFICATION	 set results returned 
WITHOUT_CLASSIFICATION	 next put row into corresponding hash partition 
WITHOUT_CLASSIFICATION	 list locks protect the above list 
WITHOUT_CLASSIFICATION	 partitions 
WITHOUT_CLASSIFICATION	 they will different values 
WITHOUT_CLASSIFICATION	 flush any catalog objects held the metastore implementation note that this does not flush statistics objects this should called the beginning each query 
WITHOUT_CLASSIFICATION	 concurrent increase and revocation then another increase after the message sent 
WITHOUT_CLASSIFICATION	 add all the names for previous batches 
WITHOUT_CLASSIFICATION	 represents column information exposed queryblock 
WITHOUT_CLASSIFICATION	 the struct null and level dynamicserde will call writenull 
WITHOUT_CLASSIFICATION	 cached use serialize data 
WITHOUT_CLASSIFICATION	 the group operator has null key 
WITHOUT_CLASSIFICATION	 all remaining functions simply delegate objectstore 
WITHOUT_CLASSIFICATION	 column not found target table its new column its schema mapstringstring 
WITHOUT_CLASSIFICATION	 printstream that stores messages logged list 
WITHOUT_CLASSIFICATION	 set default spark configurations 
WITHOUT_CLASSIFICATION	 test that fetching nonexistent dbname yields objectnotfound 
WITHOUT_CLASSIFICATION	 break into multiple batches remove duplicates first 
WITHOUT_CLASSIFICATION	 this second attempt create should throw 
WITHOUT_CLASSIFICATION	 recursively remove this task from its childrens parent task 
WITHOUT_CLASSIFICATION	 replace flag not set caller then default set true maintain backward compatibility 
WITHOUT_CLASSIFICATION	 fill host with tasks leave host empty try running both tasks host single preemption triggered followed allocation followed another preemption 
WITHOUT_CLASSIFICATION	 reduce sink row resolver used generate map join 
WITHOUT_CLASSIFICATION	 theres some access get acls assume means free for all 
WITHOUT_CLASSIFICATION	 cleanup 
WITHOUT_CLASSIFICATION	 this expected noop will return null when use local metastore 
WITHOUT_CLASSIFICATION	 introduce project rel above original leftright inputs cast 
WITHOUT_CLASSIFICATION	 with data unsupported for the test case 
WITHOUT_CLASSIFICATION	 test that overflow produces null 
WITHOUT_CLASSIFICATION	 outer join specific members 
WITHOUT_CLASSIFICATION	 tracks vertices for which notifications have been registered 
WITHOUT_CLASSIFICATION	 descendants tasks who subscribe feeds from this task 
WITHOUT_CLASSIFICATION	 dont fail the query just return null caller should skip cache lookup 
WITHOUT_CLASSIFICATION	 consider validation type information 
WITHOUT_CLASSIFICATION	 for unit testing harm hardcoding allocator ceiling longmaxvalue 
WITHOUT_CLASSIFICATION	 create parquet file with specific data 
WITHOUT_CLASSIFICATION	 get writable object 
WITHOUT_CLASSIFICATION	 loginfoencrypted shuffle enabled sslfactory new conf sslfactoryinit 
WITHOUT_CLASSIFICATION	 cache 
WITHOUT_CLASSIFICATION	 are now positioned the end this fields bytes 
WITHOUT_CLASSIFICATION	 clean 
WITHOUT_CLASSIFICATION	 the exception has been logged the lower layer 
WITHOUT_CLASSIFICATION	 try construct object 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream 
WITHOUT_CLASSIFICATION	 what the index the row beyond the set rows that match this pattern 
WITHOUT_CLASSIFICATION	 array level array level 
WITHOUT_CLASSIFICATION	 minihs cluster let run until someone kills the test 
WITHOUT_CLASSIFICATION	 type 
WITHOUT_CLASSIFICATION	 check all field inspectors are initialized 
WITHOUT_CLASSIFICATION	 larger scale will knock off lower digits and round 
WITHOUT_CLASSIFICATION	 context handler 
WITHOUT_CLASSIFICATION	 ignore the first child since the variable 
WITHOUT_CLASSIFICATION	 demuxoperator and directly connects childop will add this muxoperator between demuxoperator and childop 
WITHOUT_CLASSIFICATION	 add partition key ensure its reflected the schema 
WITHOUT_CLASSIFICATION	 framework expects mapwork instances that have physical parents union parent fine broadcast parent isnt 
WITHOUT_CLASSIFICATION	 getxxx returns for numeric types false for boolean and null for other 
WITHOUT_CLASSIFICATION	 with that mind determine disk ranges readget from cache not stream 
WITHOUT_CLASSIFICATION	 guard against poor configuration noconditional task size let hash table grow till memory available for containerexecutor 
WITHOUT_CLASSIFICATION	 this processing only needs happen for the small tables 
WITHOUT_CLASSIFICATION	 verify that the created table identical sourcetable 
WITHOUT_CLASSIFICATION	 required string destinputname 
WITHOUT_CLASSIFICATION	 row the column name 
WITHOUT_CLASSIFICATION	 round fractional must not allowed throw away digits 
WITHOUT_CLASSIFICATION	 direct heap allocations need safer 
WITHOUT_CLASSIFICATION	 intialize the tables 
WITHOUT_CLASSIFICATION	 null all tablesviews are returned 
WITHOUT_CLASSIFICATION	 passed the test load 
WITHOUT_CLASSIFICATION	 happen the normal course business need log them errors all the time 
WITHOUT_CLASSIFICATION	 instance fields 
WITHOUT_CLASSIFICATION	 not cast 
WITHOUT_CLASSIFICATION	 are done 
WITHOUT_CLASSIFICATION	 authtype 
WITHOUT_CLASSIFICATION	 aggregate above sum the subtotals 
WITHOUT_CLASSIFICATION	 oldpath subdir destf but could not cleaned 
WITHOUT_CLASSIFICATION	 can always equality predicate just need make sure get appropriate representation constant filter condition can other comparisons only storage format hbase either binary are dealing with string types since there lexicographic ordering will suffice 
WITHOUT_CLASSIFICATION	 from the value arrays and our isrepeated selected isnull arrays generate the batch 
WITHOUT_CLASSIFICATION	 mixed sign cases 
WITHOUT_CLASSIFICATION	 must honored 
WITHOUT_CLASSIFICATION	 make sure the parent directory exists not error recreate existing directory 
WITHOUT_CLASSIFICATION	 scope openedclosed once multiple opens not count 
WITHOUT_CLASSIFICATION	 for now dont take any pool action future might restore the session based this and get rid the logic outside the pool that replacesreopensetc 
WITHOUT_CLASSIFICATION	 call the executor which will execute the appropriate command based the parsed options 
WITHOUT_CLASSIFICATION	 order expressions which will only get set and used for range windowing type 
WITHOUT_CLASSIFICATION	 phase verify get the expected objects created all threads 
WITHOUT_CLASSIFICATION	 check whether output works when merge the operators will collide work work merge work work work cannot merge the reason that tez currently does not support parallel edges multiple edges from same work 
WITHOUT_CLASSIFICATION	 stores big table rows bytes for native vector map join 
WITHOUT_CLASSIFICATION	 forward compatible 
WITHOUT_CLASSIFICATION	 try outer row resolver 
WITHOUT_CLASSIFICATION	 check value argument 
WITHOUT_CLASSIFICATION	 nonjavadoc see int javalangstring 
WITHOUT_CLASSIFICATION	 test setting fetch size above max 
WITHOUT_CLASSIFICATION	 test that read blocks exclusive 
WITHOUT_CLASSIFICATION	 not authorize dummy readentity writeentity 
WITHOUT_CLASSIFICATION	 are the leftmost child 
WITHOUT_CLASSIFICATION	 divide tenscale check for rounding 
WITHOUT_CLASSIFICATION	 definitely not int 
WITHOUT_CLASSIFICATION	 from tableacidtbl where select from tablenonacidorctbl 
WITHOUT_CLASSIFICATION	 using utility method above that doesnt have used here this helps avoid adding jdo dependency for hcatalog client uses 
WITHOUT_CLASSIFICATION	 try regular properties 
WITHOUT_CLASSIFICATION	 this because the pending events will sent via the succeededfailed messages taskdone set before are sent out which what causes the thread exit 
WITHOUT_CLASSIFICATION	 read notification from metastore 
WITHOUT_CLASSIFICATION	 required string vertexname 
WITHOUT_CLASSIFICATION	 roundpower fastscale 
WITHOUT_CLASSIFICATION	 noone can take this buffer out and thus change the level after lock and they take out before lock then will fail lock same 
WITHOUT_CLASSIFICATION	 defer allocation until really need since the common case there crlf substitution 
WITHOUT_CLASSIFICATION	 concern isrepeating 
WITHOUT_CLASSIFICATION	 getpartitions will parse out the catalog and names itself 
WITHOUT_CLASSIFICATION	 super set the grouping columns are abc and the sorting columns are grouping columns sorting columns grouping columns are superset sorting columns similarly means subset intersection between sort columns and bucketcols sort cols group cols partial match group cols sort cols match group cols sort cols partial match bucketcols sortcols bucket columns either same prefix sort columns sort cols group cols complete match group cols sort cols match group cols sort cols complete match group cols bucketcols partial match otherwise bucketcols sortcols bucket columns superset sorting columns group cols sort cols partial match group cols sort cols match one exception this rule groupbycols sortcols and all bucketing columns are part sorting columns any order complete match 
WITHOUT_CLASSIFICATION	 see planindexreading only read nonrowindex streams involved sargs 
WITHOUT_CLASSIFICATION	 the partition was dropped before got around cleaning 
WITHOUT_CLASSIFICATION	 clean 
WITHOUT_CLASSIFICATION	 for multistatement txns you may have multiple events for the same row the same current transaction want collapse these just the last one regardless whether are minor compacting consider insertupdateupdate the same row the same txn there benefit passing along anything except the last event did want pass along wed have include statementid the row returned that compaction could write out make minor minor compaction understand how write out delta files deltaxxxyyystid format there doesnt seem any value this todo this could simplified since acid even you update the same row times txn will have different rowids there such thing multiple versions the same physical row leave for now since this acid reader should away altogether and will used 
WITHOUT_CLASSIFICATION	 show cannot create child the directory with permissions 
WITHOUT_CLASSIFICATION	 serialization methods 
WITHOUT_CLASSIFICATION	 defined with skewed columns and skewed values metadata 
WITHOUT_CLASSIFICATION	 for hive the only thing missing the last stripe index information get the information from the last stripe and append the existing index the actual stripe data can written asis similar 
WITHOUT_CLASSIFICATION	 tests the beeline behaves like default mode there userspecific connection configuration file 
WITHOUT_CLASSIFICATION	 note dont take the scheduling lock here although the call queue still 
WITHOUT_CLASSIFICATION	 schema retriever has been provided well attempt read the write schema from the retriever 
WITHOUT_CLASSIFICATION	 equal groups return what can handle 
WITHOUT_CLASSIFICATION	 keep buckets from the streaming relation 
WITHOUT_CLASSIFICATION	 wait for startup complete 
WITHOUT_CLASSIFICATION	 alter testdatabaseops via objectstore 
WITHOUT_CLASSIFICATION	 get list aliases for the same column 
WITHOUT_CLASSIFICATION	 wont try too strict checking this because were comparing table create intents with observed tables created does have location though will compare with external tables 
WITHOUT_CLASSIFICATION	 modify below part imposing view column names 
WITHOUT_CLASSIFICATION	 only tablename pattern and 
WITHOUT_CLASSIFICATION	 will put fork the plan the source the reduce sink 
WITHOUT_CLASSIFICATION	 standard overrides methods 
WITHOUT_CLASSIFICATION	 move lockstep one entry added each the same time 
WITHOUT_CLASSIFICATION	 this likely indicates that instance has recently restarted 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 floatcompare treats and different 
WITHOUT_CLASSIFICATION	 the types array tells the number columns the data 
WITHOUT_CLASSIFICATION	 get partition column stats for this table 
WITHOUT_CLASSIFICATION	 one them null replace the old params with the new one 
WITHOUT_CLASSIFICATION	 map keys are required primitive and may serialized binary format 
WITHOUT_CLASSIFICATION	 skip this column 
WITHOUT_CLASSIFICATION	 create separate thread send the events 
WITHOUT_CLASSIFICATION	 this the genericudaf name 
WITHOUT_CLASSIFICATION	 when column name specified describe table ddl colpath will will tablenamecolumnname 
WITHOUT_CLASSIFICATION	 test that really the maximum value the 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 order matters assuming later that timegranularity comes first then druidpartitionkey 
WITHOUT_CLASSIFICATION	 read the contents and make sure they match 
WITHOUT_CLASSIFICATION	 fraction digit continue into highest longword 
WITHOUT_CLASSIFICATION	 table dropped when dump progress just skip partitions dump 
WITHOUT_CLASSIFICATION	 find the appropriate storage descriptor 
WITHOUT_CLASSIFICATION	 will bail out not want end with limits all over the tree 
WITHOUT_CLASSIFICATION	 this should probably never happen see 
WITHOUT_CLASSIFICATION	 runctx and ctx share the configuration but not isexplainplan 
WITHOUT_CLASSIFICATION	 outpath will nonunion case union case 
WITHOUT_CLASSIFICATION	 round the next 
WITHOUT_CLASSIFICATION	 this should work comments are stripped hivecli 
WITHOUT_CLASSIFICATION	 location set null here can overwritten the import stmt 
WITHOUT_CLASSIFICATION	 lets see can one step further and just uber this puppy 
WITHOUT_CLASSIFICATION	 create and delete temp file 
WITHOUT_CLASSIFICATION	 case theres delay for the heartbeat txn should able commit 
WITHOUT_CLASSIFICATION	 finalization 
WITHOUT_CLASSIFICATION	 principal specified authorize 
WITHOUT_CLASSIFICATION	 original bucket files and delta directory should stay until cleaner kicks 
WITHOUT_CLASSIFICATION	 use util function aggr stats 
WITHOUT_CLASSIFICATION	 now correct error and rounding 
WITHOUT_CLASSIFICATION	 store the new joincontext 
WITHOUT_CLASSIFICATION	 construct list bucketing location mappings from subdirectory name 
WITHOUT_CLASSIFICATION	 setup uncaught exception handler for the current thread 
WITHOUT_CLASSIFICATION	 changing the tags 
WITHOUT_CLASSIFICATION	 for partitioned tables get the size all the partitions after pruning 
WITHOUT_CLASSIFICATION	 the reason that set the txn manager for the cxt here because each query has its own ctx object the txn mgr shared across the 
WITHOUT_CLASSIFICATION	 this the one are expecting 
WITHOUT_CLASSIFICATION	 not visited yet 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 mock operation and ophandle for operationmanager 
WITHOUT_CLASSIFICATION	 confirm that the function now gone 
WITHOUT_CLASSIFICATION	 most tests are done with ansi sql mode enabled set back true 
WITHOUT_CLASSIFICATION	 since hivemetastoreclient not threadsafe hive clients are not shared across threads thread local variable containing each threads unique used one the keys for the cache 
WITHOUT_CLASSIFICATION	 update min min lesser than the smallest value seen far 
WITHOUT_CLASSIFICATION	 this rather convoluted simplify for perf could call getrawkeyvalue instead writable and serialize based java type opposed 
WITHOUT_CLASSIFICATION	 used datumreader applications should not call 
WITHOUT_CLASSIFICATION	 materialized view not heuristic approach normal costing 
WITHOUT_CLASSIFICATION	 update the work name which referred operators following works 
WITHOUT_CLASSIFICATION	 skew the table take care 
WITHOUT_CLASSIFICATION	 try with ascii chars 
WITHOUT_CLASSIFICATION	 recall setup that get object store with the metrics initalized 
WITHOUT_CLASSIFICATION	 note must handle downgradedtask after this the end outside the lock 
WITHOUT_CLASSIFICATION	 note that this not typical list accumulator theres call finalize the last list instead add list first well locally add elements 
WITHOUT_CLASSIFICATION	 deserialize the ondisk hash table 
WITHOUT_CLASSIFICATION	 buildup the expr ast 
WITHOUT_CLASSIFICATION	 where mapreduce not present 
WITHOUT_CLASSIFICATION	 localize the nonconf resources that are missing from the current list 
WITHOUT_CLASSIFICATION	 the result decimal precision 
WITHOUT_CLASSIFICATION	 insertdata 
WITHOUT_CLASSIFICATION	 not sure how get around that 
WITHOUT_CLASSIFICATION	 ignore the first static partition 
WITHOUT_CLASSIFICATION	 keep copy hiveconf session conf changes may need get new hms client 
WITHOUT_CLASSIFICATION	 get number partitions 
WITHOUT_CLASSIFICATION	 descending 
WITHOUT_CLASSIFICATION	 clean request 
WITHOUT_CLASSIFICATION	 trim line 
WITHOUT_CLASSIFICATION	 nonjavadoc see int int 
WITHOUT_CLASSIFICATION	 generates sequence list indexes 
WITHOUT_CLASSIFICATION	 check random one these can change whive versions 
WITHOUT_CLASSIFICATION	 string entirely whitespace need apply trailingspaces 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the above members are initialized the constructor and must not transient 
WITHOUT_CLASSIFICATION	 generate sqloperator for merging the aggregations 
WITHOUT_CLASSIFICATION	 update the sql string with parameters set setxxx methods link preparedstatement param sql param parameters return updated sql string throws sqlexception 
WITHOUT_CLASSIFICATION	 fill the column vector with the provided value 
WITHOUT_CLASSIFICATION	 store list have consistent order between gettests and the test argument generation 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 process help 
WITHOUT_CLASSIFICATION	 init file 
WITHOUT_CLASSIFICATION	 fill host with tasks leave host empty 
WITHOUT_CLASSIFICATION	 actually temp table does not support partitions cascade not applicable here 
WITHOUT_CLASSIFICATION	 drop tables before dropping 
WITHOUT_CLASSIFICATION	 fetchtype 
WITHOUT_CLASSIFICATION	 where the first event refers source path and second event refers path 
WITHOUT_CLASSIFICATION	 generate spark plan 
WITHOUT_CLASSIFICATION	 dont allow query that returns everything will blow stuff 
WITHOUT_CLASSIFICATION	 check the configure job properties called from input output for setting asymmetric properties 
WITHOUT_CLASSIFICATION	 there should one exception message per file 
WITHOUT_CLASSIFICATION	 after the stats phase might have some cyclic dependencies that need 
WITHOUT_CLASSIFICATION	 insert search the beginning would have failed these parents didnt exist 
WITHOUT_CLASSIFICATION	 columnexprmap has the reverse what need mapping the internal column names the exprnodedesc from the previous operation find the keyvalue where the exprnodedesc value matches the column are searching for 
WITHOUT_CLASSIFICATION	 conflict updates else update deletes from insert and new part 
WITHOUT_CLASSIFICATION	 only valid true 
WITHOUT_CLASSIFICATION	 for base type nothing other types like structs may initialize internal data structures 
WITHOUT_CLASSIFICATION	 value found 
WITHOUT_CLASSIFICATION	 every thread created this thread pool will use the same handler 
WITHOUT_CLASSIFICATION	 time part 
WITHOUT_CLASSIFICATION	 test 
WITHOUT_CLASSIFICATION	 check are llap needs determined should use bmj dphj 
WITHOUT_CLASSIFICATION	 names 
WITHOUT_CLASSIFICATION	 inlined 
WITHOUT_CLASSIFICATION	 skip all the insert events 
WITHOUT_CLASSIFICATION	 serdename 
WITHOUT_CLASSIFICATION	 kryo docs say are taken strange things happen you dont set when registering classes 
WITHOUT_CLASSIFICATION	 create map join tasks 
WITHOUT_CLASSIFICATION	 username and password are added the http request header 
WITHOUT_CLASSIFICATION	 this writer will created when writing the first row order get information about how inspect the record data 
WITHOUT_CLASSIFICATION	 constructors 
WITHOUT_CLASSIFICATION	 lastly should dedupednondistirefs 
WITHOUT_CLASSIFICATION	 the dispatcher finds smb and there semijoin optimization before removes 
WITHOUT_CLASSIFICATION	 generates initial key 
WITHOUT_CLASSIFICATION	 write uri locking done for this 
WITHOUT_CLASSIFICATION	 production 
WITHOUT_CLASSIFICATION	 populate the cache 
WITHOUT_CLASSIFICATION	 update the keys use operator name 
WITHOUT_CLASSIFICATION	 could start move its being evicted but lets not for now 
WITHOUT_CLASSIFICATION	 test that two shared read locks can share partition 
WITHOUT_CLASSIFICATION	 hive job credstore location not set but hadoop credential provider set jobconf should contain hadoop credstore location and password should from 
WITHOUT_CLASSIFICATION	 over all the tasks and dump out the plans 
WITHOUT_CLASSIFICATION	 round double decimal places 
WITHOUT_CLASSIFICATION	 store the row see comments above for why need new copy the row 
WITHOUT_CLASSIFICATION	 group name counter name outputrecords 
WITHOUT_CLASSIFICATION	 over the list and find reducer not needed 
WITHOUT_CLASSIFICATION	 array hash map results can lookups the whole batch before output result 
WITHOUT_CLASSIFICATION	 finally add the files the existing any the old code seems this twice first for all the new resources regardless type and then for all the session resources that are not type file see branch calls from updatesession and with resourcemap from submit 
WITHOUT_CLASSIFICATION	 add proj top 
WITHOUT_CLASSIFICATION	 figure out table level partition level 
WITHOUT_CLASSIFICATION	 read the record with existing record reader and same evolved schema 
WITHOUT_CLASSIFICATION	 user did not specfied alias names infer names from outputoi 
WITHOUT_CLASSIFICATION	 its standard map reduce task check what anything inferred about 
WITHOUT_CLASSIFICATION	 returns true long there more data mockresultdata array 
WITHOUT_CLASSIFICATION	 generate reducesink operator 
WITHOUT_CLASSIFICATION	 this data structure needed create the new project 
WITHOUT_CLASSIFICATION	 arithmetic with type date longcolumnvector storing epoch days and type intervalyearmonth longcolumnvector storing 
WITHOUT_CLASSIFICATION	 report progress for each stderr line but more frequently than once per minute 
WITHOUT_CLASSIFICATION	 setboolean setboolean setshort setint setfloat setdouble setstring setlong setbyte setbyte setstring settimestamp 
WITHOUT_CLASSIFICATION	 rows should returned 
WITHOUT_CLASSIFICATION	 run other optimizations that not need stats 
WITHOUT_CLASSIFICATION	 confirm default managed table paths 
WITHOUT_CLASSIFICATION	 reenable the node task completed due preemption capacity has become available and may have been able communicate with the node 
WITHOUT_CLASSIFICATION	 assign simplify hashcode 
WITHOUT_CLASSIFICATION	 need sort tuples based the value some their columns 
WITHOUT_CLASSIFICATION	 unescape the partition name 
WITHOUT_CLASSIFICATION	 local scratch dir 
WITHOUT_CLASSIFICATION	 cancel 
WITHOUT_CLASSIFICATION	 byteval 
WITHOUT_CLASSIFICATION	 all output columns used for bucketingsorting the destination table should belong the same input table insert overwrite table select tkey tkey udftvalue tvalue from join tkey tkey and tkey tkey not optimized whereas the insert optimized the select list either changed tkey tkey udftvalue tvalue tkey tkey udftvalue tvalue 
WITHOUT_CLASSIFICATION	 they both require dbtxnmanager and both need recordvalidtxns when acquiring locks driver 
WITHOUT_CLASSIFICATION	 mapping from expression node expression containing only partition virtual column constants 
WITHOUT_CLASSIFICATION	 bit set element was already cache should have been replaced 
WITHOUT_CLASSIFICATION	 add some multibyte characters test length routine later 
WITHOUT_CLASSIFICATION	 spark has its own config for merging 
WITHOUT_CLASSIFICATION	 generate pruned column list for all relevant operators 
WITHOUT_CLASSIFICATION	 for replicating hcatpartition definition 
WITHOUT_CLASSIFICATION	 operator native 
WITHOUT_CLASSIFICATION	 generate reducesinkoperator 
WITHOUT_CLASSIFICATION	 from 
WITHOUT_CLASSIFICATION	 set the conf variable values for this test 
WITHOUT_CLASSIFICATION	 blank byte cyrillic capital dje bytes 
WITHOUT_CLASSIFICATION	 operation handle guid string 
WITHOUT_CLASSIFICATION	 infrastructure place 
WITHOUT_CLASSIFICATION	 nodemap registration not required since theres taskid association 
WITHOUT_CLASSIFICATION	 has open txn 
WITHOUT_CLASSIFICATION	 set the operation handle information driver that thrift api users can use the operation handle they receive lookup query information 
WITHOUT_CLASSIFICATION	 first branch query second branch 
WITHOUT_CLASSIFICATION	 any operator which does not allow mapside conversion present the mapper dont convert into conditional task 
WITHOUT_CLASSIFICATION	 this configuration used only for client side configuration 
WITHOUT_CLASSIFICATION	 the basic premise here that will rsync the directory first working drone then execute local rsync the node the other drones this keeps from executing tons rsyncs the master node conserving cpu 
WITHOUT_CLASSIFICATION	 date comparisons 
WITHOUT_CLASSIFICATION	 create directory 
WITHOUT_CLASSIFICATION	 remove the last 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 its list join keys 
WITHOUT_CLASSIFICATION	 check what happens when ignore these errors 
WITHOUT_CLASSIFICATION	 connection secret show the child processs command line 
WITHOUT_CLASSIFICATION	 else ptn already exists but nothing with 
WITHOUT_CLASSIFICATION	 consdier need type name parser for typedescription 
WITHOUT_CLASSIFICATION	 lru cache using linked hash map 
WITHOUT_CLASSIFICATION	 message kept around for debugging 
WITHOUT_CLASSIFICATION	 objtorefresh 
WITHOUT_CLASSIFICATION	 lengths stream could empty stream already reached end stream before present stream 
WITHOUT_CLASSIFICATION	 the catalog name isnt set need through and set 
WITHOUT_CLASSIFICATION	 may have already created the tables and thus dont need redo 
WITHOUT_CLASSIFICATION	 result jobcallable task after successful task completion this expected set the thread which executes jobcallable task 
WITHOUT_CLASSIFICATION	 failure happens here the intermediate archive files wont 
WITHOUT_CLASSIFICATION	 make sure the map does not expand should able find space 
WITHOUT_CLASSIFICATION	 all the insert update and delete tests assume two tables and each with columns and partitioned additional column these are created parseandanalyze and removed cleanuptables 
WITHOUT_CLASSIFICATION	 remove the tasks above without state checks just reset all metrics 
WITHOUT_CLASSIFICATION	 the outputtype float cast the arguments float replicate the overflow behavior nonvectorized udf genericudfposmod 
WITHOUT_CLASSIFICATION	 for cardinality values use numrows default try use colstats available 
WITHOUT_CLASSIFICATION	 singlecolumn long hash table import 
WITHOUT_CLASSIFICATION	 extract the real user from the given token string 
WITHOUT_CLASSIFICATION	 allows for unescaped ascii control characters json values 
WITHOUT_CLASSIFICATION	 the same without specifying writer time zone this tests deserialization older records 
WITHOUT_CLASSIFICATION	 skip over any leading xffs 
WITHOUT_CLASSIFICATION	 skip the last partitioning column since always nonnull 
WITHOUT_CLASSIFICATION	 make sure put the instance back again case was removed part 
WITHOUT_CLASSIFICATION	 rank the tables 
WITHOUT_CLASSIFICATION	 cars 
WITHOUT_CLASSIFICATION	 used for union all copartitionedge partitionedge 
WITHOUT_CLASSIFICATION	 original transaction 
WITHOUT_CLASSIFICATION	 hive pending rename before 
WITHOUT_CLASSIFICATION	 setup stats the operator plan 
WITHOUT_CLASSIFICATION	 capture arguments authorizer impl call and verify addresses passed 
WITHOUT_CLASSIFICATION	 update the nextwriteid for the given table after incrementing number write ids allocated 
WITHOUT_CLASSIFICATION	 nonvectorized validates that explicitly during udf init 
WITHOUT_CLASSIFICATION	 unset distribution keys fixed can change reducer count order can concat adjacent buckets can redistribute into buckets uniformly group can not wait for downstream tasks 
WITHOUT_CLASSIFICATION	 set our current entry null since its done and try again 
WITHOUT_CLASSIFICATION	 resourceplanname 
WITHOUT_CLASSIFICATION	 add hacks for wellknown collections and maps avoid estimating them 
WITHOUT_CLASSIFICATION	 read dpp outputs 
WITHOUT_CLASSIFICATION	 minopenwriteid 
WITHOUT_CLASSIFICATION	 case describe formatted tablename columnname statement colpath will contain tablenamecolumnname columnname not specified colpath will equal tablename this how can differentiate are describing table column 
WITHOUT_CLASSIFICATION	 detect there are attributes join key 
WITHOUT_CLASSIFICATION	 this happens you change bucketcount 
WITHOUT_CLASSIFICATION	 for now totalresource taskresource for llap 
WITHOUT_CLASSIFICATION	 will come here exception was thrown map reduce hadoop always call close even exception was thrown map reduce 
WITHOUT_CLASSIFICATION	 millisperday 
WITHOUT_CLASSIFICATION	 autogenerate column aliases 
WITHOUT_CLASSIFICATION	 compressed 
WITHOUT_CLASSIFICATION	 recurse 
WITHOUT_CLASSIFICATION	 compaction preserves location rows wrt bucketstranches for now 
WITHOUT_CLASSIFICATION	 the ndv the side dont match and the side filter the key column then scale the ndv the side described peter boncz such cases can off large margin the join cardinality estimate the provides the join storesales and datedim the tpcds dataset since the datedim populated for years into the future while the storesales only has years worth data there are times fewer distinct dates storesales general hard infer the range for the foreign key arbitrary expression for the ndv for dayofweek the same irrespective ndv the number unique days whereas the ndv quarters has the same ratio the ndv the keys but for expressions that apply only columns that have the same ndv the key implying that they are alternate keys can apply the ratio the case storesales datedim joins for predicate the ddate column can apply the scaling factor 
WITHOUT_CLASSIFICATION	 change the plan this structure note that the aggregaterel removed projecta replace corvar input ref from the join join replace corvar input ref from leftinputrel leftinputrel filterinputrel 
WITHOUT_CLASSIFICATION	 serves lock for itself 
WITHOUT_CLASSIFICATION	 remote spark context property 
WITHOUT_CLASSIFICATION	 this reduce sink has been processed already the work for the parentrs exists 
WITHOUT_CLASSIFICATION	 order matters all these block 
WITHOUT_CLASSIFICATION	 handles nulls items 
WITHOUT_CLASSIFICATION	 return hadoopnative available otherwise construct our own copy its logic 
WITHOUT_CLASSIFICATION	 set start with this will decremented for all columns for which events are generated this source which eventually used determine number expected 
WITHOUT_CLASSIFICATION	 determines whether the current node was actually closed and pushed this should only called the final user action node scope 
WITHOUT_CLASSIFICATION	 infer foreign key candidates positions 
WITHOUT_CLASSIFICATION	 generate the second groupbyoperator for the group plan parseinfogetxxxdest the new groupbyoperator will the second aggregation based the partial aggregation results param mode the mode aggregation final param the mapping from aggregation stringtree the return the new groupbyoperator throws semanticexception 
WITHOUT_CLASSIFICATION	 java primitive class 
WITHOUT_CLASSIFICATION	 signature xxx 
WITHOUT_CLASSIFICATION	 couldnt convince you otherwise well then lets llap 
WITHOUT_CLASSIFICATION	 best effort 
WITHOUT_CLASSIFICATION	 local scratch dir 
WITHOUT_CLASSIFICATION	 only for incremental load need validate event newer than the database 
WITHOUT_CLASSIFICATION	 partkey constant can check whether the partitions have been already filtered 
WITHOUT_CLASSIFICATION	 try query stats for column for which stats doesnt exist 
WITHOUT_CLASSIFICATION	 the the size because the main work 
WITHOUT_CLASSIFICATION	 copy conf file 
WITHOUT_CLASSIFICATION	 mydouble 
WITHOUT_CLASSIFICATION	 get the bucketing version 
WITHOUT_CLASSIFICATION	 never locked for eviction java object 
WITHOUT_CLASSIFICATION	 expected exception will occur the partitions have custom location 
WITHOUT_CLASSIFICATION	 verify that the two are equal 
WITHOUT_CLASSIFICATION	 initialize aux data structures 
WITHOUT_CLASSIFICATION	 determine type udaf 
WITHOUT_CLASSIFICATION	 make easy write unit tests instead unique generation however this does mean that writing tests have aware that repl dump will clash with prior dumps and thus have clean properly 
WITHOUT_CLASSIFICATION	 grouping sets this point 
WITHOUT_CLASSIFICATION	 the replacement changed make sure redo tostring again 
WITHOUT_CLASSIFICATION	 store the total memory that this mapjoin going use which calculated totalsizebuckets with totalsize 
WITHOUT_CLASSIFICATION	 failure hooks are run after hivestatement closed wait sometime for failure hook execute 
WITHOUT_CLASSIFICATION	 tries optimize from clause multiinsert attempt optimize insert clauses the query returns true rewriting successful false otherwise 
WITHOUT_CLASSIFICATION	 have reached new key group 
WITHOUT_CLASSIFICATION	 write delta 
WITHOUT_CLASSIFICATION	 retry with same dump with which was already loaded should resume the bootstrap load 
WITHOUT_CLASSIFICATION	 descending null last default for descending order 
WITHOUT_CLASSIFICATION	 root must exist already 
WITHOUT_CLASSIFICATION	 total running 
WITHOUT_CLASSIFICATION	 this enables vectorization rowid 
WITHOUT_CLASSIFICATION	 invariant reducerhash null 
WITHOUT_CLASSIFICATION	 check incompatible versions 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 depending the server setup 
WITHOUT_CLASSIFICATION	 need update the mapcorvartocorrel update the output position for the cor vars only pass the cor vars that are not used 
WITHOUT_CLASSIFICATION	 char between 
WITHOUT_CLASSIFICATION	 setup the actual ports the configuration 
WITHOUT_CLASSIFICATION	 now disregard null second pass 
WITHOUT_CLASSIFICATION	 link the list record the first element 
WITHOUT_CLASSIFICATION	 dummy final assignments 
WITHOUT_CLASSIFICATION	 schema that exists the avro data file 
WITHOUT_CLASSIFICATION	 remove the current active server 
WITHOUT_CLASSIFICATION	 create test database and base tables once for all the test 
WITHOUT_CLASSIFICATION	 nonempty java opts with xmx specified 
WITHOUT_CLASSIFICATION	 fetch the partition referred the message and compare 
WITHOUT_CLASSIFICATION	 perform dynamic partition pruning 
WITHOUT_CLASSIFICATION	 and restore the state before walking each child 
WITHOUT_CLASSIFICATION	 todo create immutable copies all maps 
WITHOUT_CLASSIFICATION	 careful with the range here now not want read the whole base file like deltas 
WITHOUT_CLASSIFICATION	 records whether delta dir type deletedeltaxy 
WITHOUT_CLASSIFICATION	 processing files 
WITHOUT_CLASSIFICATION	 note the following section metadataonly import handling logic interleaved with regular replimport logic the rule thumb being followed here that mdonly imports are essentially alters they not load data and should not creating any metadata they should replacing instead the only place makes sense for mdonly import create the case table thats been dropped and recreated the case unpartitioned table all other cases should behave like noop pure alter 
WITHOUT_CLASSIFICATION	 for tez dont use appid distinguish the tokens 
WITHOUT_CLASSIFICATION	 theres small number buffers and they all live the heap 
WITHOUT_CLASSIFICATION	 did not kill this session expect everything present 
WITHOUT_CLASSIFICATION	 make copy that not modify hookcontext conf 
WITHOUT_CLASSIFICATION	 generate the map join operator already checked the map join 
WITHOUT_CLASSIFICATION	 stores negative values count columns eventually set tasks columns after the source vertex completes 
WITHOUT_CLASSIFICATION	 end reldecorrelatorjava 
WITHOUT_CLASSIFICATION	 get the tablesviews for the desired pattern populate the output stream 
WITHOUT_CLASSIFICATION	 task startup 
WITHOUT_CLASSIFICATION	 right border the max 
WITHOUT_CLASSIFICATION	 otherwise this not sampling predicate need process 
WITHOUT_CLASSIFICATION	 should detect double 
WITHOUT_CLASSIFICATION	 fail try access offset out bounds 
WITHOUT_CLASSIFICATION	 llap dynamic value registry might already cached 
WITHOUT_CLASSIFICATION	 via the environment context 
WITHOUT_CLASSIFICATION	 theres rpc waiting for reply exception was most probably caught while processing the rpc send error 
WITHOUT_CLASSIFICATION	 ranges use the ranges from the child 
WITHOUT_CLASSIFICATION	 put something writeset 
WITHOUT_CLASSIFICATION	 datarelated configuration properties 
WITHOUT_CLASSIFICATION	 remove expression node desc and all children from mapping 
WITHOUT_CLASSIFICATION	 the generic sqlstate for syntax error 
WITHOUT_CLASSIFICATION	 determine all digits below power zero 
WITHOUT_CLASSIFICATION	 now populate structure use apply delete events 
WITHOUT_CLASSIFICATION	 testing for equality doubles after math operation not always reliable use this tolerance 
WITHOUT_CLASSIFICATION	 all must selected otherwise size would zero repeating property will not change 
WITHOUT_CLASSIFICATION	 very simple counter keep track number rows processed the reducer dumps every million times and quickly before that 
WITHOUT_CLASSIFICATION	 how many records the writer buffers before writes disk 
WITHOUT_CLASSIFICATION	 clear all the reading variables 
WITHOUT_CLASSIFICATION	 were going update the average variable row size sampling the current batch 
WITHOUT_CLASSIFICATION	 mark task failed due comm failure 
WITHOUT_CLASSIFICATION	 the lock manager still null then means arent using 
WITHOUT_CLASSIFICATION	 this count transform countnullindicator the null indicator located the end 
WITHOUT_CLASSIFICATION	 for this one dont specify location make sure gets put the catalog directory 
WITHOUT_CLASSIFICATION	 comletor add space after last delimeter 
WITHOUT_CLASSIFICATION	 end synchronized 
WITHOUT_CLASSIFICATION	 recursively the first phase semantic analysis for the subquery 
WITHOUT_CLASSIFICATION	 flushed 
WITHOUT_CLASSIFICATION	 custom dynamic location provided need rename final output path 
WITHOUT_CLASSIFICATION	 check that the table partition isnt sorted dont yet support that 
WITHOUT_CLASSIFICATION	 get partitionpath for gridxyz place the partition outside the tablepath 
WITHOUT_CLASSIFICATION	 output strips off the partition columns and retains other columns 
WITHOUT_CLASSIFICATION	 rewrite all insert references all the node values for this key 
WITHOUT_CLASSIFICATION	 note compaction creates delta wont replace existing base dir the txn the base dir wont part deltas range otoh compaction creates base dont care about this value because bases dont have min txn the name however logically this should also take base into account its included 
WITHOUT_CLASSIFICATION	 the cookie secured where the client connect does not use ssl 
WITHOUT_CLASSIFICATION	 ahead with the estimation 
WITHOUT_CLASSIFICATION	 below value for bucket for txn logically 
WITHOUT_CLASSIFICATION	 replace any occurrence dummyvectoroperator with our tablescanoperator 
WITHOUT_CLASSIFICATION	 see heapifydown comment 
WITHOUT_CLASSIFICATION	 dont exit the loop early because want extract the error code corresponding the bottommost error coded exception 
WITHOUT_CLASSIFICATION	 parsecontext 
WITHOUT_CLASSIFICATION	 catchall rule when none the others apply 
WITHOUT_CLASSIFICATION	 register tasks nodes with dependency vertex completing 
WITHOUT_CLASSIFICATION	 setup the map work for this thread pruning modified the work instance potentially remove partitions the same work instance must used when generating splits 
WITHOUT_CLASSIFICATION	 perform repl 
WITHOUT_CLASSIFICATION	 here are some positive cases which can executed below 
WITHOUT_CLASSIFICATION	 here means concrurrent acquirelock inserted the key 
WITHOUT_CLASSIFICATION	 input name position map 
WITHOUT_CLASSIFICATION	 true here indicates that sqcountcheck for innot subqueries 
WITHOUT_CLASSIFICATION	 the third one has the base file shouldnt combined but could base 
WITHOUT_CLASSIFICATION	 read field that under complex type may primitive type deeper complex type 
WITHOUT_CLASSIFICATION	 add replstatelogtask only pending table load tasks left for next cycle 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqltime 
WITHOUT_CLASSIFICATION	 are making new output vectorized row batch 
WITHOUT_CLASSIFICATION	 but have nulls initially 
WITHOUT_CLASSIFICATION	 for caching tablewrapper objects key aggregate database name and table name 
WITHOUT_CLASSIFICATION	 the outdated see whether should consider for rewriting not 
WITHOUT_CLASSIFICATION	 cctxnextoperatorid 
WITHOUT_CLASSIFICATION	 check srcf contains nested subdirectories 
WITHOUT_CLASSIFICATION	 put compaction request the queue 
WITHOUT_CLASSIFICATION	 this should not null because were allowed bind with this username safe check case were able bind anonymously 
WITHOUT_CLASSIFICATION	 otherwise wouldnt here 
WITHOUT_CLASSIFICATION	 any aggregate call has filter bail out 
WITHOUT_CLASSIFICATION	 schema diff return false 
WITHOUT_CLASSIFICATION	 control characeters according json rfc 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 access 
WITHOUT_CLASSIFICATION	 this will guarantee file name uniqueness 
WITHOUT_CLASSIFICATION	 must sync with toperationstate order 
WITHOUT_CLASSIFICATION	 note the vectorptfdesc has already been allocated and populated 
WITHOUT_CLASSIFICATION	 propagate new conf meta store 
WITHOUT_CLASSIFICATION	 release buffers are done with all the streams also see torelease comment 
WITHOUT_CLASSIFICATION	 for local file and hdfs key and value are same 
WITHOUT_CLASSIFICATION	 this may invoked before container ever assigned task allocatetask app decides 
WITHOUT_CLASSIFICATION	 try with regular decimal output type 
WITHOUT_CLASSIFICATION	 disabled hive disabled hive disabled hive disabled hive disabled hive disabled hive disabled hive 
WITHOUT_CLASSIFICATION	 myenumstructlistmap 
WITHOUT_CLASSIFICATION	 private static properties props 
WITHOUT_CLASSIFICATION	 partitioned tables dont have tabledesc set the fetchtask instead they have list partitiondesc objects each with table desc lets try fetch the desc for the first partition and use its deserializer 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 partial aggregation performed the mapper distinct processing the reducer 
WITHOUT_CLASSIFICATION	 taking precedence the case partitioned tables 
WITHOUT_CLASSIFICATION	 with nulls 
WITHOUT_CLASSIFICATION	 get the escape information 
WITHOUT_CLASSIFICATION	 note this writes into scratch buffer within hivedecimalwritable 
WITHOUT_CLASSIFICATION	 the root interface for vector map join hash map 
WITHOUT_CLASSIFICATION	 nonempty java opts with xmx specified 
WITHOUT_CLASSIFICATION	 the correlate 
WITHOUT_CLASSIFICATION	 dummy group 
WITHOUT_CLASSIFICATION	 add the finalop after the union 
WITHOUT_CLASSIFICATION	 its hard tell wsome code paths like udfsois etc that are used many places 
WITHOUT_CLASSIFICATION	 ideally hadoop should let know whether map execution failed not 
WITHOUT_CLASSIFICATION	 for lazystruct 
WITHOUT_CLASSIFICATION	 delim does not exist str 
WITHOUT_CLASSIFICATION	 neginfinity end exclusive 
WITHOUT_CLASSIFICATION	 must escaped binarysortable 
WITHOUT_CLASSIFICATION	 note this readlock prevent race with querycomplete operations and mutations within this lock need concurrent structures 
WITHOUT_CLASSIFICATION	 for the char and varchar data types the maximum character length the column otherwise 
WITHOUT_CLASSIFICATION	 this semaphore provides two functions forces cap the number outstanding async writes channel ensures that channel isnt closed there are any outstanding async writes 
WITHOUT_CLASSIFICATION	 this subdir under the hdfssessionpath will removed along with that dir 
WITHOUT_CLASSIFICATION	 statsobj 
WITHOUT_CLASSIFICATION	 compare hosts 
WITHOUT_CLASSIFICATION	 get stats for the metastore does not break see hive for motivation 
WITHOUT_CLASSIFICATION	 set the necessary accumulo information 
WITHOUT_CLASSIFICATION	 test reverse order 
WITHOUT_CLASSIFICATION	 insertonly tables dont have conform acid requirement like orc bucketing 
WITHOUT_CLASSIFICATION	 hive more restrictive hivepig works 
WITHOUT_CLASSIFICATION	 for the argument 
WITHOUT_CLASSIFICATION	 finally create session paths for this session local nonlocal tmp location configurable however the same across 
WITHOUT_CLASSIFICATION	 walk through the task graph and invoke 
WITHOUT_CLASSIFICATION	 dont want acquire read locks during update delete well acquiring write locks instead also theres need lock temp tables since theyre session wide 
WITHOUT_CLASSIFICATION	 get the input format 
WITHOUT_CLASSIFICATION	 perform reconnect with the proper user context 
WITHOUT_CLASSIFICATION	 generator 
WITHOUT_CLASSIFICATION	 test multithreaded implementation checker find out missing partitions 
WITHOUT_CLASSIFICATION	 this can happen for view index 
WITHOUT_CLASSIFICATION	 shuffle needed for union only hashpartition shuffle keys are not sorted any way rangepartition shuffle keys are total sorted hashpartition shuffle keys are sorted partition 
WITHOUT_CLASSIFICATION	 since have result for all rows dont need conditional null maintenance turn off nonulls 
WITHOUT_CLASSIFICATION	 reader count already incremented during cache lookup 
WITHOUT_CLASSIFICATION	 this not set default value set during config initialization default value cant set this constructor would refer names other confvars 
WITHOUT_CLASSIFICATION	 add the new readentity that were added readentitymap planutilsaddinput 
WITHOUT_CLASSIFICATION	 functionality remove semijoin optimization 
WITHOUT_CLASSIFICATION	 this method only returns the result when activating resource plan could also add boolean flag specified the caller see when the result might needed 
WITHOUT_CLASSIFICATION	 generates the plan from the operator tree 
WITHOUT_CLASSIFICATION	 abcabc 
WITHOUT_CLASSIFICATION	 used for 
WITHOUT_CLASSIFICATION	 extract only split hdfs 
WITHOUT_CLASSIFICATION	 logerrorgot 
WITHOUT_CLASSIFICATION	 check dbtablepartition doesnt have replsourcefor props also ensure ckpt property 
WITHOUT_CLASSIFICATION	 make sure the jar containing the custom compositerowid included the mapreduce jobs classpath libjars 
WITHOUT_CLASSIFICATION	 throw timeoutexception caller 
WITHOUT_CLASSIFICATION	 the queue esp conjunction with and rerun them 
WITHOUT_CLASSIFICATION	 verify when first argument boolean flags repeating 
WITHOUT_CLASSIFICATION	 tables 
WITHOUT_CLASSIFICATION	 pass flag hide prefixes 
WITHOUT_CLASSIFICATION	 evaluate the key expressions once 
WITHOUT_CLASSIFICATION	 noarg constructor make kyro happy 
WITHOUT_CLASSIFICATION	 boolean 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 put back and one additional table 
WITHOUT_CLASSIFICATION	 estimate the number hash table entries based the size each entry since the size entry 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 rundroppartitions the main function that gets called with different options partcount total number partitions that will deleted batchsize maximum number partitions that can deleted batch based the above the test will check that the batch sizes are expected exceptionstatus can take values noexception exception expected oneexception first call throws exception since will retry this will succeed after the first failure allexception failure case where everything fails will test that the test fails after retrying based maxretries when specified based decaying factor 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 this will trigger new calls metastore collect metadata 
WITHOUT_CLASSIFICATION	 not required 
WITHOUT_CLASSIFICATION	 try find the default postfix dont check two last components least there should table and file could also try throw away partitionbucketacid stuff 
WITHOUT_CLASSIFICATION	 weve encountered new key must save current one cant forward yet the aggregators have not been evaluated 
WITHOUT_CLASSIFICATION	 for null just write out the type 
WITHOUT_CLASSIFICATION	 make sure get exceptions strategies might have thrown 
WITHOUT_CLASSIFICATION	 used dummy root operator attach vectorized operators that will built parallel the current nonvectorized operator tree 
WITHOUT_CLASSIFICATION	 should use for columnnamedelimiter column name contains comma but should also take care the backward compatibility 
WITHOUT_CLASSIFICATION	 for now old class 
WITHOUT_CLASSIFICATION	 allowing this increased via config breaks the merge impl per vector smaller 
WITHOUT_CLASSIFICATION	 fix the query for materialization rebuild 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 bail out encountered 
WITHOUT_CLASSIFICATION	 distinct aggregate rewrite 
WITHOUT_CLASSIFICATION	 add constant size for unions tags 
WITHOUT_CLASSIFICATION	 also minhistorylevel wont have any entries reference for open txns 
WITHOUT_CLASSIFICATION	 wont write the set expressions the rewritten query well patch that later the set list from update should the second child index 
WITHOUT_CLASSIFICATION	 runtime constants deterministic functions can folded 
WITHOUT_CLASSIFICATION	 during repl load foreign key shall ignore the foreign table may not part the replication 
WITHOUT_CLASSIFICATION	 end tests that check values from pig that are out range for target column 
WITHOUT_CLASSIFICATION	 process listhapeers 
WITHOUT_CLASSIFICATION	 required required required required required 
WITHOUT_CLASSIFICATION	 log with int input and double base 
WITHOUT_CLASSIFICATION	 resfile pctx roottasks fetchtask analyzer explainconfig cboinfo 
WITHOUT_CLASSIFICATION	 vectorized row batch reader 
WITHOUT_CLASSIFICATION	 subclasses must override this with function that implements the desired logic 
WITHOUT_CLASSIFICATION	 write information about the old value which becomes our next the beginning our new value 
WITHOUT_CLASSIFICATION	 multifile load for dynamic partitions when some partitions not need merge and they can simply moved the target directory 
WITHOUT_CLASSIFICATION	 dont validate column count encodings for vectors 
WITHOUT_CLASSIFICATION	 case spark the credential provider location provided the jobconf when the job submitted 
WITHOUT_CLASSIFICATION	 this checked ddlsemanticanalyzer 
WITHOUT_CLASSIFICATION	 for wdw specs that refer window defns inherit missing components 
WITHOUT_CLASSIFICATION	 row resolvers for input output 
WITHOUT_CLASSIFICATION	 lookup byte array key the hash multiset param keybytes byte array containing the key within range param keystart the offset the beginning the key param keylength the length the key param hashmultisetresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 the previous rules can pull projections through join operators 
WITHOUT_CLASSIFICATION	 create dummy partitions 
WITHOUT_CLASSIFICATION	 with extra structs 
WITHOUT_CLASSIFICATION	 call redundant cast then bail out casttrueboolean 
WITHOUT_CLASSIFICATION	 need remember the input object inspector that need know the input type 
WITHOUT_CLASSIFICATION	 negative range bigger than positive range there risk overflow here 
WITHOUT_CLASSIFICATION	 need remove those branches that ended with reducesinkoperator and the reducesinkoperators name not the same childreducername also the cloned work not the first remove all leaf operators except 
WITHOUT_CLASSIFICATION	 add original files obsolete list any 
WITHOUT_CLASSIFICATION	 delim chars 
WITHOUT_CLASSIFICATION	 parallelism shouldnt set for cartesian product vertex 
WITHOUT_CLASSIFICATION	 comparison 
WITHOUT_CLASSIFICATION	 already have the merge work corresponding this merge join operator 
WITHOUT_CLASSIFICATION	 the else expression either identityexpression column 
WITHOUT_CLASSIFICATION	 bigint 
WITHOUT_CLASSIFICATION	 would nice there was way determine quotes are needed 
WITHOUT_CLASSIFICATION	 debatable this correct but thats how its implemented 
WITHOUT_CLASSIFICATION	 have prefix with wildcard 
WITHOUT_CLASSIFICATION	 replace right key input ref 
WITHOUT_CLASSIFICATION	 vectorization 
WITHOUT_CLASSIFICATION	 scheduling run will happen which may may not pick this task the test 
WITHOUT_CLASSIFICATION	 using system classloader the parent using thread context 
WITHOUT_CLASSIFICATION	 use boundarytype boundaryamt sort key order behavior case current row any any scan forward until row such that rsk rsk end ridx following unb any any end partitionsize 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream long 
WITHOUT_CLASSIFICATION	 test the idempotent behavior open and commit txn 
WITHOUT_CLASSIFICATION	 stats aggregator not present clear the current aggregator stats for merge being performed stats already collected aggregator numrows etc are still valid however load file being performed the old stats collected aggregator are not valid might good idea clear them instead leaving wrong and old stats since hive maintain the old stats although may wrong for cbo purpose use flag columnstatsaccurate show the accuracy the stats 
WITHOUT_CLASSIFICATION	 local file system using pfile link 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader 
WITHOUT_CLASSIFICATION	 all the pending get requests should just requeued elsewhere note that never queue session reuse sessiontoreuse would null 
WITHOUT_CLASSIFICATION	 partitionname 
WITHOUT_CLASSIFICATION	 get the string representing action type its non default action type 
WITHOUT_CLASSIFICATION	 but kept unchanged throughout the operator tree for one row 
WITHOUT_CLASSIFICATION	 verification 
WITHOUT_CLASSIFICATION	 initialize stats publishing table for noscan which has only stats task the rest task following stats task initializes execdriverjava 
WITHOUT_CLASSIFICATION	 anchor the pattern the startend the whole string 
WITHOUT_CLASSIFICATION	 list need refactored out done only once 
WITHOUT_CLASSIFICATION	 shortcut length means fields 
WITHOUT_CLASSIFICATION	 get big table 
WITHOUT_CLASSIFICATION	 all checks passed return null 
WITHOUT_CLASSIFICATION	 dpp check actually refers same target column etc 
WITHOUT_CLASSIFICATION	 replace the output expression with the input expression that 
WITHOUT_CLASSIFICATION	 bitset array 
WITHOUT_CLASSIFICATION	 replace the synthetic predicate with true and bail out 
WITHOUT_CLASSIFICATION	 for serialization only 
WITHOUT_CLASSIFICATION	 make the conditional task the child the current leaf task 
WITHOUT_CLASSIFICATION	 unfortunately quick path lets scale updown 
WITHOUT_CLASSIFICATION	 pkfq relationship ndv selcolsourcestat superset what tscolstat 
WITHOUT_CLASSIFICATION	 not outer join the postcondition filters are empty the row passed them 
WITHOUT_CLASSIFICATION	 actually temp table does not support partitions cascade not applicable here 
WITHOUT_CLASSIFICATION	 precondition should always directory 
WITHOUT_CLASSIFICATION	 flag indicate these counters are subject change across different test runs 
WITHOUT_CLASSIFICATION	 numrecords fetch all records hence skip all the below checks when numrecords 
WITHOUT_CLASSIFICATION	 catchall due some exec time dependencies session state that would cause otherwise 
WITHOUT_CLASSIFICATION	 write the items the output stream 
WITHOUT_CLASSIFICATION	 test using the same cache where first rows are inserted then cache cleared next reuse the same cache and insert another rows and verify the cache stores correctly this simulates reusing the same cache over and over again 
WITHOUT_CLASSIFICATION	 the output partial aggregation list doubles representing the histogram being constructed the first element the list the userspecified number bins the histogram and the histogram itself represented pairs following the first element the list length should always odd 
WITHOUT_CLASSIFICATION	 revert default keystore path 
WITHOUT_CLASSIFICATION	 create partitions for the partitioned table 
WITHOUT_CLASSIFICATION	 min allow tez pick 
WITHOUT_CLASSIFICATION	 have data all for this part the stream could unneeded skip 
WITHOUT_CLASSIFICATION	 global contains includes individual modules can only contain additional includes 
WITHOUT_CLASSIFICATION	 need provide the minimum number columns read separator parser does not waste time 
WITHOUT_CLASSIFICATION	 since are closing the previous fsps record writers need see can get 
WITHOUT_CLASSIFICATION	 task currently running 
WITHOUT_CLASSIFICATION	 list and expressions that need distribute 
WITHOUT_CLASSIFICATION	 verify null output data entry not but rather the value specified design 
WITHOUT_CLASSIFICATION	 this class 
WITHOUT_CLASSIFICATION	 string length should work after enforcemaxlength 
WITHOUT_CLASSIFICATION	 safety check for get parentop although very unlikely that stack size less than there only one mergejoinoperator the stack 
WITHOUT_CLASSIFICATION	 the incoming vectorization context describes the input big table vectorized row batch 
WITHOUT_CLASSIFICATION	 export metadata delta bucket 
WITHOUT_CLASSIFICATION	 one more major compaction 
WITHOUT_CLASSIFICATION	 limit that for now 
WITHOUT_CLASSIFICATION	 column names lose the enumness this schema 
WITHOUT_CLASSIFICATION	 offset length 
WITHOUT_CLASSIFICATION	 get txn tables that are being written 
WITHOUT_CLASSIFICATION	 lookup byte array key the hash set param keybytes byte array containing the key within range param keystart the offset the beginning the key param keylength the length the key param hashsetresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 point for linear scan has been identified which point this value unset 
WITHOUT_CLASSIFICATION	 extrapolation not needed for columns noextracolumnnames 
WITHOUT_CLASSIFICATION	 config settings 
WITHOUT_CLASSIFICATION	 update the byte size the struct 
WITHOUT_CLASSIFICATION	 try merge rest operators 
WITHOUT_CLASSIFICATION	 this test requires disruptor jar classpath 
WITHOUT_CLASSIFICATION	 map join operator default has bucket cols and num reduce sinks 
WITHOUT_CLASSIFICATION	 type intervaldaytime storing nanosecond interval primitives 
WITHOUT_CLASSIFICATION	 matches only forwardoperators which are preceded some other operator the tree particular cant reducer and hence cannot one the forwardoperators 
WITHOUT_CLASSIFICATION	 only set the default catalog the client 
WITHOUT_CLASSIFICATION	 required required optional optional 
WITHOUT_CLASSIFICATION	 this the same the setchildren method below but for empty tables 
WITHOUT_CLASSIFICATION	 test that attempting unlock locks associated with transaction generates error 
WITHOUT_CLASSIFICATION	 move the clock forward and trigger run 
WITHOUT_CLASSIFICATION	 there are more than one skewed values 
WITHOUT_CLASSIFICATION	 run the cleaner thread until cache cleanuntil occupied 
WITHOUT_CLASSIFICATION	 copyfrom 
WITHOUT_CLASSIFICATION	 done 
WITHOUT_CLASSIFICATION	 contained within set msb 
WITHOUT_CLASSIFICATION	 update the stats which not require complete scan 
WITHOUT_CLASSIFICATION	 the bottom the call stack the front the array the elements are follows getstacktrace shouldskip caller test method 
WITHOUT_CLASSIFICATION	 input value serde needs array support different serde for different tags 
WITHOUT_CLASSIFICATION	 calculate filter propagation directions for each alias 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 tez will only localize file script operators the 
WITHOUT_CLASSIFICATION	 noop hmshander not needed this impl 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl 
WITHOUT_CLASSIFICATION	 this test for llap command authz added hive which require access for pass 
WITHOUT_CLASSIFICATION	 set timestamp before moving cmroot can avoid race condition remove the file before setting 
WITHOUT_CLASSIFICATION	 key design point for repl dump not have any txns older than current txn which dump runs this needed ensure that repl dump doesnt copy any data files written any open txns mainly for streaming ingest case where one delta file shall have data from txns may also have data inconsistency the ongoing txns doesnt have corresponding openwrite events captured which means catchup incremental phase wont able replicate those txns the logic wait for configured amount time see all open txns current txn getting abortedcommitted not then forcefully abort those txns just like 
WITHOUT_CLASSIFICATION	 evict all results grouped with this index cannot any key further the batch evict key from this batch the keys grouped with cannot earlier that that key 
WITHOUT_CLASSIFICATION	 total entries valid fake 
WITHOUT_CLASSIFICATION	 custom vertex edge custom vertex and custom edge but single input custom vertex custom edge and multi input custom vertex custom edge multi input 
WITHOUT_CLASSIFICATION	 optimize the loops pulling special end cases and global decisions like isescaped out 
WITHOUT_CLASSIFICATION	 the correct result that the blank value not there 
WITHOUT_CLASSIFICATION	 this will hit theres large number mapids single request determined the cache size further which case disk again 
WITHOUT_CLASSIFICATION	 expressions macro table used when deserialize the query from calcite plan 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that inner join multikey and only big table columns appear the join result hash multiset used 
WITHOUT_CLASSIFICATION	 not supposed compactable table 
WITHOUT_CLASSIFICATION	 inserted enforce bucketing sorting need remove since will not merge them single inserted enforce bucketingsorting will have bucketing column reduce sink key whereas inserted this optimization will have partition columns followed bucket number followed sort columns the reduce sink key since both key columns are not prefix subset will not merge them together resulting jobs 
WITHOUT_CLASSIFICATION	 temporarily 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 have bytes data for this part for now 
WITHOUT_CLASSIFICATION	 create new source files with same filenames 
WITHOUT_CLASSIFICATION	 confirm the batch sizes were the three calls create partitions 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktbl 
WITHOUT_CLASSIFICATION	 typeinfo 
WITHOUT_CLASSIFICATION	 comments 
WITHOUT_CLASSIFICATION	 test with txnabort 
WITHOUT_CLASSIFICATION	 expression does not have where clause there can common filter 
WITHOUT_CLASSIFICATION	 writer for producing row from input batch 
WITHOUT_CLASSIFICATION	 this map maintains the ptfinvocationspec for each ptf chain invocation this 
WITHOUT_CLASSIFICATION	 simulate unknown type 
WITHOUT_CLASSIFICATION	 check that files produced compaction still have the version marker 
WITHOUT_CLASSIFICATION	 are trying adding map joins handle skew keys and map join right now does not work with outer joins 
WITHOUT_CLASSIFICATION	 blah foo bar form 
WITHOUT_CLASSIFICATION	 after closing the path set null 
WITHOUT_CLASSIFICATION	 comma separated list work names used prefix 
WITHOUT_CLASSIFICATION	 create table and insert two file the same content 
WITHOUT_CLASSIFICATION	 only support changing the serde mapping and the state 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 looks for the most encrypted table location may return null there are not tables encrypted are not part hdfs 
WITHOUT_CLASSIFICATION	 lru extreme frequency accesses should ignored only order matters 
WITHOUT_CLASSIFICATION	 this the first pptf the chain and there partition specified then assume the user wants include the entire input partition 
WITHOUT_CLASSIFICATION	 filters are using index which should match rows 
WITHOUT_CLASSIFICATION	 test case sensitivity 
WITHOUT_CLASSIFICATION	 done processing far 
WITHOUT_CLASSIFICATION	 are just relay send pause encoded data producer 
WITHOUT_CLASSIFICATION	 equivalent java type for the backing structure need recurse and build list 
WITHOUT_CLASSIFICATION	 bgenjjtree fieldrequiredness 
WITHOUT_CLASSIFICATION	 close with null reporter 
WITHOUT_CLASSIFICATION	 these flags are intended only for the bigkey map work 
WITHOUT_CLASSIFICATION	 can only push down stuff which appears part pure conjunction reject case etc 
WITHOUT_CLASSIFICATION	 invoking init method basehandler this way since adds the retry logic case transient failures init method 
WITHOUT_CLASSIFICATION	 only support countsumminmax for the single count distinct optimization 
WITHOUT_CLASSIFICATION	 for bucket join testing 
WITHOUT_CLASSIFICATION	 not completely accurate since oob heartbeats could out 
WITHOUT_CLASSIFICATION	 this get should succeed because its variance within past maxvariance 
WITHOUT_CLASSIFICATION	 convert dphj 
WITHOUT_CLASSIFICATION	 maintain list nonnull column ids 
WITHOUT_CLASSIFICATION	 reporter member variable the operator class 
WITHOUT_CLASSIFICATION	 week 
WITHOUT_CLASSIFICATION	 want end the binary search 
WITHOUT_CLASSIFICATION	 for each bucket file big table get the corresponding bucket file name the small table more than partition the big table the mapping for each partition 
WITHOUT_CLASSIFICATION	 find all leaf tasks and make the ddltask dependent task all them 
WITHOUT_CLASSIFICATION	 weve found and its already been marked acquired 
WITHOUT_CLASSIFICATION	 then definitely this will end zero 
WITHOUT_CLASSIFICATION	 create the map needed 
WITHOUT_CLASSIFICATION	 note interestingly this would exclude llap app jars that the session adds for llap case course doesnt matter because vertices run llap and have those jars and moreover anyway dont localize jars for the vertices llap but theory this still crappy code that assumes theres one and only app jar 
WITHOUT_CLASSIFICATION	 update error for some session that was actually already killed 
WITHOUT_CLASSIFICATION	 check all fields start with key value then unflatten adding additional level nested key and value structs example keyreducesinkkeyint keyreducesinkkey int valuecolint becomes 
WITHOUT_CLASSIFICATION	 return null since this will handled special case 
WITHOUT_CLASSIFICATION	 check the contents the first row 
WITHOUT_CLASSIFICATION	 filter any has filter expression 
WITHOUT_CLASSIFICATION	 was empty stream 
WITHOUT_CLASSIFICATION	 consider approximate map side parallelism table data size 
WITHOUT_CLASSIFICATION	 giving 
WITHOUT_CLASSIFICATION	 main entrypointname 
WITHOUT_CLASSIFICATION	 strings are interned and can thus compared like this 
WITHOUT_CLASSIFICATION	 and should the deletedelta directory 
WITHOUT_CLASSIFICATION	 return the current blocks length 
WITHOUT_CLASSIFICATION	 should have some ast 
WITHOUT_CLASSIFICATION	 binary join 
WITHOUT_CLASSIFICATION	 name 
WITHOUT_CLASSIFICATION	 try the first again would not combined and wed retain the old base less files 
WITHOUT_CLASSIFICATION	 the sargs are closely tied 
WITHOUT_CLASSIFICATION	 delete the table from the database 
WITHOUT_CLASSIFICATION	 the new partition should similar the original partition 
WITHOUT_CLASSIFICATION	 note above looks funny because seems like were instantiating static var and then nonstatic var the rule but the reason this required because rules are not allowed static but wind needing initialized from static context bcompat initialzed static context but this rule initialized before the tests run and will pick initialized value bcompat 
WITHOUT_CLASSIFICATION	 big table value expressions apply all matching and nonmatching rows 
WITHOUT_CLASSIFICATION	 this set copy the arguments objects avoid serializing 
WITHOUT_CLASSIFICATION	 check table only should not exist 
WITHOUT_CLASSIFICATION	 the result not null the buffer was evicted during the move 
WITHOUT_CLASSIFICATION	 insert reduceside 
WITHOUT_CLASSIFICATION	 case parenthesis 
WITHOUT_CLASSIFICATION	 that create empty bucket files when needed but see hive 
WITHOUT_CLASSIFICATION	 execute all implementation variations 
WITHOUT_CLASSIFICATION	 param partinfolist return the size the list partitions throws see 
WITHOUT_CLASSIFICATION	 null projection 
WITHOUT_CLASSIFICATION	 join selectivity ndv 
WITHOUT_CLASSIFICATION	 the operator groupby and are referencing the grouping 
WITHOUT_CLASSIFICATION	 fast pass worked all txns were asked heartbeat were open expected 
WITHOUT_CLASSIFICATION	 multiply the integer quotient back out can subtract from the original get 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 group contains the columns needed need aggregate from children 
WITHOUT_CLASSIFICATION	 serialize the struct 
WITHOUT_CLASSIFICATION	 columnname column position map 
WITHOUT_CLASSIFICATION	 insert reduceside 
WITHOUT_CLASSIFICATION	 singlecolumn string specific declarations 
WITHOUT_CLASSIFICATION	 add primitive types 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 append filter tag 
WITHOUT_CLASSIFICATION	 generate sql stmts execute 
WITHOUT_CLASSIFICATION	 get some number 
WITHOUT_CLASSIFICATION	 connect any edges required for minmax pushdown 
WITHOUT_CLASSIFICATION	 uniform autoparallel maxed out 
WITHOUT_CLASSIFICATION	 only remove information column not key 
WITHOUT_CLASSIFICATION	 test that overflow produces null 
WITHOUT_CLASSIFICATION	 partitionnames 
WITHOUT_CLASSIFICATION	 constructs standard group plan there other subquery with the same group bydistinct keys there are aggregations representative query for the group and there group that representative query the data skewed the conf variable used control combining group bys into single reducer false 
WITHOUT_CLASSIFICATION	 this table needs converted 
WITHOUT_CLASSIFICATION	 collect name output columns which result function 
WITHOUT_CLASSIFICATION	 set backup task 
WITHOUT_CLASSIFICATION	 clienttool end the tasks have completed control back the tool 
WITHOUT_CLASSIFICATION	 logging configuration 
WITHOUT_CLASSIFICATION	 lower this for big key testing 
WITHOUT_CLASSIFICATION	 killthreads option has already done force shutdown need again 
WITHOUT_CLASSIFICATION	 nulls possible 
WITHOUT_CLASSIFICATION	 now trigger some needed optimization rules again 
WITHOUT_CLASSIFICATION	 test mode want the operation logs regardless the settings 
WITHOUT_CLASSIFICATION	 for har files 
WITHOUT_CLASSIFICATION	 create conf for nway 
WITHOUT_CLASSIFICATION	 test right input repeating 
WITHOUT_CLASSIFICATION	 sync start 
WITHOUT_CLASSIFICATION	 test failure user not set 
WITHOUT_CLASSIFICATION	 grantorname 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 add map all the referenced positions relative each input rel 
WITHOUT_CLASSIFICATION	 scale the raw data size split level based ratio split wrt file length 
WITHOUT_CLASSIFICATION	 sqlstate errorcode should set appropriate values 
WITHOUT_CLASSIFICATION	 just translate 
WITHOUT_CLASSIFICATION	 only right input repeating 
WITHOUT_CLASSIFICATION	 get the partitions for the table and populate the output 
WITHOUT_CLASSIFICATION	 create new sparktask for the specified sparkwork recursively compute 
WITHOUT_CLASSIFICATION	 see the path fsop that calls fsexists finalpath 
WITHOUT_CLASSIFICATION	 handle minimum integer case that doesnt have abs 
WITHOUT_CLASSIFICATION	 remembered the offset just after the key length the list record read the absolute offset the value 
WITHOUT_CLASSIFICATION	 the session cannot have been killed just now this happens after all the kills the current iteration would have cleared sessiontoreuse when killing this 
WITHOUT_CLASSIFICATION	 strings including single escaped characters 
WITHOUT_CLASSIFICATION	 remember the bad estimates for future reference 
WITHOUT_CLASSIFICATION	 the metric value must zeroed 
WITHOUT_CLASSIFICATION	 ctas 
WITHOUT_CLASSIFICATION	 forward conditional the survival the corresponding key currently indexes 
WITHOUT_CLASSIFICATION	 runs 
WITHOUT_CLASSIFICATION	 single row 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 probably dont have jobconf here but can still try 
WITHOUT_CLASSIFICATION	 check that destination does not exist otherwise will overwriting data 
WITHOUT_CLASSIFICATION	 this run pre post execution hook writes message sessionstateerr causing cached cachingprintstream being used run failure hook will write what has been cached the cachingprintstream sessionstateout for verification 
WITHOUT_CLASSIFICATION	 push next 
WITHOUT_CLASSIFICATION	 update creation metadata 
WITHOUT_CLASSIFICATION	 overflow 
WITHOUT_CLASSIFICATION	 note not rename serviceacl hadoop generates hosts setting name from this resulting collision with existing and bizarre errors these are read hadoop ipc you should check the usage and naming conventions blocked string hardcoded hadoop and defaults are enforced elsewhere hive 
WITHOUT_CLASSIFICATION	 nontemp tables should use underlying client 
WITHOUT_CLASSIFICATION	 expressions subq that are joined the outer query 
WITHOUT_CLASSIFICATION	 link the update repl state task with dependency collection task 
WITHOUT_CLASSIFICATION	 sourcetask for not changed currently but that might changed various optimizers autoconvertjoin for example 
WITHOUT_CLASSIFICATION	 delete some data this will generate only delete deltas and insert deltas deletedelta 
WITHOUT_CLASSIFICATION	 now lost finishable state 
WITHOUT_CLASSIFICATION	 figure out how many tasks want for each bucket 
WITHOUT_CLASSIFICATION	 repeated string groupvertices 
WITHOUT_CLASSIFICATION	 char tests 
WITHOUT_CLASSIFICATION	 when deleterecordid currrecordidinbatch have move look the next record the batch but before that can shortcircuit and skip the entire batch itself checking the deleterecordid lastrecordinbatch 
WITHOUT_CLASSIFICATION	 for file sink operator change the directory name 
WITHOUT_CLASSIFICATION	 task for given input return empty list with index 
WITHOUT_CLASSIFICATION	 update tab 
WITHOUT_CLASSIFICATION	 renamepartition event partitioned table 
WITHOUT_CLASSIFICATION	 should not reach here 
WITHOUT_CLASSIFICATION	 nonjavadoc see int int 
WITHOUT_CLASSIFICATION	 check these belong the same task and work with withindagpriority 
WITHOUT_CLASSIFICATION	 make sure dont reference any old buffer 
WITHOUT_CLASSIFICATION	 copy the data over that the internal state text will set 
WITHOUT_CLASSIFICATION	 prevent infinite loop 
WITHOUT_CLASSIFICATION	 truncate ascii maximum length large 
WITHOUT_CLASSIFICATION	 avgcollen 
WITHOUT_CLASSIFICATION	 create list with variables that match some the regexes 
WITHOUT_CLASSIFICATION	 required required required optional required 
WITHOUT_CLASSIFICATION	 nonrepeating input column use any nonnull values for unassigned rows 
WITHOUT_CLASSIFICATION	 note this not the calling user but rather the user under which this session will 
WITHOUT_CLASSIFICATION	 note that output storage handlers never sees partition columns data schema 
WITHOUT_CLASSIFICATION	 after analyzeinternal hiveop get set query since are passing ast for select query reset 
WITHOUT_CLASSIFICATION	 retrieving can expensive and unnecessary only when required 
WITHOUT_CLASSIFICATION	 containers likely come soon 
WITHOUT_CLASSIFICATION	 for each directory add once 
WITHOUT_CLASSIFICATION	 find the jdbc driver 
WITHOUT_CLASSIFICATION	 for bucket columns all the columns match the parent put them the bucket cols else add empty list for sort columns keep the subset all the columns long order maintained 
WITHOUT_CLASSIFICATION	 multikey value hash map optimized for vector map join the key stored the provided bytes uninterpreted 
WITHOUT_CLASSIFICATION	 determine partition level privileges should checked for input tables 
WITHOUT_CLASSIFICATION	 null considered numeric type for arithmetic operators 
WITHOUT_CLASSIFICATION	 besteffort check cannot good check against caller thread since refcount could still someone else locked this used for asserts and logs 
WITHOUT_CLASSIFICATION	 find the jar local maven repo for testing 
WITHOUT_CLASSIFICATION	 shall never happen 
WITHOUT_CLASSIFICATION	 this case current task the root tasks add this new task into root tasks and remove the current task from root tasks 
WITHOUT_CLASSIFICATION	 any event there and name known then dump the start and end logs 
WITHOUT_CLASSIFICATION	 create list operator nodes start the walking 
WITHOUT_CLASSIFICATION	 now handle actual returns sessions may returned the pool may trigger expires 
WITHOUT_CLASSIFICATION	 each column the row 
WITHOUT_CLASSIFICATION	 connect the work correctly 
WITHOUT_CLASSIFICATION	 this being read because dependency view 
WITHOUT_CLASSIFICATION	 either have user functions metastore old version filter names locally 
WITHOUT_CLASSIFICATION	 first call resultsetnext should return true 
WITHOUT_CLASSIFICATION	 handle null with selected use 
WITHOUT_CLASSIFICATION	 insert the number elements plus trigger evictions 
WITHOUT_CLASSIFICATION	 lesser than both nexttxnidntxnnext and minminhistorylevel mhlminopentxnid 
WITHOUT_CLASSIFICATION	 builds and starts the mini dfs and mapreduce clusters 
WITHOUT_CLASSIFICATION	 cannot backtrack any the columns bail out 
WITHOUT_CLASSIFICATION	 fact were adding this table map table 
WITHOUT_CLASSIFICATION	 process these first that can instantiate sessionstate appropriately 
WITHOUT_CLASSIFICATION	 map string string lazymap allows emptystring style key null but not null style key null 
WITHOUT_CLASSIFICATION	 the table name might null are retrieving the constraint from the side 
WITHOUT_CLASSIFICATION	 try store the first key topnhashes arent active always forward 
WITHOUT_CLASSIFICATION	 remove them from current spark work 
WITHOUT_CLASSIFICATION	 even with tez some jobs are run disable the flag the conf that the backend runs fully 
WITHOUT_CLASSIFICATION	 references keys the hashtable the index hash the key collisions are resolved using open addressing with quadratic probing reference format offset into writebuffers state byte has list flag part hash used optimize probing offset tail offset the first record for the key the one containing the key not necessary store bits particular optimize probing fact when always store the hash not necessary but have nothing else with them todo actually could also use few bits store for each wed stop earlier read collision need profile real queries 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 scale 
WITHOUT_CLASSIFICATION	 writes data out series chunks the form chunk sizechunk byteschunk sizechunk bytes closing the output stream will send final length chunk which will indicate end input 
WITHOUT_CLASSIFICATION	 add temp table info current session 
WITHOUT_CLASSIFICATION	 check the existing entry contains the new 
WITHOUT_CLASSIFICATION	 query specific info 
WITHOUT_CLASSIFICATION	 add ctrlb delimiter between the fields this necessary because for structs case delimiter provided hive automatically adds ctrlb default delimiter between fields 
WITHOUT_CLASSIFICATION	 orc store rows inside root struct hive writes this way when populate column vectors skip over the root struct 
WITHOUT_CLASSIFICATION	 batch batch 
WITHOUT_CLASSIFICATION	 store the vertex name the operator pipeline 
WITHOUT_CLASSIFICATION	 this column doesnt come from any table 
WITHOUT_CLASSIFICATION	 note these references are with respect the output 
WITHOUT_CLASSIFICATION	 todo reduce the duplicated code 
WITHOUT_CLASSIFICATION	 get the last repl corresponding all insert events except rename 
WITHOUT_CLASSIFICATION	 should try tolerate corruption default 
WITHOUT_CLASSIFICATION	 alias alias alias all can selected but overriden biggest one alias 
WITHOUT_CLASSIFICATION	 write the current set valid write ids for the operated acid tables into the conf file 
WITHOUT_CLASSIFICATION	 partition the bucketing column 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create the consumer encoded data will coordinate decoding cvbs 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream long 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 comparisons using strings for event ids wrong this should numbers since lexical string comparison and numeric comparision differ this requires broader change where return the dump long and not string fixing this here for now was observed one the builds where compareto results failure the assertion below 
WITHOUT_CLASSIFICATION	 aint 
WITHOUT_CLASSIFICATION	 method deserialize allocwriteidmessage instance 
WITHOUT_CLASSIFICATION	 the current buffer contains multiple parts split 
WITHOUT_CLASSIFICATION	 avoid storing headers with data since expect binary size allocations 
WITHOUT_CLASSIFICATION	 make null safe check the job submission has gone through and job valid 
WITHOUT_CLASSIFICATION	 drivercontext could released the query and close processes same 
WITHOUT_CLASSIFICATION	 result object 
WITHOUT_CLASSIFICATION	 done processing the operator 
WITHOUT_CLASSIFICATION	 this place compatible with the shufflehandler requests from shuffleinputs arrive with job prefix 
WITHOUT_CLASSIFICATION	 this tablescanoperator could part semijoin optimization 
WITHOUT_CLASSIFICATION	 hdfs counters should relatively consistent across test runs when compared local file system counters 
WITHOUT_CLASSIFICATION	 after processing all the nonstreaming groups batches with evaluategroupbatch and isgroupresultnull false the aggregation result value based 
WITHOUT_CLASSIFICATION	 call the operator specific close routine 
WITHOUT_CLASSIFICATION	 set the data structures before any notifications come 
WITHOUT_CLASSIFICATION	 class hcatpartitionspec 
WITHOUT_CLASSIFICATION	 finds column name hcatschema not found returns null 
WITHOUT_CLASSIFICATION	 there are any leadlag functions this expression tree setup duplicate evaluator for the arg the llfuncdesc initialize using the inputinfo provided for this expr tree set the duplicate evaluator the lludf instance 
WITHOUT_CLASSIFICATION	 first process the current key 
WITHOUT_CLASSIFICATION	 generating the final row count relying the basic comparator evaluation methods 
WITHOUT_CLASSIFICATION	 put record into compactionqueue and nothing 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 otherwise need make sure that there subquery any level 
WITHOUT_CLASSIFICATION	 cleanup structures 
WITHOUT_CLASSIFICATION	 this operator the last operator summarize the noninlined 
WITHOUT_CLASSIFICATION	 sample path worker the sequence number which will retained until session timeout worker does not respond due communication interruptions will retain the same sequence number when returns back session timeout expires the node will deleted and new addition the same node restart will get next sequence number 
WITHOUT_CLASSIFICATION	 define schema 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 use serialize the nonpartitioning columns the input row 
WITHOUT_CLASSIFICATION	 the logger 
WITHOUT_CLASSIFICATION	 create information for vector map operator the member onerootoperator has been set 
WITHOUT_CLASSIFICATION	 tables with various types 
WITHOUT_CLASSIFICATION	 for partitioned tables get the size all the partitions 
WITHOUT_CLASSIFICATION	 col col 
WITHOUT_CLASSIFICATION	 start the heartbeat after delay which shorter than the hivetxntimeout 
WITHOUT_CLASSIFICATION	 error code indicates proper shutdown 
WITHOUT_CLASSIFICATION	 all the getters try the metastore value name first not set try the hive value name 
WITHOUT_CLASSIFICATION	 the cluster running mode return null 
WITHOUT_CLASSIFICATION	 the number hive operations that are waiting enter the compile block 
WITHOUT_CLASSIFICATION	 benefit rows filtered from selectivity rows 
WITHOUT_CLASSIFICATION	 element for key byte hash table hashset 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 input fulltablename expected format dbnametablename 
WITHOUT_CLASSIFICATION	 grantor type for public role hence the null check 
WITHOUT_CLASSIFICATION	 get the largest table alias from order 
WITHOUT_CLASSIFICATION	 check only when its terminal state 
WITHOUT_CLASSIFICATION	 table node 
WITHOUT_CLASSIFICATION	 support some virtual columns vectorization for this table scan 
WITHOUT_CLASSIFICATION	 fetch the data from parquet data page for next call 
WITHOUT_CLASSIFICATION	 hive code has assertionerrors some cases want record what happens 
WITHOUT_CLASSIFICATION	 find the record identifier column there and return possibly new objectinspector that 
WITHOUT_CLASSIFICATION	 now rememember what supported for this query and any support that was 
WITHOUT_CLASSIFICATION	 test mode then the created extra log file containing only 
WITHOUT_CLASSIFICATION	 take filter pushdown into account while calculating splits this allows prune off regions immediately note that although the javadoc for the superclass getsplits says that returns one split per region the implementation actually takes the scan definition into account and excludes regions which dont satisfy 
WITHOUT_CLASSIFICATION	 the waitqueue avoid object comparison 
WITHOUT_CLASSIFICATION	 optional string eventtype 
WITHOUT_CLASSIFICATION	 datasource the table properties insertinsert overwrite the datasource name was already persisted otherwise ctctas and need get the name from the job properties that are set the druidstoragehandler 
WITHOUT_CLASSIFICATION	 clean the mapred work 
WITHOUT_CLASSIFICATION	 token remove the placeholder arg 
WITHOUT_CLASSIFICATION	 maptask and currtask should merged and joinunion operator genmrunion which has multiple topops assert maptask currtask maptaskid maptaskgetid currtaskid currtaskgetid 
WITHOUT_CLASSIFICATION	 add inputs ops remove 
WITHOUT_CLASSIFICATION	 major compaction check data files 
WITHOUT_CLASSIFICATION	 implied 
WITHOUT_CLASSIFICATION	 transformation the join operation 
WITHOUT_CLASSIFICATION	 make sure the hidden keys didnt get published 
WITHOUT_CLASSIFICATION	 testtablekey 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 maps application identifiers jobids the associated user for the app 
WITHOUT_CLASSIFICATION	 for now expecting single row minmax aggregated bloom filter rows 
WITHOUT_CLASSIFICATION	 verify that the new configs are effect 
WITHOUT_CLASSIFICATION	 this could overridden thru the jobconf 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check input objects length doesnt match then output new primitive with the correct params 
WITHOUT_CLASSIFICATION	 fetchnext execute the same sql again 
WITHOUT_CLASSIFICATION	 check the table file has header skip 
WITHOUT_CLASSIFICATION	 add constants there select top 
WITHOUT_CLASSIFICATION	 trigger failover minihs 
WITHOUT_CLASSIFICATION	 setup our outer join specific members 
WITHOUT_CLASSIFICATION	 see 
WITHOUT_CLASSIFICATION	 digit measuring stick 
WITHOUT_CLASSIFICATION	 return the child directly the conversion redundant 
WITHOUT_CLASSIFICATION	 set output column 
WITHOUT_CLASSIFICATION	 optional bytes initialeventbytes 
WITHOUT_CLASSIFICATION	 for cbo 
WITHOUT_CLASSIFICATION	 alternate 
WITHOUT_CLASSIFICATION	 which could lead lot extra unnecessary scratch columns 
WITHOUT_CLASSIFICATION	 were starting new command 
WITHOUT_CLASSIFICATION	 convert the absolute big decimal string 
WITHOUT_CLASSIFICATION	 form result from lower and high words 
WITHOUT_CLASSIFICATION	 window based aggregations are handled differently 
WITHOUT_CLASSIFICATION	 dynamic partition pruning 
WITHOUT_CLASSIFICATION	 add string value numdistinctvalue estimator 
WITHOUT_CLASSIFICATION	 execution mode 
WITHOUT_CLASSIFICATION	 check for map which occupies levels key separator and keyvalue pair separator 
WITHOUT_CLASSIFICATION	 firstname null 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 most the cases this column reference 
WITHOUT_CLASSIFICATION	 load hivesitexml from classpath 
WITHOUT_CLASSIFICATION	 should careful when authorizing table based just the table name columns have separate authorization domain 
WITHOUT_CLASSIFICATION	 numdvs 
WITHOUT_CLASSIFICATION	 note assume this never happens for serde reader the batch would never have vectors 
WITHOUT_CLASSIFICATION	 fill values down for equal value series 
WITHOUT_CLASSIFICATION	 methods for metrics integration each threadlocal perflogger will openclose scope during each perflog method 
WITHOUT_CLASSIFICATION	 oops this should have been caught before trying componentize 
WITHOUT_CLASSIFICATION	 cannot convert complex types 
WITHOUT_CLASSIFICATION	 test invalid table name 
WITHOUT_CLASSIFICATION	 constructors 
WITHOUT_CLASSIFICATION	 programmatically set root logger level info default logjtestproperties not available root logger will use error log level 
WITHOUT_CLASSIFICATION	 temporarily reusing the tez view acls property for individual dag access control hive may want setup its own parameters wants control per dag access setting the tezproperty per dag should work for now 
WITHOUT_CLASSIFICATION	 type 
WITHOUT_CLASSIFICATION	 exponent that derives from the fractional part under normal circumstatnces the negative the number digits however very long the last digits get dropped otherwise long with large negative exponent could cause unnecessary overflow alone this case fracexp incremented one for each dropped digit 
WITHOUT_CLASSIFICATION	 need some conversions here 
WITHOUT_CLASSIFICATION	 host dead before scheduling 
WITHOUT_CLASSIFICATION	 this also ensures that txn still there expected state 
WITHOUT_CLASSIFICATION	 nothing migration just validate that the tables automatically determine the table should managed external migrate tables external tables migrate tables managed transactional tables 
WITHOUT_CLASSIFICATION	 all partitions are filtered partition pruning 
WITHOUT_CLASSIFICATION	 there should new directory basexxxxxxx 
WITHOUT_CLASSIFICATION	 all the entries map are representing null then return true 
WITHOUT_CLASSIFICATION	 were done processing events 
WITHOUT_CLASSIFICATION	 semanticanalyzer inject sel before aggregation the columns this sel are derived from the table schema and not reflect the actual columns being selected the current query this case skip the merge and just use the path from the child ops 
WITHOUT_CLASSIFICATION	 load the defaults 
WITHOUT_CLASSIFICATION	 unknown field return well continue from the next field onwards 
WITHOUT_CLASSIFICATION	 the first version rcfile used the sequence file header 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 moving files depends the parenttask still want the dependencytask depend the parenttask 
WITHOUT_CLASSIFICATION	 check that aborted operation didnt become committed 
WITHOUT_CLASSIFICATION	 srsrwait lock are examining waiting this case keep looking its possible that something front blocking 
WITHOUT_CLASSIFICATION	 each iteration this loop tries split blocks from one level the free list into target size blocks cannot satisfy the allocation from the free list containing the 
WITHOUT_CLASSIFICATION	 this prevent dropping archived partition which archived 
WITHOUT_CLASSIFICATION	 dont test all the combinations because least currently the logic inherited the same testcollower which checks all the cases 
WITHOUT_CLASSIFICATION	 isnull 
WITHOUT_CLASSIFICATION	 note not change without changing the corresponding reference 
WITHOUT_CLASSIFICATION	 this stream will restarted with the same random seed over and over 
WITHOUT_CLASSIFICATION	 create some partitions 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 integer digits stop zeroes above 
WITHOUT_CLASSIFICATION	 last one for union column 
WITHOUT_CLASSIFICATION	 junk the destination for the pass 
WITHOUT_CLASSIFICATION	 with bucketing using two different versions version for exiting tables and version for new tables all the inputs the smb must from same version this only applies tables read directly and not 
WITHOUT_CLASSIFICATION	 the merge file being currently processed 
WITHOUT_CLASSIFICATION	 get the clusterby aliases these are aliased the entries the select list 
WITHOUT_CLASSIFICATION	 have row for current group indicate not the last batch 
WITHOUT_CLASSIFICATION	 copy data values over 
WITHOUT_CLASSIFICATION	 query might have been canceled stop the background processing 
WITHOUT_CLASSIFICATION	 create invalid table which has wrong column type 
WITHOUT_CLASSIFICATION	 initialize all forward operator 
WITHOUT_CLASSIFICATION	 add field collations 
WITHOUT_CLASSIFICATION	 finally add project project out the columns 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 source complexpbproto 
WITHOUT_CLASSIFICATION	 timebased 
WITHOUT_CLASSIFICATION	 alias confict should not happen here 
WITHOUT_CLASSIFICATION	 wait while for tasks respond being cancelled 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader long 
WITHOUT_CLASSIFICATION	 aggregate had count aggregate call the corresponding aggregate aggregate must sum for other aggregates remains the same 
WITHOUT_CLASSIFICATION	 initialize with data row type conversion parameters 
WITHOUT_CLASSIFICATION	 pass the null value along the escaping process determine what the dir should 
WITHOUT_CLASSIFICATION	 create embedded metastore 
WITHOUT_CLASSIFICATION	 recursively clone the children 
WITHOUT_CLASSIFICATION	 case the statement create table 
WITHOUT_CLASSIFICATION	 not expected access the same class 
WITHOUT_CLASSIFICATION	 determine need read this slice for the split 
WITHOUT_CLASSIFICATION	 convert dynamic arrays and maps simple arrays 
WITHOUT_CLASSIFICATION	 cached copy valid 
WITHOUT_CLASSIFICATION	 dot not affected 
WITHOUT_CLASSIFICATION	 offset within compression buffer where the row group begins 
WITHOUT_CLASSIFICATION	 this case both should dhj operators prschildmj can only guarantee 
WITHOUT_CLASSIFICATION	 hashjoin 
WITHOUT_CLASSIFICATION	 failed event should not create new notification 
WITHOUT_CLASSIFICATION	 database 
WITHOUT_CLASSIFICATION	 there branch remove prune sink operator branch the basework there branch remove the whole basework 
WITHOUT_CLASSIFICATION	 configured not ignore this 
WITHOUT_CLASSIFICATION	 traverse the txntowriteid see any the input txns already have allocated write for the same dbtable yes then need reuse else have allocate new one the write would have been already allocated case multistatement txns where 
WITHOUT_CLASSIFICATION	 grouping sets are involved early return 
WITHOUT_CLASSIFICATION	 uses string type for binary before 
WITHOUT_CLASSIFICATION	 lets just the safe expensive way 
WITHOUT_CLASSIFICATION	 consider should this for all vector expressions that can 
WITHOUT_CLASSIFICATION	 emit the rest word 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqltime 
WITHOUT_CLASSIFICATION	 test some extreme cases 
WITHOUT_CLASSIFICATION	 need this get the task path and set for mapred implementation since cant done automatically because mapreducemapred abstraction 
WITHOUT_CLASSIFICATION	 divide power equal scale logically shift the digits places right scale positions eliminate them 
WITHOUT_CLASSIFICATION	 have create the tables here rather than setup because need the hive connection which conveniently created the semantic analyzer 
WITHOUT_CLASSIFICATION	 reply copy only happens for jars hdfs not otherwise 
WITHOUT_CLASSIFICATION	 need close because some filesystem implementations flush noop close the file handle hdfs file but stderrstdout skip since webhcat not supposed close 
WITHOUT_CLASSIFICATION	 this the first batch process after switching from hash mode 
WITHOUT_CLASSIFICATION	 default lexicographicalasc 
WITHOUT_CLASSIFICATION	 check that the partition folders exist disk 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 http mode use nosasl the default auth type 
WITHOUT_CLASSIFICATION	 implicit casts possible 
WITHOUT_CLASSIFICATION	 name the class report for 
WITHOUT_CLASSIFICATION	 verify whether the sql operation log generated and fetch correctly 
WITHOUT_CLASSIFICATION	 note the following test will fail you are running this test root setting permission the database folder will not preclude root from being able create the necessary files 
WITHOUT_CLASSIFICATION	 row 
WITHOUT_CLASSIFICATION	 this default case with setugi off for both client and server 
WITHOUT_CLASSIFICATION	 comment passed table params 
WITHOUT_CLASSIFICATION	 xxx from oazktclientbase 
WITHOUT_CLASSIFICATION	 avoid the npe 
WITHOUT_CLASSIFICATION	 overwhelmingly executes once 
WITHOUT_CLASSIFICATION	 utility udfs 
WITHOUT_CLASSIFICATION	 inplace progress update related variables 
WITHOUT_CLASSIFICATION	 with nulls 
WITHOUT_CLASSIFICATION	 provide iterator the rows partition iterator exposes the index the next location client can invoke leadlag relative the next location 
WITHOUT_CLASSIFICATION	 ranges just return the empty list 
WITHOUT_CLASSIFICATION	 when converting from char stringvarchar strip any trailing spaces 
WITHOUT_CLASSIFICATION	 realign the columns appearing after startpostionsay column such that column becomes column offset column becomes column offset and 
WITHOUT_CLASSIFICATION	 classify leaf predicate equi non equi 
WITHOUT_CLASSIFICATION	 all outside elements should ignored from stat estimation 
WITHOUT_CLASSIFICATION	 one live session 
WITHOUT_CLASSIFICATION	 move the next child from tree 
WITHOUT_CLASSIFICATION	 wait for all futures complete check for abort while waiting for each future any the futures cancelled aborted cancel all subsequent futures 
WITHOUT_CLASSIFICATION	 new number register bits for higher accuracy 
WITHOUT_CLASSIFICATION	 emit the rest word 
WITHOUT_CLASSIFICATION	 lbstruct needs bytearrayref 
WITHOUT_CLASSIFICATION	 brute force may discard twice many buffers 
WITHOUT_CLASSIFICATION	 the field the configured string representing null 
WITHOUT_CLASSIFICATION	 there should delta dirs the location 
WITHOUT_CLASSIFICATION	 hive streaming ingest settings 
WITHOUT_CLASSIFICATION	 for select createasselect query populate the partition column parcols table columns mapping tabcols 
WITHOUT_CLASSIFICATION	 currently druid supports only mapbasedrow jackson serde should safe cast without check 
WITHOUT_CLASSIFICATION	 lint 
WITHOUT_CLASSIFICATION	 row 
WITHOUT_CLASSIFICATION	 minimum number clauses needed transform into clauses 
WITHOUT_CLASSIFICATION	 evaluate result for position using bytes avoid storage allocation costs and set position the output vector the result 
WITHOUT_CLASSIFICATION	 check that mpart does not exist but mpart still does 
WITHOUT_CLASSIFICATION	 use bucketized hive input format that makes sure that one mapper reads the entire file 
WITHOUT_CLASSIFICATION	 doesnt have notion tiny and saves the full value int overflow expectednull but was 
WITHOUT_CLASSIFICATION	 must the same file system the current destination 
WITHOUT_CLASSIFICATION	 atlas would interested lineage information for insertloadcreate etc 
WITHOUT_CLASSIFICATION	 row 
WITHOUT_CLASSIFICATION	 supervised kafka tasks should respect instead 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 methods that need data object this function key has the same structure the map expects most cases key will primitive type its rare cases that key not primitive the user responsible for defining 
WITHOUT_CLASSIFICATION	 clear out the set dont need anymore 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream int 
WITHOUT_CLASSIFICATION	 all the code paths below propagate nulls even arg has nulls this reduce the number code paths and shorten the code the expense maybe doing unnecessary work neither input has nulls this could improved the future expanding the number code paths 
WITHOUT_CLASSIFICATION	 check the join columns contains all bucket columns table bucketized column but the join key and easy see joining different buckets yield empty results 
WITHOUT_CLASSIFICATION	 since the exceptions and the range question overlap count the 
WITHOUT_CLASSIFICATION	 strip leading zeroes although there shouldnt any for decimal 
WITHOUT_CLASSIFICATION	 still valid nothing more 
WITHOUT_CLASSIFICATION	 simple case 
WITHOUT_CLASSIFICATION	 there are nulls the structcolumnvector 
WITHOUT_CLASSIFICATION	 initialize transaction table properties with default string value 
WITHOUT_CLASSIFICATION	 based 
WITHOUT_CLASSIFICATION	 calling close explicitly clean the staging dirs 
WITHOUT_CLASSIFICATION	 verify table for key byte hash table hashmultiset 
WITHOUT_CLASSIFICATION	 str 
WITHOUT_CLASSIFICATION	 add dummy data for not removed etc 
WITHOUT_CLASSIFICATION	 the mutex pools should ideally somewhat larger since some operations require connection from each pool and want avoid taking connection from primary pool and then blocking because mutex pool empty there only thread any hms trying mutex each mutexkey except mutexkeychecklock the checklock operation gets connection from connpool first then connpoolmutex all others the opposite order not very elegant number connection requests for connpoolmutex cannot exceed size connpool 
WITHOUT_CLASSIFICATION	 the collist the output columns used child operators they are different from input columns the current operator need find out which input columns are used 
WITHOUT_CLASSIFICATION	 which big table and small table columns are bytecolumnvector and need have their data buffer manually reset for some join result processing 
WITHOUT_CLASSIFICATION	 prepend column names with starts with 
WITHOUT_CLASSIFICATION	 this mapper operator used initialize all the operators 
WITHOUT_CLASSIFICATION	 minimum values 
WITHOUT_CLASSIFICATION	 hive external tables should not considered have uptodate stats 
WITHOUT_CLASSIFICATION	 possible that some other hms instance could have created the guid the same time due which this instance could not create guid above such case return the guid already generated 
WITHOUT_CLASSIFICATION	 host 
WITHOUT_CLASSIFICATION	 set kafka producer 
WITHOUT_CLASSIFICATION	 neither side repeating 
WITHOUT_CLASSIFICATION	 extract rows and call process per row 
WITHOUT_CLASSIFICATION	 the first small table 
WITHOUT_CLASSIFICATION	 this exception has been handled 
WITHOUT_CLASSIFICATION	 show that their positions are recorded 
WITHOUT_CLASSIFICATION	 exception thrown scheduled tasks further tasks will scheduled hence this ugly catch 
WITHOUT_CLASSIFICATION	 thread killing the query 
WITHOUT_CLASSIFICATION	 any additions the subqueries select clause 
WITHOUT_CLASSIFICATION	 offsetlimit smaller than rowcount the input operator thus return the offsetlimit 
WITHOUT_CLASSIFICATION	 add the part lastbatch track the parition being dropped 
WITHOUT_CLASSIFICATION	 there are more nodes signal timeout monitor start timer 
WITHOUT_CLASSIFICATION	 checksum failure 
WITHOUT_CLASSIFICATION	 test that separate databases dont coalesce 
WITHOUT_CLASSIFICATION	 all parsing done were now good start the export process 
WITHOUT_CLASSIFICATION	 check access columns from readentity 
WITHOUT_CLASSIFICATION	 second child node could partitionspec column 
WITHOUT_CLASSIFICATION	 figure out and encode what files need read this here rather than getsplits below because part this discover our minimum and maximum transactions and discovering that getsplits too late then have way pass our mapper 
WITHOUT_CLASSIFICATION	 write operation always start txn 
WITHOUT_CLASSIFICATION	 just ran major compaction should have basex tblname that has the new files 
WITHOUT_CLASSIFICATION	 copy full bucket context 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 need disable these that automatic merge doesnt merge the files 
WITHOUT_CLASSIFICATION	 existing table not view 
WITHOUT_CLASSIFICATION	 this semijoin branch the stack should look like 
WITHOUT_CLASSIFICATION	 try delete other directories possible 
WITHOUT_CLASSIFICATION	 process the list 
WITHOUT_CLASSIFICATION	 divide rows array into different sized batches modify the rows array for isrepeating null patterns provide iterator that will fill vrb with the divided rows 
WITHOUT_CLASSIFICATION	 add dependency between the two work items 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite partition with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 close shouldnt matter 
WITHOUT_CLASSIFICATION	 stringscompleter matches against predefined wordlist 
WITHOUT_CLASSIFICATION	 any filter sorted filter its sorted filter 
WITHOUT_CLASSIFICATION	 the intersection only support both kept 
WITHOUT_CLASSIFICATION	 make sure there dynamic addition virtual cols 
WITHOUT_CLASSIFICATION	 can converted tez event this sufficient decide preemption 
WITHOUT_CLASSIFICATION	 with vectorization 
WITHOUT_CLASSIFICATION	 accumulo iterators werent getting serialized into the inputsplit but can compensate because still have that info 
WITHOUT_CLASSIFICATION	 orc cannot anything here for now all the logic the proxy 
WITHOUT_CLASSIFICATION	 for stringcharvarchar buffering when there are escapes 
WITHOUT_CLASSIFICATION	 column names also can inferred from result udtf 
WITHOUT_CLASSIFICATION	 note tokwindowvalues means range type tokwindowrange means rows type 
WITHOUT_CLASSIFICATION	 build the final custom path string replacing each column name with 
WITHOUT_CLASSIFICATION	 will move all the files the tablepartition directories into the first directory then commit the first write 
WITHOUT_CLASSIFICATION	 shortcircuit quickly eat all rows 
WITHOUT_CLASSIFICATION	 gather the common factors and return them 
WITHOUT_CLASSIFICATION	 create list topop nodes 
WITHOUT_CLASSIFICATION	 caller looking for temp table handle here otherwise pass underlying client 
WITHOUT_CLASSIFICATION	 reference already accounted for the directsize 
WITHOUT_CLASSIFICATION	 this fulfill the contract the interface which states that exception shall thrown when saslserver cannot created due error but null should returned when server cant created due the parameters supplied and the only thing plainsaslserver can fail nonsupported authentication mechanism thats why return null instead throwing the exception 
WITHOUT_CLASSIFICATION	 production 
WITHOUT_CLASSIFICATION	 normal 
WITHOUT_CLASSIFICATION	 for external tables not need anything else 
WITHOUT_CLASSIFICATION	 simulate concurrent session 
WITHOUT_CLASSIFICATION	 wait for the delete complete 
WITHOUT_CLASSIFICATION	 gather columns used aggregate operator 
WITHOUT_CLASSIFICATION	 after insert into operation get the last repl 
WITHOUT_CLASSIFICATION	 the sender will take care this the value didnt change 
WITHOUT_CLASSIFICATION	 obtain the byte buffer from the input string can traverse code point code point 
WITHOUT_CLASSIFICATION	 reset the parameters can compare 
WITHOUT_CLASSIFICATION	 antisymmetric 
WITHOUT_CLASSIFICATION	 overwrite buffer size and stripe size for delta writer based basedeltaratio 
WITHOUT_CLASSIFICATION	 for arrow serde 
WITHOUT_CLASSIFICATION	 make sure minihs not leader 
WITHOUT_CLASSIFICATION	 write estimated count 
WITHOUT_CLASSIFICATION	 have propagated the value the task 
WITHOUT_CLASSIFICATION	 call open read data split mockmocktable 
WITHOUT_CLASSIFICATION	 make sure nullscanfilesystem can loaded hive 
WITHOUT_CLASSIFICATION	 get the forward 
WITHOUT_CLASSIFICATION	 this the last range for this compression block yay 
WITHOUT_CLASSIFICATION	 not dynamic partitioning then bail out 
WITHOUT_CLASSIFICATION	 folder 
WITHOUT_CLASSIFICATION	 null then the major version number should match 
WITHOUT_CLASSIFICATION	 must deterministic order set for consistent qtest output across java versions 
WITHOUT_CLASSIFICATION	 conversion requires source placed writable can call upon vectorassignrow convert and assign the row column 
WITHOUT_CLASSIFICATION	 walk the tree long the operators between the union and the filesink not involve reducer and they can pushed above the union makes sense push them above the union and remove the union interface has been added the operator denote whether 
WITHOUT_CLASSIFICATION	 instanceof instanceof instanceof 
WITHOUT_CLASSIFICATION	 this version hadoop does not support getpassword just retrieve password from conf 
WITHOUT_CLASSIFICATION	 create materialized view 
WITHOUT_CLASSIFICATION	 write data the target 
WITHOUT_CLASSIFICATION	 some tests control the execution the background update thread 
WITHOUT_CLASSIFICATION	 copy within row 
WITHOUT_CLASSIFICATION	 this without checking for parents 
WITHOUT_CLASSIFICATION	 tail 
WITHOUT_CLASSIFICATION	 boolean 
WITHOUT_CLASSIFICATION	 just alter the table 
WITHOUT_CLASSIFICATION	 the input the gby has tab alias for the column then add entry based that tabalias for this query select count from group needs tabaliasb colaliasx the gby tabaliasb comes from looking the rowresolver that the ancestor before any gbyreducesinks added for the gby operation 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 write null field 
WITHOUT_CLASSIFICATION	 add empty string the list aliases some operators groupby add 
WITHOUT_CLASSIFICATION	 mark partitions with new schema with different blurb 
WITHOUT_CLASSIFICATION	 filecache might empty see can remove trywritelock 
WITHOUT_CLASSIFICATION	 backing store for this cache 
WITHOUT_CLASSIFICATION	 assume that there are locked blocks the list they are they can dropped therefore always evict one contiguous sequence from the tail can find one pass splice out and then finalize the eviction outside the list lock 
WITHOUT_CLASSIFICATION	 this zero need check might become zero after scaling down this not easy because there might rounding 
WITHOUT_CLASSIFICATION	 committed anymore 
WITHOUT_CLASSIFICATION	 rewrite logic filter references correlated field its filter condition rewrite the filter filter joincross product originalfilterinput distinct sets correlated variables and rewrite the correlated fieldaccess the filter condition reference the join output filter does not reference correlated variables simply rewrite the filter condition using new input 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 dont need update tmp paths when there depth difference paths 
WITHOUT_CLASSIFICATION	 end addpartitions tests 
WITHOUT_CLASSIFICATION	 assumes that getmapvalueelement object will work with key from object the equality implemented fully the greaterthanlessthan values not implement transitive relation 
WITHOUT_CLASSIFICATION	 convert nonacidorctbl acid table with splitupdate enabled txnpropsdefault 
WITHOUT_CLASSIFICATION	 populated column struct array map types struct type contains schema the struct array type contains schema one the elements 
WITHOUT_CLASSIFICATION	 example dump dirs need able handle for hivereplrootdir staging then repl dumps will created stagingdumpdir singledbdump stagingblah will contain dir for the specified blah default metadata tbl metadata files tbl tbl unptntbl metadata files multidbdump stagingbar will contain dirs for each covered staging bar default sales single tabledump stagingbaz will contain table object dump inside staging baz metadata files incremental dump stagingblue will contain dirs for each event inside staging blue 
WITHOUT_CLASSIFICATION	 files exists input path then has this code path gets triggered only order queries which expected write only one file written one reducer 
WITHOUT_CLASSIFICATION	 null null 
WITHOUT_CLASSIFICATION	 batches will sized 
WITHOUT_CLASSIFICATION	 first give this task and put back the original list 
WITHOUT_CLASSIFICATION	 complains you reset already set value just dont care 
WITHOUT_CLASSIFICATION	 only support querytime merge for tables dont handle this 
WITHOUT_CLASSIFICATION	 ddl time munging thrift mode 
WITHOUT_CLASSIFICATION	 parse out the number histogram bins only once havent already done before need least bins otherwise there point creating histogram 
WITHOUT_CLASSIFICATION	 data stream could empty stream already reached end stream before present stream this can happen all values stream are nulls last row group values are all null 
WITHOUT_CLASSIFICATION	 make sure fail the channel something goes wrong internally handle all the expected exceptions log lot information here 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 cant find hadoop home can least try usrbinhadoop 
WITHOUT_CLASSIFICATION	 delete any contents the warehouse dir 
WITHOUT_CLASSIFICATION	 nothing done here 
WITHOUT_CLASSIFICATION	 optional string groupname 
WITHOUT_CLASSIFICATION	 big input cumulative row count 
WITHOUT_CLASSIFICATION	 tasks qualify preemptable 
WITHOUT_CLASSIFICATION	 not removing this here will removed when taken off the queue and discovered have pending tasks 
WITHOUT_CLASSIFICATION	 for condition independently compute and update stats 
WITHOUT_CLASSIFICATION	 read table stats via cachedstore 
WITHOUT_CLASSIFICATION	 comment out these checks when are happy 
WITHOUT_CLASSIFICATION	 check metaexception has one any exception thrown from objectstore which means that the authorization checks passed 
WITHOUT_CLASSIFICATION	 disable filter pushdown for mapreduce when there are more than one table aliases 
WITHOUT_CLASSIFICATION	 columns 
WITHOUT_CLASSIFICATION	 clone the filesinkdesc the final filesink and create similar filesinks 
WITHOUT_CLASSIFICATION	 set havent done before 
WITHOUT_CLASSIFICATION	 permissions set 
WITHOUT_CLASSIFICATION	 these are insert events original txn current txn for all rows 
WITHOUT_CLASSIFICATION	 add all data for element listcolumnvector get out the loop there data the data for new element 
WITHOUT_CLASSIFICATION	 rebuilt the querydef why that the exprnodedescriptors the querydef are based the select operators rowresolver 
WITHOUT_CLASSIFICATION	 expected nextwriteid doesnt have entry for this table and hence directly insert 
WITHOUT_CLASSIFICATION	 user privileges testdb testtable testtable testtable testtable testdb 
WITHOUT_CLASSIFICATION	 checklock makes diagnostics easier 
WITHOUT_CLASSIFICATION	 dont release the buffer contains any data beyond the acceptable boundary 
WITHOUT_CLASSIFICATION	 not using the path child cache here there could more than path per host worker and slot znodes 
WITHOUT_CLASSIFICATION	 include shim and admin specified libjars 
WITHOUT_CLASSIFICATION	 cannot merge bail out 
WITHOUT_CLASSIFICATION	 ignore file with same content already exist cmroot 
WITHOUT_CLASSIFICATION	 try serialize 
WITHOUT_CLASSIFICATION	 normalized display width based maximum display size and label size 
WITHOUT_CLASSIFICATION	 first lookup the file sink operator from the load work 
WITHOUT_CLASSIFICATION	 test code path 
WITHOUT_CLASSIFICATION	 always create partition and virtual columns 
WITHOUT_CLASSIFICATION	 the validateptfoperator method will update vectorptfdesc 
WITHOUT_CLASSIFICATION	 ordinal position this column the schema 
WITHOUT_CLASSIFICATION	 entityname 
WITHOUT_CLASSIFICATION	 sanity check all tasks must submit events for succeed 
WITHOUT_CLASSIFICATION	 null signifies nodes that are irrelevant the generation 
WITHOUT_CLASSIFICATION	 this config contains all the configuration that master node wants provide 
WITHOUT_CLASSIFICATION	 having key select where minbvalue 
WITHOUT_CLASSIFICATION	 oid for kerberos principal name 
WITHOUT_CLASSIFICATION	 next verify that the destination table not offline nonnative table 
WITHOUT_CLASSIFICATION	 failed try clean the metastore 
WITHOUT_CLASSIFICATION	 here there attempt set transactional something other than true and not the same value was before 
WITHOUT_CLASSIFICATION	 unit test convenience method for putting key and value into the hash table using the actual types 
WITHOUT_CLASSIFICATION	 batchindex classname match duplicate 
WITHOUT_CLASSIFICATION	 last function name replicated null function replication was progress when created this state 
WITHOUT_CLASSIFICATION	 used for listprovided cases 
WITHOUT_CLASSIFICATION	 swsracquired lock are examining acquired need keep looking because there may may not another shared write front 
WITHOUT_CLASSIFICATION	 binary keys values and hashcodes rows lined index 
WITHOUT_CLASSIFICATION	 prepare new query string 
WITHOUT_CLASSIFICATION	 and convert string yay 
WITHOUT_CLASSIFICATION	 call this reinitialize the node stack called automatically the parsers reinit method 
WITHOUT_CLASSIFICATION	 both are not empty merge two lists 
WITHOUT_CLASSIFICATION	 left and right repeat and right null 
WITHOUT_CLASSIFICATION	 transaction manager used for the query this will set compile time based 
WITHOUT_CLASSIFICATION	 get the path names for the row only 
WITHOUT_CLASSIFICATION	 here each group looks something like map reducer just parse the numbers and ignore one from map and from its there 
WITHOUT_CLASSIFICATION	 there only one element tuple contained bag throw away the tuple 
WITHOUT_CLASSIFICATION	 put the script content temp file 
WITHOUT_CLASSIFICATION	 new state not running user not active any more 
WITHOUT_CLASSIFICATION	 bucketedsorted columns for the destination table 
WITHOUT_CLASSIFICATION	 default that its not repl scope default full exportimport not metadataonly 
WITHOUT_CLASSIFICATION	 nothing lock mode 
WITHOUT_CLASSIFICATION	 set nonzk lock manager avoid trying connect zookeeper 
WITHOUT_CLASSIFICATION	 data structures store original values 
WITHOUT_CLASSIFICATION	 not all partitions are scanned all mappers this could null 
WITHOUT_CLASSIFICATION	 two int 
WITHOUT_CLASSIFICATION	 this wraps instance exprnodedesc and makes equals work like issame see comment 
WITHOUT_CLASSIFICATION	 success file required for oozie indicate availability data source 
WITHOUT_CLASSIFICATION	 create the databases and reload them from the metastore 
WITHOUT_CLASSIFICATION	 start all mrmultimr inputs 
WITHOUT_CLASSIFICATION	 file dir for each for now plus export data dir this could changed flat file list later 
WITHOUT_CLASSIFICATION	 update 
WITHOUT_CLASSIFICATION	 same 
WITHOUT_CLASSIFICATION	 old api assumed all partitions belong the same table keep the same assumption 
WITHOUT_CLASSIFICATION	 the writing thread has given object wait 
WITHOUT_CLASSIFICATION	 astringintint can matched with methods like astringintint astringintinteger astringintegerint and 
WITHOUT_CLASSIFICATION	 open connections 
WITHOUT_CLASSIFICATION	 tez session settings 
WITHOUT_CLASSIFICATION	 now vary isrepeating nulls possible only left 
WITHOUT_CLASSIFICATION	 case external table drop the table contents well 
WITHOUT_CLASSIFICATION	 only one result column 
WITHOUT_CLASSIFICATION	 the heartbeat consistent with what have 
WITHOUT_CLASSIFICATION	 get the order aliases these are aliased the entries the select list 
WITHOUT_CLASSIFICATION	 primarykeycols 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 inner classes defined after this point 
WITHOUT_CLASSIFICATION	 the authorization check 
WITHOUT_CLASSIFICATION	 determine are dealing with numeric date arithmetic operation 
WITHOUT_CLASSIFICATION	 maintain count outstanding requests for tokenidentifier 
WITHOUT_CLASSIFICATION	 remove entries from txncomponents there may aborted txn components 
WITHOUT_CLASSIFICATION	 for better performance longdouble dont want the conditional statements inside the for loop 
WITHOUT_CLASSIFICATION	 acquire lock for the given materialized view only one rebuild per materialized view can triggered given time otherwise might produce incorrect results incremental maintenance triggered 
WITHOUT_CLASSIFICATION	 prevent view cycles 
WITHOUT_CLASSIFICATION	 aid testing through files authenticator passed argument the interface this helps being able switch the user within session need check the user has changed 
WITHOUT_CLASSIFICATION	 second request for host single invocation since the last has not completed 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 next one new key bail out from the inner loop 
WITHOUT_CLASSIFICATION	 loop iulrindex obtaining digits onebyone paper 
WITHOUT_CLASSIFICATION	 return new hash set result implementation specific object the object can used access access spill information when the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 descending 
WITHOUT_CLASSIFICATION	 ignore the outdated updates for the same version ignore nonnull updates because assume that removal the last thing that happens for any given version 
WITHOUT_CLASSIFICATION	 getconvertedoi with caching store settable properties the object inspector caching might help when the object inspector contains complex nested data types caching not explicitly required for the returned object inspector across multiple invocations since the already takes care 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 check the comparison supported for this type 
WITHOUT_CLASSIFICATION	 convert the constants available strings the corresponding objects 
WITHOUT_CLASSIFICATION	 try preempting lower priority task any case 
WITHOUT_CLASSIFICATION	 support decimal 
WITHOUT_CLASSIFICATION	 abstract class hold info required for the implementation 
WITHOUT_CLASSIFICATION	 add listener handle any cleanup for when the connection closed 
WITHOUT_CLASSIFICATION	 float 
WITHOUT_CLASSIFICATION	 for after tname for the end version between 
WITHOUT_CLASSIFICATION	 creates scratch directories needed the context object 
WITHOUT_CLASSIFICATION	 the following conditions are for native vector reducesink 
WITHOUT_CLASSIFICATION	 handles expr like structkeyvaluekey follows same rules which equivalent version parsing such expression from ast 
WITHOUT_CLASSIFICATION	 does not need type conversions because 
WITHOUT_CLASSIFICATION	 setup the hadoop vars specify the user 
WITHOUT_CLASSIFICATION	 that already exist existingparts will not generate notifications 
WITHOUT_CLASSIFICATION	 since integer always some products here are not included 
WITHOUT_CLASSIFICATION	 support for statistics yet 
WITHOUT_CLASSIFICATION	 write the empty base 
WITHOUT_CLASSIFICATION	 queryload can contain insert overwrite 
WITHOUT_CLASSIFICATION	 calculate selectivity 
WITHOUT_CLASSIFICATION	 werent interrupted just propagate the error 
WITHOUT_CLASSIFICATION	 hiveobject 
WITHOUT_CLASSIFICATION	 ssl conf 
WITHOUT_CLASSIFICATION	 optional 
WITHOUT_CLASSIFICATION	 for both graceful forceful shutdown wait for tasks terminate such that appropriate exceptions are raised and stored 
WITHOUT_CLASSIFICATION	 generate the map work for this aliasid pass both confirmed and unknown partitions through the mapreduce 
WITHOUT_CLASSIFICATION	 safety checks 
WITHOUT_CLASSIFICATION	 url was not specified with but was present use that 
WITHOUT_CLASSIFICATION	 timeseries query results 
WITHOUT_CLASSIFICATION	 sets delimiter tab ascii 
WITHOUT_CLASSIFICATION	 throw error the user asked for bucketed mapjoin enforced and 
WITHOUT_CLASSIFICATION	 reset the time only want count the loop 
WITHOUT_CLASSIFICATION	 set permissions appropriately for each the partitions just created 
WITHOUT_CLASSIFICATION	 this only there for the use case where are doing table only replication and not database level 
WITHOUT_CLASSIFICATION	 setting repl dump and repl load all requiring admin privileges might wind loosening this the future but right now not want individual object based checks every object possible and thus asking for broad privilege such this the best route forward repl status should use privileges similar describe dbtable and asks for 
WITHOUT_CLASSIFICATION	 this class used verify that hivemetastore calls the nontransactional listeners with the current event set the class 
WITHOUT_CLASSIFICATION	 the group operator has already been processed 
WITHOUT_CLASSIFICATION	 spilled tables are loaded always sharing clear 
WITHOUT_CLASSIFICATION	 truncate byte array maximum number characters and return byte array with only truncated bytes 
WITHOUT_CLASSIFICATION	 set lineage info 
WITHOUT_CLASSIFICATION	 remove the tag for inmemory side mapjoin 
WITHOUT_CLASSIFICATION	 the data does not belong recognized chunk split wrong 
WITHOUT_CLASSIFICATION	 referencing another table instead self for the primary key 
WITHOUT_CLASSIFICATION	 derby fails creating multiple stats aggregator concurrently 
WITHOUT_CLASSIFICATION	 mybinary 
WITHOUT_CLASSIFICATION	 headernames 
WITHOUT_CLASSIFICATION	 few over 
WITHOUT_CLASSIFICATION	 get information from yarn service 
WITHOUT_CLASSIFICATION	 metaexception 
WITHOUT_CLASSIFICATION	 mssql this means communication link failure 
WITHOUT_CLASSIFICATION	 initially all columns are projected and the same order 
WITHOUT_CLASSIFICATION	 maxweight 
WITHOUT_CLASSIFICATION	 lock 
WITHOUT_CLASSIFICATION	 are good 
WITHOUT_CLASSIFICATION	 while updating local structures note this actually called under the epic writelock 
WITHOUT_CLASSIFICATION	 each parent 
WITHOUT_CLASSIFICATION	 this works assuming the executor running within yarn 
WITHOUT_CLASSIFICATION	 done 
WITHOUT_CLASSIFICATION	 clear vertices list 
WITHOUT_CLASSIFICATION	 print out the uncompressed sizes each column 
WITHOUT_CLASSIFICATION	 here only deals with nonpartition columns deal with partition columns next 
WITHOUT_CLASSIFICATION	 test the last day month 
WITHOUT_CLASSIFICATION	 subtract for twos compliment adjustment 
WITHOUT_CLASSIFICATION	 gives list any new paths that may have been created maintain the persistent ephemeral node 
WITHOUT_CLASSIFICATION	 mybitint 
WITHOUT_CLASSIFICATION	 write the delimiter the stream which means dont need outputformatstring anymore 
WITHOUT_CLASSIFICATION	 unionmstringstring 
WITHOUT_CLASSIFICATION	 red hat centosubuntudebian oel sles 
WITHOUT_CLASSIFICATION	 couldnt create the tracker node dont create the main node 
WITHOUT_CLASSIFICATION	 new session 
WITHOUT_CLASSIFICATION	 for left outer join join and skip further 
WITHOUT_CLASSIFICATION	 not boolean columns possible which case return false count 
WITHOUT_CLASSIFICATION	 executed twice once with the typed setters once with the generic setobject 
WITHOUT_CLASSIFICATION	 the last columns are vcol and 
WITHOUT_CLASSIFICATION	 verify outputs 
WITHOUT_CLASSIFICATION	 the avg result type has the same number integer digits and more decimal digits 
WITHOUT_CLASSIFICATION	 logj utilize gcfree layoutencode method taken care superclass 
WITHOUT_CLASSIFICATION	 test valid case 
WITHOUT_CLASSIFICATION	 the roundpower same scale means all zeroes below round point 
WITHOUT_CLASSIFICATION	 get empty container when the small table empty 
WITHOUT_CLASSIFICATION	 todo extend taskrunner see api with callbacks will work 
WITHOUT_CLASSIFICATION	 set the value the writable from the decimal digits that were written with dot 
WITHOUT_CLASSIFICATION	 asian character bytes 
WITHOUT_CLASSIFICATION	 mixture input big table columns and new scratch columns 
WITHOUT_CLASSIFICATION	 add some margin the wait avoid rechecking close the boundary 
WITHOUT_CLASSIFICATION	 the data not cache 
WITHOUT_CLASSIFICATION	 drive the doprocessbatch logic with the same batch but different grouping set and null variation performance note not try reuse columns and generate the keywrappers anew 
WITHOUT_CLASSIFICATION	 invoke delete leader endpoint for failover 
WITHOUT_CLASSIFICATION	 check the schema table with one field partition keys 
WITHOUT_CLASSIFICATION	 walk hive ast and translate the hive column names their equivalent mappings this basically cheat 
WITHOUT_CLASSIFICATION	 resolve parse tree 
WITHOUT_CLASSIFICATION	 whether the method takes array like object string etc the last argument 
WITHOUT_CLASSIFICATION	 delete source overwrite destination 
WITHOUT_CLASSIFICATION	 otherwise countnull should directly return 
WITHOUT_CLASSIFICATION	 note only used from index related classes 
WITHOUT_CLASSIFICATION	 store the partitions temporarily until processed 
WITHOUT_CLASSIFICATION	 all represented seconds 
WITHOUT_CLASSIFICATION	 boolean long min and max 
WITHOUT_CLASSIFICATION	 the number tez tasks executed the hiveserver since the last restart 
WITHOUT_CLASSIFICATION	 numtxns 
WITHOUT_CLASSIFICATION	 lvmerge the above order 
WITHOUT_CLASSIFICATION	 force spark configs cloned default 
WITHOUT_CLASSIFICATION	 canhandleqbforcbo returns null the query can handled 
WITHOUT_CLASSIFICATION	 temporary work arrays 
WITHOUT_CLASSIFICATION	 get list with the current classpath components 
WITHOUT_CLASSIFICATION	 specifies 
WITHOUT_CLASSIFICATION	 replace the commar 
WITHOUT_CLASSIFICATION	 keycount 
WITHOUT_CLASSIFICATION	 pad output 
WITHOUT_CLASSIFICATION	 localdatetime immutable 
WITHOUT_CLASSIFICATION	 this case can happen with llap able deserialize and cache data from the input format will deliver that cached data vrbs 
WITHOUT_CLASSIFICATION	 quote always for fields that mimic sql keywords like desc 
WITHOUT_CLASSIFICATION	 need register minimum open txnid for current transactions into minhistory table 
WITHOUT_CLASSIFICATION	 get the vector description from the operator 
WITHOUT_CLASSIFICATION	 divide and round 
WITHOUT_CLASSIFICATION	 best answer would 
WITHOUT_CLASSIFICATION	 path file specified can relative treat target file name hadoop put behavior 
WITHOUT_CLASSIFICATION	 delete partition level column stats exists 
WITHOUT_CLASSIFICATION	 required required optional required optional optional 
WITHOUT_CLASSIFICATION	 new method that distributes the scan query creating splits containing information about different druid nodes that have the data for the given query 
WITHOUT_CLASSIFICATION	 nunknown hive type infoot map 
WITHOUT_CLASSIFICATION	 shallow copy aliastoopinfo wont want clone the operator tree here 
WITHOUT_CLASSIFICATION	 describe table partition 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the writer local the process 
WITHOUT_CLASSIFICATION	 generate the list bucketing pruning predicate separate clauses containing the partitioning and nonpartitioning columns 
WITHOUT_CLASSIFICATION	 clean the temp files 
WITHOUT_CLASSIFICATION	 could null theres race between the threads processing requests with dag finish processed earlier 
WITHOUT_CLASSIFICATION	 counters for debugging cannot use existing counters cntr and nextcntr 
WITHOUT_CLASSIFICATION	 metastore has record this lock most likely timed out either way there point tracking here any longer 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this would refresh any conf resources and also local resources 
WITHOUT_CLASSIFICATION	 the jobtracker has exceeded its threshold and doing 
WITHOUT_CLASSIFICATION	 are modifying divisor here make local copy 
WITHOUT_CLASSIFICATION	 dcname 
WITHOUT_CLASSIFICATION	 check for list found recursively init its members 
WITHOUT_CLASSIFICATION	 this thread was interrupted this case the call might return sooner than long polling timeout 
WITHOUT_CLASSIFICATION	 update description 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject int int 
WITHOUT_CLASSIFICATION	 future this may examine readentity andor config return appropriate hcatreader 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 within the proxy object wrap the method call 
WITHOUT_CLASSIFICATION	 token found 
WITHOUT_CLASSIFICATION	 offset truncation 
WITHOUT_CLASSIFICATION	 the beginning the list record will the value length 
WITHOUT_CLASSIFICATION	 opentxns 
WITHOUT_CLASSIFICATION	 all keys vcol vcolc 
WITHOUT_CLASSIFICATION	 check for this pattern the pattern matching could simplified rules can applied during decorrelation correlaterelleft correlation condition true leftinputrel projecta rexnode aggregate groupby agg agg 
WITHOUT_CLASSIFICATION	 see paper for alpha initialization 
WITHOUT_CLASSIFICATION	 size sparse map excess the threshold convert the sparse map dense register and switch dense encoding 
WITHOUT_CLASSIFICATION	 query not compiled state executing state which carried over from combined compileexecute runinternal throws the error 
WITHOUT_CLASSIFICATION	 heartbeat the lockid first assure that our lock still valid then look the lock info hopefully the cache these locks 
WITHOUT_CLASSIFICATION	 number distribution keys crs chosen only when numdistkeys prs less all other cases distribution the keys based the prs which more generic than crs examples case prs sort key and crs sort key and number distribution keys are and resp then after merge the sort keys will while the number distribution keys will case prs sort key empty and number distribution keys and crs sort key and number distribution keys then after merge new sort key will and number distribution keys will 
WITHOUT_CLASSIFICATION	 test stringstring version 
WITHOUT_CLASSIFICATION	 get pushdown predicates for this operators predicate 
WITHOUT_CLASSIFICATION	 check there exists bloom filter size entry 
WITHOUT_CLASSIFICATION	 initialization isnt finished until all parents all operators are initialized for broadcast joins that means initializing the 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 validate principals 
WITHOUT_CLASSIFICATION	 first shot without waiting 
WITHOUT_CLASSIFICATION	 have more input but did start with something valid 
WITHOUT_CLASSIFICATION	 precisionscale exceed maximum precision result must adjusted 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check the group operator has already been processed 
WITHOUT_CLASSIFICATION	 merge the result last evaluate previous evaluate 
WITHOUT_CLASSIFICATION	 throws exception not initialized 
WITHOUT_CLASSIFICATION	 evaluate the aggregation functions over the group batch 
WITHOUT_CLASSIFICATION	 this entry the pathenv directory see the required file this directory 
WITHOUT_CLASSIFICATION	 insert row nonacid table 
WITHOUT_CLASSIFICATION	 can not judge assuming replication not enabled 
WITHOUT_CLASSIFICATION	 list 
WITHOUT_CLASSIFICATION	 this simply verify that the hooks were fact run 
WITHOUT_CLASSIFICATION	 row resolver for the operator 
WITHOUT_CLASSIFICATION	 yarn ats 
WITHOUT_CLASSIFICATION	 this just for smb join usecase the numbuckets would equal that the big table and the small table could have lesser number buckets this case want send the data from the right buckets the big table side for big table has buckets and small table has buckets bucket small table needs sent bucket the big table 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 two objects 
WITHOUT_CLASSIFICATION	 decimal 
WITHOUT_CLASSIFICATION	 writable class 
WITHOUT_CLASSIFICATION	 test functionality 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 invariant bucketcol literals type bucketfield 
WITHOUT_CLASSIFICATION	 this breaks all the tests files 
WITHOUT_CLASSIFICATION	 the one big table rows values repeat 
WITHOUT_CLASSIFICATION	 keydne 
WITHOUT_CLASSIFICATION	 map avros primitive types hives for those that are supported both 
WITHOUT_CLASSIFICATION	 this close function does not need synchronized since called its parents main thread 
WITHOUT_CLASSIFICATION	 assumption batchindex increasing was called 
WITHOUT_CLASSIFICATION	 insert overwrite 
WITHOUT_CLASSIFICATION	 use complexnewbuilder construct 
WITHOUT_CLASSIFICATION	 tblnames 
WITHOUT_CLASSIFICATION	 eof 
WITHOUT_CLASSIFICATION	 select query results 
WITHOUT_CLASSIFICATION	 zero dividend 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 adding mysql jdbc driver exists 
WITHOUT_CLASSIFICATION	 should also remove when pulling from accumulo 
WITHOUT_CLASSIFICATION	 get input path and remove this alias from pathtoalias 
WITHOUT_CLASSIFICATION	 column indexes corresponding data storage layer 
WITHOUT_CLASSIFICATION	 hive attempt fold expression like where case sssolddate when then else null end where sssolddate 
WITHOUT_CLASSIFICATION	 indicates read buffer has data number rows the temporary read buffer cursor during reading total number objects output 
WITHOUT_CLASSIFICATION	 verify 
WITHOUT_CLASSIFICATION	 add jars containing the specified classes 
WITHOUT_CLASSIFICATION	 this method takes input operator and subset its output column names and generates the input column names for the operator corresponding those outputs the mapping from the input column name the output column name not simple the method returns false else returns true the list output column names modified this method the list corresponding input column names 
WITHOUT_CLASSIFICATION	 after load from this dump all target tablespartitions will have initial set data 
WITHOUT_CLASSIFICATION	 this method returns the flip bigendian representation value 
WITHOUT_CLASSIFICATION	 since integer always some products here are not included 
WITHOUT_CLASSIFICATION	 only precision specified 
WITHOUT_CLASSIFICATION	 make deep copy the aliases that they are not changed the context 
WITHOUT_CLASSIFICATION	 rewrite logic permute the group keys the front the input aggregate produces correlated variables add them the group list change aggcalls reference the new project 
WITHOUT_CLASSIFICATION	 some the strings can passed unicode for example the delimiter can passed first check the string unicode number else back the old behavior 
WITHOUT_CLASSIFICATION	 has the user explicitly asked not sample this table 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 children means were the bottom there are more operators scan 
WITHOUT_CLASSIFICATION	 maxed out capacity this move should fail the session 
WITHOUT_CLASSIFICATION	 this will also move the iterator ahead one code point 
WITHOUT_CLASSIFICATION	 end the month behavior 
WITHOUT_CLASSIFICATION	 look for file which can move the existing file with external services its possible for the service marked complete after each fragment 
WITHOUT_CLASSIFICATION	 continue 
WITHOUT_CLASSIFICATION	 doclipping 
WITHOUT_CLASSIFICATION	 converts timestamp timestamptz 
WITHOUT_CLASSIFICATION	 reset tableprops with reloaded tblproperties 
WITHOUT_CLASSIFICATION	 prspjoincrs 
WITHOUT_CLASSIFICATION	 production 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 tables have been replicated over and verified identical now couple alters the source 
WITHOUT_CLASSIFICATION	 skip check for import here because already handle above ctas check 
WITHOUT_CLASSIFICATION	 store the ugi transport and then continue usual 
WITHOUT_CLASSIFICATION	 for unpartitioned table partitionvals are not specified 
WITHOUT_CLASSIFICATION	 delete 
WITHOUT_CLASSIFICATION	 extract the launcher job submitstart time and use that scope down the search interval when look for child jobs 
WITHOUT_CLASSIFICATION	 finishtime 
WITHOUT_CLASSIFICATION	 then finishable must always precede nonfinishable 
WITHOUT_CLASSIFICATION	 still the local jvm use the usernamepassword kerberos credentials 
WITHOUT_CLASSIFICATION	 init 
WITHOUT_CLASSIFICATION	 override with user defined properties 
WITHOUT_CLASSIFICATION	 apply isnull and instr not supported pushdown via name filtering 
WITHOUT_CLASSIFICATION	 locks present 
WITHOUT_CLASSIFICATION	 lets take look the operators were checking for user code those will not run that llap 
WITHOUT_CLASSIFICATION	 the registration znode 
WITHOUT_CLASSIFICATION	 first try quickly lock some the correctsized free lists and allocate from them 
WITHOUT_CLASSIFICATION	 adjust file column index for orc struct 
WITHOUT_CLASSIFICATION	 remove this server instance from zookeeper dynamic service discovery set 
WITHOUT_CLASSIFICATION	 not want keep state two separate places remove from hive table properties 
WITHOUT_CLASSIFICATION	 done 
WITHOUT_CLASSIFICATION	 for temporary tables set the location something the sessions scratch dir has the same life cycle the tmp table 
WITHOUT_CLASSIFICATION	 toepochmilli returns utc time regardless 
WITHOUT_CLASSIFICATION	 resulting output object inspector can used make the rowresolver 
WITHOUT_CLASSIFICATION	 the new objectinspector the same the old one directly return true 
WITHOUT_CLASSIFICATION	 power with double power 
WITHOUT_CLASSIFICATION	 replace back 
WITHOUT_CLASSIFICATION	 orcrecordupdaterrow this somewhat fragile note this guarantees that physical column ids are order 
WITHOUT_CLASSIFICATION	 dont want include the root struct acid case would cause the whole struct get read without projection 
WITHOUT_CLASSIFICATION	 refer 
WITHOUT_CLASSIFICATION	 negative means the key didnt exist the original stream changed the tree 
WITHOUT_CLASSIFICATION	 determine minimum all nonnull decimal column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 cant escaped the separator 
WITHOUT_CLASSIFICATION	 skip the auth embedded mode the auth disabled 
WITHOUT_CLASSIFICATION	 same file different offset 
WITHOUT_CLASSIFICATION	 restore repeating and nulls indicators 
WITHOUT_CLASSIFICATION	 for inner joins may apply the filters now 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 need create some extra sessions wed just like startup does 
WITHOUT_CLASSIFICATION	 improved later 
WITHOUT_CLASSIFICATION	 todo this test method the first run then the parameters does not contain totalsize and numfiles this runs after other tests setupdropdatabase successful then the 
WITHOUT_CLASSIFICATION	 weve added insert clauses order when items whenclauses 
WITHOUT_CLASSIFICATION	 most operations cannot run asynchronously 
WITHOUT_CLASSIFICATION	 should emit exception 
WITHOUT_CLASSIFICATION	 callback separate thread that when task completes the thread the main queue 
WITHOUT_CLASSIFICATION	 filter the list locations those that have least the 
WITHOUT_CLASSIFICATION	 events for events for 
WITHOUT_CLASSIFICATION	 ignore empty strings 
WITHOUT_CLASSIFICATION	 when partition column type not string the values from will null 
WITHOUT_CLASSIFICATION	 apply the plan again enable 
WITHOUT_CLASSIFICATION	 unpartitioned table 
WITHOUT_CLASSIFICATION	 test february nonleap year 
WITHOUT_CLASSIFICATION	 class partitionspecproxy 
WITHOUT_CLASSIFICATION	 get failed attempts from jobfailuresjsp 
WITHOUT_CLASSIFICATION	 simd loop 
WITHOUT_CLASSIFICATION	 get list temp table names 
WITHOUT_CLASSIFICATION	 case thread count set use single thread 
WITHOUT_CLASSIFICATION	 then iterate through all the operators that have candidate fks again assume the first joining with the that just selected and apply the pkfk relationship when compute the newrows and ndv after that join the result with all the other fks not assume the pkfk relationship anymore and just compute the 
WITHOUT_CLASSIFICATION	 case error reset the file system object 
WITHOUT_CLASSIFICATION	 replicate all the events happened after bootstrap 
WITHOUT_CLASSIFICATION	 executorruntime etc 
WITHOUT_CLASSIFICATION	 runasync querytimeout makes sense only for sqloperation pass the original statement sqloperation sql parser can remove comments itself 
WITHOUT_CLASSIFICATION	 the big table has more buckets than the current small table use mod get small table bucket names for example the big table has buckets and the small table has buckets then the mapping should 
WITHOUT_CLASSIFICATION	 the reader pointer has moved the end next block the end current record 
WITHOUT_CLASSIFICATION	 need some extra checks get the running owner 
WITHOUT_CLASSIFICATION	 yearsmonths represented months 
WITHOUT_CLASSIFICATION	 buddy block allocated higher level allocation than are have reached the top level add whatever have got the current free list 
WITHOUT_CLASSIFICATION	 not public since must have the field count column sort order information 
WITHOUT_CLASSIFICATION	 create simple table and test create drop get 
WITHOUT_CLASSIFICATION	 repeated nulls skip this input column 
WITHOUT_CLASSIFICATION	 changes the owner group and verify the change 
WITHOUT_CLASSIFICATION	 the default different the client and server its null here 
WITHOUT_CLASSIFICATION	 compare set fields 
WITHOUT_CLASSIFICATION	 the cases create partition the time this event fires the partition object has not yet come into existence and thus will not yet have location but these are needed create qlmetadatapartition use the tables the only place this used the authorization hooks will not affect code flow the metastore itself 
WITHOUT_CLASSIFICATION	 and finally return them flat array 
WITHOUT_CLASSIFICATION	 the following deltas includes all kinds delta files including insert delete deltas 
WITHOUT_CLASSIFICATION	 drop table will clean the table entry from the compaction queue and hence cleaner have effect 
WITHOUT_CLASSIFICATION	 many restrictions 
WITHOUT_CLASSIFICATION	 check user have erroneously specified nonexistent partitioning columns 
WITHOUT_CLASSIFICATION	 manually modify the underlying metastore reflect statistics corresponding 
WITHOUT_CLASSIFICATION	 the rangeinputsplit should have all the necesary information contained which alleviates from reparsing our configuration from the and resetting into the configuration like did getsplits thus should unnecessary reinvoke configure 
WITHOUT_CLASSIFICATION	 for asc nulls first for desc nulls last 
WITHOUT_CLASSIFICATION	 all good 
WITHOUT_CLASSIFICATION	 fully specified partition spec 
WITHOUT_CLASSIFICATION	 ensure data structures are updated the main taskscheduler 
WITHOUT_CLASSIFICATION	 retry with path 
WITHOUT_CLASSIFICATION	 filter are the output and the same position 
WITHOUT_CLASSIFICATION	 txnididtxnupdate 
WITHOUT_CLASSIFICATION	 execution mode vectorized 
WITHOUT_CLASSIFICATION	 such abc 
WITHOUT_CLASSIFICATION	 special case both constants are not equal then return 
WITHOUT_CLASSIFICATION	 honor custom location for external table apart from what metadata specifies 
WITHOUT_CLASSIFICATION	 dont use the caches copy the buffer 
WITHOUT_CLASSIFICATION	 the decimal value currently valid 
WITHOUT_CLASSIFICATION	 update pathsi from queryid 
WITHOUT_CLASSIFICATION	 read should get rows immutable mutable 
WITHOUT_CLASSIFICATION	 search like binary search minimize comparisons 
WITHOUT_CLASSIFICATION	 this doing lot copying here this could improved enforcing length the same time escaping rather than separate steps 
WITHOUT_CLASSIFICATION	 when data prematurely ended the fieldposition will more than the end 
WITHOUT_CLASSIFICATION	 see the alias has lateral view chain the lateral view operator 
WITHOUT_CLASSIFICATION	 element for key long hash table hashset 
WITHOUT_CLASSIFICATION	 need openssl 
WITHOUT_CLASSIFICATION	 only input side has nulls 
WITHOUT_CLASSIFICATION	 sort columns make output deterministic 
WITHOUT_CLASSIFICATION	 crs being used for distinct the two reduce sinks are incompatible 
WITHOUT_CLASSIFICATION	 replicate the changes the replicatedtable 
WITHOUT_CLASSIFICATION	 add the distinct aggregate columns the groupby columns 
WITHOUT_CLASSIFICATION	 design note the future this function can implemented directly translate input output without creating new objects performance can probably improved significantly its implemented the simplest way now just calling the existing builtin function 
WITHOUT_CLASSIFICATION	 bgenjjtree flagargs 
WITHOUT_CLASSIFICATION	 txnididtxnupdate 
WITHOUT_CLASSIFICATION	 not need connect its parent its counterpart they have the same parents 
WITHOUT_CLASSIFICATION	 archived partitions have hartoharfile their location the original directory was saved params 
WITHOUT_CLASSIFICATION	 patterns that are included execution logging level execution mode show only select logger messages 
WITHOUT_CLASSIFICATION	 return constant vector expression 
WITHOUT_CLASSIFICATION	 not include the dummy grouping set column the output pass outputkeylength instead 
WITHOUT_CLASSIFICATION	 the tokens should ignore when are trying table masking 
WITHOUT_CLASSIFICATION	 this batch full break out for loop execute 
WITHOUT_CLASSIFICATION	 given byte array consisting serialized bloomkfilter gives the offset from for the start the serialized long values that make the bitset 
WITHOUT_CLASSIFICATION	 not read thus records 
WITHOUT_CLASSIFICATION	 txnididtxnupdate 
WITHOUT_CLASSIFICATION	 the jsonobject for this operator 
WITHOUT_CLASSIFICATION	 basic case 
WITHOUT_CLASSIFICATION	 intentionally using deprecated method 
WITHOUT_CLASSIFICATION	 disable llap wrapper doesnt propagate extra acid columns correctly 
WITHOUT_CLASSIFICATION	 default executors max slots noconditional task size will oversubscribed 
WITHOUT_CLASSIFICATION	 delete the incorrectly copied file and retry with path 
WITHOUT_CLASSIFICATION	 map integer integer 
WITHOUT_CLASSIFICATION	 hiveconfdir not defined file not found hiveconfdir then check hivehomeconf 
WITHOUT_CLASSIFICATION	 create partcount dummy partitions 
WITHOUT_CLASSIFICATION	 nothing done 
WITHOUT_CLASSIFICATION	 over all the aggregation classes and and get the size the fields fixed length keep track the variable length 
WITHOUT_CLASSIFICATION	 check how much memory left memory 
WITHOUT_CLASSIFICATION	 unregister for the dirwatcher for the specific dagidentifier either case 
WITHOUT_CLASSIFICATION	 txnididtxnupdate 
WITHOUT_CLASSIFICATION	 tests with queries which cannot executed with directsql because type mismatch the type the num column string but the parameters used the where clause are numbers after falling back orm the number partitions cannot fetched the method they are fetched the method 
WITHOUT_CLASSIFICATION	 while done 
WITHOUT_CLASSIFICATION	 current vector map operator read type and context 
WITHOUT_CLASSIFICATION	 set needed cols tsdesc 
WITHOUT_CLASSIFICATION	 serialize some data the schema before altered 
WITHOUT_CLASSIFICATION	 logger with default base 
WITHOUT_CLASSIFICATION	 another single call get all the partition objects 
WITHOUT_CLASSIFICATION	 rows qualify 
WITHOUT_CLASSIFICATION	 only user belonging admin role can list role 
WITHOUT_CLASSIFICATION	 this should not happen 
WITHOUT_CLASSIFICATION	 demuxoperator should have least one child 
WITHOUT_CLASSIFICATION	 currently not support merging windowing functions with other windowing functions embedding windowing functions within each other 
WITHOUT_CLASSIFICATION	 set values look for 
WITHOUT_CLASSIFICATION	 check whether will end with same operators inputing same work work merge work work work work cannot merge the reason the same above currently tez does not support parallel edges the check exclude the inputs the root operator that are trying 
WITHOUT_CLASSIFICATION	 both were the same and can replaced the new were loading into 
WITHOUT_CLASSIFICATION	 this join has been converted smb join the hive optimizer the user did not give mapjoin hint the query the hive optimizer figured out that the join can 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite table with new exclusive coalesces 
WITHOUT_CLASSIFICATION	 binary conversions supported genericudfdecimal genericudftimestamp 
WITHOUT_CLASSIFICATION	 todo couldshould trace seek destinations pps needs peek method 
WITHOUT_CLASSIFICATION	 source prep 
WITHOUT_CLASSIFICATION	 get the scale factor turn big decimal into decimal this relies the bigdecimal precision value which hive 
WITHOUT_CLASSIFICATION	 cancel the timer 
WITHOUT_CLASSIFICATION	 cache the results for table authorization 
WITHOUT_CLASSIFICATION	 only set output dir partition fully materialized 
WITHOUT_CLASSIFICATION	 column buffers otherwise the end when closeop called things get printed multiple times 
WITHOUT_CLASSIFICATION	 fall through delta 
WITHOUT_CLASSIFICATION	 not not 
WITHOUT_CLASSIFICATION	 keep column expression map explain plan uses this display 
WITHOUT_CLASSIFICATION	 this set 
WITHOUT_CLASSIFICATION	 proceed with binary search 
WITHOUT_CLASSIFICATION	 for once actually want reference equality java 
WITHOUT_CLASSIFICATION	 should convert the datetime the format hive understands default either yyyymmdd hhmmss seconds since epoch date crdatetimeepoch dgettime 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 first row all nulls 
WITHOUT_CLASSIFICATION	 dont need this for now not support 
WITHOUT_CLASSIFICATION	 following mapping enable udfname udf while generating expression for default value operator tree 
WITHOUT_CLASSIFICATION	 have repeated value the sum increases value batchsize 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 always set the explain conditions 
WITHOUT_CLASSIFICATION	 verify that the table does not already exist dumptable only used check the conflict for nontemporary tables 
WITHOUT_CLASSIFICATION	 objectinspector 
WITHOUT_CLASSIFICATION	 data from delta 
WITHOUT_CLASSIFICATION	 setup cache enabled 
WITHOUT_CLASSIFICATION	 the init method hmshandler throws exception for the first time while creating retryinghmshandler should retried 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 setting 
WITHOUT_CLASSIFICATION	 use this constructor when there output column 
WITHOUT_CLASSIFICATION	 generate groupby operator hash mode for mapside partial 
WITHOUT_CLASSIFICATION	 return true this task ancestor itself parameter desc 
WITHOUT_CLASSIFICATION	 skip all the events belong other dbstables 
WITHOUT_CLASSIFICATION	 lookup key hashcode hashcode 
WITHOUT_CLASSIFICATION	 need merging the move local file system 
WITHOUT_CLASSIFICATION	 the sequence must prsselcrs 
WITHOUT_CLASSIFICATION	 these operators need linked enable runtime statistics gatheredused correctly 
WITHOUT_CLASSIFICATION	 remove bloomfilter stats generated 
WITHOUT_CLASSIFICATION	 for now disable operating decimal column vectors for semijoin reduction have make sure same decimal type should used during bloom filter creation and bloom filter probing 
WITHOUT_CLASSIFICATION	 since repeating only happens when offset length 
WITHOUT_CLASSIFICATION	 optional authzid 
WITHOUT_CLASSIFICATION	 testlazybinaryfast source rows serde serdefewer primitivetypeinfos useincludecolumns true dowritefewercolumns true 
WITHOUT_CLASSIFICATION	 exact same state above 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 optional string hivequeryid 
WITHOUT_CLASSIFICATION	 groupby for the grouping set corresponding the rollup 
WITHOUT_CLASSIFICATION	 optimize running value expressions only over the matched rows 
WITHOUT_CLASSIFICATION	 not have location locked someone else 
WITHOUT_CLASSIFICATION	 look the current sessions setting 
WITHOUT_CLASSIFICATION	 tree form tokptblfunction name alias partitioningspec arguments can tablereference subquery another ptf invocation 
WITHOUT_CLASSIFICATION	 boolean that says whether tez auto reduce parallelism should used 
WITHOUT_CLASSIFICATION	 this plugin avoid getting serialized event runtime 
WITHOUT_CLASSIFICATION	 leaving this the hive catalog rather than choosing the default from the configuration because all the default udfs are that catalog and think thats would people really want here 
WITHOUT_CLASSIFICATION	 byte 
WITHOUT_CLASSIFICATION	 needs refresh param passed should return new object 
WITHOUT_CLASSIFICATION	 create ssl socket and connect 
WITHOUT_CLASSIFICATION	 hivetablescan not found not sequence project and filter operators execute the original getuniquekeys method 
WITHOUT_CLASSIFICATION	 inppartspec mapping from partition column name its value 
WITHOUT_CLASSIFICATION	 writer should match the orc configuration from the original file 
WITHOUT_CLASSIFICATION	 will remember completed dag for hour avoid execution outoforder fragments 
WITHOUT_CLASSIFICATION	 get column type 
WITHOUT_CLASSIFICATION	 backtrack key exprs child parent and compare with parents 
WITHOUT_CLASSIFICATION	 highest priority this point should have come out 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaneturl 
WITHOUT_CLASSIFICATION	 always positive 
WITHOUT_CLASSIFICATION	 remove branch 
WITHOUT_CLASSIFICATION	 are going create the map for each view the given database 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 returns true the aggregation result will streamed 
WITHOUT_CLASSIFICATION	 old bucketing logic 
WITHOUT_CLASSIFICATION	 hive insert overwrite local directory uses task dir name setting the jobconf helps have the similar dir name 
WITHOUT_CLASSIFICATION	 make sure one calls this 
WITHOUT_CLASSIFICATION	 else osx gives ugly temp path which screws approvals 
WITHOUT_CLASSIFICATION	 group the list dedup 
WITHOUT_CLASSIFICATION	 blank byte blank byte blank byte asian character bytes 
WITHOUT_CLASSIFICATION	 this will stop run from pushing records along with potentially blocking initialization 
WITHOUT_CLASSIFICATION	 how much can read from current read buffer out what need 
WITHOUT_CLASSIFICATION	 input path operator context 
WITHOUT_CLASSIFICATION	 wait for scheduling run few times 
WITHOUT_CLASSIFICATION	 enforce certain order when the reutilization particular use size table number reads 
WITHOUT_CLASSIFICATION	 singlecolumn long get key 
WITHOUT_CLASSIFICATION	 message must transacted before return 
WITHOUT_CLASSIFICATION	 push filter top children for retainable 
WITHOUT_CLASSIFICATION	 mark any scratch small table scratch columns that would normally receive copy the key null too 
WITHOUT_CLASSIFICATION	 the input should textinputformat 
WITHOUT_CLASSIFICATION	 set nonzk lock manager prevent from trying connect 
WITHOUT_CLASSIFICATION	 retry logic 
WITHOUT_CLASSIFICATION	 test 
WITHOUT_CLASSIFICATION	 verify moveonlytask optimized 
WITHOUT_CLASSIFICATION	 keep the dynamic partition context conditional task resolver context 
WITHOUT_CLASSIFICATION	 public boolean droppartitionstring dbname string tablename string partname boolean deletedata boolean ifpurge throws metaexception texception return droppartitiondbname tablename partname deletedata ifpurge null 
WITHOUT_CLASSIFICATION	 lowest field 
WITHOUT_CLASSIFICATION	 other failure testcases 
WITHOUT_CLASSIFICATION	 cannot validate the list may unset 
WITHOUT_CLASSIFICATION	 new state changed running from something else user active 
WITHOUT_CLASSIFICATION	 deal with the last entry 
WITHOUT_CLASSIFICATION	 the number reduce tasks per job hadoop sets this value default setting this property hive will automatically determine the correct 
WITHOUT_CLASSIFICATION	 set completed there nothing the server has report for example ddl statements 
WITHOUT_CLASSIFICATION	 step remove this mapjoin task 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 the string has more code points make sure traverse too 
WITHOUT_CLASSIFICATION	 the log 
WITHOUT_CLASSIFICATION	 app base dir dagdir appbaseoutput 
WITHOUT_CLASSIFICATION	 the method not exposed and dont use 
WITHOUT_CLASSIFICATION	 name required for routing error out not set 
WITHOUT_CLASSIFICATION	 this call accessed from client jdbc side 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 process grouping set for the reduce sink operator for consider select key value count from group key value with rollup assuming mapside aggregation and skew the plan would look like tablescan select groupby reducesink groupby select filesink this function called for reducesink add the additional grouping keys introduced 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 coalesce special case because can take variable number arguments nvl specialization the coalesce 
WITHOUT_CLASSIFICATION	 use case 
WITHOUT_CLASSIFICATION	 try render place update progress bar only the operations logs update least once this will hopefully allow printing the metadata information like query application etc have remove these notifiers when the operation logs get merged into getoperationstatus 
WITHOUT_CLASSIFICATION	 convert the first param into datewritablev value 
WITHOUT_CLASSIFICATION	 tries get lock and gets waiting state 
WITHOUT_CLASSIFICATION	 sem input 
WITHOUT_CLASSIFICATION	 source table input format 
WITHOUT_CLASSIFICATION	 change the location and see the results 
WITHOUT_CLASSIFICATION	 the current child struct expression constant struct 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 try create table with all the parameters set 
WITHOUT_CLASSIFICATION	 theres base file major compaction 
WITHOUT_CLASSIFICATION	 now replace the old evaluators with our own 
WITHOUT_CLASSIFICATION	 recurse over the subqueries fill the subquery part the plan 
WITHOUT_CLASSIFICATION	 dont want push cross join 
WITHOUT_CLASSIFICATION	 nothing for null struct 
WITHOUT_CLASSIFICATION	 mapoutputinfomap used share the lookups with the caller 
WITHOUT_CLASSIFICATION	 query 
WITHOUT_CLASSIFICATION	 attempt create the table taking external into consideration 
WITHOUT_CLASSIFICATION	 found all possible rows which will not filtered 
WITHOUT_CLASSIFICATION	 first stripes will satisfy the predicate and merged single split last stripe will 
WITHOUT_CLASSIFICATION	 finally throw exception 
WITHOUT_CLASSIFICATION	 test partition listing with partial spec specified but not 
WITHOUT_CLASSIFICATION	 this operator this used for connecting them later 
WITHOUT_CLASSIFICATION	 this the first call open the session 
WITHOUT_CLASSIFICATION	 set these two invalid values any attempt use them 
WITHOUT_CLASSIFICATION	 set fastscale 
WITHOUT_CLASSIFICATION	 startix inclusive endix exclusive 
WITHOUT_CLASSIFICATION	 not sure these large cols could resultschema ignore this for now 
WITHOUT_CLASSIFICATION	 now run lower priority task 
WITHOUT_CLASSIFICATION	 remote dirs 
WITHOUT_CLASSIFICATION	 keep backward compat explain for singlefile copy tasks 
WITHOUT_CLASSIFICATION	 exists and not empty exists and empty doesnt exist 
WITHOUT_CLASSIFICATION	 integer digit fraction digits trailing zeroes are suppressed 
WITHOUT_CLASSIFICATION	 text byte value 
WITHOUT_CLASSIFICATION	 query 
WITHOUT_CLASSIFICATION	 resourceuris 
WITHOUT_CLASSIFICATION	 sleep for longer than servers idletimeout and execute query 
WITHOUT_CLASSIFICATION	 can stored string text some other classes 
WITHOUT_CLASSIFICATION	 can add current batch 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 test that existing exclusive with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 longrunning application 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 request task 
WITHOUT_CLASSIFICATION	 the globalhook stuff theres proper way insert this add everywhere 
WITHOUT_CLASSIFICATION	 create rows file and empty 
WITHOUT_CLASSIFICATION	 right now assume that the group arraylist object may 
WITHOUT_CLASSIFICATION	 since tasks themselves can graphs want limit the number created 
WITHOUT_CLASSIFICATION	 the serialize routine uses this build 
WITHOUT_CLASSIFICATION	 empties the projection columns 
WITHOUT_CLASSIFICATION	 implies fetch everything 
WITHOUT_CLASSIFICATION	 partitions dropped this batch 
WITHOUT_CLASSIFICATION	 this called only during move session handling removing session already checks this this not expected remove failing will not even invoke this method 
WITHOUT_CLASSIFICATION	 the collectable stats for the aggregator needs cleared for file being loaded the old number rows are not valid 
WITHOUT_CLASSIFICATION	 its parent hierarchy indicates how many these have completed 
WITHOUT_CLASSIFICATION	 same above 
WITHOUT_CLASSIFICATION	 req 
WITHOUT_CLASSIFICATION	 drop them from the proper catalog 
WITHOUT_CLASSIFICATION	 more reliable then attempting parse the error message from the sqlexception 
WITHOUT_CLASSIFICATION	 check can handle the subquery 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 singlecolumn string specific save key and lookup 
WITHOUT_CLASSIFICATION	 read cost 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 cannot open the lock file for writing must held live process 
WITHOUT_CLASSIFICATION	 repl noop export nonexistant table has replnoop does not error import repl noop dump error 
WITHOUT_CLASSIFICATION	 call open read split mockmocktable 
WITHOUT_CLASSIFICATION	 rely ndc information the logs map counters attempt that not available appid should either passed extracted from ndc 
WITHOUT_CLASSIFICATION	 aggregation buffer methods 
WITHOUT_CLASSIFICATION	 for smb the key columns should same bucket columns and sort columns 
WITHOUT_CLASSIFICATION	 make sure delta appears before delta 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 javameta javaref javaarraymeta javaref 
WITHOUT_CLASSIFICATION	 replicate drop partition event and verify 
WITHOUT_CLASSIFICATION	 test batchsize not divisible decaying factor 
WITHOUT_CLASSIFICATION	 all new versions acid tables created after the introduction acid versiontype system can have property defined this parameter can used change the operational behavior acid however this parameter not defined the new acid tables will still behave the old ones this done preserve the behavior case rolling downgrade 
WITHOUT_CLASSIFICATION	 these are the possible types referenced type below 
WITHOUT_CLASSIFICATION	 since take the rhs set exactly was input dont need deal with quotingescaping columntable names 
WITHOUT_CLASSIFICATION	 create union above all the branches the schema union looks like this 
WITHOUT_CLASSIFICATION	 let hashtable the child this parent 
WITHOUT_CLASSIFICATION	 fill high long and middle from some lower long 
WITHOUT_CLASSIFICATION	 visible for testing 
WITHOUT_CLASSIFICATION	 project nulls for the extra fields maybe subclass table has 
WITHOUT_CLASSIFICATION	 invariants 
WITHOUT_CLASSIFICATION	 cancel the deleg tokens that were acquired for this job now that are done should cancel the tokens were acquired hcatoutputformat and not they were supplied oozie the latter case the property the conf will not set 
WITHOUT_CLASSIFICATION	 multiple transactions only happen for streaming ingest which only allows inserts 
WITHOUT_CLASSIFICATION	 overlap between the two ranges 
WITHOUT_CLASSIFICATION	 found source which not stored memory 
WITHOUT_CLASSIFICATION	 evaluate children 
WITHOUT_CLASSIFICATION	 topn query results records types defined metastore 
WITHOUT_CLASSIFICATION	 sequence the following insert with segments original segment still present the datasource insert overwrite with segments datasource empty insert overwrite with segments datasource empty insert with segments datasource empty insert with one segment datasource has one segment insert overwrite with one segment datasource has one segment insert with one segment datasource has two segments insert overwrite with segments datasource empty 
WITHOUT_CLASSIFICATION	 same depth using natural order 
WITHOUT_CLASSIFICATION	 context for using deserialize each row from the input file format into the vectorizedrowbatch 
WITHOUT_CLASSIFICATION	 map that keeps track work that need linked while 
WITHOUT_CLASSIFICATION	 use only reducer order present 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 cant this with before because want able control when the thread starts 
WITHOUT_CLASSIFICATION	 long avro 
WITHOUT_CLASSIFICATION	 try expire the session its not use use bail 
WITHOUT_CLASSIFICATION	 intermediate reduction case column stats hash aggregation grouping sets numrows case column stats hash aggregation grouping sets numrows sizeofgroupingset case column stats hash aggregation grouping sets minnumrows ndvproduct parallelism case column stats hash aggregation grouping sets minnumrows sizeofgroupingset ndvproduct parallelism sizeofgroupingset case column stats hash aggregation grouping sets numrows case column stats hash aggregation grouping sets numrows sizeofgroupingset 
WITHOUT_CLASSIFICATION	 with big table 
WITHOUT_CLASSIFICATION	 mapside 
WITHOUT_CLASSIFICATION	 initialize map operator 
WITHOUT_CLASSIFICATION	 put them and also roll them while were 
WITHOUT_CLASSIFICATION	 all the tasks should have deallocated their stuff make sure can allocate everything 
WITHOUT_CLASSIFICATION	 regular vertices 
WITHOUT_CLASSIFICATION	 depending whether are using beeline sqlline the line endings have handled differently 
WITHOUT_CLASSIFICATION	 reset key and value columns and batchsize 
WITHOUT_CLASSIFICATION	 extrapolation needed for extracolumnnames 
WITHOUT_CLASSIFICATION	 assume can only used the beginning line 
WITHOUT_CLASSIFICATION	 cors check 
WITHOUT_CLASSIFICATION	 set 
WITHOUT_CLASSIFICATION	 computes the sig every object 
WITHOUT_CLASSIFICATION	 have just added new node signal timeout monitor reset timer 
WITHOUT_CLASSIFICATION	 configure header size 
WITHOUT_CLASSIFICATION	 field not included query 
WITHOUT_CLASSIFICATION	 set operator plan 
WITHOUT_CLASSIFICATION	 left outer join produced list with values 
WITHOUT_CLASSIFICATION	 the default encoding for this table when not otherwise specified 
WITHOUT_CLASSIFICATION	 recreate the database 
WITHOUT_CLASSIFICATION	 illformed query like select from having 
WITHOUT_CLASSIFICATION	 and firstname john sue and 
WITHOUT_CLASSIFICATION	 query does not contain cube rollup grouping sets and thus grouping should return 
WITHOUT_CLASSIFICATION	 kick out previous overflow batch results 
WITHOUT_CLASSIFICATION	 output nonulls set false need reset the isnull array 
WITHOUT_CLASSIFICATION	 storage vars 
WITHOUT_CLASSIFICATION	 process per vertex counters that are available only via vertex progress 
WITHOUT_CLASSIFICATION	 this arraylist holds the maxmin this the 
WITHOUT_CLASSIFICATION	 canretainbyteref 
WITHOUT_CLASSIFICATION	 start with the following locks path shared path exclusive path shared path shared 
WITHOUT_CLASSIFICATION	 select query results records types defined metastore 
WITHOUT_CLASSIFICATION	 add the new aggregate column and recompute data size 
WITHOUT_CLASSIFICATION	 alter the table 
WITHOUT_CLASSIFICATION	 column level statistics are required only for the columns that are needed 
WITHOUT_CLASSIFICATION	 read the warehouse dir which can changed multiple metastore tests could run 
WITHOUT_CLASSIFICATION	 this point are going make copy needed avoid array boundaries 
WITHOUT_CLASSIFICATION	 how this different from the outputshape set the tabledef this the the object coming out the ptf put output partition whose serde usually lazybinaryserde the next ptf operator the chain gets lazybinarystruct 
WITHOUT_CLASSIFICATION	 only primitive fields supports for now 
WITHOUT_CLASSIFICATION	 case max list members max query string length 
WITHOUT_CLASSIFICATION	 linuxyes windowsno 
WITHOUT_CLASSIFICATION	 the user assumptions 
WITHOUT_CLASSIFICATION	 avoid copy when oldtmpjars null empty 
WITHOUT_CLASSIFICATION	 semijoin case 
WITHOUT_CLASSIFICATION	 current random sampling implementation inputsampler always returns value index which can same with previous partition key that induces split points are out order exception causing hive 
WITHOUT_CLASSIFICATION	 vertex 
WITHOUT_CLASSIFICATION	 now clone the tree above selop 
WITHOUT_CLASSIFICATION	 populated for smb joins only for all the small tables 
WITHOUT_CLASSIFICATION	 nontransitive 
WITHOUT_CLASSIFICATION	 reset the bytebuffer 
WITHOUT_CLASSIFICATION	 set the specific parameters needed 
WITHOUT_CLASSIFICATION	 run the map join task set task tag 
WITHOUT_CLASSIFICATION	 instantiate the chosen transaction manager 
WITHOUT_CLASSIFICATION	 run count times and get average 
WITHOUT_CLASSIFICATION	 cannot swap the inputs can try 
WITHOUT_CLASSIFICATION	 ide support for running tez jobs 
WITHOUT_CLASSIFICATION	 need set the clients keyprovider the nns for jks else the updates not get flushed properly 
WITHOUT_CLASSIFICATION	 name child class 
WITHOUT_CLASSIFICATION	 view column access info carried 
WITHOUT_CLASSIFICATION	 too large have effect 
WITHOUT_CLASSIFICATION	 the table containing the partitions not yet loaded cache 
WITHOUT_CLASSIFICATION	 use reflection set lockmanager since creating the object using the relection dummytxnmanager wont take mocked object 
WITHOUT_CLASSIFICATION	 mapping from operator the columns which its output bucketed 
WITHOUT_CLASSIFICATION	 generating join output results 
WITHOUT_CLASSIFICATION	 the token file location initial hiveconf arg 
WITHOUT_CLASSIFICATION	 first add original keys 
WITHOUT_CLASSIFICATION	 first try without qualifiers would resolve builtintemp functions otherwise try qualifying with current name 
WITHOUT_CLASSIFICATION	 reducesink also needs mapjoin child 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 multiply and subtract some digits 
WITHOUT_CLASSIFICATION	 this resets vectors both batches 
WITHOUT_CLASSIFICATION	 the storage root 
WITHOUT_CLASSIFICATION	 the subquery has where clause there nothing rewrite decompose subquery where clause into list top level conjuncts for each conjunct break down the conjunct into leftexpr leftexprtype rightexpr rightexprtype the top level operator equality operator will break down into left and right all other case there only lhs the exprtype based whether the expr refers the parent query table sources refers the subquery sources both assume unqualified column refers subquery table source this because require parent column references qualified within the subquery the lhs rhs expr refers both parent and subquery sources flag this unsupported the conjunct whole only refers the parent query sources flag this error conjunct correlated the lhs refers subquery sources and rhs refers parent query sources the reverse say the lhs refers subquery and rhs refers parent query sources the other case handled analogously remove this conjunct from the subquery where clause for the subquery expressionlhs construct new alias the correlated predicate replace the subquery expressionlhs with the alias ast add this altered predicate the join predicate tracked the qbsubquery object add the alias ast list this list used the case outer joins add null check predicates the outer querys where clause add the subquery expression with the alias selectitem the subquerys selectlist case this subquery contains aggregation expressions add this subquery expression its groupby add the front the groupby predicate not correlated let remain the subquery where clause additional things for having clause correlation predicate may refer aggregation expression this introduces twists the rewrite when analyzing equality predicates need analyze each side see aggregation expression from the outer query for this valid correlation predicate minry where outer table reference and subquery table reference when hoisting the correlation predicate join predicate need rewrite the form the join code allows the predict needs contain qualified column references handle this generating new name for the aggregation expression like rgbysqcol and adding this mapping the outer querys row resolver then construct joining predicate using this new name our the condition would rgbysqcol 
WITHOUT_CLASSIFICATION	 the list table names could contain duplicates only guarantees returning duplicate table objects one batch need 
WITHOUT_CLASSIFICATION	 check that compacted base dir has version file with expected value 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 performed smb join based all the tablespartitions being joined 
WITHOUT_CLASSIFICATION	 this the last key need process 
WITHOUT_CLASSIFICATION	 initialize set unprocessed small tables 
WITHOUT_CLASSIFICATION	 this code has been only added for testing 
WITHOUT_CLASSIFICATION	 singlecolumn long specific members 
WITHOUT_CLASSIFICATION	 create input bytearrayref 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 need convert these the generated column names can see the join operator 
WITHOUT_CLASSIFICATION	 spot check correctness decimal scalar subtract decimal column the case for addition checks all the cases for the template dont that redundantly here 
WITHOUT_CLASSIFICATION	 deliver vector batch the operator tree the vectorized input file format reader has already set the partition column values reset and filled the batch etc pass the vectorizedrowbatch through here return return true the operator tree not done yet 
WITHOUT_CLASSIFICATION	 one the digits was the point 
WITHOUT_CLASSIFICATION	 dummy private constructor since this class collection static utility methods 
WITHOUT_CLASSIFICATION	 remember which parent belongs which tag 
WITHOUT_CLASSIFICATION	 introduce derived table above one child this selfjoin since user provided aliases are lost this point 
WITHOUT_CLASSIFICATION	 get column names and types 
WITHOUT_CLASSIFICATION	 math methods 
WITHOUT_CLASSIFICATION	 cast string 
WITHOUT_CLASSIFICATION	 whether the files output this filesink can merged they are put into 
WITHOUT_CLASSIFICATION	 reset the data 
WITHOUT_CLASSIFICATION	 create converters beforehand that the converters can reuse the same object for returning conversion results 
WITHOUT_CLASSIFICATION	 first branch should have the query with write filter conditions 
WITHOUT_CLASSIFICATION	 test set random divisions high precision 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 use default values from fsdefaultname 
WITHOUT_CLASSIFICATION	 also check config that has default hiveconf 
WITHOUT_CLASSIFICATION	 initialize input 
WITHOUT_CLASSIFICATION	 pass lineagestate when driver instantiates another driver run compile another query 
WITHOUT_CLASSIFICATION	 select with grant for exporting contents 
WITHOUT_CLASSIFICATION	 neither select nor compaction which select wil work after this 
WITHOUT_CLASSIFICATION	 statistics object that combination statistics from all 
WITHOUT_CLASSIFICATION	 the wrapper for byte array 
WITHOUT_CLASSIFICATION	 overlay the confvars note that this ignores confvars with null values 
WITHOUT_CLASSIFICATION	 code below does not deal with the connection serverobject 
WITHOUT_CLASSIFICATION	 there transaction then this lock will released commitabortrollback instead 
WITHOUT_CLASSIFICATION	 ignore this exception actually this exception wont thrown from 
WITHOUT_CLASSIFICATION	 that beeline wont kill the jvm 
WITHOUT_CLASSIFICATION	 finally remove the role 
WITHOUT_CLASSIFICATION	 write the given version metastore 
WITHOUT_CLASSIFICATION	 test http mode with ssl properties specified url 
WITHOUT_CLASSIFICATION	 test getcolumns 
WITHOUT_CLASSIFICATION	 this the key less than the lowest key need process 
WITHOUT_CLASSIFICATION	 map 
WITHOUT_CLASSIFICATION	 predicate expression userid and subtype 
WITHOUT_CLASSIFICATION	 literals 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 its multiexpr filter and with previous exprs 
WITHOUT_CLASSIFICATION	 this the first expression 
WITHOUT_CLASSIFICATION	 generate vectorized expression 
WITHOUT_CLASSIFICATION	 force the cache clear know its empty 
WITHOUT_CLASSIFICATION	 udf requiring additional jars cant determine the result 
WITHOUT_CLASSIFICATION	 since there can multiple rounds this run all which will tied the same query generated compile phase adding additional uuid the end print each run separate files 
WITHOUT_CLASSIFICATION	 extract input refs they will serve input for the function invocation 
WITHOUT_CLASSIFICATION	 here can one states map join work null implies that have not yet traversed the big table side just need see can find reduce sink operator the big table side this would imply reduce side operation dont find reducesink has the case that map side operation have already created work item for the big table side need see can find table scan operator the big table side this would imply map side operation dont find table scan operator has reduce side operation 
WITHOUT_CLASSIFICATION	 max this table either the big table cannot convert 
WITHOUT_CLASSIFICATION	 create the lazystruct from the lazystructinspector 
WITHOUT_CLASSIFICATION	 the layout for acid files will always only writing delta files except iow which writes basex the buckets created filesinkoperator will look like extdeltabucket need move that into the above structure for the first mover there will delta directory can move the whole directory for everyone else will need just move the buckets under the existing delta directory 
WITHOUT_CLASSIFICATION	 for join identify key then can infer the join cardinality rowcountt selectivityt this like semijoin where the tfact sidefk side filtered factor based the selectivity the pkdim table side both and are keys then use the larger one the side case outer joins the side should the null preserving side doesnt make sense apply this heuristic case dim loj fact fact roj dim the selectivity factor applied the fact table should 
WITHOUT_CLASSIFICATION	 find the next chunk size 
WITHOUT_CLASSIFICATION	 the data written and read withing the same job thus should never written one version hive and read another see 
WITHOUT_CLASSIFICATION	 unable parse the use command 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the table partitioned have put the partition clause 
WITHOUT_CLASSIFICATION	 for updatedelete always write exactly most actually the partitions read 
WITHOUT_CLASSIFICATION	 the can used for multiple queries this indication that single query complete dont have good mechanism know when app ends removing this right now ensures 
WITHOUT_CLASSIFICATION	 error already occurred but still want get the error code the child process possible 
WITHOUT_CLASSIFICATION	 based index which row are 
WITHOUT_CLASSIFICATION	 this particular sparksessionmanager implementation dont recycle returned sessions references session are still valid 
WITHOUT_CLASSIFICATION	 make fully qualified path for further use 
WITHOUT_CLASSIFICATION	 deduce resultset schema 
WITHOUT_CLASSIFICATION	 have valid pruning expressions only retrieve qualifying partitions 
WITHOUT_CLASSIFICATION	 take the pattern and split the get all the composing patterns 
WITHOUT_CLASSIFICATION	 just individual column 
WITHOUT_CLASSIFICATION	 there change here prev version had transadtional one beofre acid 
WITHOUT_CLASSIFICATION	 get the last operator for processing big tables 
WITHOUT_CLASSIFICATION	 the authorization 
WITHOUT_CLASSIFICATION	 based the new key count keycount smaller than threshold then just load the entire restored hashmap into memory 
WITHOUT_CLASSIFICATION	 this call checks under lock can actually preempt the task possible have race where the update thats also under lock makes the task finishable guaranteed between the remove and kill but its the same timing 
WITHOUT_CLASSIFICATION	 when true indicates that this object being read part update delete this important because that case shouldnt acquire lock for authorize the read 
WITHOUT_CLASSIFICATION	 make sure theres default database associated with each catalog 
WITHOUT_CLASSIFICATION	 add order token 
WITHOUT_CLASSIFICATION	 now prepare partnames with partitions tabparttabpart but with overlap 
WITHOUT_CLASSIFICATION	 combined all good dont combine with that but may combine with others dont combine with with that and make that base for new combines 
WITHOUT_CLASSIFICATION	 bgenjjtree headerlist 
WITHOUT_CLASSIFICATION	 the partition that has exchanged 
WITHOUT_CLASSIFICATION	 represents the column name corresponding distinct aggr any 
WITHOUT_CLASSIFICATION	 multi parents cant handle that right now not remove projection top lateralviewforward operators 
WITHOUT_CLASSIFICATION	 allmatchesindex allmatchesindex duplicatecount duplicatecount count count 
WITHOUT_CLASSIFICATION	 trimming again necessary because rounding may introduce new trailing 
WITHOUT_CLASSIFICATION	 test for timestamp type 
WITHOUT_CLASSIFICATION	 populate byte from timestamp 
WITHOUT_CLASSIFICATION	 get the last repl corresponding all insertaltercreate events except drop 
WITHOUT_CLASSIFICATION	 may used acid table 
WITHOUT_CLASSIFICATION	 lastaccesstime 
WITHOUT_CLASSIFICATION	 test for both colnames and partnames being empty 
WITHOUT_CLASSIFICATION	 client proxies connection use forwarded ipaddresses instead just the gateway 
WITHOUT_CLASSIFICATION	 for now just use this hold the object inspector there are writevalue setvalue etc methods yet 
WITHOUT_CLASSIFICATION	 log which resources were adding apart from the hive exec 
WITHOUT_CLASSIFICATION	 javadoc for statement interface states that the value zero then fetch size hint ignored this case means reverting the default value 
WITHOUT_CLASSIFICATION	 run exhaustive ppd add not null filters transitive inference 
WITHOUT_CLASSIFICATION	 properties 
WITHOUT_CLASSIFICATION	 since the user didnt supply customized typechecking context use default settings 
WITHOUT_CLASSIFICATION	 mingle existing tblproperties with those specified the alter table command 
WITHOUT_CLASSIFICATION	 left and right repeat 
WITHOUT_CLASSIFICATION	 validation the bitset size hash functions 
WITHOUT_CLASSIFICATION	 one less integer digit 
WITHOUT_CLASSIFICATION	 enabled check 
WITHOUT_CLASSIFICATION	 should only downcast and elimination precision within valid range 
WITHOUT_CLASSIFICATION	 ignore the char after escapechar 
WITHOUT_CLASSIFICATION	 positive test 
WITHOUT_CLASSIFICATION	 get the big table row bytes container for native vector map join 
WITHOUT_CLASSIFICATION	 need maintain the unique that target map works can read the output 
WITHOUT_CLASSIFICATION	 the function should support both short date and full timestamp format 
WITHOUT_CLASSIFICATION	 tabledesc needed for dynamic partitioned hash join 
WITHOUT_CLASSIFICATION	 send out the preempted request outside the lock 
WITHOUT_CLASSIFICATION	 only small table values appear join output result 
WITHOUT_CLASSIFICATION	 modulo operator with overflowzerodivide check 
WITHOUT_CLASSIFICATION	 now both txns concurrently updated tab but different partitions 
WITHOUT_CLASSIFICATION	 scale with this done via 
WITHOUT_CLASSIFICATION	 use list bucketing prunners path 
WITHOUT_CLASSIFICATION	 test extra field names 
WITHOUT_CLASSIFICATION	 because that point need access the objects 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 skip the row 
WITHOUT_CLASSIFICATION	 todo for now get the secure username out ugi after signing can take 
WITHOUT_CLASSIFICATION	 write the necessary config info hadoopsitexml 
WITHOUT_CLASSIFICATION	 for udtfs skip the function name get the expressions 
WITHOUT_CLASSIFICATION	 esw lock are examining shared write 
WITHOUT_CLASSIFICATION	 this may happen one the following case fil sel dummystore mergejoin fil sel 
WITHOUT_CLASSIFICATION	 destdb 
WITHOUT_CLASSIFICATION	 make possible for tests check that the right type was 
WITHOUT_CLASSIFICATION	 join operator contains big subtree there chance that its 
WITHOUT_CLASSIFICATION	 ptf function must provide the external names the columns its output 
WITHOUT_CLASSIFICATION	 add this parent the children 
WITHOUT_CLASSIFICATION	 record reader 
WITHOUT_CLASSIFICATION	 nope look see hives home dir has been explicitly set 
WITHOUT_CLASSIFICATION	 release all transient locks simply closing the client 
WITHOUT_CLASSIFICATION	 this point know that the extracted files are the intermediate extracted dir the the original directory 
WITHOUT_CLASSIFICATION	 you are admin you have all privileges missing privileges 
WITHOUT_CLASSIFICATION	 initialize unionexpr for reduceside reduce key has union field the last field there are distinct 
WITHOUT_CLASSIFICATION	 now really scan 
WITHOUT_CLASSIFICATION	 the calculation strongly dependent the assumption that all splits came from the same file 
WITHOUT_CLASSIFICATION	 the delimiter seen and the line isnt inside quoted string then treat lineindex single command 
WITHOUT_CLASSIFICATION	 all the code paths below propagate nulls even arg has nulls this reduce the number code paths and shorten the code the expense maybe doing unnecessary work neither input has nulls this could improved the future expanding the number code paths 
WITHOUT_CLASSIFICATION	 support the old syntax hivemetastore port but complain 
WITHOUT_CLASSIFICATION	 wrap the static accumuloinputformat methods with methods that can verify were correctly called via mockito 
WITHOUT_CLASSIFICATION	 shared plan utils for spark 
WITHOUT_CLASSIFICATION	 need 
WITHOUT_CLASSIFICATION	 use this function make the union flat for both execution and explain 
WITHOUT_CLASSIFICATION	 addpartitions 
WITHOUT_CLASSIFICATION	 guid 
WITHOUT_CLASSIFICATION	 match for workeridentity 
WITHOUT_CLASSIFICATION	 tresolver 
WITHOUT_CLASSIFICATION	 checks default connection configuration file present and uses connect found noop the file not present 
WITHOUT_CLASSIFICATION	 the implementation may may not set output isrepeting 
WITHOUT_CLASSIFICATION	 can ignore the check failed 
WITHOUT_CLASSIFICATION	 availableslots waves desired slots fill weight for particular bucket weights add 
WITHOUT_CLASSIFICATION	 implementation row iterator 
WITHOUT_CLASSIFICATION	 subquerytorelnode might null subquery expression anywhere other than expected filter wherehaving should throw appropriate error message 
WITHOUT_CLASSIFICATION	 can happen race condition where another process adds zlock under this parent just before about deleted should not problem since this parent can eventually deleted the process which hold its last child zlock 
WITHOUT_CLASSIFICATION	 given byte array consisting serialized bloomfilter gives the offset from for the start the serialized long values that make the bitset 
WITHOUT_CLASSIFICATION	 called many times 
WITHOUT_CLASSIFICATION	 sqlstate 
WITHOUT_CLASSIFICATION	 setup the context for reading from the next partition file 
WITHOUT_CLASSIFICATION	 even the state has changed dont log twice 
WITHOUT_CLASSIFICATION	 now process the delta files for normal read these should only delete deltas for compaction these may any deltaxy the files inside any deltaxy may acid format with acid metadata columns original 
WITHOUT_CLASSIFICATION	 checkcorrect codec checkcorrect codec 
WITHOUT_CLASSIFICATION	 else recurse the parents 
WITHOUT_CLASSIFICATION	 skip compaction theres delta files and theres original files 
WITHOUT_CLASSIFICATION	 real object 
WITHOUT_CLASSIFICATION	 not relevant for load 
WITHOUT_CLASSIFICATION	 for this may null should default which most restrictive 
WITHOUT_CLASSIFICATION	 been processed 
WITHOUT_CLASSIFICATION	 consider the query select count from group with cube where being executed single mapreduce job the plan tablescan groupby reducesink groupby filesink groupby already added the grouping part the row this function called for groupby add grouping part the groupby keys 
WITHOUT_CLASSIFICATION	 noarg ctor required for kyro 
WITHOUT_CLASSIFICATION	 todo could also vectorize some formats based llap enabled and are going run llap however dont know end llap not this stage dont this now may need add force option 
WITHOUT_CLASSIFICATION	 alter the tablepartition stats and also notify truncate table event 
WITHOUT_CLASSIFICATION	 byte 
WITHOUT_CLASSIFICATION	 assumed throughout the code that reducer has single child add 
WITHOUT_CLASSIFICATION	 reference hdfs based resource directly use distribute cache efficiently 
WITHOUT_CLASSIFICATION	 druids time column always not null 
WITHOUT_CLASSIFICATION	 find the last stripe 
WITHOUT_CLASSIFICATION	 this means that not need value generator 
WITHOUT_CLASSIFICATION	 hadoop might return null cannot locate the job may still able retrieve the job status ignore 
WITHOUT_CLASSIFICATION	 for runtime query may have finished 
WITHOUT_CLASSIFICATION	 writeentity typepartition writetypeinsert isdpfalse 
WITHOUT_CLASSIFICATION	 determine maximum all nonnull long column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 tests expect configuration changes applied directly metastore 
WITHOUT_CLASSIFICATION	 this piece code runs master node and gets necessary context 
WITHOUT_CLASSIFICATION	 release the background thread 
WITHOUT_CLASSIFICATION	 were dealing with input that array strings 
WITHOUT_CLASSIFICATION	 avoid allocationcopy nulls because potentially expensive branch instead 
WITHOUT_CLASSIFICATION	 all the versions should place this list 
WITHOUT_CLASSIFICATION	 drop tablepartition corresponding records txncomponents and should disappear 
WITHOUT_CLASSIFICATION	 for multiplicands with scale trim trailing zeroes 
WITHOUT_CLASSIFICATION	 constant projection 
WITHOUT_CLASSIFICATION	 the modification cannot affect active plan 
WITHOUT_CLASSIFICATION	 for each the the logical this should called seperately 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 generate reduce sink and project operator 
WITHOUT_CLASSIFICATION	 both are partitioned tables 
WITHOUT_CLASSIFICATION	 bucketing and sorting keys should exactly match 
WITHOUT_CLASSIFICATION	 distinct partitions modified 
WITHOUT_CLASSIFICATION	 could derive the expected number ams pass note pass null token here the tokens talk plugin endpoints will only known once the ams register and they are different for every unlike llap token 
WITHOUT_CLASSIFICATION	 start third batch abort everything dont properly close 
WITHOUT_CLASSIFICATION	 store credentials private hash map and not the udf context 
WITHOUT_CLASSIFICATION	 overlay the sasl transport top the base socket transport ssl nonssl 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 filter disabled injection enabled exception not expected 
WITHOUT_CLASSIFICATION	 only used for semijoin with residual predicates 
WITHOUT_CLASSIFICATION	 and udf rowid and rowid rowid 
WITHOUT_CLASSIFICATION	 all sources are initialized front events from different sources will end getting added the same list pruning disabled either source sends event which causes pruning skipped 
WITHOUT_CLASSIFICATION	 rest cases 
WITHOUT_CLASSIFICATION	 get the operationlog object from the operation 
WITHOUT_CLASSIFICATION	 first comparison unsigned 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 test that the all columns will read default 
WITHOUT_CLASSIFICATION	 optional int 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 test that setting read column ids set read all columns false 
WITHOUT_CLASSIFICATION	 process wdw functions 
WITHOUT_CLASSIFICATION	 add column info for non partion cols object inspector fields 
WITHOUT_CLASSIFICATION	 optimizing for readfield 
WITHOUT_CLASSIFICATION	 create the event and send tez tez will route appropriate processor 
WITHOUT_CLASSIFICATION	 test readfully 
WITHOUT_CLASSIFICATION	 add maxlength parameter udfs that have char varchar output 
WITHOUT_CLASSIFICATION	 change textinputformat 
WITHOUT_CLASSIFICATION	 the existing entry and newer entry are subset one another 
WITHOUT_CLASSIFICATION	 projection mode not yet supported for not between return null vectorizer knows revert rowatatime execution 
WITHOUT_CLASSIFICATION	 trim the trailing zero fraction digits dont cause unnecessary precision overflow later 
WITHOUT_CLASSIFICATION	 function class will not cause exception 
WITHOUT_CLASSIFICATION	 vectorized row batch being created 
WITHOUT_CLASSIFICATION	 anonymous element arrayelement 
WITHOUT_CLASSIFICATION	 the position this table 
WITHOUT_CLASSIFICATION	 verify scratch dir paths and permission 
WITHOUT_CLASSIFICATION	 uses pattern 
WITHOUT_CLASSIFICATION	 properties for remote driver rpc and yarn properties for spark yarn mode 
WITHOUT_CLASSIFICATION	 convert mapreduce job 
WITHOUT_CLASSIFICATION	 the cached buffer the middle the requested range the remaining tail the latter may still available further 
WITHOUT_CLASSIFICATION	 all good 
WITHOUT_CLASSIFICATION	 leave works output may read further sparkworkfetchwork should not combine leave works without notifying further sparkworkfetchwork 
WITHOUT_CLASSIFICATION	 filter enabled injection disabled exception expected 
WITHOUT_CLASSIFICATION	 not supported for tables for now 
WITHOUT_CLASSIFICATION	 scanning the filesystem get file lengths 
WITHOUT_CLASSIFICATION	 sortbased aggregations 
WITHOUT_CLASSIFICATION	 our hash tables are immutable can safely reference string charvarchar etc 
WITHOUT_CLASSIFICATION	 this insert into statement might need add constraint check 
WITHOUT_CLASSIFICATION	 want remove the dpp with bigger data size 
WITHOUT_CLASSIFICATION	 this can only come from brute force discard for now dont discard blocks larger than the target block could discard and add remainder free lists definition are fragmented there should smaller buffer somewhere 
WITHOUT_CLASSIFICATION	 adjust right input fields nonequiconds previous call modified the input 
WITHOUT_CLASSIFICATION	 hcat input format related errors 
WITHOUT_CLASSIFICATION	 run hcat expression and return just the json outout 
WITHOUT_CLASSIFICATION	 looks much possible like original query 
WITHOUT_CLASSIFICATION	 continue merging with next alias 
WITHOUT_CLASSIFICATION	 open the log file and read line then feed the line into 
WITHOUT_CLASSIFICATION	 all rows from left side will present resultset 
WITHOUT_CLASSIFICATION	 todo periodically reload new hiveconf check stats reporting enabled 
WITHOUT_CLASSIFICATION	 regrettable that have wrap the hcatexception into runtimeexception but throwing the exception the appropriate result here and hasnext signature will only allow runtimeexceptions iteratorhasnext really should have allowed ioexceptions 
WITHOUT_CLASSIFICATION	 from insert for each update stmt 
WITHOUT_CLASSIFICATION	 some the information the source complete dont need fetch from the context 
WITHOUT_CLASSIFICATION	 back seconds 
WITHOUT_CLASSIFICATION	 this internalname represents constant parameter aggregation parameters 
WITHOUT_CLASSIFICATION	 its parents also 
WITHOUT_CLASSIFICATION	 test with same schema with include 
WITHOUT_CLASSIFICATION	 least know they are not equal the one with the larger scale has nonzero digits below the others scale since the scale does not include trailing zeroes 
WITHOUT_CLASSIFICATION	 authorize for the old location and new location 
WITHOUT_CLASSIFICATION	 get our multikey hash map information for this specialized class 
WITHOUT_CLASSIFICATION	 today acid tables are only orc and that format vectorizable verify these assumptions 
WITHOUT_CLASSIFICATION	 add the hadoop token back the job the configuration still has the necessary 
WITHOUT_CLASSIFICATION	 todo for ordinal types you can produce range between 
WITHOUT_CLASSIFICATION	 make sure the accumulo token set the configuration only stub the accumulo authentiationtoken serialized not the entire token configurejobconf may called multiple times with the same jobconf which results error from accumulo 
WITHOUT_CLASSIFICATION	 walk through the tree decide value example skewed column index expression tree and 
WITHOUT_CLASSIFICATION	 the size present the metastore use 
WITHOUT_CLASSIFICATION	 implicit conversion from source type target type 
WITHOUT_CLASSIFICATION	 this branch had already cookie that did expire therefore need resend valid kerberos challenge 
WITHOUT_CLASSIFICATION	 need separate stmt for executeupdate otherwise will close the resultsethive 
WITHOUT_CLASSIFICATION	 boolean array instead single number 
WITHOUT_CLASSIFICATION	 wrap around the end buffer 
WITHOUT_CLASSIFICATION	 this required otherwise correct work object repl load wont created 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the last line didnt match pattern probably error message part string stack traces related the same error message add the stack trace 
WITHOUT_CLASSIFICATION	 map valid write ids list for all the tables read the current txn 
WITHOUT_CLASSIFICATION	 send the delegationtoken down the configuration for accumulo use 
WITHOUT_CLASSIFICATION	 for caching column stats for partitioned table 
WITHOUT_CLASSIFICATION	 map tasks and reduce tasks are finishable state then priority given the task the following order dag start time within dag priority attempt start time vertex parallelism 
WITHOUT_CLASSIFICATION	 order events dagstart and fragmentstart was guaranteed could just create the cache when dag starts and blindly return execution here 
WITHOUT_CLASSIFICATION	 the logjconfiguration property hasnt already been explicitly set use hives default logj configuration 
WITHOUT_CLASSIFICATION	 there least one record put the map 
WITHOUT_CLASSIFICATION	 add the entry mapredwork 
WITHOUT_CLASSIFICATION	 now clean them and check that they are removed from the count 
WITHOUT_CLASSIFICATION	 have found the colname 
WITHOUT_CLASSIFICATION	 its countcol case 
WITHOUT_CLASSIFICATION	 genericudfcase and genericudfwhen are implemented with the udf adaptor because their complexity and generality the future variations these can optimized run faster for the vectorized code path for example case col when then one when then two else other end example genericudfcase that has all constant arguments except for the first argument this probably common case and good candidate for fast specialpurpose vectorexpression then the udf adaptor code path could used catchall for nonoptimized general cases 
WITHOUT_CLASSIFICATION	 start column for columns array values corresponding columns 
WITHOUT_CLASSIFICATION	 level top level will have rexcall kept map 
WITHOUT_CLASSIFICATION	 create reducesink operator 
WITHOUT_CLASSIFICATION	 interrupt the runner thread 
WITHOUT_CLASSIFICATION	 union hard handle for instance the following case fil fil sel sel union join treat this case then after the removed would create two mapworks for each the each these mapwork will contain operator which wrong otherwise could try break the tree the union and create two mapworks for the branches above then will the following reducework 
WITHOUT_CLASSIFICATION	 the join alias modified before being inserted for consumption sortmerge join queries the join part subquery the alias modified include 
WITHOUT_CLASSIFICATION	 this represents subquery predicate then this will point the subquery object 
WITHOUT_CLASSIFICATION	 were dealing with input that array arrays strings 
WITHOUT_CLASSIFICATION	 cqstate 
WITHOUT_CLASSIFICATION	 the join keys are available the reducesinkoperators before join 
WITHOUT_CLASSIFICATION	 setup the table column stats 
WITHOUT_CLASSIFICATION	 create database specific location absolute nonqualified path 
WITHOUT_CLASSIFICATION	 second table union query with view parent 
WITHOUT_CLASSIFICATION	 base means originals longer matter 
WITHOUT_CLASSIFICATION	 load from modified dump event directories 
WITHOUT_CLASSIFICATION	 optional 
WITHOUT_CLASSIFICATION	 amount time thread can alive thread pool before cleaning this core threads will not cleanup from thread pool 
WITHOUT_CLASSIFICATION	 create expressions for project operators before and after the sort 
WITHOUT_CLASSIFICATION	 the map overloaded keep track mapjoins also 
WITHOUT_CLASSIFICATION	 not found 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 add added archives 
WITHOUT_CLASSIFICATION	 remove task from the pending list 
WITHOUT_CLASSIFICATION	 init output object inspectors 
WITHOUT_CLASSIFICATION	 indexes have only one entry per value could linear from here want use this for any sorted table well need continue the search 
WITHOUT_CLASSIFICATION	 cases where column expression map row schema missing just pass the parent column stats this could happen cases like fil where fil does not map input column names 
WITHOUT_CLASSIFICATION	 need new connection object well check the cache size after connection close 
WITHOUT_CLASSIFICATION	 future 
WITHOUT_CLASSIFICATION	 store the required fields information the udfcontext that 
WITHOUT_CLASSIFICATION	 the number reducers set user inferred 
WITHOUT_CLASSIFICATION	 are going validate the schema make sure dont create 
WITHOUT_CLASSIFICATION	 object inspectors for the tags for the input and output unionss 
WITHOUT_CLASSIFICATION	 start the service 
WITHOUT_CLASSIFICATION	 when token created the renewer the token stored shortname this seems like inconsistency because while cancelling the token uses the shortname compare the renewer while does not use shortname during token renewal use getshortusername until its fixed hadoop 
WITHOUT_CLASSIFICATION	 fallback mapjoin bucket scaling 
WITHOUT_CLASSIFICATION	 convert array typeinfo using library routine since parses the information and can handle use different separators etc cannot use the raw type string for comparison the map because the different separators used 
WITHOUT_CLASSIFICATION	 compute keys and values standardobjects use nonoptimized key 
WITHOUT_CLASSIFICATION	 this file sink desc has been processed due linked file sink desc 
WITHOUT_CLASSIFICATION	 have force cleanup all expired entries here because its possible that the expired entries will still counted cachesize taken from 
WITHOUT_CLASSIFICATION	 remove from all tables 
WITHOUT_CLASSIFICATION	 null just add 
WITHOUT_CLASSIFICATION	 bootstrap test 
WITHOUT_CLASSIFICATION	 the number nonnull keys they have associated hash codes and key data 
WITHOUT_CLASSIFICATION	 create temporary function using the jar 
WITHOUT_CLASSIFICATION	 query hooks that execute before compilation and after execution 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqldate javautilcalendar 
WITHOUT_CLASSIFICATION	 creates the commandline parameters for distcp 
WITHOUT_CLASSIFICATION	 this should not happen 
WITHOUT_CLASSIFICATION	 add shutdown hook cleanup the beeline for smooth exit 
WITHOUT_CLASSIFICATION	 insert row acid table 
WITHOUT_CLASSIFICATION	 the mapping that doesnt exist still shouldnt work 
WITHOUT_CLASSIFICATION	 have eager evaluators anywhere below then are eager too 
WITHOUT_CLASSIFICATION	 test select rootcol from 
WITHOUT_CLASSIFICATION	 return short string with the parameters the vector expression that will shown explain output etc 
WITHOUT_CLASSIFICATION	 find one session dir remove 
WITHOUT_CLASSIFICATION	 driverruninsert overwrite table orc select from inpy 
WITHOUT_CLASSIFICATION	 set progress bar completed when hive query execution has completed 
WITHOUT_CLASSIFICATION	 warn user could get stats for required columns 
WITHOUT_CLASSIFICATION	 skip the name and metadata 
WITHOUT_CLASSIFICATION	 expected exception remote metastore 
WITHOUT_CLASSIFICATION	 ownername 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 error out 
WITHOUT_CLASSIFICATION	 make sure row the output 
WITHOUT_CLASSIFICATION	 instance 
WITHOUT_CLASSIFICATION	 the cleaner will removed aborted txns datametadata but cannot remove aborted txn from txntowriteid there open txn aborted txn the aborted txn open txn and will removed also committed txn open txn retained 
WITHOUT_CLASSIFICATION	 cancelled the job request and return client 
WITHOUT_CLASSIFICATION	 the current element the current element 
WITHOUT_CLASSIFICATION	 loading the extra configuration options 
WITHOUT_CLASSIFICATION	 case the query served hiveserver dont pad with spaces hiveserver output consumed jdbcodbc clients 
WITHOUT_CLASSIFICATION	 non nulls 
WITHOUT_CLASSIFICATION	 noop 
WITHOUT_CLASSIFICATION	 nothing copy and cache 
WITHOUT_CLASSIFICATION	 restrictionh subquery predicates can appear only top level conjuncts 
WITHOUT_CLASSIFICATION	 the hive configs are received from with clause repl load repl status commands 
WITHOUT_CLASSIFICATION	 unknown unknown 
WITHOUT_CLASSIFICATION	 restore the old path information back this just prevent incompatibilities with previous versions hive 
WITHOUT_CLASSIFICATION	 set dynamic partitioning nonstrict that queries not need any partition references todo this may perf issue prevents the optimizer not 
WITHOUT_CLASSIFICATION	 level create gbr all keys vcol count for each 
WITHOUT_CLASSIFICATION	 the task the cache noop 
WITHOUT_CLASSIFICATION	 may null 
WITHOUT_CLASSIFICATION	 even there not data 
WITHOUT_CLASSIFICATION	 initialize the conversion related arrays assumes inittoplevelfield has already been called 
WITHOUT_CLASSIFICATION	 store char and varchar without pads write with string 
WITHOUT_CLASSIFICATION	 the fastbigintegerbytes method returns bit byte words and possible sign byte 
WITHOUT_CLASSIFICATION	 send state update for vertex completion this triggers status update sent out 
WITHOUT_CLASSIFICATION	 the join outputs concatenation all the inputs 
WITHOUT_CLASSIFICATION	 return positive modulo 
WITHOUT_CLASSIFICATION	 otherwise handle like normal generic udf 
WITHOUT_CLASSIFICATION	 regression test for defect reported hive 
WITHOUT_CLASSIFICATION	 assume the archive the original dir check exists 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 tests may leave this unitialized better set 
WITHOUT_CLASSIFICATION	 config name used find the maximum time job request can executed 
WITHOUT_CLASSIFICATION	 join 
WITHOUT_CLASSIFICATION	 cast string group string varchar string etc 
WITHOUT_CLASSIFICATION	 replace expression 
WITHOUT_CLASSIFICATION	 values 
WITHOUT_CLASSIFICATION	 replica memory 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 note there expectation that all fields will readthru 
WITHOUT_CLASSIFICATION	 the mapjoin has the same schema the join operator 
WITHOUT_CLASSIFICATION	 for thread safety allocate private writable objects for our use only 
WITHOUT_CLASSIFICATION	 return true the partition bucketedsorted the specified positions the number buckets the sort order should also match along with the 
WITHOUT_CLASSIFICATION	 double not between 
WITHOUT_CLASSIFICATION	 small table 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 minor comp ignore base files all deletes end first since 
WITHOUT_CLASSIFICATION	 optional string hivequeryid 
WITHOUT_CLASSIFICATION	 update expectedentries based factor and minentries configurations 
WITHOUT_CLASSIFICATION	 used convert decimal 
WITHOUT_CLASSIFICATION	 write the mutation 
WITHOUT_CLASSIFICATION	 preliminary checks the version the code these used done via another walk here done inline 
WITHOUT_CLASSIFICATION	 get the job request time out value this configuration value set then job request will wait until finishes 
WITHOUT_CLASSIFICATION	 since basex doesnt have suffix neither does pre acid write 
WITHOUT_CLASSIFICATION	 now that bootstrap has dumped all objects related have account for the changes that occurred while bootstrap was happening have look through all events during the bootstrap period and consolidate them with our dump 
WITHOUT_CLASSIFICATION	 populate the filters and filtermap structure needed the join descriptor 
WITHOUT_CLASSIFICATION	 template classname valuetype ifdefined 
WITHOUT_CLASSIFICATION	 user specified row limit set the query 
WITHOUT_CLASSIFICATION	 nobody can see this exception the threadpool just log 
WITHOUT_CLASSIFICATION	 reached endoffile 
WITHOUT_CLASSIFICATION	 the rpc library takes care timing out this 
WITHOUT_CLASSIFICATION	 add the queue only the first time this registered and 
WITHOUT_CLASSIFICATION	 link always used for vectorized reads acid tables some cases this cannot used from llap elevator because link not currently available there but required generate rowids for original files param hasdeletes there are any deletes that apply this split todo hive 
WITHOUT_CLASSIFICATION	 mark this small table being processed 
WITHOUT_CLASSIFICATION	 specified 
WITHOUT_CLASSIFICATION	 some columns from tables are not used 
WITHOUT_CLASSIFICATION	 the previous character isnt escape characters its the separator 
WITHOUT_CLASSIFICATION	 check that data has moved 
WITHOUT_CLASSIFICATION	 all children are base classes 
WITHOUT_CLASSIFICATION	 its either count count case 
WITHOUT_CLASSIFICATION	 assigning higher priority than filesystem shutdown hook that streaming connection gets closed first before filesystem close avoid 
WITHOUT_CLASSIFICATION	 break encountered union 
WITHOUT_CLASSIFICATION	 tostandardduration assumes days are always and hours are always minutes which may not always the case there are daylight saving changes 
WITHOUT_CLASSIFICATION	 only refresh once 
WITHOUT_CLASSIFICATION	 password must present 
WITHOUT_CLASSIFICATION	 create the nondeferred realargument 
WITHOUT_CLASSIFICATION	 the mapjoin has already been handled 
WITHOUT_CLASSIFICATION	 there should delta dir plus base dir the location 
WITHOUT_CLASSIFICATION	 set the table properties 
WITHOUT_CLASSIFICATION	 single long value hash map based the serialize the long key into binarysortable format into output buffer accepted 
WITHOUT_CLASSIFICATION	 newerclass 
WITHOUT_CLASSIFICATION	 check there already exists semijoin branch 
WITHOUT_CLASSIFICATION	 the row key column becomes string 
WITHOUT_CLASSIFICATION	 now try the table owner and see get better luck 
WITHOUT_CLASSIFICATION	 bail out ungracefully should never hit this here but would have hit semanticanalyzer 
WITHOUT_CLASSIFICATION	 already processed see backtracking 
WITHOUT_CLASSIFICATION	 privileges 
WITHOUT_CLASSIFICATION	 decrement batch size when this gets the batch will executed 
WITHOUT_CLASSIFICATION	 handle remaining middle long word digits 
WITHOUT_CLASSIFICATION	 test lazymap with bad entries empty key empty entries where and dont exist only for notation purpose stx with value entry separator etx with keyvalue separator 
WITHOUT_CLASSIFICATION	 toksubqueryexpr should have either children 
WITHOUT_CLASSIFICATION	 set memory threshold memory used after 
WITHOUT_CLASSIFICATION	 test with predicates such that partition pruning doesnt kick 
WITHOUT_CLASSIFICATION	 here means the last branch the multiinsert cardinality validation 
WITHOUT_CLASSIFICATION	 test set random subtracts high precision 
WITHOUT_CLASSIFICATION	 need keep predicate kind equal not equal that later while decorrelating logicalcorrelate appropriate join predicate generated 
WITHOUT_CLASSIFICATION	 infer pkfk relationship single attribute join case 
WITHOUT_CLASSIFICATION	 initialize pathtotableinfo 
WITHOUT_CLASSIFICATION	 add the property only exists table properties 
WITHOUT_CLASSIFICATION	 ambiguous call two methods with the same number implicit 
WITHOUT_CLASSIFICATION	 extract the information necessary create the predicate for the 
WITHOUT_CLASSIFICATION	 map tasks and reduce tasks are finishable state then priority given the task that has less number pending tasks shortest job 
WITHOUT_CLASSIFICATION	 dont push down any expressions that refer aliases that cant pushed down per getqualifiedaliases 
WITHOUT_CLASSIFICATION	 serde may not have this optional annotation defined which case conservative and say conversion needed 
WITHOUT_CLASSIFICATION	 all can handle limitoperator filteroperator selectoperator and final for nonaggressive mode minimal sampling not allowed for partitioned table all filters should targeted partition column 
WITHOUT_CLASSIFICATION	 backtrack sel columns prs 
WITHOUT_CLASSIFICATION	 remove the biggest key 
WITHOUT_CLASSIFICATION	 transient members initialized transientinit method temporary location for building number string 
WITHOUT_CLASSIFICATION	 free output columns inputs have nonleaf expression trees 
WITHOUT_CLASSIFICATION	 floor date operator need rewrite 
WITHOUT_CLASSIFICATION	 check null cols schemas for partition 
WITHOUT_CLASSIFICATION	 column the record identifier indicates record unique within transaction 
WITHOUT_CLASSIFICATION	 invalid paths 
WITHOUT_CLASSIFICATION	 load partition that doesnt exist 
WITHOUT_CLASSIFICATION	 the perbatch setup for inner bigonly join 
WITHOUT_CLASSIFICATION	 all partitions should miss target was marked virtually dropped 
WITHOUT_CLASSIFICATION	 need either move tmp files remove them 
WITHOUT_CLASSIFICATION	 todo nothing else should done for this task move 
WITHOUT_CLASSIFICATION	 drop connection without calling close hms thread deletecontext 
WITHOUT_CLASSIFICATION	 map key will list typeinfo isescaped escapechar 
WITHOUT_CLASSIFICATION	 note this could made more generic may common problem for the endpoints that can move around dynamically for now only handle this for the update 
WITHOUT_CLASSIFICATION	 convert from node 
WITHOUT_CLASSIFICATION	 since left integer always some products here are not included 
WITHOUT_CLASSIFICATION	 create hiveconf again run initialization code see value changes 
WITHOUT_CLASSIFICATION	 then continue use this perf logger 
WITHOUT_CLASSIFICATION	 process map joins with reducers pattern 
WITHOUT_CLASSIFICATION	 logj configuration file found successfully use hiveconf property value 
WITHOUT_CLASSIFICATION	 conflicts module configuration what will used weve already verified that includes and excludes are not present the same time for individual modules 
WITHOUT_CLASSIFICATION	 should not happen edit check false 
WITHOUT_CLASSIFICATION	 fail remove its probably internal error wed try handle the same way above restarting the session wed fail the caller avoid exceeding parallelism 
WITHOUT_CLASSIFICATION	 this point were dealing with all return types except 
WITHOUT_CLASSIFICATION	 ignore errors cleaning minimr 
WITHOUT_CLASSIFICATION	 dont create extra the metastore that has references 
WITHOUT_CLASSIFICATION	 entries should lru order the keyset iterator 
WITHOUT_CLASSIFICATION	 check mapreduce job needed merge the files the current size smaller than the target merge 
WITHOUT_CLASSIFICATION	 not much can about here 
WITHOUT_CLASSIFICATION	 generate enough delta files that initiator can trigger auto compaction 
WITHOUT_CLASSIFICATION	 append the separator needed 
WITHOUT_CLASSIFICATION	 test regular inputformat 
WITHOUT_CLASSIFICATION	 loginfoallocated alloccount size adebugdump 
WITHOUT_CLASSIFICATION	 now multiply and add sign bit 
WITHOUT_CLASSIFICATION	 struct 
WITHOUT_CLASSIFICATION	 shouldve done several heartbeats 
WITHOUT_CLASSIFICATION	 fall through 
WITHOUT_CLASSIFICATION	 the output column projection the vectorized row batch and the type infos the output 
WITHOUT_CLASSIFICATION	 the createtime will set the server side the comparison should skip 
WITHOUT_CLASSIFICATION	 txnhighwatermark 
WITHOUT_CLASSIFICATION	 keyseq 
WITHOUT_CLASSIFICATION	 specific test for hive tests timestamp assignment 
WITHOUT_CLASSIFICATION	 create only neededincluded columns data columns 
WITHOUT_CLASSIFICATION	 recurse over all the source tables 
WITHOUT_CLASSIFICATION	 there prefix then dont cut anything 
WITHOUT_CLASSIFICATION	 look for databases but not find any 
WITHOUT_CLASSIFICATION	 set codahale enabled cannot tag the values just prefix them for the jmx view 
WITHOUT_CLASSIFICATION	 cached buffer has the same lower offset the requested buffer 
WITHOUT_CLASSIFICATION	 each these should fail 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 now metastore connection should fail 
WITHOUT_CLASSIFICATION	 use the default separators 
WITHOUT_CLASSIFICATION	 later 
WITHOUT_CLASSIFICATION	 tablename either tablename dbnametablename given 
WITHOUT_CLASSIFICATION	 now have lock 
WITHOUT_CLASSIFICATION	 windowframe specifies the range which window function should applied for the current row its specified istarti and iendi boundary 
WITHOUT_CLASSIFICATION	 convert integer value representing timestamp nanoseconds one that represents timestamp seconds since the epoch 
WITHOUT_CLASSIFICATION	 pairwise columnhasnulls columnisrepeating 
WITHOUT_CLASSIFICATION	 major compact create base that has acid schema 
WITHOUT_CLASSIFICATION	 the seed port 
WITHOUT_CLASSIFICATION	 blank byte new tai lue letter low bytes 
WITHOUT_CLASSIFICATION	 the threshold percent then there throttling 
WITHOUT_CLASSIFICATION	 its not column table alias 
WITHOUT_CLASSIFICATION	 genericudf 
WITHOUT_CLASSIFICATION	 for some attempts check inheritance 
WITHOUT_CLASSIFICATION	 verify table for key long hash table hashmap 
WITHOUT_CLASSIFICATION	 assert that the table created still has hcat instrumentation 
WITHOUT_CLASSIFICATION	 todo call setremoteuser higher 
WITHOUT_CLASSIFICATION	 the data that need for this 
WITHOUT_CLASSIFICATION	 vectorized expression that selects rows 
WITHOUT_CLASSIFICATION	 check that the agg the entire input 
WITHOUT_CLASSIFICATION	 want abx come from finite field size where prime number prime for hence bitvectorsize has pick abx didnt come from finite field mod and mod will not pair wise independent consequence the hash values will not distribute uniformly from thus introducing errors the estimates 
WITHOUT_CLASSIFICATION	 noop this needed able instantiate the class from the name 
WITHOUT_CLASSIFICATION	 schemaname 
WITHOUT_CLASSIFICATION	 the corresponding reducesinkoperator 
WITHOUT_CLASSIFICATION	 find the highest failure count 
WITHOUT_CLASSIFICATION	 always use index the write methods dont write separator 
WITHOUT_CLASSIFICATION	 look top bits and return appropriate enum 
WITHOUT_CLASSIFICATION	 validate resultset columns 
WITHOUT_CLASSIFICATION	 init fails but the session also killed before that 
WITHOUT_CLASSIFICATION	 all the agg expressions are distinct and have the same 
WITHOUT_CLASSIFICATION	 the path the tracking root 
WITHOUT_CLASSIFICATION	 set the escape 
WITHOUT_CLASSIFICATION	 get the table names out 
WITHOUT_CLASSIFICATION	 look for under the old hive name 
WITHOUT_CLASSIFICATION	 left pad longer strings with multibyte characters 
WITHOUT_CLASSIFICATION	 find this parentcolname its parents 
WITHOUT_CLASSIFICATION	 all the vectors have the same length 
WITHOUT_CLASSIFICATION	 before making changes copypasting these 
WITHOUT_CLASSIFICATION	 second time will complete silently 
WITHOUT_CLASSIFICATION	 whether there are more than rows 
WITHOUT_CLASSIFICATION	 the partition doesnt qualify the global limit optimization for some reason 
WITHOUT_CLASSIFICATION	 fail early the columns specified for column statistics are not valid 
WITHOUT_CLASSIFICATION	 normalize the case for source replication parameter 
WITHOUT_CLASSIFICATION	 later 
WITHOUT_CLASSIFICATION	 clip off one byte and expect get eofexception the write field 
WITHOUT_CLASSIFICATION	 test second argument repeating 
WITHOUT_CLASSIFICATION	 end sync stuff 
WITHOUT_CLASSIFICATION	 get the new nextkvreader with lowest key 
WITHOUT_CLASSIFICATION	 from this point session creation will wait for the default pool sessions 
WITHOUT_CLASSIFICATION	 sort does not contain limit operation limit bail out 
WITHOUT_CLASSIFICATION	 test altering the table 
WITHOUT_CLASSIFICATION	 second branch should only have the 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 this for transactional tables 
WITHOUT_CLASSIFICATION	 drop the partitions and get list locations which need deleted 
WITHOUT_CLASSIFICATION	 while are the process setting valid 
WITHOUT_CLASSIFICATION	 issue command with bad options 
WITHOUT_CLASSIFICATION	 given the data partition evaluate the result for the next row for streaming and batch mode 
WITHOUT_CLASSIFICATION	 interpolation needed because lower position and higher position has the same key 
WITHOUT_CLASSIFICATION	 comment for reviewers updatetabcols needed separate from tabcols because pass tabcols gethiveprivobjects for the output case will trip insertselects since the insert will get passed the columns from the select 
WITHOUT_CLASSIFICATION	 response true 
WITHOUT_CLASSIFICATION	 this the vectorized row batch description the output the native vectorized ptf operator based the incoming vectorization context its projection may include 
WITHOUT_CLASSIFICATION	 nothing updated yet 
WITHOUT_CLASSIFICATION	 clear the mask for array reuse this avoid masks array allocation inner loop 
WITHOUT_CLASSIFICATION	 ptf node form tokptblfunction name alias partitioningspec expression guaranteed have alias here check done processjoin 
WITHOUT_CLASSIFICATION	 that something didnt expect itd more likely fail 
WITHOUT_CLASSIFICATION	 run the operator pipeline 
WITHOUT_CLASSIFICATION	 optional bytes workspecsignature 
WITHOUT_CLASSIFICATION	 and metadata gets created 
WITHOUT_CLASSIFICATION	 all keys vcol 
WITHOUT_CLASSIFICATION	 todo longer term should pass this from client somehow this would optimization once that place make sure build and test writeset below using operationtype not locktype with static partitions assume that the query modifies exactly the partitions locked not entirely realistic since updatedelete may have some predicate that filters out all records out some partitions but plausible for acquire locks very wide all known partitions but for most queries only fraction will actually updated tells exactly which ones were written thus using this trick kill query early for queries may too restrictive 
WITHOUT_CLASSIFICATION	 not put the tab for the last column 
WITHOUT_CLASSIFICATION	 small value bytes 
WITHOUT_CLASSIFICATION	 between has args here but can vectorized like this 
WITHOUT_CLASSIFICATION	 filters are using index which should match rows 
WITHOUT_CLASSIFICATION	 this column not appearing keyexprs the 
WITHOUT_CLASSIFICATION	 flush the last record when reader out records 
WITHOUT_CLASSIFICATION	 rename based output schema join operator 
WITHOUT_CLASSIFICATION	 only dealing with special join types here 
WITHOUT_CLASSIFICATION	 the will look for there 
WITHOUT_CLASSIFICATION	 case dynamic partitioning lock the table 
WITHOUT_CLASSIFICATION	 update max executors now that cluster info definitely available 
WITHOUT_CLASSIFICATION	 current replication state must set the partition object only for bootstrap dump event replication state will null case bootstrap dump 
WITHOUT_CLASSIFICATION	 propagate nulls 
WITHOUT_CLASSIFICATION	 adjust the compression block position 
WITHOUT_CLASSIFICATION	 jobclose needs execute successfully otherwise fail task 
WITHOUT_CLASSIFICATION	 stats delete forgivable error 
WITHOUT_CLASSIFICATION	 step create temp table object 
WITHOUT_CLASSIFICATION	 execute optimization 
WITHOUT_CLASSIFICATION	 tests single threaded implementation checkmetastore 
WITHOUT_CLASSIFICATION	 rename the data directory 
WITHOUT_CLASSIFICATION	 owner like owner and owner like test 
WITHOUT_CLASSIFICATION	 there are union all operators means that the walking context contains union all operators please see more details gentezworkwalker 
WITHOUT_CLASSIFICATION	 base javaobject primitives javafieldref javaarray entry javaobject javafieldref primitives 
WITHOUT_CLASSIFICATION	 for now dont higher than the default batch size unless more work verify every vectorized operator downstream can handle larger batch size 
WITHOUT_CLASSIFICATION	 rewriting cannot performed 
WITHOUT_CLASSIFICATION	 this matches the list structure that hive writes 
WITHOUT_CLASSIFICATION	 this sets dependencies such that child task dependant the parent complete 
WITHOUT_CLASSIFICATION	 hashmap javafieldref primitives hashmapentry javafieldref 
WITHOUT_CLASSIFICATION	 are secure mode login using keytab 
WITHOUT_CLASSIFICATION	 join key exprs are represented terms the original table columns 
WITHOUT_CLASSIFICATION	 concatenate keeping the old logic for nonmm tables with temp directories and stuff 
WITHOUT_CLASSIFICATION	 infer uniquenes rowcountcol ndvcol tbd for numerics maxcol mincol rowcountcol why are intercepting project and not tablescan because have method for tablescan will not know which columns check for inferring uniqueness for all columns very expensive right now the flip side doing this only works post field trimming 
WITHOUT_CLASSIFICATION	 there nothing project are projecting everything then need introduce another relnode 
WITHOUT_CLASSIFICATION	 make sure has chance dump 
WITHOUT_CLASSIFICATION	 minimum value seen far 
WITHOUT_CLASSIFICATION	 vectorized because there inputfilename 
WITHOUT_CLASSIFICATION	 this point tablepath part hdfs and encrypted 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 for any exception conversion decimal produce null 
WITHOUT_CLASSIFICATION	 localjobrunner does not work with mapreduce outputcommitter need 
WITHOUT_CLASSIFICATION	 accumulo instance name with quorum 
WITHOUT_CLASSIFICATION	 pad with empty rows the number values group less than top num 
WITHOUT_CLASSIFICATION	 default all columns that are not metrics timestamp are treated dimensions 
WITHOUT_CLASSIFICATION	 calculate the length the utf strings input vector and place results output vector 
WITHOUT_CLASSIFICATION	 daemon 
WITHOUT_CLASSIFICATION	 analyze and process the position alias step out 
WITHOUT_CLASSIFICATION	 txnid 
WITHOUT_CLASSIFICATION	 check common conditions for both optimized and fast hash tables 
WITHOUT_CLASSIFICATION	 wants create new base for iow instead delta dir should specify here 
WITHOUT_CLASSIFICATION	 clear all memory partitions first 
WITHOUT_CLASSIFICATION	 check predicate needed predicate needed either input pruning not enough input pruning not possible 
WITHOUT_CLASSIFICATION	 now will return true 
WITHOUT_CLASSIFICATION	 exhausted reading all records close the reader 
WITHOUT_CLASSIFICATION	 pass the message the user essentially something about the table information passed hcatoutputformat was not right 
WITHOUT_CLASSIFICATION	 the delegation token not applicable the given deployment mode such hms not kerberos secured 
WITHOUT_CLASSIFICATION	 attach this sel the operator right before 
WITHOUT_CLASSIFICATION	 create backtrack selectop 
WITHOUT_CLASSIFICATION	 want send the heartbeat interval that less than the timeout 
WITHOUT_CLASSIFICATION	 for negative tests which succeeded need print the query string 
WITHOUT_CLASSIFICATION	 virtual columns start after the last partition column 
WITHOUT_CLASSIFICATION	 get the the spark job that was launched returns spark job was launched 
WITHOUT_CLASSIFICATION	 this check case the decrypted plaintext actually makes sense some way 
WITHOUT_CLASSIFICATION	 not map reduce task not conditional task just skip 
WITHOUT_CLASSIFICATION	 position not field name field names arent guaranteed the order fields recordidentifier writeid bucketid rowid 
WITHOUT_CLASSIFICATION	 check column defined not 
WITHOUT_CLASSIFICATION	 transition success state 
WITHOUT_CLASSIFICATION	 mystringset 
WITHOUT_CLASSIFICATION	 check filter condition type first extract the correlation out the filter 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 payload 
WITHOUT_CLASSIFICATION	 create the index table does not exist 
WITHOUT_CLASSIFICATION	 netty defaults processors can changed via 
WITHOUT_CLASSIFICATION	 cannot just deallocate the buffer can hypothetically have users 
WITHOUT_CLASSIFICATION	 just give each table the same amount memory 
WITHOUT_CLASSIFICATION	 remove the lock specified 
WITHOUT_CLASSIFICATION	 set some values use for getting conf vars 
WITHOUT_CLASSIFICATION	 there should blocking operation recordprocessor creation otherwise the abort operation will not register since they are synchronized the same 
WITHOUT_CLASSIFICATION	 empty list 
WITHOUT_CLASSIFICATION	 partitions archived before introducing multiple archiving 
WITHOUT_CLASSIFICATION	 hivetxntimeout defined heartbeat interval will hivetxntimeout 
WITHOUT_CLASSIFICATION	 list maintain the incremental dumps for each operation 
WITHOUT_CLASSIFICATION	 the app state running get additional information from yarn service 
WITHOUT_CLASSIFICATION	 use lowercase table name prefix here statstask get table name from metastore fetch counter 
WITHOUT_CLASSIFICATION	 nothing got modified 
WITHOUT_CLASSIFICATION	 set the root the temporary path where dynamic partition columns will populate 
WITHOUT_CLASSIFICATION	 port 
WITHOUT_CLASSIFICATION	 can safely remove the condition replacing with true 
WITHOUT_CLASSIFICATION	 will increase the size the array demand 
WITHOUT_CLASSIFICATION	 bgenjjtree namespace 
WITHOUT_CLASSIFICATION	 any the table requests are null then need pull all the 
WITHOUT_CLASSIFICATION	 test string 
WITHOUT_CLASSIFICATION	 friday august 
WITHOUT_CLASSIFICATION	 serialize densesparse registers dense registers are bitpacked whereas 
WITHOUT_CLASSIFICATION	 short 
WITHOUT_CLASSIFICATION	 mergeisdirectflag need merge isdirect flag even newinput does not have parent 
WITHOUT_CLASSIFICATION	 reference the current row 
WITHOUT_CLASSIFICATION	 adding mssql jdbc driver exists 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 adjust all longs using power divisionremainder 
WITHOUT_CLASSIFICATION	 multikey hash multiset optimized for vector map join the key stored the provided bytes uninterpreted 
WITHOUT_CLASSIFICATION	 out 
WITHOUT_CLASSIFICATION	 buffer needed bridge 
WITHOUT_CLASSIFICATION	 this handles the common logic for destroy and return everything except the invalid combination destroy and return themselves well the actual statement that destroys returns 
WITHOUT_CLASSIFICATION	 see someone else evicted this parallel 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 verify vectorized expression 
WITHOUT_CLASSIFICATION	 should the cache size updated here after the result data has actually been deleted 
WITHOUT_CLASSIFICATION	 test that exclusive lock blocks shared reads 
WITHOUT_CLASSIFICATION	 bug the field has not sec override only set precisely sec 
WITHOUT_CLASSIFICATION	 now there are more than sources then have join case 
WITHOUT_CLASSIFICATION	 have use this ifelse since switchcase string supported java onwards 
WITHOUT_CLASSIFICATION	 for static partitions values would obtained from partitionkeyvalue clause 
WITHOUT_CLASSIFICATION	 ptf node form tokptblfunction name alias partitioningspec expression guranteed have lias here check done processjoin 
WITHOUT_CLASSIFICATION	 gce settings 
WITHOUT_CLASSIFICATION	 clear out the mapjoin set dont need anymore 
WITHOUT_CLASSIFICATION	 located the same position the input newproject 
WITHOUT_CLASSIFICATION	 primarily for debugging purposes atm since theres some unexplained tasktimeouts which are currently being observed 
WITHOUT_CLASSIFICATION	 make more explicit below that processhooks needs called last 
WITHOUT_CLASSIFICATION	 join positions for even index filter lengths for odd index 
WITHOUT_CLASSIFICATION	 fourth lower priority result canfinish being set false 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 fix the query for views 
WITHOUT_CLASSIFICATION	 dont use the username member may may not have been set get the value from conf which calls into getugi figure out who the process running 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 validate and setup resultexprstr 
WITHOUT_CLASSIFICATION	 change 
WITHOUT_CLASSIFICATION	 date str 
WITHOUT_CLASSIFICATION	 heartbeat indicates task has duck this must reverted 
WITHOUT_CLASSIFICATION	 have failed before building newcachedata deallocate other the allocated 
WITHOUT_CLASSIFICATION	 test that two different partitions dont collide their locks 
WITHOUT_CLASSIFICATION	 reset the previously stored rootnode string 
WITHOUT_CLASSIFICATION	 noscan uses hdfs apis retrieve such information from namenode 
WITHOUT_CLASSIFICATION	 create inline sql operator 
WITHOUT_CLASSIFICATION	 number columns the aliases does not match with number columns generated the lateral view 
WITHOUT_CLASSIFICATION	 add column expression for bloom filter 
WITHOUT_CLASSIFICATION	 this hashtable stores references array longs index the array hash the key these references point into infinite byte buffer see below this buffer contains records written one after another there are several simple record formats single record for the key key bytesvalue bytesvlong value lengthvlong key lengthpadding leave padding ensure have least bytes after key and value first multiple records for the key updated from single value for the key key bytesvalue bytesbyte long offset list start record list start record vlong value lengthvlong key lengthbyte long offset the list record lengths are preserved from the first record offset discussed above subsequent values the list value bytesvalue lengthvlong relative offset next record summary because have separate list record have very little list overhead for the typical case primary key join where theres list for any key large lists also dont have lot relative overhead also see the todo below the record looks follows for one value per key hash fixed bytes and stored expand rehashing and more efficiently deal with collision key hash refs offset wbs hash key val vlkl after that refs dont change they are not pictured when add the value rewrite lengths with relative offset the list start record that way the first record points the list record ref wbs hash key val offset vlkl after that refs dont change they are not pictured list record points the value ref wbs hash key val offset vlkloffset val add another value overwrite the list record dont need overwrite any vlongs and suffer because that ref wbs hash key val offset vlkloffset val val vloffset and another value for example vlkloffset val val vloffset val vloffset 
WITHOUT_CLASSIFICATION	 this loop fills the selected vector with all the index positions that are selected 
WITHOUT_CLASSIFICATION	 indicate that the query will use cached result 
WITHOUT_CLASSIFICATION	 hive try find the fake merge work for smb join that really another mapwork 
WITHOUT_CLASSIFICATION	 union 
WITHOUT_CLASSIFICATION	 shouldl return nulls end 
WITHOUT_CLASSIFICATION	 note not testing table rename because table rename replication not supported for tablelevel repl 
WITHOUT_CLASSIFICATION	 use separate method make easier create saslhandler without having 
WITHOUT_CLASSIFICATION	 aggregation buffer definition and manipulation methods 
WITHOUT_CLASSIFICATION	 verify number digits and each number has more digits 
WITHOUT_CLASSIFICATION	 current dest has distinct keys 
WITHOUT_CLASSIFICATION	 for tez used remember which position maps which logical input 
WITHOUT_CLASSIFICATION	 gather operators that belong root works works containing operators and share the same input operator these will the first target for extended shared work optimization 
WITHOUT_CLASSIFICATION	 include column names from serde the partition and virtual columns 
WITHOUT_CLASSIFICATION	 are checking the desired action 
WITHOUT_CLASSIFICATION	 verify the serialized format for dtype 
WITHOUT_CLASSIFICATION	 muxdesc only generated from corresponding reducesinkdesc 
WITHOUT_CLASSIFICATION	 cancel again current thread also interrupted 
WITHOUT_CLASSIFICATION	 set the parameters 
WITHOUT_CLASSIFICATION	 create drop priv requirement there the owner should have write permission 
WITHOUT_CLASSIFICATION	 the special case zero logic the beginning should have caught this 
WITHOUT_CLASSIFICATION	 inverse populated runtime 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 update lru 
WITHOUT_CLASSIFICATION	 unionentry 
WITHOUT_CLASSIFICATION	 this helpful when sqoop installed each node the cluster make sure relevant jars jdbc particular are present the node running the command 
WITHOUT_CLASSIFICATION	 shared between threads including sessionstate 
WITHOUT_CLASSIFICATION	 used for rehashing get last set values current array size have minimum fill factor 
WITHOUT_CLASSIFICATION	 there should delta dir plus base dir the location steve 
WITHOUT_CLASSIFICATION	 privileges filter 
WITHOUT_CLASSIFICATION	 into originalversion 
WITHOUT_CLASSIFICATION	 check hiveserver config gets loaded when started 
WITHOUT_CLASSIFICATION	 single byte array value hash map optimized for vector map join 
WITHOUT_CLASSIFICATION	 normal case evict the items from the list 
WITHOUT_CLASSIFICATION	 special cases for avro with orc make table properties that avro interested available jobconf runtime 
WITHOUT_CLASSIFICATION	 esr lock are examining shared read 
WITHOUT_CLASSIFICATION	 write the output temporary directory and move the final location the end 
WITHOUT_CLASSIFICATION	 there hint but none the operators removed throw error 
WITHOUT_CLASSIFICATION	 nonacid case then all files would the base data path just return 
WITHOUT_CLASSIFICATION	 sort the queue may have put items here out order 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 work can still null there merge work for this input 
WITHOUT_CLASSIFICATION	 this key evaluator translates from the vectorized format 
WITHOUT_CLASSIFICATION	 using parseinto avoids throwing exception when parsing 
WITHOUT_CLASSIFICATION	 iterate thru the file cache this besteffort 
WITHOUT_CLASSIFICATION	 get column names and sort order 
WITHOUT_CLASSIFICATION	 skip the column since wont have vector after reading the text source 
WITHOUT_CLASSIFICATION	 add 
WITHOUT_CLASSIFICATION	 decimal integer conversion 
WITHOUT_CLASSIFICATION	 getters 
WITHOUT_CLASSIFICATION	 nothing not mapreduce task 
WITHOUT_CLASSIFICATION	 reduce sink needed the query contains cluster distribute 
WITHOUT_CLASSIFICATION	 these are used cache previously created string 
WITHOUT_CLASSIFICATION	 optional processordescriptor 
WITHOUT_CLASSIFICATION	 cost transferring map outputs join operator 
WITHOUT_CLASSIFICATION	 the case tablesample the input paths are pointing files rather than directories need get the parent directory the filtering path that all files the same parent directory will grouped into one pool but not files from different parent directories this guarantees that split will combine all files the same partition but wont cross multiple partitions the user has asked path not directory 
WITHOUT_CLASSIFICATION	 skip command which readonly older postgres versions like see 
WITHOUT_CLASSIFICATION	 write string itself 
WITHOUT_CLASSIFICATION	 template classnameprefix returntype operandtype funcname operandcast 
WITHOUT_CLASSIFICATION	 now for each path that for the given versionnumber delete the znode from zookeeper 
WITHOUT_CLASSIFICATION	 for caching database objects key database name 
WITHOUT_CLASSIFICATION	 command should redacted avoid logging sensitive data 
WITHOUT_CLASSIFICATION	 replace all reducesinkoperators which are not the bottom 
WITHOUT_CLASSIFICATION	 tested 
WITHOUT_CLASSIFICATION	 user has told run local mode doesnt want autolocal mode 
WITHOUT_CLASSIFICATION	 set the new value the output string variable 
WITHOUT_CLASSIFICATION	 getallurls will parse zkjdbcurl and will plugin the active hss hostport 
WITHOUT_CLASSIFICATION	 now test tblproperties specified alter table compact statement 
WITHOUT_CLASSIFICATION	 must tezwork 
WITHOUT_CLASSIFICATION	 look for matches vertex specific counters 
WITHOUT_CLASSIFICATION	 stage 
WITHOUT_CLASSIFICATION	 need semishared 
WITHOUT_CLASSIFICATION	 create dummy mapreduce task 
WITHOUT_CLASSIFICATION	 singlecolumn string specific imports 
WITHOUT_CLASSIFICATION	 note that regular serde doesnt tolerate fewer columns 
WITHOUT_CLASSIFICATION	 build row type from field type name 
WITHOUT_CLASSIFICATION	 max number nodes when converting cnf 
WITHOUT_CLASSIFICATION	 check entries beyond first one 
WITHOUT_CLASSIFICATION	 need override this one too droptable breaks because doesnt find the table when checks 
WITHOUT_CLASSIFICATION	 hivedecimal number double 
WITHOUT_CLASSIFICATION	 note both fullacid and insertonly sinks 
WITHOUT_CLASSIFICATION	 trim off the ending any 
WITHOUT_CLASSIFICATION	 loginfogot 
WITHOUT_CLASSIFICATION	 test testing that the filters introduced eventutils are working correctly 
WITHOUT_CLASSIFICATION	 stage 
WITHOUT_CLASSIFICATION	 the startstop row conditions hbase 
WITHOUT_CLASSIFICATION	 using previously cached result 
WITHOUT_CLASSIFICATION	 size bigtable 
WITHOUT_CLASSIFICATION	 calculate typeinfo 
WITHOUT_CLASSIFICATION	 need not perform this optimization and should bail out 
WITHOUT_CLASSIFICATION	 for queries using script the optimization cannot applied without users confirmation 
WITHOUT_CLASSIFICATION	 the materialization was created otherwise query returns rows 
WITHOUT_CLASSIFICATION	 output from the script 
WITHOUT_CLASSIFICATION	 convert the mapjoin bucketized mapjoin 
WITHOUT_CLASSIFICATION	 check session files are removed 
WITHOUT_CLASSIFICATION	 check that hook disable transforms has been added 
WITHOUT_CLASSIFICATION	 and later way finding sasl property 
WITHOUT_CLASSIFICATION	 pass job initialize metastore conf overrides for embedded metastore case hivemetastoreuris 
WITHOUT_CLASSIFICATION	 table null either these then they are claiming lock the whole database and need check otherwise 
WITHOUT_CLASSIFICATION	 ignore all the other events logged above 
WITHOUT_CLASSIFICATION	 default handles this for both pool and nonpool session 
WITHOUT_CLASSIFICATION	 add the hadoop token the jobconf 
WITHOUT_CLASSIFICATION	 overlord and coordinator both run same jvm 
WITHOUT_CLASSIFICATION	 also dependent the udfexampleadd class within that jar 
WITHOUT_CLASSIFICATION	 child not rexcall instance can bail out 
WITHOUT_CLASSIFICATION	 xxx this could easily become hotspot 
WITHOUT_CLASSIFICATION	 nonempty java opts with xmx specified 
WITHOUT_CLASSIFICATION	 the following code should gone after hive using topological order 
WITHOUT_CLASSIFICATION	 set load server conf booleans false 
WITHOUT_CLASSIFICATION	 these represent the sorted columns 
WITHOUT_CLASSIFICATION	 presence grouping sets cant remove sqcountcheck 
WITHOUT_CLASSIFICATION	 expr built ddlsa should only contain part cols and simple ops 
WITHOUT_CLASSIFICATION	 now try find the file based sha and name currently require exact name match could also allow cutting off versions and other stuff provided that 
WITHOUT_CLASSIFICATION	 process hosts from which doas requests are authorized 
WITHOUT_CLASSIFICATION	 for now 
WITHOUT_CLASSIFICATION	 multithreaded file statuses and split strategy 
WITHOUT_CLASSIFICATION	 this should now fine since increased the configured header size 
WITHOUT_CLASSIFICATION	 bytes key hash map optimized for vector map join this the abstract base for the multikey and string bytes key hash map implementations 
WITHOUT_CLASSIFICATION	 only select operators among the allowed operators can cause changes the column names 
WITHOUT_CLASSIFICATION	 future this may examine writeentity andor config return appropriate hcatwriter 
WITHOUT_CLASSIFICATION	 get the next byte 
WITHOUT_CLASSIFICATION	 there integer room above 
WITHOUT_CLASSIFICATION	 load multiple random sets long values 
WITHOUT_CLASSIFICATION	 build reloptabstracttable 
WITHOUT_CLASSIFICATION	 mockresultset 
WITHOUT_CLASSIFICATION	 note that while this improvement over static initialization still not technically valid cause nothing prevents from connecting several metastores 
WITHOUT_CLASSIFICATION	 add this the list top operators always start from table 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 verify that drops were replicated this can either from tables ptns not existing and thus throwing returning nulls select returning empty depending what were testing 
WITHOUT_CLASSIFICATION	 mapside aggregation should reduce the entries atleast half 
WITHOUT_CLASSIFICATION	 this partitions set columns the same the parent tables use the parent tables not create duplicate column descriptor thereby saving space 
WITHOUT_CLASSIFICATION	 order forwarded ips per xforwardedfor http spec client proxy proxy 
WITHOUT_CLASSIFICATION	 special cases for orc need check table properties see couple parameters such compression parameters are defined they are then copy them job properties that will available jobconf runtime see hive for details 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 although instance objectstore accessed one thread there may many threads with objectstore instances the static variables pmf and prop need protected with locks 
WITHOUT_CLASSIFICATION	 set insiderview that can skip the column authorization for this 
WITHOUT_CLASSIFICATION	 check the location the result partition should located the destination table 
WITHOUT_CLASSIFICATION	 entries were added via conf initiate our defaults 
WITHOUT_CLASSIFICATION	 sql standard return null for zero one elements 
WITHOUT_CLASSIFICATION	 this method invoked for unqualified column references join conditions this passed the alias operator mapping the queryblock far try resolve the unqualified column against each the operator row resolvers the column present only one rowresolver treat this reference that operator the column resolves with more than one rowresolver treat ambiguous reference the column doesnt resolve with any rowresolver treat this invalid reference 
WITHOUT_CLASSIFICATION	 verify more invocations case success 
WITHOUT_CLASSIFICATION	 the total size local tables may not under the limit after merge mapjoinlocalwork and childlocalwork not merge 
WITHOUT_CLASSIFICATION	 this means that the derived colalias collides with existing ones 
WITHOUT_CLASSIFICATION	 add the left hand side the clause which contains the struct definition 
WITHOUT_CLASSIFICATION	 this can relaxed the future there requirement 
WITHOUT_CLASSIFICATION	 return true this custom udf custom genericudf 
WITHOUT_CLASSIFICATION	 scalarcolumn 
WITHOUT_CLASSIFICATION	 trigger transformation 
WITHOUT_CLASSIFICATION	 then nonfinishable must always precede finishable 
WITHOUT_CLASSIFICATION	 required build against reporter and statusreporter 
WITHOUT_CLASSIFICATION	 insert another row newlyconverted acid table 
WITHOUT_CLASSIFICATION	 lookup long the hash map param key the long key param hashmapresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 lets see this constant was folded because optimization 
WITHOUT_CLASSIFICATION	 now column can have alternate mapping this captures the alternate mapping the primaryfirst mapping still only held invrslvmap 
WITHOUT_CLASSIFICATION	 the input table bucketed choose the first bucket 
WITHOUT_CLASSIFICATION	 create empty database load 
WITHOUT_CLASSIFICATION	 validation has limited size 
WITHOUT_CLASSIFICATION	 entry and entries per partition 
WITHOUT_CLASSIFICATION	 since skew join optimization makes copy the tree above joins and there multiquery optimization place let not use skew join optimizations for now 
WITHOUT_CLASSIFICATION	 for now assume partition may have files perhaps better count them 
WITHOUT_CLASSIFICATION	 verify table for key byte hash table hashset 
WITHOUT_CLASSIFICATION	 handle default case for isrepeating setting for output this will set true later the special cases where that necessary 
WITHOUT_CLASSIFICATION	 used serialization only 
WITHOUT_CLASSIFICATION	 create and backtrack select operator top 
WITHOUT_CLASSIFICATION	 explicit pool specification invalid theres mapping that matches 
WITHOUT_CLASSIFICATION	 convert the lower case because events will have these names lower case 
WITHOUT_CLASSIFICATION	 part lowest word survives 
WITHOUT_CLASSIFICATION	 get the key and value 
WITHOUT_CLASSIFICATION	 test when second argument has nulls and repeats 
WITHOUT_CLASSIFICATION	 otherwise for each child run this method recursively 
WITHOUT_CLASSIFICATION	 drop table and check that trash works 
WITHOUT_CLASSIFICATION	 ordered columns are the output columns 
WITHOUT_CLASSIFICATION	 far have all the data from the beginning the part 
WITHOUT_CLASSIFICATION	 not all messages are parametrized even those that should have been link invalidtable invalidtable usually used with link getmsgstring this method can also used with invalidtable and the like and will match getmsgstring behavior another example link invalidpartition ideally you want the message have parameters one for partition name one for table name since this already defined any parameters one can still call code table name this way the message text will slightly different but least the errorcode will match note this should not abused adding anything other than what should have been parameter names keep msg text standardized 
WITHOUT_CLASSIFICATION	 there should residual since already negotiated that earlier however with pushes the original filter back down again since pusheddown filters are not omitted the higher levels and thus the contract negotiation ignored anyway just ignore the residuals reassess this when negotiation honored and the duplicate evaluation removed this ignores residual parsing from 
WITHOUT_CLASSIFICATION	 this the job tracker url 
WITHOUT_CLASSIFICATION	 finally write out the pieces sign power digits 
WITHOUT_CLASSIFICATION	 captured function 
WITHOUT_CLASSIFICATION	 parse the rewritten query string 
WITHOUT_CLASSIFICATION	 nonjavadoc this provides lazyboolean like class which can initialized from data stored binary format see int int 
WITHOUT_CLASSIFICATION	 only one bucket file 
WITHOUT_CLASSIFICATION	 important remove after unlock case fails 
WITHOUT_CLASSIFICATION	 theres buffer and another move reserving this 
WITHOUT_CLASSIFICATION	 skip the column that the root structure 
WITHOUT_CLASSIFICATION	 find out number partitions for each small table should same across tables 
WITHOUT_CLASSIFICATION	 double 
WITHOUT_CLASSIFICATION	 add the expressions that correspond the aggregation 
WITHOUT_CLASSIFICATION	 binary join nway join first biggest small table unconditionally create hashmap for the first hash partition 
WITHOUT_CLASSIFICATION	 sets the job state failed returns true failed status set otherwise returns false 
WITHOUT_CLASSIFICATION	 finally add project project out the column 
WITHOUT_CLASSIFICATION	 column stats hivecolstats might not the same order the columns reorder hivecolstats can build hivecolstatsmap using below 
WITHOUT_CLASSIFICATION	 required string fragmentid 
WITHOUT_CLASSIFICATION	 make sure the partitions directory not hdfs 
WITHOUT_CLASSIFICATION	 primary entry point factory method instead ctor 
WITHOUT_CLASSIFICATION	 lock states 
WITHOUT_CLASSIFICATION	 causing not sort the entire table due not knowing how selective the filter 
WITHOUT_CLASSIFICATION	 theres pending fragments queue some the cleanup for later point locks log rolling 
WITHOUT_CLASSIFICATION	 deep copy new mapred work 
WITHOUT_CLASSIFICATION	 sharedread 
WITHOUT_CLASSIFICATION	 add another via objectstore 
WITHOUT_CLASSIFICATION	 tinyint 
WITHOUT_CLASSIFICATION	 now try with hivehost principal 
WITHOUT_CLASSIFICATION	 primitive fields refs above with alignment 
WITHOUT_CLASSIFICATION	 projrel 
WITHOUT_CLASSIFICATION	 something happened and were not able rename the temp file attempt remove 
WITHOUT_CLASSIFICATION	 try fold constant expression 
WITHOUT_CLASSIFICATION	 one second before and after 
WITHOUT_CLASSIFICATION	 strings test 
WITHOUT_CLASSIFICATION	 add list running jobs kill case abnormal shutdown 
WITHOUT_CLASSIFICATION	 indicates this instance beeline running compatibility mode beeline mode 
WITHOUT_CLASSIFICATION	 dirs 
WITHOUT_CLASSIFICATION	 parse numrecords integer 
WITHOUT_CLASSIFICATION	 type timestamp minus type timestamp produces 
WITHOUT_CLASSIFICATION	 close the reader for this entry 
WITHOUT_CLASSIFICATION	 above returned null then the does not exist probably drop database exists clause dont try authorize then 
WITHOUT_CLASSIFICATION	 get rexnodes for original projections from below 
WITHOUT_CLASSIFICATION	 dropping expressions 
WITHOUT_CLASSIFICATION	 not call endgroup operators below because are batching rows output batch and the semantics will not work superendgroup 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 column pruner 
WITHOUT_CLASSIFICATION	 system defaults usually replica disk 
WITHOUT_CLASSIFICATION	 the username not already available the url add the one provided 
WITHOUT_CLASSIFICATION	 throw away lowest word 
WITHOUT_CLASSIFICATION	 the accessed columns query 
WITHOUT_CLASSIFICATION	 try the repeating case 
WITHOUT_CLASSIFICATION	 save the original selected vector 
WITHOUT_CLASSIFICATION	 assume including everything means the vrb will have everything todo this rather brittle esp view schema evolution abstract not currently implemented hive the compile should supply the columns expects see which not all any schema vrb row cvs the right mechanism for that who knows perhaps resolve schema evolution 
WITHOUT_CLASSIFICATION	 queryid 
WITHOUT_CLASSIFICATION	 make sure can actually get session still parallelismetc should not affected 
WITHOUT_CLASSIFICATION	 test lastcolumntakesrest 
WITHOUT_CLASSIFICATION	 generate floatingpoint number that represents the exponent this processing the exponent one bit time combine many powers then combine the exponent with the fraction 
WITHOUT_CLASSIFICATION	 the poolsized list being fully drained 
WITHOUT_CLASSIFICATION	 remove the pwd from conf file that job tracker doesnt show this 
WITHOUT_CLASSIFICATION	 cant eliminate stripes there are deltas because the deltas may change the rows making them match the predicate todo see hive 
WITHOUT_CLASSIFICATION	 validate the first parameter which the expression compute over this should array strings type array arrays strings 
WITHOUT_CLASSIFICATION	 sort the files 
WITHOUT_CLASSIFICATION	 patch the optimized query back into original from clause 
WITHOUT_CLASSIFICATION	 uses default 
WITHOUT_CLASSIFICATION	 store the partitions that are currently being processed 
WITHOUT_CLASSIFICATION	 create initialize and test the serde 
WITHOUT_CLASSIFICATION	 allow latter 
WITHOUT_CLASSIFICATION	 map dbnametblname tsoperator 
WITHOUT_CLASSIFICATION	 union extra fields 
WITHOUT_CLASSIFICATION	 increased visibility this method only for providing better test coverage 
WITHOUT_CLASSIFICATION	 check that the directory created 
WITHOUT_CLASSIFICATION	 for the udtf operator 
WITHOUT_CLASSIFICATION	 create formatter that includes all the input patterns 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 additional bits pad long array block size 
WITHOUT_CLASSIFICATION	 setup list conf vars that are not allowed change runtime 
WITHOUT_CLASSIFICATION	 can continue 
WITHOUT_CLASSIFICATION	 actually available for execution and will not potentially result rejectedexecution 
WITHOUT_CLASSIFICATION	 for use wildcard pattern test like 
WITHOUT_CLASSIFICATION	 there materialized view update desc create introduce the end the tree 
WITHOUT_CLASSIFICATION	 add user metadata footer case any 
WITHOUT_CLASSIFICATION	 the path used above should not used second try each dump request written unique location however want keep the dump location clean might want delete the paths 
WITHOUT_CLASSIFICATION	 larger allocations will specialcased and will not use the normal buffer buffernextfree will set newly allocated array just for the current row 
WITHOUT_CLASSIFICATION	 used the joinas field access 
WITHOUT_CLASSIFICATION	 transform the operator tree 
WITHOUT_CLASSIFICATION	 for partial and complete objectinspectors for original data 
WITHOUT_CLASSIFICATION	 difference from standard file appender locking not supported and buffering cannot switched off 
WITHOUT_CLASSIFICATION	 check and transform group this will only happen for select distinct here the genselectplan being leveraged the main benefits are remove virtual columns that should not included the group add the fully qualified column names unparsetranslator that view supported the drawback that additional sel added not necessary will removed optimizer because will match 
WITHOUT_CLASSIFICATION	 remove all the entries from the parameters which are added repl tasks internally 
WITHOUT_CLASSIFICATION	 nothing process 
WITHOUT_CLASSIFICATION	 the heap over the keys storing indexes the array 
WITHOUT_CLASSIFICATION	 this before next update that deltedelta properly sorted 
WITHOUT_CLASSIFICATION	 set genericudfs which require need implicit type casting decimal parameters vectorization for mathmatical functions currently depends decimal params automatically being converted the return type see which not correct 
WITHOUT_CLASSIFICATION	 start overlapping txn 
WITHOUT_CLASSIFICATION	 child the optional constraint 
WITHOUT_CLASSIFICATION	 found files under subdirectory under table base path possible that the table empty and hence there are partition subdirectories created under base path 
WITHOUT_CLASSIFICATION	 check all elements are contained 
WITHOUT_CLASSIFICATION	 same but repeating input not null 
WITHOUT_CLASSIFICATION	 the vectorized input file format reader responsible for setting the partition column values resetting and filling the batch etc 
WITHOUT_CLASSIFICATION	 here then must compacting 
WITHOUT_CLASSIFICATION	 insert the records simulate hive table 
WITHOUT_CLASSIFICATION	 update the memory monitor info for llap 
WITHOUT_CLASSIFICATION	 looking for map reduce 
WITHOUT_CLASSIFICATION	 ascii string 
WITHOUT_CLASSIFICATION	 give evaluator chance setup for output execution setup output shape 
WITHOUT_CLASSIFICATION	 frac 
WITHOUT_CLASSIFICATION	 schema 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 attempts make connection using default connection config file available there connection not made return false 
WITHOUT_CLASSIFICATION	 tracks running fragments and completing fragments completing since have race the being notified and the task actually 
WITHOUT_CLASSIFICATION	 pass false for canretainbyteref since will not keeping byte references the input bytes with the method 
WITHOUT_CLASSIFICATION	 user specified perms invalid format 
WITHOUT_CLASSIFICATION	 configurations are always published servicerecord readapply configs jdbc connection params 
WITHOUT_CLASSIFICATION	 always remove the condition replacing with true 
WITHOUT_CLASSIFICATION	 couldnt find proper parent column expr 
WITHOUT_CLASSIFICATION	 returns the number children the stack the current node scope 
WITHOUT_CLASSIFICATION	 since local mode only runs with reducers make sure that the 
WITHOUT_CLASSIFICATION	 introduce and before the operator tree already contains 
WITHOUT_CLASSIFICATION	 short 
WITHOUT_CLASSIFICATION	 create rexsubquery node 
WITHOUT_CLASSIFICATION	 note the vectorgroupbydesc has already been allocated and will updated here 
WITHOUT_CLASSIFICATION	 execute the driver locally 
WITHOUT_CLASSIFICATION	 elemoi 
WITHOUT_CLASSIFICATION	 override 
WITHOUT_CLASSIFICATION	 dont combine inputformat 
WITHOUT_CLASSIFICATION	 tez merge file work will become tez merge file 
WITHOUT_CLASSIFICATION	 chain pattern 
WITHOUT_CLASSIFICATION	 make sure big table bytescolumnvectors have room for string values the overflow batch 
WITHOUT_CLASSIFICATION	 this necessary sometimes semantic analyzers mapping different than operators own alias 
WITHOUT_CLASSIFICATION	 for tokjoin and tokfullouterjoin 
WITHOUT_CLASSIFICATION	 keep record all the input path for this alias 
WITHOUT_CLASSIFICATION	 session vars 
WITHOUT_CLASSIFICATION	 whether any the node outputs unknown whether all the node outputs are divided 
WITHOUT_CLASSIFICATION	 member variables 
WITHOUT_CLASSIFICATION	 start exclusive infinity inclusive 
WITHOUT_CLASSIFICATION	 traverse the given node find all correlated variables note that correlated variables are supported filter only where having 
WITHOUT_CLASSIFICATION	 initialize the first value the delete reader 
WITHOUT_CLASSIFICATION	 the first split for base 
WITHOUT_CLASSIFICATION	 set the regular provided qualifier names 
WITHOUT_CLASSIFICATION	 this file unknown metastore 
WITHOUT_CLASSIFICATION	 create map join task and set big table 
WITHOUT_CLASSIFICATION	 filter out the deleted records 
WITHOUT_CLASSIFICATION	 infer column stats state 
WITHOUT_CLASSIFICATION	 look for functions with empty pattern 
WITHOUT_CLASSIFICATION	 whether any acid table insertonly table involved query 
WITHOUT_CLASSIFICATION	 lifted from but supports offset 
WITHOUT_CLASSIFICATION	 see marker class comment 
WITHOUT_CLASSIFICATION	 for these input with exponents have this point intermediate decimal exponent power and result intermediate input decimal exponent result scale scale scale scale scale scale scale scale scale scale scale scale 
WITHOUT_CLASSIFICATION	 sum all nonnull decimal column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 optional string astring 
WITHOUT_CLASSIFICATION	 any condition contains nonns nonns 
WITHOUT_CLASSIFICATION	 close will move the temp files into the right place for the fetch task the job has failed will clean the files 
WITHOUT_CLASSIFICATION	 set other parameters 
WITHOUT_CLASSIFICATION	 build the value reference word will return that will kept the caller 
WITHOUT_CLASSIFICATION	 reset defaults 
WITHOUT_CLASSIFICATION	 any the children contains null then return null 
WITHOUT_CLASSIFICATION	 note currently read variations only apply top level data types 
WITHOUT_CLASSIFICATION	 invalidwriteids 
WITHOUT_CLASSIFICATION	 firstname sue 
WITHOUT_CLASSIFICATION	 able acquire the lock and finish successfully 
WITHOUT_CLASSIFICATION	 add shutdown hook 
WITHOUT_CLASSIFICATION	 needs bit words 
WITHOUT_CLASSIFICATION	 table sampled some situation really can leverage row limit order safe not use now 
WITHOUT_CLASSIFICATION	 this transformation needs first because changes the work item itself 
WITHOUT_CLASSIFICATION	 this batch used vectorrow deserializer readers 
WITHOUT_CLASSIFICATION	 futureproofing the parser will actually not allow this 
WITHOUT_CLASSIFICATION	 cleanup just case something left over from previous run 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 range starts here 
WITHOUT_CLASSIFICATION	 case this has not been initialized elsewhere 
WITHOUT_CLASSIFICATION	 test for double type 
WITHOUT_CLASSIFICATION	 the aliases that are allowed map null scan 
WITHOUT_CLASSIFICATION	 only this table has spilled big table rows 
WITHOUT_CLASSIFICATION	 head 
WITHOUT_CLASSIFICATION	 following matchergroup would fail anyway and dont want skip files since that may data loss scenario 
WITHOUT_CLASSIFICATION	 run worker execute compaction 
WITHOUT_CLASSIFICATION	 may not have active resource plan the start 
WITHOUT_CLASSIFICATION	 restore the settings 
WITHOUT_CLASSIFICATION	 now suck the digits the mantissa use two integers collect digits each this faster than using floatingpoint the mantissa has more than digits ignore the extras since they cant affect the value anyway 
WITHOUT_CLASSIFICATION	 americalosangeles dst dates 
WITHOUT_CLASSIFICATION	 corresponding the input file stored name the output bucket file appropriately 
WITHOUT_CLASSIFICATION	 rows 
WITHOUT_CLASSIFICATION	 serialize some data the schema after altered 
WITHOUT_CLASSIFICATION	 theory the below call isnt needed non thriftmode but lets not 
WITHOUT_CLASSIFICATION	 fetchcolumns not called because had columns fetch 
WITHOUT_CLASSIFICATION	 note retries weve configured total attempts connect failure simulation count left after this 
WITHOUT_CLASSIFICATION	 delete dependency only other resource depends 
WITHOUT_CLASSIFICATION	 numbuckets 
WITHOUT_CLASSIFICATION	 close the pipedoutputstream before close the outermost outputstream 
WITHOUT_CLASSIFICATION	 the explain vectorization option was specified 
WITHOUT_CLASSIFICATION	 hive job credential provider set but not set use the jobconf 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 set scratch dir permission 
WITHOUT_CLASSIFICATION	 test this supposed restrict events those that match the dbname and tblname provided the filter the tblname passed the filter null then restricts itself 
WITHOUT_CLASSIFICATION	 singlecolumn string hash table import 
WITHOUT_CLASSIFICATION	 there valid bucket pruning filter 
WITHOUT_CLASSIFICATION	 test invalid case with multiple versions 
WITHOUT_CLASSIFICATION	 predicate not deterministic 
WITHOUT_CLASSIFICATION	 could not parse the view 
WITHOUT_CLASSIFICATION	 need explicitly update proxyusers 
WITHOUT_CLASSIFICATION	 the begin the real elements 
WITHOUT_CLASSIFICATION	 look evaluator get output type info 
WITHOUT_CLASSIFICATION	 retry from same dump when the database empty also not allowed 
WITHOUT_CLASSIFICATION	 happens due side preemption the asking for task die theres hooks the moment get information over 
WITHOUT_CLASSIFICATION	 check that get the right files disk 
WITHOUT_CLASSIFICATION	 try put the most common first 
WITHOUT_CLASSIFICATION	 this code modified version that lets detect can return reference the bytes directly 
WITHOUT_CLASSIFICATION	 retry with same dump with which was already loaded also fails 
WITHOUT_CLASSIFICATION	 restart pool sessions 
WITHOUT_CLASSIFICATION	 should already true 
WITHOUT_CLASSIFICATION	 for these positions some variable primitive type string used size 
WITHOUT_CLASSIFICATION	 allow only keys that start with hive hdfs mapred for security dont allow access password 
WITHOUT_CLASSIFICATION	 hope this respected properly 
WITHOUT_CLASSIFICATION	 verify that cleans all old notifications 
WITHOUT_CLASSIFICATION	 when renaming partition should update partition location partition column stats there are any because partname field hms table partcolstats 
WITHOUT_CLASSIFICATION	 the nanosecond part fits bits 
WITHOUT_CLASSIFICATION	 not used yet since the writable rpc engine does not support this policy 
WITHOUT_CLASSIFICATION	 public long rdatetimeepoch the format hive understands default 
WITHOUT_CLASSIFICATION	 srswacquired lock are examining acquired can acquire because read can share with write and there must 
WITHOUT_CLASSIFICATION	 means that there stats 
WITHOUT_CLASSIFICATION	 blank byte ring above uda bytes 
WITHOUT_CLASSIFICATION	 reload related configuration changed 
WITHOUT_CLASSIFICATION	 tests whether the needs automatic setting parallelism 
WITHOUT_CLASSIFICATION	 save the info that required query time resolve dynamicruntime values 
WITHOUT_CLASSIFICATION	 add struct 
WITHOUT_CLASSIFICATION	 always need the base row 
WITHOUT_CLASSIFICATION	 sqlstate for cancel operation 
WITHOUT_CLASSIFICATION	 generate the aggregate see the reference example above 
WITHOUT_CLASSIFICATION	 union consisted bunch mapreduce jobs and has been split the union 
WITHOUT_CLASSIFICATION	 this either alter table add foreign key add primary key command 
WITHOUT_CLASSIFICATION	 for unregistered patterns fail 
WITHOUT_CLASSIFICATION	 class members for cookie based authentication 
WITHOUT_CLASSIFICATION	 verify that the sourcetable was created successfully 
WITHOUT_CLASSIFICATION	 not allow embedded metastore llap unless are test 
WITHOUT_CLASSIFICATION	 todo api changed 
WITHOUT_CLASSIFICATION	 havent processed all the parent sinks and need them done order compute the parallelism for this sink this case skip should visit this again from another path 
WITHOUT_CLASSIFICATION	 type when implemented 
WITHOUT_CLASSIFICATION	 jobconf will hold all the configuration for hadoop tez and hive 
WITHOUT_CLASSIFICATION	 multiple hash tables with hybrid grace partitioning 
WITHOUT_CLASSIFICATION	 has separate first step because dont set the default values the config object 
WITHOUT_CLASSIFICATION	 get the small table keyvalue container 
WITHOUT_CLASSIFICATION	 limit means that not need any task 
WITHOUT_CLASSIFICATION	 said before here use rewrite allcolref 
WITHOUT_CLASSIFICATION	 operator 
WITHOUT_CLASSIFICATION	 but there should more active calls 
WITHOUT_CLASSIFICATION	 run worker 
WITHOUT_CLASSIFICATION	 parsing elements one one 
WITHOUT_CLASSIFICATION	 set list bucketing context 
WITHOUT_CLASSIFICATION	 drop one database see what remains 
WITHOUT_CLASSIFICATION	 estimated hash table size 
WITHOUT_CLASSIFICATION	 clear the value 
WITHOUT_CLASSIFICATION	 per jdbc spec the request defines hint the driver enable database optimizations the readonly mode for this connection disabled and cannot enabled isreadonly always returns false 
WITHOUT_CLASSIFICATION	 set start which the timeout 
WITHOUT_CLASSIFICATION	 match 
WITHOUT_CLASSIFICATION	 though clone the tree were still using the same mapworkreducework 
WITHOUT_CLASSIFICATION	 generate that can make entry 
WITHOUT_CLASSIFICATION	 commaseparated intervals without brackets 
WITHOUT_CLASSIFICATION	 tcp server 
WITHOUT_CLASSIFICATION	 logic does not work unless the reducesink connected parent the mapjoin but this point have already removed all the parents from the mapjoin 
WITHOUT_CLASSIFICATION	 just return that case drop needed 
WITHOUT_CLASSIFICATION	 for serialization only 
WITHOUT_CLASSIFICATION	 now that let calcite process subqueries might have more than one 
WITHOUT_CLASSIFICATION	 genericudaf 
WITHOUT_CLASSIFICATION	 typical line length 
WITHOUT_CLASSIFICATION	 user grants 
WITHOUT_CLASSIFICATION	 todigitsonlybytes 
WITHOUT_CLASSIFICATION	 skewed value 
WITHOUT_CLASSIFICATION	 child tablescan there need push this predicate 
WITHOUT_CLASSIFICATION	 get the join keys from old parent reducesink operators 
WITHOUT_CLASSIFICATION	 create standard java object inspector 
WITHOUT_CLASSIFICATION	 test february nonleap year viewd due days diff 
WITHOUT_CLASSIFICATION	 its table alias will process that later dot 
WITHOUT_CLASSIFICATION	 flip the highestorder bit the sevenbyte representation seconds make negative values come before positive ones 
WITHOUT_CLASSIFICATION	 should have new root now 
WITHOUT_CLASSIFICATION	 the result the comparison the last row processed 
WITHOUT_CLASSIFICATION	 restore the original hdfs root 
WITHOUT_CLASSIFICATION	 this needed specifcally for hive tez addition 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 take integer fractional portion 
WITHOUT_CLASSIFICATION	 other writetypes related dmls 
WITHOUT_CLASSIFICATION	 the digits must fit without rounding 
WITHOUT_CLASSIFICATION	 nametotypeptr 
WITHOUT_CLASSIFICATION	 resfile 
WITHOUT_CLASSIFICATION	 call the method recursively over all the internal fields the given avro 
WITHOUT_CLASSIFICATION	 determine join type todo what about tokcrossjoin tokmapjoin 
WITHOUT_CLASSIFICATION	 after processing all the groups batches with evaluategroupbatch the nonstreaming 
WITHOUT_CLASSIFICATION	 with this map project the big table batch make look like output batch 
WITHOUT_CLASSIFICATION	 bgenjjtree constvalue 
WITHOUT_CLASSIFICATION	 decaying where the batchsize keeps reducing half 
WITHOUT_CLASSIFICATION	 both are non null first compare the table names 
WITHOUT_CLASSIFICATION	 required for hivereldecorrelator 
WITHOUT_CLASSIFICATION	 the default works bug 
WITHOUT_CLASSIFICATION	 propagate this change till the next 
WITHOUT_CLASSIFICATION	 todo this 
WITHOUT_CLASSIFICATION	 different instance same value 
WITHOUT_CLASSIFICATION	 used for logdir failure messages etc 
WITHOUT_CLASSIFICATION	 can merge 
WITHOUT_CLASSIFICATION	 should not materialized view 
WITHOUT_CLASSIFICATION	 path itself missing cannot recover from this error 
WITHOUT_CLASSIFICATION	 required optional optional 
WITHOUT_CLASSIFICATION	 store data 
WITHOUT_CLASSIFICATION	 this tablescandesc flag strictly set the vectorizer class for vectorized mapwork vertices 
WITHOUT_CLASSIFICATION	 the ref must table look for column name right child dot 
WITHOUT_CLASSIFICATION	 this map records such information 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 query should pass and create the table 
WITHOUT_CLASSIFICATION	 finish the last query 
WITHOUT_CLASSIFICATION	 streamingeval wrap regular eval getnext remove first row from values and return 
WITHOUT_CLASSIFICATION	 determine the length storage for value and key lengths the first record 
WITHOUT_CLASSIFICATION	 schemacounter name validation will done grammar part hive 
WITHOUT_CLASSIFICATION	 writable needed for this data type 
WITHOUT_CLASSIFICATION	 make sure all the numbers are converted long for size estimation 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 replication done need check the new property added 
WITHOUT_CLASSIFICATION	 only red qualifies and its entry 
WITHOUT_CLASSIFICATION	 start with the keywrapper itself 
WITHOUT_CLASSIFICATION	 eventscount 
WITHOUT_CLASSIFICATION	 offlineoffline 
WITHOUT_CLASSIFICATION	 compute knownpending tasks selfandupstream indicates task counts for current vertex and 
WITHOUT_CLASSIFICATION	 should allocate since not known 
WITHOUT_CLASSIFICATION	 maxevents 
WITHOUT_CLASSIFICATION	 only remove mvs first 
WITHOUT_CLASSIFICATION	 check there capacity dest pool move else kill the session 
WITHOUT_CLASSIFICATION	 table and all partitions have the same schema and serde need convert 
WITHOUT_CLASSIFICATION	 return session the pool can directly here 
WITHOUT_CLASSIFICATION	 defaultconstraints 
WITHOUT_CLASSIFICATION	 test deprecated 
WITHOUT_CLASSIFICATION	 some operations with new format 
WITHOUT_CLASSIFICATION	 lateral view ast has the following shape toklateralview tokselect tokselexpr tokfunction identifier params identifier tablealias 
WITHOUT_CLASSIFICATION	 knows where look compact 
WITHOUT_CLASSIFICATION	 reduce sink does not have any kids since the plan now has been broken into multiple tasks iterate over all tasks for each task over all operators recursively 
WITHOUT_CLASSIFICATION	 must different 
WITHOUT_CLASSIFICATION	 destpaths parent path doesnt exist should mkdir 
WITHOUT_CLASSIFICATION	 since remove reduce sink parents replace original expressions 
WITHOUT_CLASSIFICATION	 rejected 
WITHOUT_CLASSIFICATION	 the following two are public for any external users who wish use them 
WITHOUT_CLASSIFICATION	 are running tests 
WITHOUT_CLASSIFICATION	 rwx rwx rwx rwxrxrx rwxrwxrwx rxrxrx 
WITHOUT_CLASSIFICATION	 dont timeout because retry delay 
WITHOUT_CLASSIFICATION	 only support pattern matching via jdo since pattern matching java might different than the one used the metastore backends 
WITHOUT_CLASSIFICATION	 compare schemes 
WITHOUT_CLASSIFICATION	 patterns that match the middleend stack traces 
WITHOUT_CLASSIFICATION	 strict mode the presence order limit must specified 
WITHOUT_CLASSIFICATION	 deny this blacklist filter excludematches true and matched this whitelist filter and didnt match 
WITHOUT_CLASSIFICATION	 small table indices has priority over retain 
WITHOUT_CLASSIFICATION	 partspec doesnt exists return delegate one and the actual partition created movetask 
WITHOUT_CLASSIFICATION	 can the lazybinary format really tolerate writing fewer columns 
WITHOUT_CLASSIFICATION	 first try get from select 
WITHOUT_CLASSIFICATION	 return null 
WITHOUT_CLASSIFICATION	 the extra parameters will added server side check that the required ones are 
WITHOUT_CLASSIFICATION	 windowing specification get added child udaf invocation distinguish from similar udafs but different windows the udaf translated windowfunction invocation the ptftranslator here just return null for tokens that appear window specification when the traversal reaches the udaf invocation its exprnodedesc build using the columninfo the inputrr this similar how udafs are handled select lists the difference that there translation for window related tokens just return null 
WITHOUT_CLASSIFICATION	 collect grouping set info 
WITHOUT_CLASSIFICATION	 attempts execute will made using batchsizes throws retry exception 
WITHOUT_CLASSIFICATION	 create the layout for the queryid appender 
WITHOUT_CLASSIFICATION	 latin small letter turned bytes 
WITHOUT_CLASSIFICATION	 column reference will try resolve 
WITHOUT_CLASSIFICATION	 group them into the chunks want 
WITHOUT_CLASSIFICATION	 query using the hive command line 
WITHOUT_CLASSIFICATION	 expression for the operation 
WITHOUT_CLASSIFICATION	 each table creation itself takes more than one task give are giving max should hit multiple runs 
WITHOUT_CLASSIFICATION	 check case insensitive search 
WITHOUT_CLASSIFICATION	 reset this before calling positiontofirst 
WITHOUT_CLASSIFICATION	 powcol and powercol are special cases implemented separately from this template 
WITHOUT_CLASSIFICATION	 the caller responsible for destroying the session 
WITHOUT_CLASSIFICATION	 note previously did called oldhivedecimal 
WITHOUT_CLASSIFICATION	 execute init files always silent mode 
WITHOUT_CLASSIFICATION	 initialize source tablepartition 
WITHOUT_CLASSIFICATION	 for dynamic partitioned hash join run the logic for any 
WITHOUT_CLASSIFICATION	 the hive key and bytes writable value needed pass the key and value the collector 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 populate with parameters defined 
WITHOUT_CLASSIFICATION	 this class annotates each operator with its traits the optraits class specifies the traits that are populated for each operator 
WITHOUT_CLASSIFICATION	 note multi insert not supported 
WITHOUT_CLASSIFICATION	 partname 
WITHOUT_CLASSIFICATION	 being acquired now 
WITHOUT_CLASSIFICATION	 themap might reused the protocol 
WITHOUT_CLASSIFICATION	 the char type info need set prior initialization and must preserved when the plan serialized other processes 
WITHOUT_CLASSIFICATION	 fall through doesnt map hivehcat type here for completeness 
WITHOUT_CLASSIFICATION	 union occurs before the sortmerge join not useful convert the the sortmerge join mapjoin the number inputs for the union more than would difficult figure out the big table for the mapjoin 
WITHOUT_CLASSIFICATION	 selectivityrs selectivityjoin 
WITHOUT_CLASSIFICATION	 see all the field expressions the left hand side are expressions containing constants only partition columns coming from same table 
WITHOUT_CLASSIFICATION	 all the tablespartitions columns should sorted the same order for example tables and are being joined columns and which are the sorted and bucketed columns the join would work long 
WITHOUT_CLASSIFICATION	 note needs called inside the txn otherwise could have deduplicated this with 
WITHOUT_CLASSIFICATION	 continue the next exprnode find match 
WITHOUT_CLASSIFICATION	 get our singlecolumn long hash map information for this specialized class 
WITHOUT_CLASSIFICATION	 mapping 
WITHOUT_CLASSIFICATION	 cap writebuffersize avoid large preallocations also want limit the size writebuffer because normally have partitions that 
WITHOUT_CLASSIFICATION	 divisionremainder get lower binary word quotient will either middle decimal both high and middle decimal that requires another divisionremainder 
WITHOUT_CLASSIFICATION	 return the current blocks compressed key length 
WITHOUT_CLASSIFICATION	 first rowcall new partition 
WITHOUT_CLASSIFICATION	 ignore the struct column and just copy all the following data columns 
WITHOUT_CLASSIFICATION	 counter may exceed limitation 
WITHOUT_CLASSIFICATION	 addtable 
WITHOUT_CLASSIFICATION	 add rounding and handle carry 
WITHOUT_CLASSIFICATION	 testing acid usage when maskingfiltering present 
WITHOUT_CLASSIFICATION	 multikey hash table import 
WITHOUT_CLASSIFICATION	 overflow 
WITHOUT_CLASSIFICATION	 compatible with the oldhivedecimal version zero has factor 
WITHOUT_CLASSIFICATION	 get the hive counters hive createdfiles deserializeerrors recordsinmap recordsoutreducer 
WITHOUT_CLASSIFICATION	 otherwise load lazily via storagehandler query time 
WITHOUT_CLASSIFICATION	 fixme 
WITHOUT_CLASSIFICATION	 basic algorithm scale away fractional digits present clear integer rounding portion 
WITHOUT_CLASSIFICATION	 lookup values needed for numeric arithmetic udfs 
WITHOUT_CLASSIFICATION	 union type currently not totally supported see hive 
WITHOUT_CLASSIFICATION	 max heap size 
WITHOUT_CLASSIFICATION	 set output vector 
WITHOUT_CLASSIFICATION	 get acl provider for most outer path that nonnull 
WITHOUT_CLASSIFICATION	 the number map reduce tasks executed the hiveserver since the last restart 
WITHOUT_CLASSIFICATION	 distinct processing the reducer query like select countdistinct key from transformed into 
WITHOUT_CLASSIFICATION	 get the memory hashmap 
WITHOUT_CLASSIFICATION	 compatibility mode enabled 
WITHOUT_CLASSIFICATION	 validate the second parameter which either solitary double array doubles 
WITHOUT_CLASSIFICATION	 this seems odd but wan make sure that run 
WITHOUT_CLASSIFICATION	 note this backward compat only should removed with createunallocated 
WITHOUT_CLASSIFICATION	 common base class not 
WITHOUT_CLASSIFICATION	 check split 
WITHOUT_CLASSIFICATION	 validatefillin scheme and authority this follows logic identical filesystemgeturi conf but doesnt actually obtain file system handle 
WITHOUT_CLASSIFICATION	 finally move recovered file actual file 
WITHOUT_CLASSIFICATION	 use the serialization option switch write primitive values either variable length utf string fixed width bytes serializing binary format 
WITHOUT_CLASSIFICATION	 things log the jobconf 
WITHOUT_CLASSIFICATION	 must set this key even differences empty otherwise client and will attempt 
WITHOUT_CLASSIFICATION	 first value for next column 
WITHOUT_CLASSIFICATION	 hive 
WITHOUT_CLASSIFICATION	 create new setop whose children are the filters created above 
WITHOUT_CLASSIFICATION	 killed failed state 
WITHOUT_CLASSIFICATION	 optional string value 
WITHOUT_CLASSIFICATION	 always round down the previous period for timestamps prior origin 
WITHOUT_CLASSIFICATION	 tests null value returned when file not present any the lookup locations 
WITHOUT_CLASSIFICATION	 can set the traits for this join operator 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 not sel operator bail out 
WITHOUT_CLASSIFICATION	 optionally the next values small length could integer 
WITHOUT_CLASSIFICATION	 this pluggable policy chose the candidate mapjoin table for converting join sort merge join the largest table chosen based the size the tables 
WITHOUT_CLASSIFICATION	 initialize multikey members for this specialized class 
WITHOUT_CLASSIFICATION	 bgenjjtree fieldvalue 
WITHOUT_CLASSIFICATION	 per length means null map 
WITHOUT_CLASSIFICATION	 get fields out the lazy struct and check they match expected results 
WITHOUT_CLASSIFICATION	 want signal error the tableview doesnt exist and were configured not fail silently 
WITHOUT_CLASSIFICATION	 store column name maptargetwork 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 populate the names and order columns for the first partition the first table 
WITHOUT_CLASSIFICATION	 able combine parent join and its left input join child the left keys over which the parent join executed need the same than those the child join thus iterate over the different inputs the child checking the keys the parent are the same 
WITHOUT_CLASSIFICATION	 get the tmp uri path will hdfs path not local mode 
WITHOUT_CLASSIFICATION	 cannot deserialize throw the specific exception 
WITHOUT_CLASSIFICATION	 get join condn 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 for null and true values return every partition 
WITHOUT_CLASSIFICATION	 maintain the stack operators encountered 
WITHOUT_CLASSIFICATION	 configuration conf credentials creds 
WITHOUT_CLASSIFICATION	 database ddl 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 update primary and secondarykey 
WITHOUT_CLASSIFICATION	 required 
WITHOUT_CLASSIFICATION	 summary for top column values 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this should fail with given http response code error message since header more 
WITHOUT_CLASSIFICATION	 for now dont push anything into hbase nor store anything special hbase 
WITHOUT_CLASSIFICATION	 turn notification listener metastore 
WITHOUT_CLASSIFICATION	 minopentxn 
WITHOUT_CLASSIFICATION	 prepare blah lists for the following queries cut off the final 
WITHOUT_CLASSIFICATION	 blocking execute 
WITHOUT_CLASSIFICATION	 convert the filter one that references the child the project 
WITHOUT_CLASSIFICATION	 use the parser get the output operators 
WITHOUT_CLASSIFICATION	 parse the rewritten query string check need 
WITHOUT_CLASSIFICATION	 one fewer byte 
WITHOUT_CLASSIFICATION	 not supported cbo 
WITHOUT_CLASSIFICATION	 already have the tablescan for one side the join check this now 
WITHOUT_CLASSIFICATION	 the correlated variables 
WITHOUT_CLASSIFICATION	 create tablescans 
WITHOUT_CLASSIFICATION	 trimtrue 
WITHOUT_CLASSIFICATION	 metadataonly dump then the state the dump shouldnt the latest event the data not yet dumped and shall dumped future export 
WITHOUT_CLASSIFICATION	 possible overflow once 
WITHOUT_CLASSIFICATION	 create final loadmove work 
WITHOUT_CLASSIFICATION	 matching rows must the final block can end the binary search 
WITHOUT_CLASSIFICATION	 drain the first row which just contains column names 
WITHOUT_CLASSIFICATION	 not give out the capacity the initializing sessions the running ones expect init fast 
WITHOUT_CLASSIFICATION	 noop 
WITHOUT_CLASSIFICATION	 create batch with two string bytes columns 
WITHOUT_CLASSIFICATION	 udf 
WITHOUT_CLASSIFICATION	 close write 
WITHOUT_CLASSIFICATION	 assuming taskattemptid and are the same verify this 
WITHOUT_CLASSIFICATION	 the task done all operators are done well 
WITHOUT_CLASSIFICATION	 not delete the data 
WITHOUT_CLASSIFICATION	 add execute permission downloaded resource file needed when loading dll file 
WITHOUT_CLASSIFICATION	 standard objectinspector 
WITHOUT_CLASSIFICATION	 negative number flip all bits 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring javautilmap 
WITHOUT_CLASSIFICATION	 the process method was not called big table rows 
WITHOUT_CLASSIFICATION	 seconds wait until subsequent status minutes timeout for watch mode 
WITHOUT_CLASSIFICATION	 revert output cols sel exprnodecolumndesc 
WITHOUT_CLASSIFICATION	 estimate size aggregation buffer 
WITHOUT_CLASSIFICATION	 filter join condition filter see can pushed above the join filter cannot pushed join full outer join left outer and filter left alias join right outer and filter right alias 
WITHOUT_CLASSIFICATION	 need operate sorted data fully test binarysortable 
WITHOUT_CLASSIFICATION	 chosen noconditional task size will oversubscribed 
WITHOUT_CLASSIFICATION	 production 
WITHOUT_CLASSIFICATION	 offsets 
WITHOUT_CLASSIFICATION	 the user can specify the hadoop memory 
WITHOUT_CLASSIFICATION	 reducesinkoperators 
WITHOUT_CLASSIFICATION	 force the underlying initialize 
WITHOUT_CLASSIFICATION	 the issue with caching case bucket map join that different tasks process different buckets and the container reused join different bucket join results can incorrect the cache keyed operator and for bucket map join the operator does not change but data needed different for proper fix this requires changes the tez api with regard finding bucket and also ability schedule tasks reuse containers that have cached the specific bucket 
WITHOUT_CLASSIFICATION	 for cache 
WITHOUT_CLASSIFICATION	 nothing the moment 
WITHOUT_CLASSIFICATION	 replace filter current fil with new fil 
WITHOUT_CLASSIFICATION	 the result empty string negative start provided whose absolute value greater than the string length 
WITHOUT_CLASSIFICATION	 random sampling 
WITHOUT_CLASSIFICATION	 the scratch column information was collected the task get 
WITHOUT_CLASSIFICATION	 are going eliminate 
WITHOUT_CLASSIFICATION	 here rewrite the and also the masking table 
WITHOUT_CLASSIFICATION	 hide this doesnt look like simple property 
WITHOUT_CLASSIFICATION	 the parameters does not define any transactional properties return default type 
WITHOUT_CLASSIFICATION	 ccid monotonically increasing for any entity sorts order compaction history thus this query groups entity and withing group sorts most recent first 
WITHOUT_CLASSIFICATION	 multithreaded mode just once 
WITHOUT_CLASSIFICATION	 find the branch which this processor was invoked 
WITHOUT_CLASSIFICATION	 set custom prefix for hdfs scratch dir path 
WITHOUT_CLASSIFICATION	 get checksum file 
WITHOUT_CLASSIFICATION	 fetchworks sink used hold results each query needs separate copy fetchwork 
WITHOUT_CLASSIFICATION	 new attempt path crated add watch and scan for existing files 
WITHOUT_CLASSIFICATION	 there should still now directories the location 
WITHOUT_CLASSIFICATION	 sent over the wire from the and will take the place appiddagid queryidentifier 
WITHOUT_CLASSIFICATION	 file copied from path then need rename them using original source file name this needed avoid having duplicate files target same event applied twice 
WITHOUT_CLASSIFICATION	 partition column value null 
WITHOUT_CLASSIFICATION	 this lock used mutex commitabort and heartbeat calls 
WITHOUT_CLASSIFICATION	 hasmorerows 
WITHOUT_CLASSIFICATION	 verify that flattening and unflattening isrepeating works 
WITHOUT_CLASSIFICATION	 fall back regular api and create states without 
WITHOUT_CLASSIFICATION	 tests for gettable other catalogs are covered 
WITHOUT_CLASSIFICATION	 hive 
WITHOUT_CLASSIFICATION	 write basesplit into output 
WITHOUT_CLASSIFICATION	 set the default configs whitelist 
WITHOUT_CLASSIFICATION	 enabled get hosts property 
WITHOUT_CLASSIFICATION	 vectorized row batch 
WITHOUT_CLASSIFICATION	 the return value indicates success 
WITHOUT_CLASSIFICATION	 singlecolumn long specific declarations 
WITHOUT_CLASSIFICATION	 make sure run through the loop once before checking stop this makes testing much easier the stop value only for testing anyway and not used when called from hivemetastore 
WITHOUT_CLASSIFICATION	 the property set the metastore config the partition inherits the listed table parameters this property not set this test therefore the partition doesnt inherit the table parameters 
WITHOUT_CLASSIFICATION	 composite key class was provided but neither the types property was set and neither the getparts method hbasecompositekey was overidden the implementation flag exception 
WITHOUT_CLASSIFICATION	 first look for all the compactions that are waiting cleaned have not seen entry before look for all the locks held that table partition and record them will then only clean the partition once all those locks have been released this way avoid removing the files while they are use while the same time avoiding starving the cleaner new readers come along this works because know that any reader who comes along after the worker thread has done the compaction will read the more date version the data either newer delta newer base 
WITHOUT_CLASSIFICATION	 not need update column stats alter partition not for rename changing existing columns 
WITHOUT_CLASSIFICATION	 trigger query hooks after query completes its execution 
WITHOUT_CLASSIFICATION	 exceptions expected 
WITHOUT_CLASSIFICATION	 the raw data size 
WITHOUT_CLASSIFICATION	 simple implementation for now this will later expand dag evaluation 
WITHOUT_CLASSIFICATION	 associated with txn and handled performtimeouts 
WITHOUT_CLASSIFICATION	 initialize output 
WITHOUT_CLASSIFICATION	 todo poll periodically 
WITHOUT_CLASSIFICATION	 emulate biginteger deserialization used lazybinary and others 
WITHOUT_CLASSIFICATION	 not using fieldschemaequals comments can different 
WITHOUT_CLASSIFICATION	 walk through the operator tree 
WITHOUT_CLASSIFICATION	 adding column types used later 
WITHOUT_CLASSIFICATION	 remember mapping plan input 
WITHOUT_CLASSIFICATION	 owner testowner and lastaccesstime testparam 
WITHOUT_CLASSIFICATION	 attempt retrieving the schema from the data 
WITHOUT_CLASSIFICATION	 reorder fields record based the order columns the table 
WITHOUT_CLASSIFICATION	 existing 
WITHOUT_CLASSIFICATION	 generate the output column infos row resolver using internal names 
WITHOUT_CLASSIFICATION	 acid file would have schema like owid writerid rowid cwid could check this way onceif removed typedescription schema readergetschema liststring columns schemagetfieldnames 
WITHOUT_CLASSIFICATION	 this point have druid segments from reducers but need atomically rename and commit metadata moving druid segments and committing druid metadata one transaction 
WITHOUT_CLASSIFICATION	 create row file copy and row file copy 
WITHOUT_CLASSIFICATION	 weve eliminated the highest digit already with check above 
WITHOUT_CLASSIFICATION	 atomically move temp file the destination file 
WITHOUT_CLASSIFICATION	 required required required optional 
WITHOUT_CLASSIFICATION	 this primarily for testing avoid the host lookup 
WITHOUT_CLASSIFICATION	 equivalent algorithm exists 
WITHOUT_CLASSIFICATION	 roundcol special case and will implemented separately from this template 
WITHOUT_CLASSIFICATION	 targetpath table root unpartitioned table partition sourcepath result select part ctas statement 
WITHOUT_CLASSIFICATION	 direct handoff 
WITHOUT_CLASSIFICATION	 customized logj config log file 
WITHOUT_CLASSIFICATION	 test fields 
WITHOUT_CLASSIFICATION	 nonvectorized record reader created below 
WITHOUT_CLASSIFICATION	 are running tests are going verify that the contents the cache correspond with the contents the plan and otherwise fail this check always run when are running test mode independently whether 
WITHOUT_CLASSIFICATION	 vector expression doesnt support checked execution hold case there available checked variant 
WITHOUT_CLASSIFICATION	 dont through initiator for user initiated compactions 
WITHOUT_CLASSIFICATION	 last row group set true means the above row group spans compression buffer 
WITHOUT_CLASSIFICATION	 need instantiate the realinputformat 
WITHOUT_CLASSIFICATION	 todo parse make sure has alter table defaulttacid set tblproperties transactionaltrue alter table defaulttacidpart set tblproperties transactionaltrue 
WITHOUT_CLASSIFICATION	 repeated null fill down column 
WITHOUT_CLASSIFICATION	 either input input may have nulls 
WITHOUT_CLASSIFICATION	 compatibility 
WITHOUT_CLASSIFICATION	 create matchers for custom path string well actual dynamic partition path created 
WITHOUT_CLASSIFICATION	 walk operator tree create expression tree for list bucketing 
WITHOUT_CLASSIFICATION	 rewrite the agg calls each distinct agg becomes nondistinct call the corresponding field from the right for example countdistinct esal becomes countdistinctesal 
WITHOUT_CLASSIFICATION	 needed that the file actually loaded into configuration 
WITHOUT_CLASSIFICATION	 the name used the service registry may not match the host name were using try hostnamecanonical hostnamehost address 
WITHOUT_CLASSIFICATION	 locks 
WITHOUT_CLASSIFICATION	 remove viewbased rewriting rules from planner 
WITHOUT_CLASSIFICATION	 true return true 
WITHOUT_CLASSIFICATION	 ensure that have the correct identifier the column name 
WITHOUT_CLASSIFICATION	 useincludecolumns 
WITHOUT_CLASSIFICATION	 explicitly use link when getting the schema but store link this class serialized into the job conf 
WITHOUT_CLASSIFICATION	 wait for the initial resource plan applied 
WITHOUT_CLASSIFICATION	 mybitint 
WITHOUT_CLASSIFICATION	 check for two different classes below because initially the result but the future the system enhanced 
WITHOUT_CLASSIFICATION	 try converting the enum verify that this valid privilege type 
WITHOUT_CLASSIFICATION	 catch all errors throwable and execptions prevent subsequent tasks from being suppressed 
WITHOUT_CLASSIFICATION	 for join operators that can generate small table results reset their target scratch columns 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 for everything exceptions are aborted 
WITHOUT_CLASSIFICATION	 last try get port just use default 
WITHOUT_CLASSIFICATION	 get the mwmtrigger object from 
WITHOUT_CLASSIFICATION	 quoted string 
WITHOUT_CLASSIFICATION	 the only concurrent change that can happen when hold the heap lock list removal 
WITHOUT_CLASSIFICATION	 nosuch 
WITHOUT_CLASSIFICATION	 check the grantor matches current user 
WITHOUT_CLASSIFICATION	 need provide different record reader for every type druid query the reason that druid results format different for each type 
WITHOUT_CLASSIFICATION	 for casting integral types boolean 
WITHOUT_CLASSIFICATION	 modify copy not the original 
WITHOUT_CLASSIFICATION	 for remote jdbc client try set the hive var using set hivevarkeyvalue 
WITHOUT_CLASSIFICATION	 trigger bugs anyone who uses this hostname 
WITHOUT_CLASSIFICATION	 table name pattern 
WITHOUT_CLASSIFICATION	 writeid for table way the future 
WITHOUT_CLASSIFICATION	 get the column path return column name exists column could dot separated example lintstringelemmyint 
WITHOUT_CLASSIFICATION	 the tablepartition exist and older than the event then just apply 
WITHOUT_CLASSIFICATION	 assumed that parent directory the destf should already exist when this method called when the replace value true this method works little different from command the destf directory replaces the destf instead moving under 
WITHOUT_CLASSIFICATION	 since left integer always some products here are not included 
WITHOUT_CLASSIFICATION	 nothing register return hosthostname 
WITHOUT_CLASSIFICATION	 dowritefewercolumns 
WITHOUT_CLASSIFICATION	 called during deserialization querydef during runtime 
WITHOUT_CLASSIFICATION	 this would the case for obscure tasks like truncate column unsupported for 
WITHOUT_CLASSIFICATION	 not matching the regex 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 drop any partitions 
WITHOUT_CLASSIFICATION	 are not starting zookeeper server here because qtestutil already starts 
WITHOUT_CLASSIFICATION	 push down projections 
WITHOUT_CLASSIFICATION	 add its locations the list paths delete 
WITHOUT_CLASSIFICATION	 assigning can subset columns this the projection the batch column numbers 
WITHOUT_CLASSIFICATION	 optional string queue 
WITHOUT_CLASSIFICATION	 verify privilege objects 
WITHOUT_CLASSIFICATION	 decimal scale updown 
WITHOUT_CLASSIFICATION	 the rankvalue the last row output 
WITHOUT_CLASSIFICATION	 authorization ddl 
WITHOUT_CLASSIFICATION	 populate inputinspectors for all children this demuxoperator those are stored 
WITHOUT_CLASSIFICATION	 this will call one the specific notifyevicted overloads 
WITHOUT_CLASSIFICATION	 collect aggrel output col names 
WITHOUT_CLASSIFICATION	 backtrack key columns crs prs 
WITHOUT_CLASSIFICATION	 note are called after doesnt have nearly much does every buffer already cachechunk 
WITHOUT_CLASSIFICATION	 set the fetchsize 
WITHOUT_CLASSIFICATION	 loginfoget input agetclass bgetclass 
WITHOUT_CLASSIFICATION	 remove from hadoopclientopts any debug related options 
WITHOUT_CLASSIFICATION	 system property that matches one our conf value names set then use the value 
WITHOUT_CLASSIFICATION	 column stats generates select computestats queries disable caching for these 
WITHOUT_CLASSIFICATION	 clean system properties that were set this test 
WITHOUT_CLASSIFICATION	 folding the constant 
WITHOUT_CLASSIFICATION	 partition smaller than the lagamt the entire partition lagvalues 
WITHOUT_CLASSIFICATION	 for the mapper processing the smj not initialized need close either 
WITHOUT_CLASSIFICATION	 note not cancel any user actions here user actions actually interact with kills 
WITHOUT_CLASSIFICATION	 there should delta dirs plus base dir the location 
WITHOUT_CLASSIFICATION	 the intermediate rename wouldve failed bootstrap dump progress 
WITHOUT_CLASSIFICATION	 any the elements was not referenced every operand bail out 
WITHOUT_CLASSIFICATION	 prefix workgetaggkey 
WITHOUT_CLASSIFICATION	 table scan operators dpp sources 
WITHOUT_CLASSIFICATION	 keys for all tables are the same only the first has deserialize them 
WITHOUT_CLASSIFICATION	 right border the min 
WITHOUT_CLASSIFICATION	 when generate results into the overflow batch may still end with fewer rows the big table batch nulsel and the batchs selected array will rebuilt with just the big table rows that need forwarded minus any rows processed with the 
WITHOUT_CLASSIFICATION	 wish add new line here 
WITHOUT_CLASSIFICATION	 adjustment only needed when exceed max precision 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 assume not streamjng default 
WITHOUT_CLASSIFICATION	 string including surrogate pair character 
WITHOUT_CLASSIFICATION	 the column names and order ascendingdescending matched the first sorted columns should the same the joincols where the size join columns for the table sorted abc convert the join any combination abc abc acb cab cba bca bac 
WITHOUT_CLASSIFICATION	 now check though the schemas are the same the operator changes the order columns the output 
WITHOUT_CLASSIFICATION	 converts the negative byte into positive index 
WITHOUT_CLASSIFICATION	 get the children the set clause each which should column assignment 
WITHOUT_CLASSIFICATION	 qbjointree from innermost outer 
WITHOUT_CLASSIFICATION	 prune any nulls present map values this the typical case 
WITHOUT_CLASSIFICATION	 set fetch size 
WITHOUT_CLASSIFICATION	 need for this all subqueries are maponly queries 
WITHOUT_CLASSIFICATION	 windowing handling ptfinvocationspec ptfdesc 
WITHOUT_CLASSIFICATION	 generate the output records 
WITHOUT_CLASSIFICATION	 order outputcolumn 
WITHOUT_CLASSIFICATION	 written major compaction 
WITHOUT_CLASSIFICATION	 and notification comes from the llaptaskreporter 
WITHOUT_CLASSIFICATION	 assume 
WITHOUT_CLASSIFICATION	 orc table restrict changing the serde can break schema evolution 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqltimestamp javautilcalendar 
WITHOUT_CLASSIFICATION	 first load the defaults from sparkdefaultsconf available 
WITHOUT_CLASSIFICATION	 optional bytes credentialsbinary 
WITHOUT_CLASSIFICATION	 part the partition specified create dummypartition this case since the metastore does not store partial partitions currently need store dummy partitions 
WITHOUT_CLASSIFICATION	 standardize lower case 
WITHOUT_CLASSIFICATION	 execute and exit 
WITHOUT_CLASSIFICATION	 the caller not within mapperreducer reading from the table via clidriver then may return null fall back using default constructor 
WITHOUT_CLASSIFICATION	 this test 
WITHOUT_CLASSIFICATION	 one vint with nanos 
WITHOUT_CLASSIFICATION	 this root task 
WITHOUT_CLASSIFICATION	 also note that any deletedeltas between given deltaxy range would made obsolete for example delta would make deletedelta obsolete this valid because minor compaction always compacts the normal deltas and the delete deltas for the same range that had directories delta deletedelta and delta then running minor compaction would produce delta and deletedelta 
WITHOUT_CLASSIFICATION	 assuming the objectinspector represents exactly the same type this struct this assumption should checked during query compile time 
WITHOUT_CLASSIFICATION	 since allow write operations cache while prewarm happening dont add tables that were deleted while were preparing list for prewarm 
WITHOUT_CLASSIFICATION	 modified 
WITHOUT_CLASSIFICATION	 smb join 
WITHOUT_CLASSIFICATION	 more complex methods which may wrap multiple operations 
WITHOUT_CLASSIFICATION	 the max for digits 
WITHOUT_CLASSIFICATION	 initialize the cookie based authentication related variables 
WITHOUT_CLASSIFICATION	 following parameters slated for removal prefer usage enum above that allows programmatic access 
WITHOUT_CLASSIFICATION	 this shouldnt happen 
WITHOUT_CLASSIFICATION	 end the record part the data 
WITHOUT_CLASSIFICATION	 must able take away the requisite number cant whered the ducks 
WITHOUT_CLASSIFICATION	 row specific recordwriters 
WITHOUT_CLASSIFICATION	 test perfectly divisible batchsize and decaying factor 
WITHOUT_CLASSIFICATION	 tells you what that mapping 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 initialize with estimated element size 
WITHOUT_CLASSIFICATION	 just return the object need further operation 
WITHOUT_CLASSIFICATION	 todo potential dfs call 
WITHOUT_CLASSIFICATION	 over all the children 
WITHOUT_CLASSIFICATION	 there overlap ranges create combined range 
WITHOUT_CLASSIFICATION	 the global limit optimization triggered will estimate input data actually needed based limit rows estimated input numlimit maxsizeperrow estimatedmap 
WITHOUT_CLASSIFICATION	 when neither nor are not set jobconf should contain only the credential provider path 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 pos change need since weve shrunk the string with same pos 
WITHOUT_CLASSIFICATION	 fair reentrant lock 
WITHOUT_CLASSIFICATION	 still equal compare lock ids 
WITHOUT_CLASSIFICATION	 not using saslkerberos use the password 
WITHOUT_CLASSIFICATION	 set out parameters 
WITHOUT_CLASSIFICATION	 contains nonequi join conditions bail out 
WITHOUT_CLASSIFICATION	 cars 
WITHOUT_CLASSIFICATION	 after using selected generate spills generate nonmatches any 
WITHOUT_CLASSIFICATION	 these parameters must match for all orc files involved merging does not merge the file will put into incompatible file set and will not merged 
WITHOUT_CLASSIFICATION	 get connection 
WITHOUT_CLASSIFICATION	 http transport mode 
WITHOUT_CLASSIFICATION	 joincond represents correlated predicate leftisrewritten rightisrewritten indicates either side has been replaced column alias side not rewritten get its text from the tokenstream for rewritten conditions form the text based the table and column reference 
WITHOUT_CLASSIFICATION	 see spark 
WITHOUT_CLASSIFICATION	 finally 
WITHOUT_CLASSIFICATION	 deserializes bit decimals the maximum bit precision decimal digits 
WITHOUT_CLASSIFICATION	 consider unwinding subclass 
WITHOUT_CLASSIFICATION	 partitionorder 
WITHOUT_CLASSIFICATION	 traling 
WITHOUT_CLASSIFICATION	 currently all spark work the cluster 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 add new item the tez work 
WITHOUT_CLASSIFICATION	 run the script using sqlline 
WITHOUT_CLASSIFICATION	 check the partition spec valid 
WITHOUT_CLASSIFICATION	 note listpartition getpartitions removed with hive because will result oom errors with large addpartitions 
WITHOUT_CLASSIFICATION	 get privileges for this user and its roles this object 
WITHOUT_CLASSIFICATION	 cancel 
WITHOUT_CLASSIFICATION	 this operator allowed before mapjoin eventually mapjoin hint should done away with but since bucketized mapjoin and sortmerge join depend completely needed check the operators which are allowed before mapjoin 
WITHOUT_CLASSIFICATION	 give evaluator chance setup for rawinput execution setup rawinput shape 
WITHOUT_CLASSIFICATION	 otherwise return the arguments 
WITHOUT_CLASSIFICATION	 pop struct union 
WITHOUT_CLASSIFICATION	 try with with decimal input and desired output type 
WITHOUT_CLASSIFICATION	 now union the select operators selectop and selectopclone store the operator that follows the select after the join will 
WITHOUT_CLASSIFICATION	 rows will 
WITHOUT_CLASSIFICATION	 both the handlers should same 
WITHOUT_CLASSIFICATION	 compute the geometric average the ratios all the factors which are nonzero and finite 
WITHOUT_CLASSIFICATION	 supports acidinputformat which uses the key pass rowid info 
WITHOUT_CLASSIFICATION	 restart the sessions until one them refuses restart 
WITHOUT_CLASSIFICATION	 restore the original selected vector 
WITHOUT_CLASSIFICATION	 construct ptf operator 
WITHOUT_CLASSIFICATION	 test partition listing with partial spec 
WITHOUT_CLASSIFICATION	 need the where clause below ensure consistent results with readcommitted 
WITHOUT_CLASSIFICATION	 over all aggregation buffers and see they implement estimable 
WITHOUT_CLASSIFICATION	 check isrepeating propagation 
WITHOUT_CLASSIFICATION	 note only look the schema here deal with complex types somebody has set the reader with whatever ideas they had the schema and just trust the reader produce the cvbs that was asked for however only need look top level columns 
WITHOUT_CLASSIFICATION	 store path prefix this testfilter 
WITHOUT_CLASSIFICATION	 attempt dynamic partitioned hash join since dont have big table index yet must start with estimate numreducers 
WITHOUT_CLASSIFICATION	 all will filtered out 
WITHOUT_CLASSIFICATION	 serialize aggbuffer 
WITHOUT_CLASSIFICATION	 not supported for tables right now 
WITHOUT_CLASSIFICATION	 read basesplit from input 
WITHOUT_CLASSIFICATION	 check this table name qualified not 
WITHOUT_CLASSIFICATION	 short circuit and return the current number rows this 
WITHOUT_CLASSIFICATION	 failure here will cause this query not added the cache 
WITHOUT_CLASSIFICATION	 tables test 
WITHOUT_CLASSIFICATION	 increments file modification time 
WITHOUT_CLASSIFICATION	 exceeds max value 
WITHOUT_CLASSIFICATION	 show table properties populate the output stream 
WITHOUT_CLASSIFICATION	 well formed 
WITHOUT_CLASSIFICATION	 configure http client for ssl 
WITHOUT_CLASSIFICATION	 dummy for backtracking 
WITHOUT_CLASSIFICATION	 string between 
WITHOUT_CLASSIFICATION	 intentionally corrupt some files 
WITHOUT_CLASSIFICATION	 literal double 
WITHOUT_CLASSIFICATION	 try find valid entry but settle for pending entry that all have 
WITHOUT_CLASSIFICATION	 get set recordwriter based the column values 
WITHOUT_CLASSIFICATION	 add the queryid appender the queryid based route 
WITHOUT_CLASSIFICATION	 regular case accessing nested field column 
WITHOUT_CLASSIFICATION	 starttime 
WITHOUT_CLASSIFICATION	 bad files dont pollute the filesystem 
WITHOUT_CLASSIFICATION	 locked invalidated buffer was the list just drop will readded unlock 
WITHOUT_CLASSIFICATION	 remove all the candidate filter operators when get the 
WITHOUT_CLASSIFICATION	 enforce required retain the buffer sizes old files instead orc writer inferring the optimal buffer size 
WITHOUT_CLASSIFICATION	 repeated groupinputspecproto groupedinputspecs 
WITHOUT_CLASSIFICATION	 should not matter which txnmgr used here 
WITHOUT_CLASSIFICATION	 use url param indirectly the name env var that contains the url the urlparam default would look for beelineurldefault url 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 testing multibyte string with reference set mid array 
WITHOUT_CLASSIFICATION	 dont need cancel this token the tokenrenewer for tokens takes care cancelling them 
WITHOUT_CLASSIFICATION	 position 
WITHOUT_CLASSIFICATION	 copy the join and the top sort operator 
WITHOUT_CLASSIFICATION	 test repeating null selection 
WITHOUT_CLASSIFICATION	 definitely long most longs fall here 
WITHOUT_CLASSIFICATION	 maponly subqueries can optimized future not write file 
WITHOUT_CLASSIFICATION	 the formulae jls section 
WITHOUT_CLASSIFICATION	 any table has bad size estimate need fall back sizing each table equally 
WITHOUT_CLASSIFICATION	 todo hive abort handling handling mergemapop 
WITHOUT_CLASSIFICATION	 now all the column values should always return null 
WITHOUT_CLASSIFICATION	 exception 
WITHOUT_CLASSIFICATION	 currently only column related changes can cascaded alter table 
WITHOUT_CLASSIFICATION	 when people forget quote string opop null for example select from sometable where 
WITHOUT_CLASSIFICATION	 gather statistics for the first time and the attach table scan operator 
WITHOUT_CLASSIFICATION	 new value the last isnt clobbered 
WITHOUT_CLASSIFICATION	 now 
WITHOUT_CLASSIFICATION	 the subquery predicate has been hoisted join the subquery predicate replaced true predicate the outer qbs wherehaving clause 
WITHOUT_CLASSIFICATION	 the current record needs returned based the filter conditions specified the user increment the counter 
WITHOUT_CLASSIFICATION	 bad luck handle the corner cases where bytes are multiple blocks 
WITHOUT_CLASSIFICATION	 careful maintenance the flag 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream long 
WITHOUT_CLASSIFICATION	 return loaned session goes back tez pool 
WITHOUT_CLASSIFICATION	 return true the remove was successful false otherwise 
WITHOUT_CLASSIFICATION	 distincts are not allowed with additional job 
WITHOUT_CLASSIFICATION	 this likely shutdown 
WITHOUT_CLASSIFICATION	 even though there are only two delta delta and one deletedelta 
WITHOUT_CLASSIFICATION	 type affinity does not help when type affinity does not match input args 
WITHOUT_CLASSIFICATION	 now 
WITHOUT_CLASSIFICATION	 tuesday january 
WITHOUT_CLASSIFICATION	 file named path doesnt exist nothing validate 
WITHOUT_CLASSIFICATION	 index into partitionspecs 
WITHOUT_CLASSIFICATION	 nnname 
WITHOUT_CLASSIFICATION	 number variables declared with the same data type and default 
WITHOUT_CLASSIFICATION	 issame 
WITHOUT_CLASSIFICATION	 number bytes for register 
WITHOUT_CLASSIFICATION	 everything should same before 
WITHOUT_CLASSIFICATION	 latin capital letter bytes blank byte 
WITHOUT_CLASSIFICATION	 should intercept here for possible decimal vector expression class 
WITHOUT_CLASSIFICATION	 delegation token passed from the client side not set the principal 
WITHOUT_CLASSIFICATION	 from parentrunner retried under exception notified only after exhausting retrycount 
WITHOUT_CLASSIFICATION	 clear the thread locals 
WITHOUT_CLASSIFICATION	 for these positions some variable primitive type string used for the 
WITHOUT_CLASSIFICATION	 iterate for the first time get all the names stages 
WITHOUT_CLASSIFICATION	 cancel the maintenance thread 
WITHOUT_CLASSIFICATION	 partial column statistics grouping attributes case column statistics grouping attribute missing then assume worst case gby rule will emit half the number rows ndvproduct 
WITHOUT_CLASSIFICATION	 todo add allocate overload with offset and length 
WITHOUT_CLASSIFICATION	 todo parts this should moved out tezsession reuse the clients but theres good place for that right now hive 
WITHOUT_CLASSIFICATION	 this null check specifically done the same class used handle both incremental and bootstrap replication scenarios for create function when doing bootstrap not have event for this event but rather when bootstrap started and hence pass null dmd for bootstrapthere should better way this but might required lot changes across different handlers unless this common pattern that seen leaving this here 
WITHOUT_CLASSIFICATION	 dynamic usecase called through 
WITHOUT_CLASSIFICATION	 clone rhssemijoin 
WITHOUT_CLASSIFICATION	 use the rowresolver from the input operator generate input objectinspector that can used initialize the udtf then the 
WITHOUT_CLASSIFICATION	 param conf throws see configuration 
WITHOUT_CLASSIFICATION	 give more 
WITHOUT_CLASSIFICATION	 string can not parsed converter will return null 
WITHOUT_CLASSIFICATION	 allow not but throw error for rest 
WITHOUT_CLASSIFICATION	 now 
WITHOUT_CLASSIFICATION	 inputformat 
WITHOUT_CLASSIFICATION	 get all items into array and sort them 
WITHOUT_CLASSIFICATION	 now prepare partnames with only partitions tabparttabpart 
WITHOUT_CLASSIFICATION	 use linear extrapolation more complicated one can added the 
WITHOUT_CLASSIFICATION	 given that not delete empty slot means match findreadslot key key slot slot pairindex pairindex empty slot 
WITHOUT_CLASSIFICATION	 will store all the new changed properties the job the udf context the the and setschema 
WITHOUT_CLASSIFICATION	 private methods 
WITHOUT_CLASSIFICATION	 the owner will also have select with grant privileges new view 
WITHOUT_CLASSIFICATION	 year granularity 
WITHOUT_CLASSIFICATION	 use only control chars that are very unlikely part the string the following mightlikely used text files for strings horizontal tab line feed form feed carriage return escape esc gcc only 
WITHOUT_CLASSIFICATION	 disable the expandevents for the purpose backward compatibility 
WITHOUT_CLASSIFICATION	 also make sure constant 
WITHOUT_CLASSIFICATION	 process minmax 
WITHOUT_CLASSIFICATION	 hash from the fetcher 
WITHOUT_CLASSIFICATION	 decompose and expression into its parts before checking the entire expression candidate because each part may candidate for replicating transitively over equijoin condition 
WITHOUT_CLASSIFICATION	 remove cast boolean not null boolean vice versa filter accepts nullable and notnullable conditions but cast might get the way 
WITHOUT_CLASSIFICATION	 key key key 
WITHOUT_CLASSIFICATION	 defines interface for reexecution logics fixme rethink methods 
WITHOUT_CLASSIFICATION	 exponent read from field 
WITHOUT_CLASSIFICATION	 input includes table columns partition columns 
WITHOUT_CLASSIFICATION	 check that have two deltas 
WITHOUT_CLASSIFICATION	 repeating else expression 
WITHOUT_CLASSIFICATION	 methods setreset getpartition modifier 
WITHOUT_CLASSIFICATION	 during the dfs traversal the ast descendant likely set error because subtree unlikely also group expression for example query such select concatkey from src group concatkey key will processed before concatkey and since key not group expression error will set ctx columnexprprocessor 
WITHOUT_CLASSIFICATION	 trigger 
WITHOUT_CLASSIFICATION	 the following information for creating vectorizedrowbatch and for helping with knowing how the table partitioned will stored mapwork and reducework 
WITHOUT_CLASSIFICATION	 get file metadata from cache create the reader and read 
WITHOUT_CLASSIFICATION	 maponly job can optimized instead converting mapreduce job can have another map job the same avoid the cost sorting the mapreduce phase better approach would write into local file and then have maponly job add the limit operator get the value fields 
WITHOUT_CLASSIFICATION	 stop wont able forward 
WITHOUT_CLASSIFICATION	 maintain count outstanding requests for tokenidentifier count goes safe remove the token 
WITHOUT_CLASSIFICATION	 reads all the delete events into memory during initialization and closes the delete event readers after exception gets thrown during initialization may have close any readers that are still left open 
WITHOUT_CLASSIFICATION	 note conditional vector expression not call 
WITHOUT_CLASSIFICATION	 calcite daytime interval millis value bigdecimal seconds converted millis 
WITHOUT_CLASSIFICATION	 weve seen both root and child can bail 
WITHOUT_CLASSIFICATION	 should have files 
WITHOUT_CLASSIFICATION	 remove default partition from partition names and get aggregate 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 otherwise tried 
WITHOUT_CLASSIFICATION	 anything remains this where starts 
WITHOUT_CLASSIFICATION	 iterating through the buffer should not cause the next buffer fetched 
WITHOUT_CLASSIFICATION	 now 
WITHOUT_CLASSIFICATION	 querytimeout 
WITHOUT_CLASSIFICATION	 check for dynamic partitions 
WITHOUT_CLASSIFICATION	 second check the tasks looked for must not have been accessed more than once result the traversal note that actually wind accessing times because each visit counts twice once check for existence and once visit 
WITHOUT_CLASSIFICATION	 add role privileges 
WITHOUT_CLASSIFICATION	 how much can the fraction moved 
WITHOUT_CLASSIFICATION	 check any the txns the list aborted 
WITHOUT_CLASSIFICATION	 add another via cachedstore 
WITHOUT_CLASSIFICATION	 tez internals may register the same task completing multiple times 
WITHOUT_CLASSIFICATION	 the conversion standard object inspector was necessitated hive the issue happens when select operator preceeds this operator the case subquery the select operator does not allocate new object hold the deserialized row this affects the operation the smb join which puts the object priority queue since all elements the priority queue point the same object the join was resulting incorrect results the fix make copy the object done the processop phase below this however necessitates change the object inspector that can used processing the row downstream 
WITHOUT_CLASSIFICATION	 provider setup 
WITHOUT_CLASSIFICATION	 make sure delta files are written with indexes compression and dictionary 
WITHOUT_CLASSIFICATION	 translate the positions correlated variables relative the join output leaving room for valuegenfieldoffset because valuegenerators are joined with the original left input the rel 
WITHOUT_CLASSIFICATION	 also test getting table from specific 
WITHOUT_CLASSIFICATION	 lets try parse timestamp with time zone and transform 
WITHOUT_CLASSIFICATION	 note that here ignore nanos part hive timestamp since nanos are dropped when reading hive from pig design 
WITHOUT_CLASSIFICATION	 this ptfdesc for mapside ptf operation 
WITHOUT_CLASSIFICATION	 test with predicates such that partition pruning works 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 offer accepted evicted 
WITHOUT_CLASSIFICATION	 pick unknown case 
WITHOUT_CLASSIFICATION	 these remaining combinations below definitely result overflow 
WITHOUT_CLASSIFICATION	 dont entirely believe that everything cleaned when close called based watching the trace logs 
WITHOUT_CLASSIFICATION	 replication dest will not external override set 
WITHOUT_CLASSIFICATION	 all join tables have keys doesnt matter what generate 
WITHOUT_CLASSIFICATION	 this will trigger the column pruner collect view column authorization info 
WITHOUT_CLASSIFICATION	 special handling for sql reload function 
WITHOUT_CLASSIFICATION	 why even compute syntheticprops isoriginal 
WITHOUT_CLASSIFICATION	 notifyreused implies that buffer already locked its also called once for new buffers that are not cached yet dont notify cache policy 
WITHOUT_CLASSIFICATION	 missing stripe stats old format numrows then its empty file and statistics present have differentiate stats empty file missing stats old format 
WITHOUT_CLASSIFICATION	 expected 
WITHOUT_CLASSIFICATION	 check the virtualcolumn directly 
WITHOUT_CLASSIFICATION	 have make sure theres slash after har otherwise resolve doesnt work 
WITHOUT_CLASSIFICATION	 attempting get the password should throw exception 
WITHOUT_CLASSIFICATION	 try outer row resolver 
WITHOUT_CLASSIFICATION	 disregard nulls for processing other words the arithmetic operation performed even one more inputs are null this improve speed avoiding conditional checks the inner loop 
WITHOUT_CLASSIFICATION	 first nonnull item 
WITHOUT_CLASSIFICATION	 call getsplits 
WITHOUT_CLASSIFICATION	 make sure got the right implementation columnmapping 
WITHOUT_CLASSIFICATION	 add the new entry 
WITHOUT_CLASSIFICATION	 return our mocked objects 
WITHOUT_CLASSIFICATION	 trigger update needed 
WITHOUT_CLASSIFICATION	 refactored out the equality case parsejoincondition that this can recursively called its left tree the case when only left sources are referenced predicate 
WITHOUT_CLASSIFICATION	 used for testing simulate method timeout 
WITHOUT_CLASSIFICATION	 newer versions and later support offsetfetch 
WITHOUT_CLASSIFICATION	 mainly for verification 
WITHOUT_CLASSIFICATION	 this object should the same cache otherwise must removed due init error 
WITHOUT_CLASSIFICATION	 build input output position map 
WITHOUT_CLASSIFICATION	 test rpcserveraddress configured 
WITHOUT_CLASSIFICATION	 modifier letter arrowhead bytes 
WITHOUT_CLASSIFICATION	 use the tablealias generate keybasealias 
WITHOUT_CLASSIFICATION	 replace sel selexprs 
WITHOUT_CLASSIFICATION	 tinsert and tinsert 
WITHOUT_CLASSIFICATION	 clone filtermap 
WITHOUT_CLASSIFICATION	 have plan prefer its logical result schema its available otherwise try digging out fetch task failing that give 
WITHOUT_CLASSIFICATION	 acid 
WITHOUT_CLASSIFICATION	 instantiate decimal vector expression the method sets the output column and type information 
WITHOUT_CLASSIFICATION	 nonspace character 
WITHOUT_CLASSIFICATION	 randomly pick the character corresponding the field and convert byte array 
WITHOUT_CLASSIFICATION	 this string reader should simply redirect its own seek what other types already 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 stddevpopx power sumx sumx sumx countx countx stddevsampx power sumx sumx sumx countx 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 optional bool default false 
WITHOUT_CLASSIFICATION	 flush any pending scheduler runs which may blocked wait seconds for the run complete 
WITHOUT_CLASSIFICATION	 for now dont support group decimal keys 
WITHOUT_CLASSIFICATION	 uppercase first letter 
WITHOUT_CLASSIFICATION	 above enforce certain order when the reutilization 
WITHOUT_CLASSIFICATION	 target table has cols ccc and partition col and had set currentdate want end with insert into target select currentdate where 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 make sure dont modify the config rpcconfiguration 
WITHOUT_CLASSIFICATION	 use check set columns are all partition columns true only all columns bitset are partition columns 
WITHOUT_CLASSIFICATION	 this will only send update its necessary 
WITHOUT_CLASSIFICATION	 disallow subqueries which hive doesnt currently support 
WITHOUT_CLASSIFICATION	 went through correctly then also hcatrecord and also equal the above and deepcopy and this holds through for multiple levels more serialization well 
WITHOUT_CLASSIFICATION	 shuffle write metrics 
WITHOUT_CLASSIFICATION	 txn write bucket txn write bucket bucket 
WITHOUT_CLASSIFICATION	 run this rule later stages since many calcite rules cant deal with semijoin 
WITHOUT_CLASSIFICATION	 take row from the current buffered batch 
WITHOUT_CLASSIFICATION	 the general idea here create created created created for each job submitted the node number generated and the payload the jobid basically this keeps track the order which jobs were submitted and zookeepercleanup uses this purge old job info since the jobsid node has createupdate timestamp this whole thing can removed 
WITHOUT_CLASSIFICATION	 logdebugclassname processop all keylength 
WITHOUT_CLASSIFICATION	 incase the join has extra keys other than bucketed columns partition keys need updated small tables 
WITHOUT_CLASSIFICATION	 values within the column type bounds 
WITHOUT_CLASSIFICATION	 init output object inspectors the return type for partial aggregation still list doubles but add the percentile values requested the end and handle before pass things the parent method the return type for final and complete full aggregation result which also 
WITHOUT_CLASSIFICATION	 all partitions and all but default 
WITHOUT_CLASSIFICATION	 resultsetnext blocks until the async query complete 
WITHOUT_CLASSIFICATION	 green and red qualify 
WITHOUT_CLASSIFICATION	 create map join task and set big table bigtableposition 
WITHOUT_CLASSIFICATION	 string variable 
WITHOUT_CLASSIFICATION	 and remind ourselves perform incremental normalization 
WITHOUT_CLASSIFICATION	 jaas login from ticket cache setup the client 
WITHOUT_CLASSIFICATION	 value therefore always logicalcorrelates left input which outer query 
WITHOUT_CLASSIFICATION	 createdirectories creates all nonexistent parent directories 
WITHOUT_CLASSIFICATION	 log warning hivedefaultxml found the classpath 
WITHOUT_CLASSIFICATION	 add post script 
WITHOUT_CLASSIFICATION	 number output columns array path expressions each which corresponds column array returned column values object pool nonnull text avoid creating objects all the time 
WITHOUT_CLASSIFICATION	 are now some intersecting buffer find the first intersecting buffer 
WITHOUT_CLASSIFICATION	 replace stdout and run command 
WITHOUT_CLASSIFICATION	 set query timeout second 
WITHOUT_CLASSIFICATION	 reject any clauses that are against column that isnt the rowid mapping indexed 
WITHOUT_CLASSIFICATION	 the perbatch setup for inner join 
WITHOUT_CLASSIFICATION	 swap column vectors simulate change data 
WITHOUT_CLASSIFICATION	 for negative number initializing bits 
WITHOUT_CLASSIFICATION	 put into the cache before initialized make sure can catch 
WITHOUT_CLASSIFICATION	 the fourth parameter precision factor has been specified make sure its 
WITHOUT_CLASSIFICATION	 splits are not shared across different partitions with different input formats 
WITHOUT_CLASSIFICATION	 the database newer than the drop event then noop 
WITHOUT_CLASSIFICATION	 test set and append methods 
WITHOUT_CLASSIFICATION	 get the positions for sorted and bucketed columns for sorted columns also get the order ascendingdescending that should also match for this converted maponly job get the positions for sorted and bucketed columns for sorted columns also get the order ascendingdescending that should 
WITHOUT_CLASSIFICATION	 done 
WITHOUT_CLASSIFICATION	 all other exceptions are considered emanating from unauthorized accesses 
WITHOUT_CLASSIFICATION	 expects both parameters local 
WITHOUT_CLASSIFICATION	 location specified ensure that full qualified name 
WITHOUT_CLASSIFICATION	 comment 
WITHOUT_CLASSIFICATION	 always allow the operation not replication scope 
WITHOUT_CLASSIFICATION	 need add this branch the key value info 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 implement vectorized function hexstring returning string 
WITHOUT_CLASSIFICATION	 move the next bit vector 
WITHOUT_CLASSIFICATION	 run without cascade 
WITHOUT_CLASSIFICATION	 bucketing isnt consistent there are bucket columns optimizer does not extract multiple column predicates for this 
WITHOUT_CLASSIFICATION	 debug display 
WITHOUT_CLASSIFICATION	 the number reducers should used for those bottom layer reducesinkoperators 
WITHOUT_CLASSIFICATION	 copy all the histogram bins from and other into overstuffed histogram 
WITHOUT_CLASSIFICATION	 view already exists thus should replacing 
WITHOUT_CLASSIFICATION	 larger than max compile duration used test 
WITHOUT_CLASSIFICATION	 delete any tables other than the source tables 
WITHOUT_CLASSIFICATION	 pass the row though the operator tree guaranteed that not more than row can produced from input row 
WITHOUT_CLASSIFICATION	 first write chunk length 
WITHOUT_CLASSIFICATION	 todo exprnodedesc expression tree could just use that and rid filterg 
WITHOUT_CLASSIFICATION	 null null 
WITHOUT_CLASSIFICATION	 the update has failed wed try with another candidate first but only the same priority 
WITHOUT_CLASSIFICATION	 metrics are first dumped temp file which then renamed the destination 
WITHOUT_CLASSIFICATION	 this mapping the values the small table hash table that will copied the small table result portion the output that mapping the lazybinary field order 
WITHOUT_CLASSIFICATION	 and only stmt implicit txn and uses acid resource 
WITHOUT_CLASSIFICATION	 new input column numbers 
WITHOUT_CLASSIFICATION	 check reduce vertex and its children more than check all the child ops are reduce output operators 
WITHOUT_CLASSIFICATION	 high word must zero check for overflow digits middle word 
WITHOUT_CLASSIFICATION	 returns job status for list input jobs list 
WITHOUT_CLASSIFICATION	 singlecolumn string outer null detection 
WITHOUT_CLASSIFICATION	 build rel for clause 
WITHOUT_CLASSIFICATION	 all rows from both side will present resultset 
WITHOUT_CLASSIFICATION	 the position the column keys the dummy grouping set column 
WITHOUT_CLASSIFICATION	 essentially partition values are represented strings but want the actual object type associated 
WITHOUT_CLASSIFICATION	 txn was committed but notification was not received was aborted either case can clean 
WITHOUT_CLASSIFICATION	 using metrics thus should always check whether this nonnull before using 
WITHOUT_CLASSIFICATION	 without hive with hive 
WITHOUT_CLASSIFICATION	 just move the marker according delta hit the limit the list was exhausted 
WITHOUT_CLASSIFICATION	 not present just unmanaged 
WITHOUT_CLASSIFICATION	 method that provides similar filter functionality filterholder above useful when 
WITHOUT_CLASSIFICATION	 otherwise subdirectories produced hive union operations wont readable 
WITHOUT_CLASSIFICATION	 the current members deserializeread have the field value 
WITHOUT_CLASSIFICATION	 not try finish and flush inprogress group because correct values require the last group batch 
WITHOUT_CLASSIFICATION	 set one null value for possible later use 
WITHOUT_CLASSIFICATION	 todo move into the below account for release call 
WITHOUT_CLASSIFICATION	 from tableacidtbl where inselect from tablenonacidorctbl 
WITHOUT_CLASSIFICATION	 use the move operation that created atomic 
WITHOUT_CLASSIFICATION	 this will work only the files are local files webhcat server which not very useful since users might not have access that file system this likely the hive issue 
WITHOUT_CLASSIFICATION	 truncate external table not allowed 
WITHOUT_CLASSIFICATION	 sleep twice the ttl interval things should have been cleaned then 
WITHOUT_CLASSIFICATION	 get the key column names for each side the join and check the keys are all constants 
WITHOUT_CLASSIFICATION	 set resolver and resolver context 
WITHOUT_CLASSIFICATION	 gss name for server 
WITHOUT_CLASSIFICATION	 see comments for next method 
WITHOUT_CLASSIFICATION	 can generate multiphase aggregates one distinct aggregate filter summinmaxcount nondistinct aggregate one more nondistinct aggregates 
WITHOUT_CLASSIFICATION	 password available url needs striped 
WITHOUT_CLASSIFICATION	 spot check correctness decimal column subtract decimal scalar the case for addition checks all the cases for the template dont that redundantly here 
WITHOUT_CLASSIFICATION	 need get connector look the users authorizations not otherwise specified 
WITHOUT_CLASSIFICATION	 otherwise find the row groups that were selected the client 
WITHOUT_CLASSIFICATION	 are not unique keys for 
WITHOUT_CLASSIFICATION	 enabled 
WITHOUT_CLASSIFICATION	 for expr aliases should iterated the order they are specified the query 
WITHOUT_CLASSIFICATION	 override this method forward its outputs 
WITHOUT_CLASSIFICATION	 implementation notes since only local file systems are supported there need use hadoop version path class javanio package provides modern implementation file and directory operations which better then the traditional javaio are using here particular supports atomic creation temporary files with specified permissions the specified directory this also avoids various attacks possible when temp file name generated first followed file creation see for the description nio api and for the description interoperability between legacy api nio api avoid race conditions with readers the metrics file the implementation dumps metrics temporary file the same directory the actual metrics file and then renames the destination since both are located the same filesystem this rename likely atomic long the underlying support atomic renames note this reporter very similar would good unify the two 
WITHOUT_CLASSIFICATION	 try the udf 
WITHOUT_CLASSIFICATION	 principalname 
WITHOUT_CLASSIFICATION	 disallow udfs that can potentially allow untrusted code execution 
WITHOUT_CLASSIFICATION	 nonnls 
WITHOUT_CLASSIFICATION	 task hash map 
WITHOUT_CLASSIFICATION	 perform any value expressions results will into scratch columns 
WITHOUT_CLASSIFICATION	 this akin cbo cumulative cardinality model 
WITHOUT_CLASSIFICATION	 child the boolean expression 
WITHOUT_CLASSIFICATION	 all columns got constant folded then disable this optimization 
WITHOUT_CLASSIFICATION	 the worker should remove the subdir for aborted transaction 
WITHOUT_CLASSIFICATION	 verify query result 
WITHOUT_CLASSIFICATION	 check the union fields length and offset 
WITHOUT_CLASSIFICATION	 test nonvectorized nonacid combine 
WITHOUT_CLASSIFICATION	 close abort 
WITHOUT_CLASSIFICATION	 tez blurs the boundary between map and reduce thus has its own config 
WITHOUT_CLASSIFICATION	 filesaddedchecksum 
WITHOUT_CLASSIFICATION	 number rows for the key the given table 
WITHOUT_CLASSIFICATION	 workerid 
WITHOUT_CLASSIFICATION	 for whatever failure reason including that trash has lower encryption zone retry with force delete 
WITHOUT_CLASSIFICATION	 acid write add plan output else dont bother 
WITHOUT_CLASSIFICATION	 bits minus byte 
WITHOUT_CLASSIFICATION	 verify that driver fails due older version schema 
WITHOUT_CLASSIFICATION	 count number false values seen far 
WITHOUT_CLASSIFICATION	 alter table with invalid column type 
WITHOUT_CLASSIFICATION	 get full objects for oracleetc batches 
WITHOUT_CLASSIFICATION	 cast int does not work expected still considered string order for some reason 
WITHOUT_CLASSIFICATION	 test that existing sharedread partition with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 will start new key group can call flush the buffer children from lastchildindex inclusive the last child and propagate processgroup those children 
WITHOUT_CLASSIFICATION	 todo hive propagating abort dummyops 
WITHOUT_CLASSIFICATION	 maxed out capacity this move should fail the session 
WITHOUT_CLASSIFICATION	 hiveserver auth configuration 
WITHOUT_CLASSIFICATION	 for use columnscalar and scalarcolumn arithmetic for null propagation 
WITHOUT_CLASSIFICATION	 dbname 
WITHOUT_CLASSIFICATION	 mapping from constraint name list foreign keys 
WITHOUT_CLASSIFICATION	 remove any existing entries that are contained the new one 
WITHOUT_CLASSIFICATION	 orcsplitisacid returns true know for sure acid 
WITHOUT_CLASSIFICATION	 here ther were locks that blocked any locks locksbeingchecked acquire them all 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 there are inputs the execution engine skips the operator tree prevent from happening opaque zerorows input added here when needed 
WITHOUT_CLASSIFICATION	 column names column names default partition name case null empty value maximum dynamic partitions created per mapperreducer 
WITHOUT_CLASSIFICATION	 spark memory per task and total number cores 
WITHOUT_CLASSIFICATION	 will always release copies the end 
WITHOUT_CLASSIFICATION	 schema name 
WITHOUT_CLASSIFICATION	 specifies and pattern 
WITHOUT_CLASSIFICATION	 there should only directory left base 
WITHOUT_CLASSIFICATION	 create the row resolver for this operator from the output columns 
WITHOUT_CLASSIFICATION	 create the 
WITHOUT_CLASSIFICATION	 the selected array already filled want 
WITHOUT_CLASSIFICATION	 the sign encoded the least significant bit need adjust our decimal before conversion binary positive multiply negative logic does negate the biginteger not have since fasthivedecimal stores the numbers unsigned fast fast and fast need subtract one though and then multiply and add the sign bit consider this could combined 
WITHOUT_CLASSIFICATION	 metastore ssl settings 
WITHOUT_CLASSIFICATION	 optional group containing repeated anonymous group map containing 
WITHOUT_CLASSIFICATION	 set checkpoint task dependant create table task same dump retried for 
WITHOUT_CLASSIFICATION	 set reducer parallelism 
WITHOUT_CLASSIFICATION	 below are the properties that need set based what database this test going run 
WITHOUT_CLASSIFICATION	 keep track 
WITHOUT_CLASSIFICATION	 test that committing unlocks 
WITHOUT_CLASSIFICATION	 for backwards compatibility reasons honor the older set different from default 
WITHOUT_CLASSIFICATION	 tinyint smallint int bigint double float string string struct array map bool complex decimal char varchar date timestamp binary 
WITHOUT_CLASSIFICATION	 gets file data for particular offsets the range list modified place then returned since the list head could have changed ranges are replaced with cached ranges case partial overlap with cached data full cache blocks are always returned theres capacity for partial matches return type the rules are follows the requested range starts the middle cached range that cached range will not returned default and are cached the request for will only return from cache this may configurable impls this because assume wellknown range start offsets are used rgstripe offsets request from the middle the start doesnt make sense the requested range ends the middle cached range that entire cached range will returned and are cached the request for will return both ranges should really same however currently orc uses estimated end offsets fact know such cases that partiallymatched cached block can thrown away the reader will never touch but need code the reader handle such cases avoid disk reads for these tails real unmatched ranges some sort invalidcachechunk could placed avoid them todo param base base offset for the ranges stripestream offset case orc 
WITHOUT_CLASSIFICATION	 simple division yes should something like this but later anyway this will eventually replaced intrinsics java 
WITHOUT_CLASSIFICATION	 process singlecolumn string inner join vectorized row batch 
WITHOUT_CLASSIFICATION	 hashing the string potentially expensive better branch additionally not looking values for nulls allows not reset the values 
WITHOUT_CLASSIFICATION	 create estimators 
WITHOUT_CLASSIFICATION	 the stage that this vertex belongs 
WITHOUT_CLASSIFICATION	 reenable the node task succeeded slot may have become available also reset commfailures since task was able communicate back and indicate success 
WITHOUT_CLASSIFICATION	 dont use streaming for distinct cases 
WITHOUT_CLASSIFICATION	 only persist inputoutput format metadata when explicitly specified 
WITHOUT_CLASSIFICATION	 some mix tests string char varchar char varchar string 
WITHOUT_CLASSIFICATION	 need use cleanup interval which how often the cleanup thread will kick and check see any the connections can expired dont want this too often because itd like having minigc going off every often limit minimum the client has explicitly set larger timeout the cache though respect that and use that 
WITHOUT_CLASSIFICATION	 avoid adding group for correlated inexists queries since this rewritting into semijoin 
WITHOUT_CLASSIFICATION	 insert new filter between and parent 
WITHOUT_CLASSIFICATION	 unknown unknown 
WITHOUT_CLASSIFICATION	 not from this 
WITHOUT_CLASSIFICATION	 see comments for sqlprecision 
WITHOUT_CLASSIFICATION	 insert the select operator between 
WITHOUT_CLASSIFICATION	 make sure reuse changes the fifo order the session 
WITHOUT_CLASSIFICATION	 the first line check should handle the oob 
WITHOUT_CLASSIFICATION	 get the bucket 
WITHOUT_CLASSIFICATION	 set the last query time 
WITHOUT_CLASSIFICATION	 for output rows this operator 
WITHOUT_CLASSIFICATION	 already used update the port and retry 
WITHOUT_CLASSIFICATION	 delete all things 
WITHOUT_CLASSIFICATION	 the silly looking call builder below get the default value session timeout from curator which itself exposes system property 
WITHOUT_CLASSIFICATION	 allow cte definitions views can end with hierarchy cte definitions the top level query statement where view referenced views may refer other views the scoping rules use are search for cte from the current outwards order disambiguate between ctes are different levels qualifyprefix them with the the they appear when adding them the codealiastoctescode map 
WITHOUT_CLASSIFICATION	 keep partkeyvalmap synch well 
WITHOUT_CLASSIFICATION	 partspec mapping from partition column name its value 
WITHOUT_CLASSIFICATION	 validtxnlist 
WITHOUT_CLASSIFICATION	 testing substring index starting with and length equal array length 
WITHOUT_CLASSIFICATION	 init input object inspectors 
WITHOUT_CLASSIFICATION	 second check this work has multiple reducesinks split 
WITHOUT_CLASSIFICATION	 todo enable metadatafilter using configuration path file metadatafilter filter api 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 return the settable equivalent object inspector for primitive categories for for table containing partitions and possibly different from the table return the settable inspector for the inspector for 
WITHOUT_CLASSIFICATION	 parsing the keys and values one one 
WITHOUT_CLASSIFICATION	 reduce side optimized plan 
WITHOUT_CLASSIFICATION	 set command currently not authorized through the api 
WITHOUT_CLASSIFICATION	 throw hiveexception the tablepartition bucketized 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 note after this the caller must send the downgrade message downgradedtask outside the writelock preferably before exiting 
WITHOUT_CLASSIFICATION	 errors 
WITHOUT_CLASSIFICATION	 were the only node just clear out the expression 
WITHOUT_CLASSIFICATION	 user explicitly specified queue name 
WITHOUT_CLASSIFICATION	 obtain inspector for schema 
WITHOUT_CLASSIFICATION	 get the size cache after 
WITHOUT_CLASSIFICATION	 restore 
WITHOUT_CLASSIFICATION	 byte values 
WITHOUT_CLASSIFICATION	 now test again with seed 
WITHOUT_CLASSIFICATION	 exprs 
WITHOUT_CLASSIFICATION	 skip the first which always required 
WITHOUT_CLASSIFICATION	 convert millis result back days 
WITHOUT_CLASSIFICATION	 indexcolumnvector includes the keys map 
WITHOUT_CLASSIFICATION	 javaobject primitives javaarray 
WITHOUT_CLASSIFICATION	 populate the names and order columns for the first table 
WITHOUT_CLASSIFICATION	 input files can pruned 
WITHOUT_CLASSIFICATION	 please take look the instructions hiveauthorizerjava before 
WITHOUT_CLASSIFICATION	 allowed operations intervalyearmonth intervalyearmonth intervalyearmonth intervalyearmonth date date operands reversible intervalyearmonth timestamp timestamp operands reversible intervaldaytime intervaldaytime intervaldaytime intervaldaytime date timestamp operands reversible intervaldaytime timestamp timestamp operands reversible 
WITHOUT_CLASSIFICATION	 try using permanent function and verify its only one that shows 
WITHOUT_CLASSIFICATION	 the join task converted mapjoin task this can only happen set true conditional task was 
WITHOUT_CLASSIFICATION	 todo hive report fatal error over the umbilical 
WITHOUT_CLASSIFICATION	 alias filter mapping 
WITHOUT_CLASSIFICATION	 inner map 
WITHOUT_CLASSIFICATION	 extract rest join predicate info infer the rest join condition that will added the filters join conditions that are not part 
WITHOUT_CLASSIFICATION	 the expiring session may may not the pool 
WITHOUT_CLASSIFICATION	 hive treats names that start with internalnames change the names dont run into this issue when converting back hive ast 
WITHOUT_CLASSIFICATION	 not proactively remove locked items from the heap and opportunistically try remove from the list since eviction mostly from the list eviction stumbles upon locked item either will remove from cache when unlock are going put back update depending whether this has happened this should cause most the expensive cache update work happen unlock not blocking processing 
WITHOUT_CLASSIFICATION	 for jackson 
WITHOUT_CLASSIFICATION	 dont remove the old entry still refers the old and need lookup later 
WITHOUT_CLASSIFICATION	 remove the pwd from conf file that job tracker doesnt show this logs 
WITHOUT_CLASSIFICATION	 add ngram estimation only the context matches 
WITHOUT_CLASSIFICATION	 routines cant used directly 
WITHOUT_CLASSIFICATION	 partition columns 
WITHOUT_CLASSIFICATION	 when type not set init hasnt been called yet 
WITHOUT_CLASSIFICATION	 dont wait for the thread 
WITHOUT_CLASSIFICATION	 create reduce 
WITHOUT_CLASSIFICATION	 can happen this operator does not carry forward the previous bucketing columns for another join operator which does not carry one the sides key columns 
WITHOUT_CLASSIFICATION	 read lock for get operation 
WITHOUT_CLASSIFICATION	 all the fields this event are final reason create new one for each listener 
WITHOUT_CLASSIFICATION	 the simd optimized form 
WITHOUT_CLASSIFICATION	 validations been done compile time validation needed here 
WITHOUT_CLASSIFICATION	 single double value 
WITHOUT_CLASSIFICATION	 tabledb 
WITHOUT_CLASSIFICATION	 check projrel only projects one expression check this project only projects one expression scalar 
WITHOUT_CLASSIFICATION	 our reading positioned the key 
WITHOUT_CLASSIFICATION	 get the mapredlocalwork 
WITHOUT_CLASSIFICATION	 restrictionm the case implied group correlated subquery the subquery always returns row exists subquery with implied gby will always return true whereas algebraically transforming join may not return true see specification doc for details similarly not exists subquery with implied gby will always return false 
WITHOUT_CLASSIFICATION	 columns where the length the bucketed columns 
WITHOUT_CLASSIFICATION	 config variable set via system property config variable set the cli config variable not the default list config variables and config variable the default list config variables but which has not been overridden 
WITHOUT_CLASSIFICATION	 this point are done processing the input close the record processor 
WITHOUT_CLASSIFICATION	 init output object inspectors the return type for partial aggregation still list doubles but add the percentile values requested the end and handle before pass things the parent method the return type for final and complete full aggregation result which 
WITHOUT_CLASSIFICATION	 list columns names and maps them indices 
WITHOUT_CLASSIFICATION	 perform major compaction nothing should change both deltas and base dirs should have the same name 
WITHOUT_CLASSIFICATION	 this the vectorized row batch description the output the native vectorized map join operator based the incoming vectorization context its projection may include 
WITHOUT_CLASSIFICATION	 having non null create table grants privileges causes problems the tests that compares underlying thrift table object created table with table object that was fetched from metastore this because the fetch does not populate the privileges field table 
WITHOUT_CLASSIFICATION	 todo the below assumes that all the arguments are the same type 
WITHOUT_CLASSIFICATION	 cancel the delegation token 
WITHOUT_CLASSIFICATION	 all drones where abandoned host try replacing them 
WITHOUT_CLASSIFICATION	 hbase api convoluted 
WITHOUT_CLASSIFICATION	 theres always just one file that have merged the uniondpetc should already account for the path 
WITHOUT_CLASSIFICATION	 tell our super class will evaluating our scratch bytescolumnvector 
WITHOUT_CLASSIFICATION	 verify that field changes are consistent with what hive does note could handle this 
WITHOUT_CLASSIFICATION	 finalize the previous record 
WITHOUT_CLASSIFICATION	 find the firing rule find the rule from the stack specified 
WITHOUT_CLASSIFICATION	 following the main loop which iterates through all the cookies send the client the generated cookies are the format hiveserverauthvalue cookie which identified hiveserver generated cookie validated calling the validation passes send the username for which the cookie validated the caller client side 
WITHOUT_CLASSIFICATION	 currently getimportedkeys always returns empty resultset for hive 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 fallthrough 
WITHOUT_CLASSIFICATION	 special case for handling false constants 
WITHOUT_CLASSIFICATION	 undone until fast binarysortable supports complex types disable 
WITHOUT_CLASSIFICATION	 update nondistinct key aggregations keycolxtcoly 
WITHOUT_CLASSIFICATION	 finally exclude preds that are already the subtree given the metadata provider note this the last step trying avoid the expensive call the metadata provider 
WITHOUT_CLASSIFICATION	 lookup the state the map 
WITHOUT_CLASSIFICATION	 changing string value getcharacterlength should update accordingly 
WITHOUT_CLASSIFICATION	 createtable event partitioned table 
WITHOUT_CLASSIFICATION	 create file the folder mark 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 remove the discardable operator 
WITHOUT_CLASSIFICATION	 for jackson instantiate 
WITHOUT_CLASSIFICATION	 need use the opparsecontext the child selectoperator replace the 
WITHOUT_CLASSIFICATION	 free any previously allocated buffers that are referenced vector 
WITHOUT_CLASSIFICATION	 first apply configuration applicable both hive cli and hiveserver not adding any authorization related restrictions hive cli grant all privileges for table its owner set this cli well that owner has permissions via hiveserver well 
WITHOUT_CLASSIFICATION	 generate ngrams wherever the context matches 
WITHOUT_CLASSIFICATION	 first just check that this translates 
WITHOUT_CLASSIFICATION	 this method contains various asserts warn environment variables are buggy state 
WITHOUT_CLASSIFICATION	 convert skewed table nonskewed table 
WITHOUT_CLASSIFICATION	 fall through 
WITHOUT_CLASSIFICATION	 aggr checks for sorted arglist 
WITHOUT_CLASSIFICATION	 executed relevant and used contain all the other details about the table not 
WITHOUT_CLASSIFICATION	 metastore and some partitions may have data based other filters 
WITHOUT_CLASSIFICATION	 create table select statement 
WITHOUT_CLASSIFICATION	 launch tez job 
WITHOUT_CLASSIFICATION	 noop 
WITHOUT_CLASSIFICATION	 mapper can span multiple filespartitions the serializers need reset the input file changed 
WITHOUT_CLASSIFICATION	 need test this with llap settings which requires some additional configurations set 
WITHOUT_CLASSIFICATION	 check single column for repeating 
WITHOUT_CLASSIFICATION	 this table not good big table 
WITHOUT_CLASSIFICATION	 individual column becomes string 
WITHOUT_CLASSIFICATION	 now load data into the tables and see incremental repl dropload can duplicate 
WITHOUT_CLASSIFICATION	 undone performance problem with conversion string then bytes 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 large fit into main memory 
WITHOUT_CLASSIFICATION	 after deserialization need recreate the txntowriteidlist its not under jsonproperty 
WITHOUT_CLASSIFICATION	 comment from booleanwritable date value boolean doesnt make any sense always set the output null 
WITHOUT_CLASSIFICATION	 dpp with the tree wont split and dont have remove 
WITHOUT_CLASSIFICATION	 dont call postread will have checked everything here ignore genericudfbridge its checked separately run posthook 
WITHOUT_CLASSIFICATION	 this will abort initializeop 
WITHOUT_CLASSIFICATION	 skip the first node which always required 
WITHOUT_CLASSIFICATION	 view ddl 
WITHOUT_CLASSIFICATION	 the length each field the value repeats for every entry then stored vector and isrepeating from the superclass set true 
WITHOUT_CLASSIFICATION	 following remote job may refer classes this jar and the remote job would executed different thread add this jar path jobcontext for further usage 
WITHOUT_CLASSIFICATION	 failed 
WITHOUT_CLASSIFICATION	 add new residual preds 
WITHOUT_CLASSIFICATION	 will extract the literals introduce the clause 
WITHOUT_CLASSIFICATION	 order likely currently running compactions will first lhs union 
WITHOUT_CLASSIFICATION	 read aggregated stats for one column 
WITHOUT_CLASSIFICATION	 new tai lue letter low bytes 
WITHOUT_CLASSIFICATION	 test basic truncate vector 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 start with empty wordlist and build 
WITHOUT_CLASSIFICATION	 key index nullsafe join flag 
WITHOUT_CLASSIFICATION	 adjustment array 
WITHOUT_CLASSIFICATION	 add constant object overhead for union 
WITHOUT_CLASSIFICATION	 gmt milliseconds after epoch 
WITHOUT_CLASSIFICATION	 check the source file unmodified even after copy see copied the right file 
WITHOUT_CLASSIFICATION	 metainfo 
WITHOUT_CLASSIFICATION	 run cleaner should remove the delta dirs 
WITHOUT_CLASSIFICATION	 have some disk data separate column chunk and put into cache 
WITHOUT_CLASSIFICATION	 create list top nodes 
WITHOUT_CLASSIFICATION	 there should rows the deletedelta because there have been delete events 
WITHOUT_CLASSIFICATION	 count 
WITHOUT_CLASSIFICATION	 test mode hive mode 
WITHOUT_CLASSIFICATION	 variable access should not done and use exportrootdir instead 
WITHOUT_CLASSIFICATION	 this means that the lock being committed this instant hence the cleaner should not remove even times out transaction 
WITHOUT_CLASSIFICATION	 fired only once the original node 
WITHOUT_CLASSIFICATION	 have the udf are the session registry both 
WITHOUT_CLASSIFICATION	 partition path can null the case new create partition this case try default checking the permissions the parent table partition itself can also null cases where this gets called generic 
WITHOUT_CLASSIFICATION	 not synchronizing creation mapop with invocation check immediately after creation case abort has been set relying the regular flow clean the actual operator exception thrown attempt will made cleanup the are here exit out via exception were the middle the opeartorinitialize 
WITHOUT_CLASSIFICATION	 this null for minor compaction may also null there base only deltas 
WITHOUT_CLASSIFICATION	 foreignschemaname 
WITHOUT_CLASSIFICATION	 replicate all the events happened far should fail the data files missing 
WITHOUT_CLASSIFICATION	 remove default partition from partition names and get aggregate stats again 
WITHOUT_CLASSIFICATION	 refs 
WITHOUT_CLASSIFICATION	 since the input 
WITHOUT_CLASSIFICATION	 are using oracle schema because simpler parse quotes backticks etc 
WITHOUT_CLASSIFICATION	 ckeys pkeys have constant node expressions avoid the merge 
WITHOUT_CLASSIFICATION	 topn query results records 
WITHOUT_CLASSIFICATION	 schedule major compaction all the partitionstable clean aborted data 
WITHOUT_CLASSIFICATION	 iterate over each group subqueries with the same group bydistinct keys 
WITHOUT_CLASSIFICATION	 take ones complement 
WITHOUT_CLASSIFICATION	 the driver context has been shutdown due query cancellation kill the spark job 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 tests wait queue behaviour for fragments which have reported the but have not given their executor slot 
WITHOUT_CLASSIFICATION	 xxx from oazktclientbase 
WITHOUT_CLASSIFICATION	 were updating add the rowid expression then make the following column accesses offset that dont try convert the rowid 
WITHOUT_CLASSIFICATION	 does the query have using clause 
WITHOUT_CLASSIFICATION	 now insert this record into the list 
WITHOUT_CLASSIFICATION	 serialize the rest the values the aggbuffer 
WITHOUT_CLASSIFICATION	 this block exists for debugging purposes want check whether the col stats cache working properly and are retrieving the 
WITHOUT_CLASSIFICATION	 means asc could really use enum here the thrift 
WITHOUT_CLASSIFICATION	 todo for now only support sampling two columns need change list columns 
WITHOUT_CLASSIFICATION	 errored 
WITHOUT_CLASSIFICATION	 error getting children node probably node has been deleted 
WITHOUT_CLASSIFICATION	 download resources dir 
WITHOUT_CLASSIFICATION	 token was not storage format token 
WITHOUT_CLASSIFICATION	 argh hcatrecord doesnt implement equals 
WITHOUT_CLASSIFICATION	 events insert last repl repldumpidx 
WITHOUT_CLASSIFICATION	 add the move task 
WITHOUT_CLASSIFICATION	 all small writes the first buffer should contiguous memory 
WITHOUT_CLASSIFICATION	 here start explicit txn 
WITHOUT_CLASSIFICATION	 output should have varchar type params 
WITHOUT_CLASSIFICATION	 table tbl liketbl listfieldschema cols tblgetsdgetcols assertequals colssize assertequalsnew fieldschemaa int null colsget mapstring string tblparams tblgetparameters tblparamsgethcatisd tblparamsgethcatosd hcatdriverrundrop table junitsemanalysis hcatdriverrundrop table liketbl 
WITHOUT_CLASSIFICATION	 the serdes for the partition columns 
WITHOUT_CLASSIFICATION	 for example select deptno countdistinct sal sumdistinct sal from emp group deptno becomes select deptno countdistinctsal sumdistinctsal from select distinct deptno sal distinctsal from emp group deptno group deptno 
WITHOUT_CLASSIFICATION	 unsupported conversion 
WITHOUT_CLASSIFICATION	 assign ids all vertices targets first then sources 
WITHOUT_CLASSIFICATION	 values needed for numeric arithmetic udfs 
WITHOUT_CLASSIFICATION	 size string buffer bytes 
WITHOUT_CLASSIFICATION	 gen 
WITHOUT_CLASSIFICATION	 load the column stats and table params with scale 
WITHOUT_CLASSIFICATION	 and retry 
WITHOUT_CLASSIFICATION	 return new hash multiset result implementation specific object the object can used access the count values when the key contained the multiset access spill information when the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 handle type casts that may contain type parameters 
WITHOUT_CLASSIFICATION	 instance shared utils 
WITHOUT_CLASSIFICATION	 integerbitmask 
WITHOUT_CLASSIFICATION	 specifications default utf string storage for backwards compatibility 
WITHOUT_CLASSIFICATION	 test using loadtablework 
WITHOUT_CLASSIFICATION	 condition occurs when the input has rows possible due filtering joins etc 
WITHOUT_CLASSIFICATION	 create column schema 
WITHOUT_CLASSIFICATION	 map column number type this always nonnull for useful vec context 
WITHOUT_CLASSIFICATION	 this not config that users set hivesite its only use share information between the java component the service driver and the python component 
WITHOUT_CLASSIFICATION	 breakup the original windowingspec into set windowingspecs each windowingspec executed instance ptfoperator preceded reducesink and extract the logic componentize straightforward distribute window specs from original window spec into set windowspecs based their partitioning group windowspecs subset the window invocations the queryblock that have the same order spec each group put new windowingspec and evaluated its own ptfoperator instance the order computation then inferred based the dependency between groups groups have the same dependency then the group with the function that earliest the selectlist executed first 
WITHOUT_CLASSIFICATION	 example handling dirs with shimshadoop 
WITHOUT_CLASSIFICATION	 used for the analyze command statistics used for the analyze command statistics noscan 
WITHOUT_CLASSIFICATION	 reset rowcontainers serde objectinspector and tabledesc 
WITHOUT_CLASSIFICATION	 find the closest pair bins terms coordinates break ties randomly 
WITHOUT_CLASSIFICATION	 reopening close open allowed opened state 
WITHOUT_CLASSIFICATION	 not keyword 
WITHOUT_CLASSIFICATION	 write file 
WITHOUT_CLASSIFICATION	 for local directory always write mapred intermediate store and then copy local 
WITHOUT_CLASSIFICATION	 the source table 
WITHOUT_CLASSIFICATION	 prepare future cache buffer 
WITHOUT_CLASSIFICATION	 since the user running the test belongs the group obtained above the call will succeed 
WITHOUT_CLASSIFICATION	 see setloadfiletype and setisacidiow calls elsewhere for example 
WITHOUT_CLASSIFICATION	 true this statement creates replaces materialized view 
WITHOUT_CLASSIFICATION	 single long key hash multiset optimized for vector map join 
WITHOUT_CLASSIFICATION	 concurrent forceeviction ignore 
WITHOUT_CLASSIFICATION	 lost the duck 
WITHOUT_CLASSIFICATION	 lastvalue when sort key specified then lastvalue should return the last value among rows with the same sort key value 
WITHOUT_CLASSIFICATION	 break iteration times out 
WITHOUT_CLASSIFICATION	 cleanup the scratch dir before starting 
WITHOUT_CLASSIFICATION	 size for each input alias 
WITHOUT_CLASSIFICATION	 this records how many rows have been inserted deleted separate from insertedrows 
WITHOUT_CLASSIFICATION	 readerspecific incompatibility like smb schema evolution 
WITHOUT_CLASSIFICATION	 this not used for delete commands partitioneval not set correctly needed for that 
WITHOUT_CLASSIFICATION	 theres lock dont need heartbeat start heartbeat for readonly queries which dont open transactions but requires locks for those that require transactions the heartbeat has already been started opentxn 
WITHOUT_CLASSIFICATION	 since there concept group dont invoke startgroupendgroup for mapper 
WITHOUT_CLASSIFICATION	 this implementation completely and exhaustively reverses the addgauge method above 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 row format serde 
WITHOUT_CLASSIFICATION	 smbjoin possible need correct bucketing 
WITHOUT_CLASSIFICATION	 revert 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 that semanticanalyzer finds are use 
WITHOUT_CLASSIFICATION	 double scalarcolumn 
WITHOUT_CLASSIFICATION	 only create bucket files only dynamic partitions buckets dynamic partitions will created for each newly created partition todo iow integration full acid uses the else block create acids recordupdater hivefileformatutils and that will set 
WITHOUT_CLASSIFICATION	 drop the table exists 
WITHOUT_CLASSIFICATION	 note assume that workers run the same threads repeatedly can set the session here and will reused without explicitly storing the worker 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 metrics gathered default 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 parameters used for jmx 
WITHOUT_CLASSIFICATION	 have create new object because the object belongs the code that creates and may get its value changed 
WITHOUT_CLASSIFICATION	 preserve operator before the gby well use resolve 
WITHOUT_CLASSIFICATION	 determine minimum all nonnull long column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 checks whether txn list has been invalidated while planning the query this would happen query requires exclusivesemishared lock and there has been committed transaction the table over which the lock 
WITHOUT_CLASSIFICATION	 hashmap javaobject 
WITHOUT_CLASSIFICATION	 isdynamicfunction used indicate the function not deterministic between queries 
WITHOUT_CLASSIFICATION	 date daytime interval timestamp 
WITHOUT_CLASSIFICATION	 this semijoin optimization should removed after were done iterating 
WITHOUT_CLASSIFICATION	 for now just small table values 
WITHOUT_CLASSIFICATION	 column statistics for column already found then merge the statistics 
WITHOUT_CLASSIFICATION	 here maybe compaction regular read delete event sorter the later cases should hive should compute sarg push down minmax key deletedelta 
WITHOUT_CLASSIFICATION	 commit the changes 
WITHOUT_CLASSIFICATION	 this the last keyvalue pair 
WITHOUT_CLASSIFICATION	 singlecolumn string specific members 
WITHOUT_CLASSIFICATION	 fmsketch treats the ndv all nulls but hll treates the ndv order get rid divide problem follow fmsketch 
WITHOUT_CLASSIFICATION	 allocate write ids for both tables and for all txns 
WITHOUT_CLASSIFICATION	 set the number instances which llap should run 
WITHOUT_CLASSIFICATION	 already registered 
WITHOUT_CLASSIFICATION	 clarification terms the originaldir directory represents the original directory the partitions files they now contain archived version those files the source directory the directory containing all the files that should the partitions note the har scheme 
WITHOUT_CLASSIFICATION	 nothing matches try creating custom counter 
WITHOUT_CLASSIFICATION	 when executes nonresultset query like create 
WITHOUT_CLASSIFICATION	 the following method introduces cast decimal and parent expression decimal 
WITHOUT_CLASSIFICATION	 set true 
WITHOUT_CLASSIFICATION	 create the serde 
WITHOUT_CLASSIFICATION	 check whether operation log file deleted 
WITHOUT_CLASSIFICATION	 this might seem counter intuitive but some queries like query select yearcalcsdate yrdateok from druidtableaucalcs calcs where yearcalcsdate null limit planed way where only push filter down and keep the project null hive project thus empty columns 
WITHOUT_CLASSIFICATION	 are getting the resources externally dont relocalize anything 
WITHOUT_CLASSIFICATION	 negative numbers flip all bits 
WITHOUT_CLASSIFICATION	 fallback for the case when orcsplit flags not contain hasbase and deltas 
WITHOUT_CLASSIFICATION	 tabletype 
WITHOUT_CLASSIFICATION	 small table file size 
WITHOUT_CLASSIFICATION	 optional int underscoreint 
WITHOUT_CLASSIFICATION	 make this method final improve performance 
WITHOUT_CLASSIFICATION	 analyze table partition compute statistics noscan 
WITHOUT_CLASSIFICATION	 subtree 
WITHOUT_CLASSIFICATION	 workaround for bug persisting nulls bytea column instead set empty bit vector with header 
WITHOUT_CLASSIFICATION	 pass marking invalid candidates checks based variance and ttl 
WITHOUT_CLASSIFICATION	 package visible that hmsmetricslistener can see them 
WITHOUT_CLASSIFICATION	 add granularity the row schema 
WITHOUT_CLASSIFICATION	 tolerance for long range bias and for short range bias 
WITHOUT_CLASSIFICATION	 long scalarscalar 
WITHOUT_CLASSIFICATION	 the case truncate table set the stats 
WITHOUT_CLASSIFICATION	 the dividedby logic consistent 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 hive normalizes partition spec for dates yyyymmdd format some versions java will accept other formats for datevalueof yyyymd and who knows what else the future some will not accept other formats cannot test normalization with them type check will fail before can ever happen thus test isolation 
WITHOUT_CLASSIFICATION	 end while 
WITHOUT_CLASSIFICATION	 trigger failover minihs and make sure the connections are closed 
WITHOUT_CLASSIFICATION	 uses deletedelegator kill job and ignores all exceptions 
WITHOUT_CLASSIFICATION	 console sql shell with command completion todo liuserfriendly connection promptsli lipage resultsli lihandle binary data blob fieldsli liimplement command aliasesli listored procedure executionli libinding parameters prepared statementsli liscripting languageli lixa transactionsli 
WITHOUT_CLASSIFICATION	 then this considered like the metastore server case 
WITHOUT_CLASSIFICATION	 production string 
WITHOUT_CLASSIFICATION	 check for two way join 
WITHOUT_CLASSIFICATION	 request intervalstt 
WITHOUT_CLASSIFICATION	 need keep the path part only because the hadoop will pass the path part only accept trailing the path with separator prevent partial matching 
WITHOUT_CLASSIFICATION	 into the rowmode mapjoinkey 
WITHOUT_CLASSIFICATION	 now add preconfigured users admin role 
WITHOUT_CLASSIFICATION	 stats setup 
WITHOUT_CLASSIFICATION	 delete the data the partitions which have other locations 
WITHOUT_CLASSIFICATION	 generate the groupbyoperator for the query block parseinfogetxxxdest the new groupbyoperator will child the param parseinfo param dest param param mode the mode the aggregation mergepartial partial param the mapping from aggregation stringtree the param groupingsets list grouping sets param groupingsetspresent whether grouping sets are present this query param whether grouping sets are consumed this group return the new groupbyoperator 
WITHOUT_CLASSIFICATION	 commit each partition gets moved out the job work dir 
WITHOUT_CLASSIFICATION	 patterns that are excluded verbose logging level filter out messages coming from log processing classes well run infinite loop 
WITHOUT_CLASSIFICATION	 set signum before result zero fastmultiply will set signum 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 obtain delegation tokens for the job 
WITHOUT_CLASSIFICATION	 case are stuck consume 
WITHOUT_CLASSIFICATION	 compare the column names and the order with the first tablepartition 
WITHOUT_CLASSIFICATION	 the time enter here the bytevalueslentgh and isnull must have already been compared 
WITHOUT_CLASSIFICATION	 enough compare the last level subdirectory which has the name event 
WITHOUT_CLASSIFICATION	 request llap splits for table 
WITHOUT_CLASSIFICATION	 set the catalog name hasnt been set the new table 
WITHOUT_CLASSIFICATION	 will fail 
WITHOUT_CLASSIFICATION	 pool 
WITHOUT_CLASSIFICATION	 construct list bucketing location mappings from subdirectory name 
WITHOUT_CLASSIFICATION	 generate resolved parse tree from syntax tree 
WITHOUT_CLASSIFICATION	 test the connection metastore using the config property 
WITHOUT_CLASSIFICATION	 else return 
WITHOUT_CLASSIFICATION	 run join releated optimizations 
WITHOUT_CLASSIFICATION	 build rel for having clause 
WITHOUT_CLASSIFICATION	 need initialize make sure nobody modified this table then current txn shouldnt read any data there conversion from nonacid acid table then default would assigned 
WITHOUT_CLASSIFICATION	 definite node constructed from specified number children that number nodes are popped from the stack and made the children the definite node then the definite node pushed the stack 
WITHOUT_CLASSIFICATION	 initialize the converter 
WITHOUT_CLASSIFICATION	 create filters top each setop child modifying the filter condition reference each setop child 
WITHOUT_CLASSIFICATION	 two vint without nanos 
WITHOUT_CLASSIFICATION	 should not occur since second parameter gettablewithqn false 
WITHOUT_CLASSIFICATION	 subsequent requests 
WITHOUT_CLASSIFICATION	 start the timer thread for cancelling the query when query timeout reached 
WITHOUT_CLASSIFICATION	 successfully scheduled 
WITHOUT_CLASSIFICATION	 the cumulative cardinality equal but the size bigger 
WITHOUT_CLASSIFICATION	 keep flushing until the memory under threshold 
WITHOUT_CLASSIFICATION	 since merge multiple operation paths assign new tags bottom layer reducesinkoperators this mapping used map new tags original tags associated 
WITHOUT_CLASSIFICATION	 found the proper columns 
WITHOUT_CLASSIFICATION	 add back probability qdigit 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 copy credentials 
WITHOUT_CLASSIFICATION	 write out integer part first then write out fractional part 
WITHOUT_CLASSIFICATION	 there are many ways have aux jars hive sigh 
WITHOUT_CLASSIFICATION	 make sure the new partition has the catalog value set 
WITHOUT_CLASSIFICATION	 verify that the schema now within the configuration the one passed 
WITHOUT_CLASSIFICATION	 write the current set valid transactions into the conf file 
WITHOUT_CLASSIFICATION	 readonly maps initialized once 
WITHOUT_CLASSIFICATION	 through hive 
WITHOUT_CLASSIFICATION	 dummy instance just trigger failover 
WITHOUT_CLASSIFICATION	 expected events not updated yet vertex success notification not received 
WITHOUT_CLASSIFICATION	 nothing needs done 
WITHOUT_CLASSIFICATION	 now vary isrepeating nulls possible only right 
WITHOUT_CLASSIFICATION	 get key element information 
WITHOUT_CLASSIFICATION	 check the affinity the argument passed with the accepted argument based the primitivegrouping 
WITHOUT_CLASSIFICATION	 are any the new transactions ones that care about 
WITHOUT_CLASSIFICATION	 backcheck force least output and this should have partial which empty 
WITHOUT_CLASSIFICATION	 all the tablespartitions columns should sorted the same order for example tables and are being joined columns and which are the sorted and bucketed columns the join would work long and are sorted the same order 
WITHOUT_CLASSIFICATION	 this trivial query block return 
WITHOUT_CLASSIFICATION	 existing ranges just accept the current 
WITHOUT_CLASSIFICATION	 ugi for the httphost spnego principal 
WITHOUT_CLASSIFICATION	 the real expression 
WITHOUT_CLASSIFICATION	 perform minor compaction since nothing was aborted subdirs should stay 
WITHOUT_CLASSIFICATION	 vector mode store char unpadded 
WITHOUT_CLASSIFICATION	 reduce only the parameters are significant 
WITHOUT_CLASSIFICATION	 tests that the absence stats for partitions andor absence columns 
WITHOUT_CLASSIFICATION	 only accept parse results parsed the entire string 
WITHOUT_CLASSIFICATION	 preevent listener 
WITHOUT_CLASSIFICATION	 must struct because list and map need parameterizedtype 
WITHOUT_CLASSIFICATION	 can boolean column which case return true count 
WITHOUT_CLASSIFICATION	 init output object inspectors the output partial aggregation list 
WITHOUT_CLASSIFICATION	 test retries when invocationexception wrapped metaexception wrapped jdoexception thrown 
WITHOUT_CLASSIFICATION	 initialize 
WITHOUT_CLASSIFICATION	 add spark job metrics 
WITHOUT_CLASSIFICATION	 only proceed the thenelse types were aligned 
WITHOUT_CLASSIFICATION	 this path points symlink path 
WITHOUT_CLASSIFICATION	 when have repeating values can unset the whole bitset once the repeating value not valid write 
WITHOUT_CLASSIFICATION	 grantee role check exists 
WITHOUT_CLASSIFICATION	 needed avoid file name conflict when big table partitioned 
WITHOUT_CLASSIFICATION	 same return one them 
WITHOUT_CLASSIFICATION	 every fields write null byte 
WITHOUT_CLASSIFICATION	 make sure the catalog name set the new partition 
WITHOUT_CLASSIFICATION	 note that the code removes the data from the field its passed the consumer expect have stuff remaining there only case errors 
WITHOUT_CLASSIFICATION	 allowrounding 
WITHOUT_CLASSIFICATION	 update the number entries that can fit the hash table 
WITHOUT_CLASSIFICATION	 further the dpp value needs generated from same subtree 
WITHOUT_CLASSIFICATION	 returns true thread pool created and can used for executing job request otherwise returns false 
WITHOUT_CLASSIFICATION	 change smallint and tinyint int 
WITHOUT_CLASSIFICATION	 single string key hash set optimized for vector map join the key will deserialized and just the bytes will stored 
WITHOUT_CLASSIFICATION	 initialize column buffers 
WITHOUT_CLASSIFICATION	 the dispatcher fires the processor corresponding the closest 
WITHOUT_CLASSIFICATION	 the boolean predicate 
WITHOUT_CLASSIFICATION	 validate false default disable the constraint 
WITHOUT_CLASSIFICATION	 create and add ast node with position grouping function input group clause 
WITHOUT_CLASSIFICATION	 simple case fits entirely the disk range 
WITHOUT_CLASSIFICATION	 cancel should noop either cases 
WITHOUT_CLASSIFICATION	 empty out side file 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 recursively create the exprnodedesc base cases when encounter column ref convert that into exprnodecolumndesc when encounter constant convert that into for others just 
WITHOUT_CLASSIFICATION	 need reconnect 
WITHOUT_CLASSIFICATION	 have valid batchiter and has more elements return them 
WITHOUT_CLASSIFICATION	 add sort clause that the row ids come out the correct order 
WITHOUT_CLASSIFICATION	 drop one table see what remains 
WITHOUT_CLASSIFICATION	 add the semijoin branch the map 
WITHOUT_CLASSIFICATION	 this version hadoop does not support hadoop introduces this new method 
WITHOUT_CLASSIFICATION	 constant node with value the agreed result 
WITHOUT_CLASSIFICATION	 key definitions related replication 
WITHOUT_CLASSIFICATION	 loginfoqueryplan 
WITHOUT_CLASSIFICATION	 disable web tests unless the test explicitly setting unique web port that dont mess ptests 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 note hive ops not use the normal future failure path this will not happen case actual failure the future will just done the background operation thread was aborted 
WITHOUT_CLASSIFICATION	 parse the schema file determine the tables that are expected exist 
WITHOUT_CLASSIFICATION	 replace the path the file status the input path parquet may use the path the file status open the file 
WITHOUT_CLASSIFICATION	 the output position should not change since there are corvars 
WITHOUT_CLASSIFICATION	 close the connection soon the error message sent 
WITHOUT_CLASSIFICATION	 loginfofirst tail offset create list record firsttailoffset 
WITHOUT_CLASSIFICATION	 get the name file and look its properties see orccompresssize was respected 
WITHOUT_CLASSIFICATION	 copy nop skip itthis important for avoiding spurious overlap assertions 
WITHOUT_CLASSIFICATION	 enable disable test scheduling control 
WITHOUT_CLASSIFICATION	 add hiveconf variable with modes adaptor always use vectorudfadaptor for statements good vectorize but dont optimize conditional expressions better vectorize and optimize conditional expressions 
WITHOUT_CLASSIFICATION	 but should have already checked for that before this point 
WITHOUT_CLASSIFICATION	 eventtype 
WITHOUT_CLASSIFICATION	 update the target map work the second 
WITHOUT_CLASSIFICATION	 process failover 
WITHOUT_CLASSIFICATION	 the threshold where should use repeating vectorized row batch optimization for 
WITHOUT_CLASSIFICATION	 construct joinpredicateinfo 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 table already there dont recreate 
WITHOUT_CLASSIFICATION	 rightinputrel 
WITHOUT_CLASSIFICATION	 never includes join condition the code was not modified for brevity 
WITHOUT_CLASSIFICATION	 desired schema does not include virtual columns partition columns 
WITHOUT_CLASSIFICATION	 create the dummy operators 
WITHOUT_CLASSIFICATION	 user not admin user allow the request only the user requesting for privileges for themselves role they belong 
WITHOUT_CLASSIFICATION	 two vint with nanos 
WITHOUT_CLASSIFICATION	 set the option for tolerating corruptions the read should succeed 
WITHOUT_CLASSIFICATION	 can null unit tests 
WITHOUT_CLASSIFICATION	 table newer leave 
WITHOUT_CLASSIFICATION	 unsecure case 
WITHOUT_CLASSIFICATION	 clean postconditions 
WITHOUT_CLASSIFICATION	 error out exit were not able parse the args successfully 
WITHOUT_CLASSIFICATION	 pig which uses hcat will pass this hcat that can find the metastore 
WITHOUT_CLASSIFICATION	 write some garbage the buffer that should erased 
WITHOUT_CLASSIFICATION	 when constructing the evaluator tree from exprnode tree look for any descendant leadlag function expressions they are found add them the llinfoleadlagexprs and add mapping from the expr tree root the llfunc expr 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 suppress the stages vectorization off 
WITHOUT_CLASSIFICATION	 for types with parameters like varchar need determine the type parameters that should added this type based the original typeinfos 
WITHOUT_CLASSIFICATION	 validate all tasks 
WITHOUT_CLASSIFICATION	 always set the null flag false when there value 
WITHOUT_CLASSIFICATION	 update the queryid use the generated applicationid see comment below about why this done 
WITHOUT_CLASSIFICATION	 aid testing 
WITHOUT_CLASSIFICATION	 some value set 
WITHOUT_CLASSIFICATION	 first see gbkeys 
WITHOUT_CLASSIFICATION	 import table 
WITHOUT_CLASSIFICATION	 need not validate inppartspec here 
WITHOUT_CLASSIFICATION	 which will cause the scheduler loop continue 
WITHOUT_CLASSIFICATION	 use partition level authorization 
WITHOUT_CLASSIFICATION	 the magic bytes the beginning the rcfile 
WITHOUT_CLASSIFICATION	 check work has explain annotation 
WITHOUT_CLASSIFICATION	 print attr 
WITHOUT_CLASSIFICATION	 change these parameters 
WITHOUT_CLASSIFICATION	 this should database usage privilege once supported 
WITHOUT_CLASSIFICATION	 should honor the ordering records provided order select statement 
WITHOUT_CLASSIFICATION	 exhausted the batch longer have heartbeat 
WITHOUT_CLASSIFICATION	 lists clauses 
WITHOUT_CLASSIFICATION	 unlink connection between and its parent 
WITHOUT_CLASSIFICATION	 fill the child listcolumnvector with valuelist 
WITHOUT_CLASSIFICATION	 the line doesnt end with the delimiter the line empty add the cmd part 
WITHOUT_CLASSIFICATION	 grab sign bit and shift away 
WITHOUT_CLASSIFICATION	 the hivedecimalwritable set method will quickly copy the deserialized decimal writable fields 
WITHOUT_CLASSIFICATION	 this test calling 
WITHOUT_CLASSIFICATION	 padding leading zeroesones 
WITHOUT_CLASSIFICATION	 test stats deletion partition level 
WITHOUT_CLASSIFICATION	 create the list serialized splits for each bucket 
WITHOUT_CLASSIFICATION	 joins alias alias alias alias was not eligible for big pos must deterministic order map for consistent qtest output across java versions 
WITHOUT_CLASSIFICATION	 buffer one batch time for row retrieval 
WITHOUT_CLASSIFICATION	 test that existing sharedread with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 first allocation write should add the table the nextwriteid meta table the initial value for write should and hence add with number write ids allocated here 
WITHOUT_CLASSIFICATION	 there are survivor spaces 
WITHOUT_CLASSIFICATION	 offset where this begins 
WITHOUT_CLASSIFICATION	 this method implementation preserved for backward compatibility 
WITHOUT_CLASSIFICATION	 offer accepted and gets evicted 
WITHOUT_CLASSIFICATION	 cant much expression not context filter cant treat null equivalent false here 
WITHOUT_CLASSIFICATION	 plus any correlated variables the input wants pass along 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 add struct field data the row 
WITHOUT_CLASSIFICATION	 priority 
WITHOUT_CLASSIFICATION	 short 
WITHOUT_CLASSIFICATION	 log error the acid table missing from the validwriteidlist conf 
WITHOUT_CLASSIFICATION	 are associated with transaction then heartbeat that well 
WITHOUT_CLASSIFICATION	 set list task 
WITHOUT_CLASSIFICATION	 call process once have full batch 
WITHOUT_CLASSIFICATION	 found unused port exit 
WITHOUT_CLASSIFICATION	 not put the unused duck back wed run the tasks below then assign priority 
WITHOUT_CLASSIFICATION	 merge filters from previous scan oring with filters from current scan 
WITHOUT_CLASSIFICATION	 ensures txn still there and expected state 
WITHOUT_CLASSIFICATION	 are only vectorizing reduce under tezspark 
WITHOUT_CLASSIFICATION	 partition specified get pruned partition list 
WITHOUT_CLASSIFICATION	 the constant value into and add the struct part exprnodestructs 
WITHOUT_CLASSIFICATION	 environment variables long values 
WITHOUT_CLASSIFICATION	 nodes need merged 
WITHOUT_CLASSIFICATION	 success 
WITHOUT_CLASSIFICATION	 remove all tok tokens 
WITHOUT_CLASSIFICATION	 there was preexisting work generated for the bigtable mapjoin side need hook the work generated for the associated with the rsmj pattern with the preexisting work otherwise need associate that the mapjoin linked the work associated with the rsmj pattern 
WITHOUT_CLASSIFICATION	 todobr could use combined instead list why not use expr 
WITHOUT_CLASSIFICATION	 error default value 
WITHOUT_CLASSIFICATION	 the size unavailable need assume size greater than this implies that merge cannot happen will return false 
WITHOUT_CLASSIFICATION	 the only case where duplicate elements matter the others are handled the below 
WITHOUT_CLASSIFICATION	 create scratch columns hold small table results 
WITHOUT_CLASSIFICATION	 the parsed statement contained dbtablename specification prefer that 
WITHOUT_CLASSIFICATION	 primitive writable class 
WITHOUT_CLASSIFICATION	 failed infer pkfk relationship for row count estimation fallback default logic compute denominator maxvry vsy maxvry vsy case multiattribute join 
WITHOUT_CLASSIFICATION	 mandatory property 
WITHOUT_CLASSIFICATION	 cbo did not optimize the query might need replace grouping function 
WITHOUT_CLASSIFICATION	 check multiset count 
WITHOUT_CLASSIFICATION	 secondary droptablecommand test for testing repldroptables effect partitions inside partitioned table when there exist partitions inside the table which are older than the drop event our goal this create table with repllastid say create partitions inside with repllastid and say now process drop table command with eventid should result the table and the partition with repllastid continuing exist but dropping the partition with repllastid 
WITHOUT_CLASSIFICATION	 nothing default 
WITHOUT_CLASSIFICATION	 doesnt seem like you should have this but java serialization wacky and doesnt call the default constructor 
WITHOUT_CLASSIFICATION	 the form partition 
WITHOUT_CLASSIFICATION	 stripe position within file 
WITHOUT_CLASSIFICATION	 server sets createtime 
WITHOUT_CLASSIFICATION	 create result cache object add one pair and retrieve them 
WITHOUT_CLASSIFICATION	 threshold percentage trigger the warning 
WITHOUT_CLASSIFICATION	 convert lowercase for the comparison 
WITHOUT_CLASSIFICATION	 use the keytab one was provided 
WITHOUT_CLASSIFICATION	 tokcreatefunction identifier stringliteral istempfunction toktemporary 
WITHOUT_CLASSIFICATION	 two events one for create and other for drop 
WITHOUT_CLASSIFICATION	 directory output results 
WITHOUT_CLASSIFICATION	 create groupingid column 
WITHOUT_CLASSIFICATION	 print job stages 
WITHOUT_CLASSIFICATION	 nulls and repeating 
WITHOUT_CLASSIFICATION	 complain about the deprecated syntax but still run 
WITHOUT_CLASSIFICATION	 get row size small table 
WITHOUT_CLASSIFICATION	 check elements the innermost array 
WITHOUT_CLASSIFICATION	 delete the bucket files now have empty delta dirs 
WITHOUT_CLASSIFICATION	 load next batch from disk 
WITHOUT_CLASSIFICATION	 pretend add trailing zeroes even when would exceed the 
WITHOUT_CLASSIFICATION	 right now the work graph pretty simple there preceding work have root and will generate map vertex there preceding work will generate 
WITHOUT_CLASSIFICATION	 create table database without location clause 
WITHOUT_CLASSIFICATION	 convert all nan and optionally infinity values vector null 
WITHOUT_CLASSIFICATION	 have already set the enabled conditions not met 
WITHOUT_CLASSIFICATION	 the table level and the storage handler level 
WITHOUT_CLASSIFICATION	 clear the username 
WITHOUT_CLASSIFICATION	 this not needed beyond compilation transient 
WITHOUT_CLASSIFICATION	 initialize the result 
WITHOUT_CLASSIFICATION	 not all argument elements need hold true 
WITHOUT_CLASSIFICATION	 used the framework runtime initialize the real initializer runtime 
WITHOUT_CLASSIFICATION	 setop rel 
WITHOUT_CLASSIFICATION	 commit the open txn which lets the cleanup txntowriteid 
WITHOUT_CLASSIFICATION	 close any open readers there was some exception during initialization rethrow the exception that the caller can handle 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 create default route where events without queryid 
WITHOUT_CLASSIFICATION	 applicationspecific typecodes 
WITHOUT_CLASSIFICATION	 prepare stringbuilder for partid use future queries 
WITHOUT_CLASSIFICATION	 the reason why can get list split strategies here because for acid splitupdate case when have mix original base files insert deltas will produce two independent split strategies for them there global flag isoriginal that set 
WITHOUT_CLASSIFICATION	 this set drivercontext runtime 
WITHOUT_CLASSIFICATION	 optional string requestuser 
WITHOUT_CLASSIFICATION	 extract the values 
WITHOUT_CLASSIFICATION	 consider for large fill all isnull array and use the tighter else loop 
WITHOUT_CLASSIFICATION	 chose the table descriptor none the partitions present for consider the query select mapjoint count from join tkeytkey both and and partitioned tables but does not have any partitions fetchoperator invoked for and listparts empty that case use schema get the objectinspector 
WITHOUT_CLASSIFICATION	 happens case 
WITHOUT_CLASSIFICATION	 current number open txns 
WITHOUT_CLASSIFICATION	 adds the missing schemeauthority for the new partition location 
WITHOUT_CLASSIFICATION	 helper class submit fragments llap and retry rejected submissions 
WITHOUT_CLASSIFICATION	 this map which vectorized row batch columns are the key columns 
WITHOUT_CLASSIFICATION	 the mathmin and the fact that maxallocation int ensures dont overflow 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 validate closereopen 
WITHOUT_CLASSIFICATION	 metrics throws exception dont this when the key already exists 
WITHOUT_CLASSIFICATION	 easier read logs 
WITHOUT_CLASSIFICATION	 indicates whether cache statistics should collected 
WITHOUT_CLASSIFICATION	 exactly the same 
WITHOUT_CLASSIFICATION	 use magic length value indicate big 
WITHOUT_CLASSIFICATION	 init method hmshandler should not retried there are exceptions 
WITHOUT_CLASSIFICATION	 can save ourselves from spilling once have join emit interval worth rows 
WITHOUT_CLASSIFICATION	 one writable per row 
WITHOUT_CLASSIFICATION	 privileges obtained indirectly via roles 
WITHOUT_CLASSIFICATION	 override whats placed the configuration setup 
WITHOUT_CLASSIFICATION	 immutable maps 
WITHOUT_CLASSIFICATION	 generates plan for minmax when dynamic partition pruning ruled out 
WITHOUT_CLASSIFICATION	 the fastroundinteger methods remove all fractional digits set and 
WITHOUT_CLASSIFICATION	 reinstantiate the parent expression with new arguments 
WITHOUT_CLASSIFICATION	 smallint 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 regardless our matching result keep that information make multiple use for possible series equal keys 
WITHOUT_CLASSIFICATION	 future more sophisticated our conversion check 
WITHOUT_CLASSIFICATION	 for short range use linear counting 
WITHOUT_CLASSIFICATION	 release buffers are done with all the streams also see torelease comment 
WITHOUT_CLASSIFICATION	 heartbeat from task that are not currently tracking 
WITHOUT_CLASSIFICATION	 static partition created during setup dynamic partitions 
WITHOUT_CLASSIFICATION	 output top values 
WITHOUT_CLASSIFICATION	 stores datastore jpox properties 
WITHOUT_CLASSIFICATION	 lets not guess which one correct 
WITHOUT_CLASSIFICATION	 create table and pupulate with kvtxt 
WITHOUT_CLASSIFICATION	 lengths stream could empty stream already reached end stream before present stream this can happen all values stream are nulls last row group values are all null 
WITHOUT_CLASSIFICATION	 publish columns its subscribers 
WITHOUT_CLASSIFICATION	 responsible for capturing subquery rewrites and providing the rewritten query sql 
WITHOUT_CLASSIFICATION	 start offset compression buffer corresponding above row group 
WITHOUT_CLASSIFICATION	 dag scratch dir get session from the pool may different from tez one 
WITHOUT_CLASSIFICATION	 code true means current transaction started via start transaction which means cannot include any operations which cannot rolled back drop partition write nonacid table false its single statement transaction which can include any statement this not contradiction from the user point view who doesnt know anything about the implicit txn and cannot call rollback the statement course can fail which case there nothing rollback assuming the statement well implemented this done that all commands run transaction which simplifies implementation and allows simple implementation multistatement txns which dont require lock manager capable deadlock detection todo not fully implemented elaborate how this works also critically important ensuring that everything runs transaction assigns order all operations the system needed for replicationdr dont want allow nontransactional statements user demarcated txn because the effect such statement visible immediately statement completion but the user may issue rollback but the action the statement cant undone and has possibly already been seen another txn for example start transaction insert into transactionaltable values insert into select from transactionaltable rollback the user would for surprise especially they are not aware transactional properties the tables involved side note what should the lock manager with locks for nontransactional resources should release them the end the stmt txn some interesting thoughts 
WITHOUT_CLASSIFICATION	 guaranteed just because each dummystoreoperator can part only one work 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 filter does not change the input ordering filter rel does not permute the input all corvars produced filter will have the same output positions the 
WITHOUT_CLASSIFICATION	 increment cursor for perquery inclause list 
WITHOUT_CLASSIFICATION	 note edgecase here interaction with tablelevel repl load where that nukes out tablesupdated however explicitly dont support repl that sort and error out above 
WITHOUT_CLASSIFICATION	 the set operators the works that are merging need meet some requirements particular none the works that are merging can contain union operator this not supported yet might end with cycles the tez dag there cannot more than one dummystore operator the new resulting work when the operators are merged this due assumption mergejoinproc that needs further explored any these conditions are not met cannot merge 
WITHOUT_CLASSIFICATION	 used sortbased groupby mode complete partial partial 
WITHOUT_CLASSIFICATION	 set zookeeper dynamic service discovery configs 
WITHOUT_CLASSIFICATION	 the current record should not included the output detaillist 
WITHOUT_CLASSIFICATION	 set the values totalinputfilesize and totalinputnumfiles estimating them percentage block sampling being used 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 flatten 
WITHOUT_CLASSIFICATION	 partition column 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 insert data legacy format 
WITHOUT_CLASSIFICATION	 example code 
WITHOUT_CLASSIFICATION	 this operator map join operator merge join operator 
WITHOUT_CLASSIFICATION	 first hash used locate start the block blockbaseoffset 
WITHOUT_CLASSIFICATION	 child operator object inspector converted its needed 
WITHOUT_CLASSIFICATION	 the tabledesc will null the case that all columns that table not used use dummy row denote all rows that table and the dummy row added caller 
WITHOUT_CLASSIFICATION	 this operator can removed 
WITHOUT_CLASSIFICATION	 accumuloinputformat complains you reset already set value just dont care 
WITHOUT_CLASSIFICATION	 for delete statements not null constraint need not checked 
WITHOUT_CLASSIFICATION	 bootstrap dump with open txn timeout 
WITHOUT_CLASSIFICATION	 write totalmonths dataoutput 
WITHOUT_CLASSIFICATION	 now change the resource plan change the mapping for the user 
WITHOUT_CLASSIFICATION	 only columns and constants can selected 
WITHOUT_CLASSIFICATION	 cascade 
WITHOUT_CLASSIFICATION	 determine the data and partition columns using the first partition descriptors partition count other words how split the schema columns the allcolumnnamelist and alltypeinfolist variables into the data and partition columns 
WITHOUT_CLASSIFICATION	 skip mapaggr gby 
WITHOUT_CLASSIFICATION	 ruleregexp rules are used match operators anywhere the tree ruleexactmatch rules are used specify exactly what the tree should look like particular this guarantees that the first operator the reducer and its parents are reducesinkoperators since begins walking the tree from 
WITHOUT_CLASSIFICATION	 check additional constraint 
WITHOUT_CLASSIFICATION	 stay with multikey 
WITHOUT_CLASSIFICATION	 encoded filenamechecksum files write into files 
WITHOUT_CLASSIFICATION	 the join the root but should always end with project operator 
WITHOUT_CLASSIFICATION	 state the rolling partition preceding numrowsrecived index represents the last output row preceding span rows before that are still held for subsequent rows processing the rows beyond numrowsprocessed followingspan 
WITHOUT_CLASSIFICATION	 hivedecimalwritable version 
WITHOUT_CLASSIFICATION	 functions 
WITHOUT_CLASSIFICATION	 set the configuration such that proxyuser can act behalf all users belonging the group foobargroup 
WITHOUT_CLASSIFICATION	 get the table from the client verify the name correct 
WITHOUT_CLASSIFICATION	 this relies findkeyreftoread doing key equality check and leaving read ptr where needed 
WITHOUT_CLASSIFICATION	 this used print the progress information pure text sample below map reducer map reducer map reducer map reducer 
WITHOUT_CLASSIFICATION	 limit can pushed down mapside all window functions need access rows before the current row this true for rank denserank and lead fns the window doesnt matter for lead the window for the function row based and the end boundary doesnt reference rows past the current row 
WITHOUT_CLASSIFICATION	 check any the roles this user owner 
WITHOUT_CLASSIFICATION	 the binarysortable serialization the saved key for possible series equal keys 
WITHOUT_CLASSIFICATION	 events insert last repl repldumpidx 
WITHOUT_CLASSIFICATION	 does require new job for grouping sets 
WITHOUT_CLASSIFICATION	 temporarily mark child all the 
WITHOUT_CLASSIFICATION	 should not affect schema conversion 
WITHOUT_CLASSIFICATION	 lookup long the hash multiset param key the long key param hashmultisetresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 ignore refcounts and errors for now 
WITHOUT_CLASSIFICATION	 surprise that midnight utc was local far are firm ground 
WITHOUT_CLASSIFICATION	 bootstrap repl 
WITHOUT_CLASSIFICATION	 cache indexlist 
WITHOUT_CLASSIFICATION	 use common conversion method share with 
WITHOUT_CLASSIFICATION	 for primitive object serialize plain string 
WITHOUT_CLASSIFICATION	 split starting row start will not read that row 
WITHOUT_CLASSIFICATION	 test single highprecision multiply random inputs arguments must integers with optional sign represented strings arguments must have digits and the number total digits 
WITHOUT_CLASSIFICATION	 handle decimal separately 
WITHOUT_CLASSIFICATION	 remove operator add ops new collection 
WITHOUT_CLASSIFICATION	 concurrent increase and termination increase fails 
WITHOUT_CLASSIFICATION	 exchange single partitions using complete partitionspec all partition columns 
WITHOUT_CLASSIFICATION	 column the target table that will pruned against 
WITHOUT_CLASSIFICATION	 need column expression other side 
WITHOUT_CLASSIFICATION	 call open mockmocktbl 
WITHOUT_CLASSIFICATION	 always collect input file formats 
WITHOUT_CLASSIFICATION	 not using position alias and number 
WITHOUT_CLASSIFICATION	 make new big table scratch column for the small table value 
WITHOUT_CLASSIFICATION	 nope look see hives conf dir has been explicitly set 
WITHOUT_CLASSIFICATION	 undone currently negative exponent not supported 
WITHOUT_CLASSIFICATION	 months produces type timestamp via calendar calculation 
WITHOUT_CLASSIFICATION	 optional long array length 
WITHOUT_CLASSIFICATION	 undone for now element list with null element null list 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 but for now just rely the cache put lock them before send them over 
WITHOUT_CLASSIFICATION	 references 
WITHOUT_CLASSIFICATION	 type timestamp plusminus type intervaldaytime 
WITHOUT_CLASSIFICATION	 see the execution thread has just completed operation and result available result available then return the result otherwise raise exception 
WITHOUT_CLASSIFICATION	 monotonic iff its first argument but not strict 
WITHOUT_CLASSIFICATION	 leap year 
WITHOUT_CLASSIFICATION	 globstatus api returns empty filestatus when the specified path does not exist but getfilestatus throw ioexception mimic the similar behavior will return empty array exception for external tables the path the table will not exists during table creation 
WITHOUT_CLASSIFICATION	 implicit assumption here that database level processed first before table level which will depend the iterator used since should provide the higher level directory listing before providing the lower level listing this also required such that the dbtracker tabletracker are setup correctly always 
WITHOUT_CLASSIFICATION	 any tablepartition directory not owned hive then assume table using storagebased auth set external transactional tables should still remain transactional 
WITHOUT_CLASSIFICATION	 already stopped else was never started another service failing canceled startup 
WITHOUT_CLASSIFICATION	 nothing start 
WITHOUT_CLASSIFICATION	 set temporary path communicate between the smallbig table 
WITHOUT_CLASSIFICATION	 destroy the sessions that dont need anymore 
WITHOUT_CLASSIFICATION	 filters are not over the rowid therefore scan everything 
WITHOUT_CLASSIFICATION	 switching tables between catalogs not allowed 
WITHOUT_CLASSIFICATION	 throw new not sort order and unique 
WITHOUT_CLASSIFICATION	 according the java documentation this does nothing but just case 
WITHOUT_CLASSIFICATION	 next call will eventually end which makes operation driver and starts its own clisessionstate and then closes which removes from threadloacal thus the session created this class gone after this fixed hiveendpoint 
WITHOUT_CLASSIFICATION	 see arraysbinarysearch javadoc 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 get nonssl socket transport 
WITHOUT_CLASSIFICATION	 compactor generated split for bucket that has data 
WITHOUT_CLASSIFICATION	 create row per database name 
WITHOUT_CLASSIFICATION	 add the last one 
WITHOUT_CLASSIFICATION	 the outputoi has all fields settable return 
WITHOUT_CLASSIFICATION	 case decided run everything local mode restore the 
WITHOUT_CLASSIFICATION	 before failover check are getting connection from minihs 
WITHOUT_CLASSIFICATION	 naked dot 
WITHOUT_CLASSIFICATION	 drop databaseschema exists databasename restrictcascade 
WITHOUT_CLASSIFICATION	 get credentials using the configuration instance which has hbase properties 
WITHOUT_CLASSIFICATION	 deserialize path offset length using filesplit 
WITHOUT_CLASSIFICATION	 test that adding multiple versions the same schema 
WITHOUT_CLASSIFICATION	 authenticate deny based its context completion 
WITHOUT_CLASSIFICATION	 operation emitter will used some internal druid classes 
WITHOUT_CLASSIFICATION	 set all properties specified the command line 
WITHOUT_CLASSIFICATION	 once enough rows have been output there need process input rows 
WITHOUT_CLASSIFICATION	 its key that this per hcatstorer instance object 
WITHOUT_CLASSIFICATION	 cant add null 
WITHOUT_CLASSIFICATION	 other columns provided nonnull values may able finish all rows with this input column 
WITHOUT_CLASSIFICATION	 http over thrift transport settings 
WITHOUT_CLASSIFICATION	 set child expressions 
WITHOUT_CLASSIFICATION	 make sure the bucketid max the numbuckets 
WITHOUT_CLASSIFICATION	 run major compaction 
WITHOUT_CLASSIFICATION	 need look further for checked variant this expression 
WITHOUT_CLASSIFICATION	 todo possible for heartbeats come from lost tasks those should told die which likely already happening 
WITHOUT_CLASSIFICATION	 just vint 
WITHOUT_CLASSIFICATION	 there anything wrong happen bail out 
WITHOUT_CLASSIFICATION	 select should return resultset 
WITHOUT_CLASSIFICATION	 and hash with mask out sign bit make sure its positive then know taking the result mod the range include salt argument this hash function can varied need rehash 
WITHOUT_CLASSIFICATION	 produce this sequence 
WITHOUT_CLASSIFICATION	 scsize 
WITHOUT_CLASSIFICATION	 working the assumption that single dag runs time per 
WITHOUT_CLASSIFICATION	 now create delete delta that has rowids divisible both and this will produce 
WITHOUT_CLASSIFICATION	 make sure the allocation transfered correctly return 
WITHOUT_CLASSIFICATION	 example file names are inputqoutmr inputqout 
WITHOUT_CLASSIFICATION	 jersey uses javautillogging bridge slf 
WITHOUT_CLASSIFICATION	 seek directly first record 
WITHOUT_CLASSIFICATION	 not group 
WITHOUT_CLASSIFICATION	 currently there way stop the metastore service will stopped when the test jvm exits this how other tests are also using metastore server 
WITHOUT_CLASSIFICATION	 note that shouldnt show from 
WITHOUT_CLASSIFICATION	 open session 
WITHOUT_CLASSIFICATION	 reference boolean column covert truth test 
WITHOUT_CLASSIFICATION	 set credentialprovider 
WITHOUT_CLASSIFICATION	 rows looked one repeated key need spill but filtered out rows need generated nonmatches too 
WITHOUT_CLASSIFICATION	 for operator the function name the operator text unless its our special dictionary 
WITHOUT_CLASSIFICATION	 iterate over the path partition descriptions find the partition that matches our input split 
WITHOUT_CLASSIFICATION	 the character set name starts with strip that 
WITHOUT_CLASSIFICATION	 check for escape character before the colon 
WITHOUT_CLASSIFICATION	 this colon escaped search again after 
WITHOUT_CLASSIFICATION	 joining condition 
WITHOUT_CLASSIFICATION	 processvectorgroup keybytes keylength keylength 
WITHOUT_CLASSIFICATION	 because have for true only sel and fil operators this rule will actually only match unionselfilfs the assumption here that are going have multiple operators multiple inserts current implementation does not support this more details please see hive 
WITHOUT_CLASSIFICATION	 http mode 
WITHOUT_CLASSIFICATION	 parse digits 
WITHOUT_CLASSIFICATION	 this will throw exception case the response from druid not array this case occurs for instance druid query execution returns exception instead array results 
WITHOUT_CLASSIFICATION	 remove all parts that are not partition columns see javadoc for details 
WITHOUT_CLASSIFICATION	 only keep the most significant decimaldigits digits 
WITHOUT_CLASSIFICATION	 copy all the values avoid creating more objects todo might cheaper always preserve data reset existing objects 
WITHOUT_CLASSIFICATION	 them 
WITHOUT_CLASSIFICATION	 prepare arguments for 
WITHOUT_CLASSIFICATION	 composite key types comma separated list different parts the composite keys order which they appear the key 
WITHOUT_CLASSIFICATION	 note this should never happen for tables 
WITHOUT_CLASSIFICATION	 data size still could not determined then fall back filesytem get file 
WITHOUT_CLASSIFICATION	 currently include all data partition and any vectorization available virtual columns the vrb 
WITHOUT_CLASSIFICATION	 for use ddl operations that only need shared lock such creating table for use ddl statements that not require lock 
WITHOUT_CLASSIFICATION	 set the rules for the graph walker for group and join operators 
WITHOUT_CLASSIFICATION	 were hijacking the big table evaluators and replacing them with our own custom ones 
WITHOUT_CLASSIFICATION	 there are filespartitions read need skip trying read 
WITHOUT_CLASSIFICATION	 expr alias parses but only allowed for udtfs this check not needed and invalid when there transform the 
WITHOUT_CLASSIFICATION	 add the additional postprocessing transformations needed 
WITHOUT_CLASSIFICATION	 uses the authorizer from sessionstate will need some more work get this run parallel however this should not bottle neck might not need parallelize this 
WITHOUT_CLASSIFICATION	 reducesink parents that missed 
WITHOUT_CLASSIFICATION	 note assume here that the data that was returned the caller from cache will not passed back via put right now its safe since dont anything but evict proactively will have compare objects all the way down 
WITHOUT_CLASSIFICATION	 wrong expression 
WITHOUT_CLASSIFICATION	 capacity exists 
WITHOUT_CLASSIFICATION	 allow implicit string double conversion 
WITHOUT_CLASSIFICATION	 couldnt determine common type dont cast 
WITHOUT_CLASSIFICATION	 when have yet another child beyond the current one save unselected 
WITHOUT_CLASSIFICATION	 object overhead bytes for int nanos bytes padding 
WITHOUT_CLASSIFICATION	 set ugi use kerberos have use the string constant support hadoop 
WITHOUT_CLASSIFICATION	 the creation time changed not check that 
WITHOUT_CLASSIFICATION	 ensure that get the right concrete columnmapping 
WITHOUT_CLASSIFICATION	 cast only needed for hadoop compatibility 
WITHOUT_CLASSIFICATION	 has limit use and not distribute the query 
WITHOUT_CLASSIFICATION	 typename datatype precision literalprefix literalsuffix createparams nullable casesensitive searchable unsignedattribute fixedprecscale autoincrement localtypename minimumscale maximumscale sqldatatype unused sqldatetimesub unused numprecradix 
WITHOUT_CLASSIFICATION	 optional string 
WITHOUT_CLASSIFICATION	 note support pruning the grouping set dummy key mergepartial case use the keycount passed the constructor and not 
WITHOUT_CLASSIFICATION	 can proceed with the conversion 
WITHOUT_CLASSIFICATION	 expect evicted but not failed 
WITHOUT_CLASSIFICATION	 boolean special case 
WITHOUT_CLASSIFICATION	 read each child node add results 
WITHOUT_CLASSIFICATION	 looks like some network outrage reset the file system object and retry 
WITHOUT_CLASSIFICATION	 hiveserver webui 
WITHOUT_CLASSIFICATION	 only the lightweight stuff ctor default llap coordinator created during 
WITHOUT_CLASSIFICATION	 current usage looks like its only for metadata columns but that changes then this method may need require type qualifiers aruments 
WITHOUT_CLASSIFICATION	 timeout 
WITHOUT_CLASSIFICATION	 boolean must come before the integer family its special case 
WITHOUT_CLASSIFICATION	 cascade only occurs with partitioned table 
WITHOUT_CLASSIFICATION	 the function should support both string and binary input types 
WITHOUT_CLASSIFICATION	 todo for llap assumption offheap cache 
WITHOUT_CLASSIFICATION	 match 
WITHOUT_CLASSIFICATION	 hash ineffective disable 
WITHOUT_CLASSIFICATION	 authorize drops there was drop privilege requirement and table not external external table data not dropped 
WITHOUT_CLASSIFICATION	 other rewrites 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 verify corrupted cache value gets replaced 
WITHOUT_CLASSIFICATION	 then remove the table 
WITHOUT_CLASSIFICATION	 colstatindex respond avglong avgdouble 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 number register bits 
WITHOUT_CLASSIFICATION	 preparation for hybrid grace hash join 
WITHOUT_CLASSIFICATION	 override the name provided repl load command 
WITHOUT_CLASSIFICATION	 olddbname and newdbname will the same were here 
WITHOUT_CLASSIFICATION	 this needs manually set under normal circumstances task does this 
WITHOUT_CLASSIFICATION	 for each file figure out which bucket 
WITHOUT_CLASSIFICATION	 should never happen 
WITHOUT_CLASSIFICATION	 discard context that cached for reuse per thread avoid allocating lots arrays and then resizing them down the line need bigger size 
WITHOUT_CLASSIFICATION	 all filters were executed during partition pruning 
WITHOUT_CLASSIFICATION	 nothing front this one prevent acquisition 
WITHOUT_CLASSIFICATION	 set the file owner hive the metastore run 
WITHOUT_CLASSIFICATION	 this method must return the decimal typeinfo for what getcasttodecimal will produce 
WITHOUT_CLASSIFICATION	 clean test space 
WITHOUT_CLASSIFICATION	 use boundarytype boundary amt sort key order behavior case preceding unb any any start current row any any scan backwards until row such rsk rsk start ridx 
WITHOUT_CLASSIFICATION	 this longer does expansions run commands the files used instead depends the developers have already unrolled those the files 
WITHOUT_CLASSIFICATION	 find out this synthetic predicate belongs the current cycle 
WITHOUT_CLASSIFICATION	 special entry for nondp case 
WITHOUT_CLASSIFICATION	 pruning sink operator with map join then pruning sink need not split separate tree add the pruning sink operator context and return 
WITHOUT_CLASSIFICATION	 there one and only one limit starting return the limit there limit return 
WITHOUT_CLASSIFICATION	 original files original directory base directory and delta directory 
WITHOUT_CLASSIFICATION	 first pass will drop the materialized views 
WITHOUT_CLASSIFICATION	 deserialize 
WITHOUT_CLASSIFICATION	 check null handling 
WITHOUT_CLASSIFICATION	 normalize name for mapping 
WITHOUT_CLASSIFICATION	 currently the long must fit markers setting these bit sizes determines the balance between max pool size allowed and max concurrency allowed this balance here not what want each while only objects limit but uses whole bytes and good for now delta and take the same number bits usually doesnt make sense have more delta 
WITHOUT_CLASSIFICATION	 cartesian product 
WITHOUT_CLASSIFICATION	 cache the hints before cbo runs and removes them 
WITHOUT_CLASSIFICATION	 error because thrift client have recreate base object 
WITHOUT_CLASSIFICATION	 lets see can convert aggregate into projects 
WITHOUT_CLASSIFICATION	 null null 
WITHOUT_CLASSIFICATION	 base since the presence base will make the originals obsolete 
WITHOUT_CLASSIFICATION	 this registration has done after knowntasks has been populated register for state change notifications that the waitqueue can reordered correctly the fragment moves out the finishable state 
WITHOUT_CLASSIFICATION	 require ownership there file require select insert and delete 
WITHOUT_CLASSIFICATION	 test for and precedence 
WITHOUT_CLASSIFICATION	 when splitupdate enabled for acid initialize separate deleteeventwriter that used write all the delete events case minor compaction only for major compaction history not required maintained hence the delete events are processed but not rewritten separately 
WITHOUT_CLASSIFICATION	 read the null byte again 
WITHOUT_CLASSIFICATION	 both cases this will the next day gmt 
WITHOUT_CLASSIFICATION	 figure out which factory were instantiating from hiveconf iff its not been set directly 
WITHOUT_CLASSIFICATION	 blank byte blank byte blank byte hyphenminus byte blank byte grave accent byte black sun with rays bytes 
WITHOUT_CLASSIFICATION	 javacc not edit this line 
WITHOUT_CLASSIFICATION	 check repeating 
WITHOUT_CLASSIFICATION	 command methods follow 
WITHOUT_CLASSIFICATION	 make sure that the wasnt escaped 
WITHOUT_CLASSIFICATION	 add any other header info 
WITHOUT_CLASSIFICATION	 table doesnt exist allow creating new one only the database state older than the update this inturn applicable for partitions creation well 
WITHOUT_CLASSIFICATION	 dpp work considered descendant because work needs finish for execute 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 bytes the overhead for reference 
WITHOUT_CLASSIFICATION	 using targeta breaks this 
WITHOUT_CLASSIFICATION	 component 
WITHOUT_CLASSIFICATION	 update the parentop 
WITHOUT_CLASSIFICATION	 column not column reference bail out 
WITHOUT_CLASSIFICATION	 make sure will have less digits than 
WITHOUT_CLASSIFICATION	 this port already use try use another 
WITHOUT_CLASSIFICATION	 ensure size otherwise try again 
WITHOUT_CLASSIFICATION	 sleep half second 
WITHOUT_CLASSIFICATION	 table exists and older than the update now need ensure update allowed the partition 
WITHOUT_CLASSIFICATION	 handled via adminprivops see above 
WITHOUT_CLASSIFICATION	 correct testcase executed twice once with the typed setters once with the generic setobject 
WITHOUT_CLASSIFICATION	 the map vertex 
WITHOUT_CLASSIFICATION	 these are non standard version numbers cant perform the comparison these assume that they are incompatible 
WITHOUT_CLASSIFICATION	 test that locking table prevents locking partitions the table 
WITHOUT_CLASSIFICATION	 input and output are the same 
WITHOUT_CLASSIFICATION	 dont create splits for anything past logical eof 
WITHOUT_CLASSIFICATION	 pinged before log only occasionally seconds elapsed log again 
WITHOUT_CLASSIFICATION	 this implies that locks are needed for such command 
WITHOUT_CLASSIFICATION	 its value provided 
WITHOUT_CLASSIFICATION	 used load columns value into memory 
WITHOUT_CLASSIFICATION	 add would become recursive split 
WITHOUT_CLASSIFICATION	 call open read split mockmocktable 
WITHOUT_CLASSIFICATION	 note that not set location for repl load want that autocreated 
WITHOUT_CLASSIFICATION	 get our singlecolumn string hash set information for this specialized class 
WITHOUT_CLASSIFICATION	 the oldprunerpred and the newpprpred 
WITHOUT_CLASSIFICATION	 get delegation token for the given proxy user 
WITHOUT_CLASSIFICATION	 node disable timeout higher than locality delay 
WITHOUT_CLASSIFICATION	 the query contains windowing processing 
WITHOUT_CLASSIFICATION	 not will start transform the operator tree 
WITHOUT_CLASSIFICATION	 deep one bigger less the top 
WITHOUT_CLASSIFICATION	 allow create table only create should fail for rest the tables and hence constraints 
WITHOUT_CLASSIFICATION	 here want encode the error machine readable way json ideally errorcode would always set canonical error defined errormsg practice that rarely the case the messy logic below tries tease out canonical error code can exclude stack trace from output when the error specificexpected one its written stdout for backward compatibility webhcat consumes 
WITHOUT_CLASSIFICATION	 sum all nonnull double column values for avg maintain isgroupresultnull after last row last group batch compute the group avg when sum nonnull 
WITHOUT_CLASSIFICATION	 otherwise returns the expression that originated the column 
WITHOUT_CLASSIFICATION	 get task associated with this union 
WITHOUT_CLASSIFICATION	 verify integerdigitcount given fastscale 
WITHOUT_CLASSIFICATION	 return value modulo but always the positive range since prime this gives good spread for numbers that are multiples one billion which important since timestamps internally are stored number nanoseconds and the fractional seconds part often 
WITHOUT_CLASSIFICATION	 container reuse 
WITHOUT_CLASSIFICATION	 timestamp scalar case becomes use longdouble scalar class 
WITHOUT_CLASSIFICATION	 after classifying filters there more than joining predicate dont handle this return null 
WITHOUT_CLASSIFICATION	 creating new jars for classes that have already been packaged 
WITHOUT_CLASSIFICATION	 unnormalize divide get result 
WITHOUT_CLASSIFICATION	 find the new database 
WITHOUT_CLASSIFICATION	 get the internal map structure mapkeyvalue 
WITHOUT_CLASSIFICATION	 txnid was committed and thus not open 
WITHOUT_CLASSIFICATION	 unique the list 
WITHOUT_CLASSIFICATION	 get our own instance the transaction handler 
WITHOUT_CLASSIFICATION	 using such aggregate fileid cache not bulletproof and should disableable 
WITHOUT_CLASSIFICATION	 try with chunked stream here the chunked output didnt get chance write the endofdata 
WITHOUT_CLASSIFICATION	 the pruner should not have completed 
WITHOUT_CLASSIFICATION	 distkeylength doesnt include tag but includes bucknum cachedkeys 
WITHOUT_CLASSIFICATION	 perform decorrelation 
WITHOUT_CLASSIFICATION	 there mismatch bucketingversion then should set that way smb will disabled 
WITHOUT_CLASSIFICATION	 drop stats parameter which triggers recompute stats update automatically 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 select all with the null and not null child expressions and then expect the child not invoked 
WITHOUT_CLASSIFICATION	 optional string operationid 
WITHOUT_CLASSIFICATION	 current pos larger than shrinkedlength which calculated for each split table sampling stop fetching any more early exit 
WITHOUT_CLASSIFICATION	 write this out file and import into hive 
WITHOUT_CLASSIFICATION	 create rows file copy and rows file copy 
WITHOUT_CLASSIFICATION	 http path should begin with 
WITHOUT_CLASSIFICATION	 they arent the same but may able cast 
WITHOUT_CLASSIFICATION	 whether show link the most failed task debugging tips 
WITHOUT_CLASSIFICATION	 remember the condition variables for explain regardless whether specialize not 
WITHOUT_CLASSIFICATION	 reason compute interim row count where join type isnt considered because later 
WITHOUT_CLASSIFICATION	 some these tests require intercepting systemexit using the securitymanager safer registerunregister our securitymanager during setupteardown instead doing within the individual test cases 
WITHOUT_CLASSIFICATION	 throw file already exists that should never happen 
WITHOUT_CLASSIFICATION	 that the other locker hasnt checked yet and could lock well 
WITHOUT_CLASSIFICATION	 test longdouble version 
WITHOUT_CLASSIFICATION	 unsupported for the test case 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautilmap 
WITHOUT_CLASSIFICATION	 clear the output 
WITHOUT_CLASSIFICATION	 enabling grouping the payload 
WITHOUT_CLASSIFICATION	 need handle offset with single digital hour see jdk 
WITHOUT_CLASSIFICATION	 arraynull compatible with any other arraytype 
WITHOUT_CLASSIFICATION	 and nodes need intersected 
WITHOUT_CLASSIFICATION	 all zeroes 
WITHOUT_CLASSIFICATION	 debug 
WITHOUT_CLASSIFICATION	 since dont have explicit end signal yet were going create and discard amnodeinfo instances per query 
WITHOUT_CLASSIFICATION	 the udf depends any external resources cant fold because the resources may not available compile time 
WITHOUT_CLASSIFICATION	 create view not use table alias 
WITHOUT_CLASSIFICATION	 tokens cannot used for the management protocol for now 
WITHOUT_CLASSIFICATION	 likely unsupported combination params not available yet skip benchmark cleanly 
WITHOUT_CLASSIFICATION	 only need execute get lock 
WITHOUT_CLASSIFICATION	 assume one request only for one file 
WITHOUT_CLASSIFICATION	 convert skewed table nonskewed table 
WITHOUT_CLASSIFICATION	 for calcite isdeterministic just matters for within the query 
WITHOUT_CLASSIFICATION	 nothing this eof 
WITHOUT_CLASSIFICATION	 the input already present make sure the new parent added the input 
WITHOUT_CLASSIFICATION	 this operator has not been removed include the list existing operators 
WITHOUT_CLASSIFICATION	 set http path 
WITHOUT_CLASSIFICATION	 submit the spark job 
WITHOUT_CLASSIFICATION	 now test with repeating flag 
WITHOUT_CLASSIFICATION	 based the paper daniel lemire streaming maxmin filter using more than comparisons per elem his algorithm works fixed size windows the current row for row and window computes the minmax for window the core idea keep queue max idx tuples tuple the queue represents the max value the range prev tupleidx idx using the queue data structure and following operations easy see that maxes can computed receiving the ith row drain the queue from the back any entries whose value less than the ith entry add the ith value tuple the queue ival the ith step check the element the front the queue has reached its max range influence fronttupleidx yes can remove from the queue the ith step the front the queue the max for the ith entry here modify the algorithm handle windows that are the form where numprecedingf numfollowing start outputing rows only after receiving rows the formula for influence range idx accounts for the following rows optimize for the case when numpreceding unbounded this case only max needs tarcked any given time 
WITHOUT_CLASSIFICATION	 row results should null 
WITHOUT_CLASSIFICATION	 only set the updater for insert for update and delete dont know unitl see the row 
WITHOUT_CLASSIFICATION	 simplified from position prefix synclength syncsize 
WITHOUT_CLASSIFICATION	 try instantiate the old replv task generation every event produced 
WITHOUT_CLASSIFICATION	 for each partition each table drop the partitions and get list 
WITHOUT_CLASSIFICATION	 value count rows 
WITHOUT_CLASSIFICATION	 loading metastore stats async execute simple ensure they are loaded 
WITHOUT_CLASSIFICATION	 since there was allocation failure dont try assigning tasks the next priority 
WITHOUT_CLASSIFICATION	 execute query 
WITHOUT_CLASSIFICATION	 permissions for the metrics file 
WITHOUT_CLASSIFICATION	 remove auth cookie 
WITHOUT_CLASSIFICATION	 prefix for top level properties 
WITHOUT_CLASSIFICATION	 otherwise this not sampling predicate and need 
WITHOUT_CLASSIFICATION	 cookies for requested uri authenticate user and add authentication header 
WITHOUT_CLASSIFICATION	 need use the expanded text for the materialized view will contain 
WITHOUT_CLASSIFICATION	 there are nulls our column 
WITHOUT_CLASSIFICATION	 the types for rows between range between windowing spec 
WITHOUT_CLASSIFICATION	 and expr 
WITHOUT_CLASSIFICATION	 start the split before this slice simple case will read cache from the split start offset 
WITHOUT_CLASSIFICATION	 someone else replacedremoved stale value try again 
WITHOUT_CLASSIFICATION	 the argumentcompletor allows match multiple tokens 
WITHOUT_CLASSIFICATION	 now create sparktasks from the sparkworks also set dependency 
WITHOUT_CLASSIFICATION	 determines should cache table its partitions stats etc 
WITHOUT_CLASSIFICATION	 the schema for intersect distinct like this all attributes countc cnt finally add project project out the last column 
WITHOUT_CLASSIFICATION	 otherwise fall through and proceed with nondecimal vector expression classes 
WITHOUT_CLASSIFICATION	 generate the serialized keys the batch 
WITHOUT_CLASSIFICATION	 indexes those equivalent columns 
WITHOUT_CLASSIFICATION	 nonjavadoc see langobject hand row each function provided there are enough rows for functions window call getnextobject each function output many rows possible based minimum output list 
WITHOUT_CLASSIFICATION	 relations involved join 
WITHOUT_CLASSIFICATION	 have not added this column before bail out 
WITHOUT_CLASSIFICATION	 precision char length 
WITHOUT_CLASSIFICATION	 returns single rowcolumn 
WITHOUT_CLASSIFICATION	 double 
WITHOUT_CLASSIFICATION	 even are just setting the scale when newscale greater than the current scale 
WITHOUT_CLASSIFICATION	 note hypothetically generic wmawaresession should not know about guaranteed tasks should have another subclass for however since this the only type for now this can live here 
WITHOUT_CLASSIFICATION	 principal name can user group role 
WITHOUT_CLASSIFICATION	 update clause 
WITHOUT_CLASSIFICATION	 fast check the next day directory exists return 
WITHOUT_CLASSIFICATION	 firstname john 
WITHOUT_CLASSIFICATION	 make compile happy 
WITHOUT_CLASSIFICATION	 check the function can short cut 
WITHOUT_CLASSIFICATION	 dont strip quotes 
WITHOUT_CLASSIFICATION	 start from end with 
WITHOUT_CLASSIFICATION	 reopen implies the use the reopened session for the same query that gave out for would have failed active query fail the user before its started 
WITHOUT_CLASSIFICATION	 set the session key token base encoded the headers 
WITHOUT_CLASSIFICATION	 not much can about honestly 
WITHOUT_CLASSIFICATION	 todo hive liststring materializedviews assertassertequals 
WITHOUT_CLASSIFICATION	 proper index dummy 
WITHOUT_CLASSIFICATION	 deal with dynamic partitions 
WITHOUT_CLASSIFICATION	 store away the keystore 
WITHOUT_CLASSIFICATION	 verify that the correct methods are invoked accumuloinputformat 
WITHOUT_CLASSIFICATION	 alter unpartitioned table set table property 
WITHOUT_CLASSIFICATION	 based securityutil 
WITHOUT_CLASSIFICATION	 setup inbloomfilter udf 
WITHOUT_CLASSIFICATION	 set transportmode 
WITHOUT_CLASSIFICATION	 thread spun start these other threads thats because cant start them until after the tserver has started but once tserverserve called arent given back control 
WITHOUT_CLASSIFICATION	 both delete events land corresponding buckets the original rowids 
WITHOUT_CLASSIFICATION	 first extract the information that the query provides 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 add these values mixedup green null char string with multbyte chars 
WITHOUT_CLASSIFICATION	 struct column can have null child column 
WITHOUT_CLASSIFICATION	 build with timestamp granularity column 
WITHOUT_CLASSIFICATION	 active passive enabled use default namespace 
WITHOUT_CLASSIFICATION	 need send the splits multiple buckets 
WITHOUT_CLASSIFICATION	 determine maximum all nonnull double column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 create queryid based route 
WITHOUT_CLASSIFICATION	 check that the bit the given index 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 nodeofinterest the query 
WITHOUT_CLASSIFICATION	 ignore files length 
WITHOUT_CLASSIFICATION	 todo make add asynchronous add shouldnt block the higher level calls 
WITHOUT_CLASSIFICATION	 adding name the log file extra log line easier find the original there test error 
WITHOUT_CLASSIFICATION	 the schemadestf null means the destination not the local file system 
WITHOUT_CLASSIFICATION	 exit 
WITHOUT_CLASSIFICATION	 since may have both update and delete branches auth needs know 
WITHOUT_CLASSIFICATION	 verify that partition rename succeded 
WITHOUT_CLASSIFICATION	 match found use update the tables 
WITHOUT_CLASSIFICATION	 the constructor wasnt defined the implementation class flag error 
WITHOUT_CLASSIFICATION	 create the list record copy first record valuekey lengths there 
WITHOUT_CLASSIFICATION	 this number safety limit for writables 
WITHOUT_CLASSIFICATION	 get the process the field struct type 
WITHOUT_CLASSIFICATION	 asc desc 
WITHOUT_CLASSIFICATION	 multiline 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 add dependencies for the jars 
WITHOUT_CLASSIFICATION	 save the actual values from each row opposed the string representation 
WITHOUT_CLASSIFICATION	 note regardless what the input file format returns have determined with that only columns have values want any extra columns needed the table schema were set repeating null the batch 
WITHOUT_CLASSIFICATION	 make binary integer value the bytearray 
WITHOUT_CLASSIFICATION	 use tabledir rootdir 
WITHOUT_CLASSIFICATION	 further checks not file split return equality 
WITHOUT_CLASSIFICATION	 convert parent sel parent 
WITHOUT_CLASSIFICATION	 only left input repeating 
WITHOUT_CLASSIFICATION	 offset trims 
WITHOUT_CLASSIFICATION	 inject behaviour where repeats the insert event twice with different event ids 
WITHOUT_CLASSIFICATION	 returns whether record was forwarded 
WITHOUT_CLASSIFICATION	 ordered columns are the source columns 
WITHOUT_CLASSIFICATION	 note that theory are guaranteed have session waiting for here but the expiration failures etc may cause one missing pending restart 
WITHOUT_CLASSIFICATION	 myenumstructmap 
WITHOUT_CLASSIFICATION	 verify null output entry correct 
WITHOUT_CLASSIFICATION	 for given table and its bucket full file path list only keep the base file name remove file path etc and put the new list into the new mapping 
WITHOUT_CLASSIFICATION	 get the tblproperties value 
WITHOUT_CLASSIFICATION	 insert overwrite dynamic partition 
WITHOUT_CLASSIFICATION	 make sure get table with key returns empty list 
WITHOUT_CLASSIFICATION	 this operator allowed after mapjoin eventually mapjoin hint should done away with but since bucketized mapjoin and sortmerge join depend completely needed check the operators which are allowed after mapjoin 
WITHOUT_CLASSIFICATION	 tokens 
WITHOUT_CLASSIFICATION	 ids 
WITHOUT_CLASSIFICATION	 construct dummy null row indicating empty table and construct spill table serde which used input too 
WITHOUT_CLASSIFICATION	 the request has succeeded but failed add these partitions 
WITHOUT_CLASSIFICATION	 dont move this code the parent class theres binary incompatibility between hadoop and wrt minidfscluster and need have two different shim classes even though they are 
WITHOUT_CLASSIFICATION	 process sync entry minus syncescapes length read synccheck 
WITHOUT_CLASSIFICATION	 add jars that are already the tmpjars variable 
WITHOUT_CLASSIFICATION	 running 
WITHOUT_CLASSIFICATION	 doesnt have all skewed values within its data 
WITHOUT_CLASSIFICATION	 only accept struct which means that were already nested one level deep 
WITHOUT_CLASSIFICATION	 treat all inputs string the return value will converted the appropriate type 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 script operator blackbox hive optimization here assuming that nothing can pushed above the script same with limit create filter with all children predicates 
WITHOUT_CLASSIFICATION	 devenagari letter bytes 
WITHOUT_CLASSIFICATION	 handle null both sizes not repeating 
WITHOUT_CLASSIFICATION	 read paths from each symlink file 
WITHOUT_CLASSIFICATION	 change the current thread name include parent thread executed thread pool useful extract logs specific job request and helpful debug job issues 
WITHOUT_CLASSIFICATION	 get the total available memory from memory manager 
WITHOUT_CLASSIFICATION	 prefill the pool halfway 
WITHOUT_CLASSIFICATION	 ready start execution the cluster 
WITHOUT_CLASSIFICATION	 the dpp sink has target remove the subtree 
WITHOUT_CLASSIFICATION	 first hash used locate start the block blockbaseoffset subsequent hashes are used generate bits within block words avoid branches during probe separate masks array used for each longswords within block 
WITHOUT_CLASSIFICATION	 populate list exclusive splits for every sampled alias 
WITHOUT_CLASSIFICATION	 the expression can any one double long and integer try parse the expression that order ensure that the most specific type used for conversion 
WITHOUT_CLASSIFICATION	 close files 
WITHOUT_CLASSIFICATION	 update the topops appropriately 
WITHOUT_CLASSIFICATION	 retrieve generator 
WITHOUT_CLASSIFICATION	 make sure dbname and tblname are valid 
WITHOUT_CLASSIFICATION	 read not direct not need check its autho 
WITHOUT_CLASSIFICATION	 this means are hitting nested subquery dont need further 
WITHOUT_CLASSIFICATION	 that only allocate writeid only actually adding data adding partition data 
WITHOUT_CLASSIFICATION	 modify the options reflect the event instead the base row 
WITHOUT_CLASSIFICATION	 remember remove this when were out the loop cant the loop well get concurrent modification exception 
WITHOUT_CLASSIFICATION	 setup 
WITHOUT_CLASSIFICATION	 scalar query has aggregate and windowing and gby avoid adding sqcountcheck 
WITHOUT_CLASSIFICATION	 big value len big value bytes 
WITHOUT_CLASSIFICATION	 unprocessed role get its parents add processed and call this function recursively 
WITHOUT_CLASSIFICATION	 returns mapbucketnum listrecord 
WITHOUT_CLASSIFICATION	 swap get reference the left side 
WITHOUT_CLASSIFICATION	 theres point adding task with forcelocality set since that will never exit the queue add other tasks they are not already the queue 
WITHOUT_CLASSIFICATION	 used indicate the input sorted and shoudl used 
WITHOUT_CLASSIFICATION	 lastinputpath should changed the root the operator tree execmappermap 
WITHOUT_CLASSIFICATION	 there isnt already session name ahead and create 
WITHOUT_CLASSIFICATION	 asiaindia 
WITHOUT_CLASSIFICATION	 subqueries will need outer querys row resolver 
WITHOUT_CLASSIFICATION	 test that separate tables dont coalesce 
WITHOUT_CLASSIFICATION	 allowvoidprojection 
WITHOUT_CLASSIFICATION	 only release the related resources ctx drivercontext normal 
WITHOUT_CLASSIFICATION	 this can happen are querying the getfunctions before are getting the actual function between there can drop function user which case our call will fail 
WITHOUT_CLASSIFICATION	 iterate over partition columns figure out partition name 
WITHOUT_CLASSIFICATION	 set two dummy classes authorizatin managers two instances should get created 
WITHOUT_CLASSIFICATION	 invalid table partitioned but endpoints partitionvals empty 
WITHOUT_CLASSIFICATION	 reader schema was provided 
WITHOUT_CLASSIFICATION	 rename 
WITHOUT_CLASSIFICATION	 newline 
WITHOUT_CLASSIFICATION	 empty new database name 
WITHOUT_CLASSIFICATION	 the caller remembers the small value length 
WITHOUT_CLASSIFICATION	 set internal input format for all partition descriptors 
WITHOUT_CLASSIFICATION	 comparison null will always return false 
WITHOUT_CLASSIFICATION	 with multiple users concurrently issuing insert statements the same partition has side effect that some queries may not see partition the time when theyre issued but will realize the partition actually there when trying add such partition the metastore and thus get because some earlier query just created race condition for example imagine such table created create table name char partitioned string and the following two queries are launched the same time from different sessions insert into table partition values bob today creates the partition today insert into table partition values joe today will fail with that case want retry with alterpartition 
WITHOUT_CLASSIFICATION	 replication source file does not exist try cmroot 
WITHOUT_CLASSIFICATION	 now have complete statement process write the line buffer 
WITHOUT_CLASSIFICATION	 undone more 
WITHOUT_CLASSIFICATION	 create the mapjoinoperator 
WITHOUT_CLASSIFICATION	 conditional expression 
WITHOUT_CLASSIFICATION	 for better insertion performance values are added temporary unsorted list which will merged sparse map after threshold 
WITHOUT_CLASSIFICATION	 spot check 
WITHOUT_CLASSIFICATION	 jdbc says that means return all which the default 
WITHOUT_CLASSIFICATION	 validate the row values 
WITHOUT_CLASSIFICATION	 has been committed all others open 
WITHOUT_CLASSIFICATION	 drop table ignore error 
WITHOUT_CLASSIFICATION	 create mocked metastore client that returns table objects every time called will use same size for tableiterable batch fetch size 
WITHOUT_CLASSIFICATION	 rows forwarded will received listsinkoperator which replacing 
WITHOUT_CLASSIFICATION	 ensure columnnames are unique calcite 
WITHOUT_CLASSIFICATION	 unregister functions from local system registry that are not getallfunctions 
WITHOUT_CLASSIFICATION	 delete all the objects created 
WITHOUT_CLASSIFICATION	 todo side note the above lockrequestbuilder combines the both defaultacidtblpart entries 
WITHOUT_CLASSIFICATION	 enums are one two types fudge for hive enums strings come out 
WITHOUT_CLASSIFICATION	 struct 
WITHOUT_CLASSIFICATION	 get the sizes from the key buffer and aggregate 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 update buffer 
WITHOUT_CLASSIFICATION	 mapside join aliases 
WITHOUT_CLASSIFICATION	 copy the text 
WITHOUT_CLASSIFICATION	 some small alias not known too big 
WITHOUT_CLASSIFICATION	 adapts arrow batch reader row reader 
WITHOUT_CLASSIFICATION	 ptf invocation may entail multiple ptf operators 
WITHOUT_CLASSIFICATION	 option bypass task cleanup task was introduced hadoop mapreduce 
WITHOUT_CLASSIFICATION	 output positions 
WITHOUT_CLASSIFICATION	 this was the only predicate set filter expression null 
WITHOUT_CLASSIFICATION	 not done yet 
WITHOUT_CLASSIFICATION	 finally verify the key bytes match 
WITHOUT_CLASSIFICATION	 optional optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 for binary join firstsmalltable the only small table has reference spilled big table rows for nway join since only spill once when processing the first small table only the firstsmalltable has reference the spilled big table rows 
WITHOUT_CLASSIFICATION	 partitionpruner may create more folding opportunities run constantpropagate again 
WITHOUT_CLASSIFICATION	 will swap the vectors from underlying row batch 
WITHOUT_CLASSIFICATION	 hive command operation types starts here 
WITHOUT_CLASSIFICATION	 need know cte not cte may have the same name table for example with select tab masking tab select from tab masking 
WITHOUT_CLASSIFICATION	 them they come back from restarts 
WITHOUT_CLASSIFICATION	 the abstract context for the kinds vectorized reading 
WITHOUT_CLASSIFICATION	 need the size above take effect 
WITHOUT_CLASSIFICATION	 the current table function has partition info specified inherit from the ptf the chain 
WITHOUT_CLASSIFICATION	 when splitupdate enabled not need account for buckets that arent covered this huge performance benefit splitupdate and the reason why are able because the deltas here are actually only the deletedeltas all the insertdeltas with valid user payload data has already been considered base for the covered buckets 
WITHOUT_CLASSIFICATION	 semishared can share with shared but not with itself 
WITHOUT_CLASSIFICATION	 allow anything 
WITHOUT_CLASSIFICATION	 attempt made save partition values nonpartitioned table throw error 
WITHOUT_CLASSIFICATION	 merge expressions 
WITHOUT_CLASSIFICATION	 the user explicitly importing new external table clear txn flags from the spec 
WITHOUT_CLASSIFICATION	 ceilnumsplits numpaths can get least numsplits splits 
WITHOUT_CLASSIFICATION	 not vectorize reduce 
WITHOUT_CLASSIFICATION	 apply the granularity function 
WITHOUT_CLASSIFICATION	 sourcetablename 
WITHOUT_CLASSIFICATION	 check number distinct keys greater than given max number entries 
WITHOUT_CLASSIFICATION	 can wrap inputs the execution vectorized use wrapper 
WITHOUT_CLASSIFICATION	 ascending 
WITHOUT_CLASSIFICATION	 maximum value seen far 
WITHOUT_CLASSIFICATION	 all external file systems 
WITHOUT_CLASSIFICATION	 analyze table 
WITHOUT_CLASSIFICATION	 rule cannot applied there are groupingsets 
WITHOUT_CLASSIFICATION	 original read block 
WITHOUT_CLASSIFICATION	 since set autocommit starts implicit txn close 
WITHOUT_CLASSIFICATION	 wipe out partition columns 
WITHOUT_CLASSIFICATION	 make sure get the right data back beforeafter compactions 
WITHOUT_CLASSIFICATION	 acid code path set rowid null 
WITHOUT_CLASSIFICATION	 inadequate total resources will never succeed wait for new executors become available 
WITHOUT_CLASSIFICATION	 does the same thing getfunctioninfo except for getting the function info 
WITHOUT_CLASSIFICATION	 filemetadata 
WITHOUT_CLASSIFICATION	 when the same node goes away and comes back the old entry will lost which means dont know how many fragments have actually scheduled this node 
WITHOUT_CLASSIFICATION	 bgenjjtree start 
WITHOUT_CLASSIFICATION	 fallback old mechanism which serves smb joins 
WITHOUT_CLASSIFICATION	 expected fail due old schema 
WITHOUT_CLASSIFICATION	 run hcat expression and return just the json outout 
WITHOUT_CLASSIFICATION	 null means the method does not accept number arguments passed 
WITHOUT_CLASSIFICATION	 miss locality request try picking consistent location with fallback random selection 
WITHOUT_CLASSIFICATION	 case success trigger scheduling run for pending tasks 
WITHOUT_CLASSIFICATION	 double the size the array needed 
WITHOUT_CLASSIFICATION	 combine splits only from same tables and same partitions not combine splits from multiple tables multiple partitions 
WITHOUT_CLASSIFICATION	 order which the results should output 
WITHOUT_CLASSIFICATION	 generic settings 
WITHOUT_CLASSIFICATION	 now flushforward all keysrows except the last current one 
WITHOUT_CLASSIFICATION	 returns true columns could inferred false otherwise 
WITHOUT_CLASSIFICATION	 slightly different depending where the test run specifically due file size estimation 
WITHOUT_CLASSIFICATION	 dynamic partitions 
WITHOUT_CLASSIFICATION	 merge the files the destination tablepartitions creating maponly merge job underlying data rcfile rcfileblockmerge task would created 
WITHOUT_CLASSIFICATION	 shutdown metrics 
WITHOUT_CLASSIFICATION	 flag for scan during analyze compute statistics 
WITHOUT_CLASSIFICATION	 llap not enabled noop 
WITHOUT_CLASSIFICATION	 this assumes that grouping will always used 
WITHOUT_CLASSIFICATION	 note that hive does not support udftodouble etc the query text 
WITHOUT_CLASSIFICATION	 decimal multiply 
WITHOUT_CLASSIFICATION	 hostname 
WITHOUT_CLASSIFICATION	 remove the resource plan disable all the queries die 
WITHOUT_CLASSIFICATION	 should have been able reach the union from only one side 
WITHOUT_CLASSIFICATION	 call super which calls super mapjoinoperator with 
WITHOUT_CLASSIFICATION	 set the semijoin hints parse context 
WITHOUT_CLASSIFICATION	 strip off leading blanks and check for sign 
WITHOUT_CLASSIFICATION	 hits misses tracked for candidate node 
WITHOUT_CLASSIFICATION	 best effort 
WITHOUT_CLASSIFICATION	 parse out the context and make sure isnt empty 
WITHOUT_CLASSIFICATION	 create table like tblname 
WITHOUT_CLASSIFICATION	 add the input newinput the set inputs for the query the input may may not already present the readentity also contains the parents from derived only populated case views the equals method for readentity does not compare the parents that the same input with different parents cannot added twice the input already present make sure the parents are added consider the query select from select from union all select from subq where both and depend select from select from addinput would called twice for one with parent and the other with parent when addinput called for the first time for parent added inputs when addinput called for the second time for the input from inputs picked and its parents are enhanced include and the inputs will contain parent parent parentsv the readentity already present and another readentity with same name added then the isdirect flag updated the values both 
WITHOUT_CLASSIFICATION	 bounded max executors 
WITHOUT_CLASSIFICATION	 for insert overwrite 
WITHOUT_CLASSIFICATION	 find first operator upstream with valid nonnull column expression map 
WITHOUT_CLASSIFICATION	 execute session hooks 
WITHOUT_CLASSIFICATION	 not empty least one the source tables being sampled and can not optimize 
WITHOUT_CLASSIFICATION	 for the children populate the newtooldexprmap keep track the original condition before rewriting for this operator 
WITHOUT_CLASSIFICATION	 cost must positive nudge 
WITHOUT_CLASSIFICATION	 bitset for flagging aborted write ids bit true aborted false open default value means there are open write ids the snapshot 
WITHOUT_CLASSIFICATION	 exercise broad range timestamps close the present 
WITHOUT_CLASSIFICATION	 this distinct udaf 
WITHOUT_CLASSIFICATION	 not analyze table column compute statistics statement dont any rewrites 
WITHOUT_CLASSIFICATION	 make sure that arglists size one 
WITHOUT_CLASSIFICATION	 convert mapjoin bucketed mapjoin the operator tree not changed but the mapjoin descriptor the big table enhanced keep the big table bucket small table buckets mapping 
WITHOUT_CLASSIFICATION	 factor includes scale 
WITHOUT_CLASSIFICATION	 roughly based biginteger code 
WITHOUT_CLASSIFICATION	 what the columnvector type the aggregation result 
WITHOUT_CLASSIFICATION	 need make sure that are using segment identifier 
WITHOUT_CLASSIFICATION	 not authorize temporary uris 
WITHOUT_CLASSIFICATION	 common generate join results from hash maps used inner and outer joins 
WITHOUT_CLASSIFICATION	 whose constructor would not have been called 
WITHOUT_CLASSIFICATION	 get our multikey hash set information for this specialized class 
WITHOUT_CLASSIFICATION	 create the select operator 
WITHOUT_CLASSIFICATION	 select countkey from far the reducer concerned 
WITHOUT_CLASSIFICATION	 the states where background operation progress wait for the callback also ignore any duplicate calls also dont kill failed ones handled elsewhere 
WITHOUT_CLASSIFICATION	 status change for active resource plan first activate another plan 
WITHOUT_CLASSIFICATION	 return false categories are not equal 
WITHOUT_CLASSIFICATION	 rcfile supports configurable serde 
WITHOUT_CLASSIFICATION	 this tests the case where older data has ambiguous list and not named indicating that the source considered the group significant 
WITHOUT_CLASSIFICATION	 the writes different directory 
WITHOUT_CLASSIFICATION	 numhashfunctions byte bitset array length bytes 
WITHOUT_CLASSIFICATION	 packing into vertex typically table scan union join 
WITHOUT_CLASSIFICATION	 will get regular single rows from the input file format reader that will need vectorrow deserialize 
WITHOUT_CLASSIFICATION	 parameters 
WITHOUT_CLASSIFICATION	 canevolve 
WITHOUT_CLASSIFICATION	 clear parameters lastinvoke 
WITHOUT_CLASSIFICATION	 this should fine since header should less than the configured header size 
WITHOUT_CLASSIFICATION	 since are walking backwards seek back buffer width that load the previous buffer rows 
WITHOUT_CLASSIFICATION	 test string 
WITHOUT_CLASSIFICATION	 batch already memory anyway will bypass the memory checks 
WITHOUT_CLASSIFICATION	 verify the content subdirs 
WITHOUT_CLASSIFICATION	 true are running test and the extra test file should used when the logs are 
WITHOUT_CLASSIFICATION	 and set the pattern and excludematches accordingly 
WITHOUT_CLASSIFICATION	 create orc file with small stripe size can write multiple stripes 
WITHOUT_CLASSIFICATION	 without data 
WITHOUT_CLASSIFICATION	 aborted 
WITHOUT_CLASSIFICATION	 this relies heavily what method determinesplits calls and doesnt 
WITHOUT_CLASSIFICATION	 add metastore 
WITHOUT_CLASSIFICATION	 got error might there anyway due permissions problem 
WITHOUT_CLASSIFICATION	 get the first live service instance 
WITHOUT_CLASSIFICATION	 formatteron 
WITHOUT_CLASSIFICATION	 convert input row standard objects 
WITHOUT_CLASSIFICATION	 they are both types strings that should fine 
WITHOUT_CLASSIFICATION	 consider generating column group equal value series 
WITHOUT_CLASSIFICATION	 equivalent works must have dpp lists same size 
WITHOUT_CLASSIFICATION	 only the threads need this thered most few dozen objects 
WITHOUT_CLASSIFICATION	 this should error analyze scope 
WITHOUT_CLASSIFICATION	 syllable mee bytes 
WITHOUT_CLASSIFICATION	 figure out there are any currently running compactions the same table partition 
WITHOUT_CLASSIFICATION	 nothing close here 
WITHOUT_CLASSIFICATION	 add the merge job 
WITHOUT_CLASSIFICATION	 create select operator 
WITHOUT_CLASSIFICATION	 nulls case not repeating 
WITHOUT_CLASSIFICATION	 the reason poll here that blocking queue causes the query thread spend nontrivial amount time signaling when element added wed rather that the time was wasted this background thread 
WITHOUT_CLASSIFICATION	 length each value the map 
WITHOUT_CLASSIFICATION	 writes the timestamptzs serialized value the internal byte array 
WITHOUT_CLASSIFICATION	 verify the config 
WITHOUT_CLASSIFICATION	 last singleton range 
WITHOUT_CLASSIFICATION	 this table has not been modified since materialization was created nothing 
WITHOUT_CLASSIFICATION	 have readentity defaultacidtblpart 
WITHOUT_CLASSIFICATION	 current buffer size should larger than initial size 
WITHOUT_CLASSIFICATION	 the group keys and distinct keys should the same for all dests using the first 
WITHOUT_CLASSIFICATION	 cant get any info without plan 
WITHOUT_CLASSIFICATION	 supposed dump path does not exist 
WITHOUT_CLASSIFICATION	 returns true passes the test false otherwise 
WITHOUT_CLASSIFICATION	 get cost the subset best rel may have been chosen not 
WITHOUT_CLASSIFICATION	 the txnid then there are transactions this heartbeat 
WITHOUT_CLASSIFICATION	 this the first keyvalueseparator this entry 
WITHOUT_CLASSIFICATION	 for fields descending order bit flip first 
WITHOUT_CLASSIFICATION	 returns nonnull fetchtask instance when succeeded 
WITHOUT_CLASSIFICATION	 reported once break 
WITHOUT_CLASSIFICATION	 make connection object that will throw exception 
WITHOUT_CLASSIFICATION	 bucket mapjoin llap 
WITHOUT_CLASSIFICATION	 invariant hllp 
WITHOUT_CLASSIFICATION	 sortmerge join 
WITHOUT_CLASSIFICATION	 have evicted the entire list 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader 
WITHOUT_CLASSIFICATION	 prepare data for client stat publishers any present and execute them 
WITHOUT_CLASSIFICATION	 extract the correlation out the filter 
WITHOUT_CLASSIFICATION	 multiple rules can matched with same cost last rule will choosen processor 
WITHOUT_CLASSIFICATION	 localize hiveexecjar well 
WITHOUT_CLASSIFICATION	 make sure for existing destination return false per filesystem api contract 
WITHOUT_CLASSIFICATION	 runtime that launches runnable tasks separate threads through taskrunners soon task isrunnable put queue any time most maxthreads tasks can running the main thread polls the taskrunners check they have finished 
WITHOUT_CLASSIFICATION	 for between clause 
WITHOUT_CLASSIFICATION	 read configuration parameters 
WITHOUT_CLASSIFICATION	 update sum the length the values seen far 
WITHOUT_CLASSIFICATION	 need add not null filter for constant 
WITHOUT_CLASSIFICATION	 step rename tmp output folder intermediate path after this point updates from speculative tasks still writing tmppath will not appear finalpath 
WITHOUT_CLASSIFICATION	 some existing chunk find max 
WITHOUT_CLASSIFICATION	 save some info for webui for use after plan freed 
WITHOUT_CLASSIFICATION	 now run its major compaction collapse events 
WITHOUT_CLASSIFICATION	 now duplicated field name should fail 
WITHOUT_CLASSIFICATION	 num distinct vals for col clause num distinct vals for col 
WITHOUT_CLASSIFICATION	 careful maintenance nulls 
WITHOUT_CLASSIFICATION	 grant option specified only update the privilege dont remove grant option has already been removed from the privileges the section above 
WITHOUT_CLASSIFICATION	 via hint 
WITHOUT_CLASSIFICATION	 maxparts 
WITHOUT_CLASSIFICATION	 after lead and lag call allow object associated with serde and writable associated with partition reset the value for the current index 
WITHOUT_CLASSIFICATION	 the element type not tuple subschema 
WITHOUT_CLASSIFICATION	 text 
WITHOUT_CLASSIFICATION	 first value repeated for all batches 
WITHOUT_CLASSIFICATION	 end digits 
WITHOUT_CLASSIFICATION	 jobclose has already been performed this operator 
WITHOUT_CLASSIFICATION	 simulate renaming via another metastore thrift server another hive cli instance 
WITHOUT_CLASSIFICATION	 data structures specific for vectorized operators 
WITHOUT_CLASSIFICATION	 call 
WITHOUT_CLASSIFICATION	 varchar between 
WITHOUT_CLASSIFICATION	 the client direct all calls catalog that does not yet exist 
WITHOUT_CLASSIFICATION	 copy into the current union task plan 
WITHOUT_CLASSIFICATION	 could wrapper with only size and get methods instead list sure 
WITHOUT_CLASSIFICATION	 create function desc 
WITHOUT_CLASSIFICATION	 for nonsingular args count can include null counted 
WITHOUT_CLASSIFICATION	 get buffer size and stripe size for base writer 
WITHOUT_CLASSIFICATION	 startdate sat letters day name 
WITHOUT_CLASSIFICATION	 for now keep the old logic for nonmm nondp union case should probably unified 
WITHOUT_CLASSIFICATION	 check for number created files 
WITHOUT_CLASSIFICATION	 these are from 
WITHOUT_CLASSIFICATION	 datanucleus wants autocreate but shall such thing 
WITHOUT_CLASSIFICATION	 caller will set signum 
WITHOUT_CLASSIFICATION	 reset the iter start 
WITHOUT_CLASSIFICATION	 columns and originalrr the original generated select 
WITHOUT_CLASSIFICATION	 todo should checked server side embedded metastore throws metaexception remote metastore throws ttransportexception 
WITHOUT_CLASSIFICATION	 max ndv across all column references from both sides table 
WITHOUT_CLASSIFICATION	 trailing spaces are not significant 
WITHOUT_CLASSIFICATION	 the leaves could shared the tree use set remove the duplicates 
WITHOUT_CLASSIFICATION	 source directory specified treat the target directory 
WITHOUT_CLASSIFICATION	 top the operator tree this could also reduce the amount data going the reducer 
WITHOUT_CLASSIFICATION	 the children are input 
WITHOUT_CLASSIFICATION	 done with this operator 
WITHOUT_CLASSIFICATION	 this also used for table conversion 
WITHOUT_CLASSIFICATION	 tests for the partition string partitionspecs string sourcedb string sourcetable string destdb string desttablename method 
WITHOUT_CLASSIFICATION	 set some conf vars 
WITHOUT_CLASSIFICATION	 mapping the fieldid the field 
WITHOUT_CLASSIFICATION	 for views the entities can nested default entities are the top level 
WITHOUT_CLASSIFICATION	 multigby singlers todo 
WITHOUT_CLASSIFICATION	 bogus encoding 
WITHOUT_CLASSIFICATION	 initialize with for nonacid and nonmm tables 
WITHOUT_CLASSIFICATION	 clone postjoinfilters 
WITHOUT_CLASSIFICATION	 shouldnt any others 
WITHOUT_CLASSIFICATION	 these are the filters which are common for every qtest 
WITHOUT_CLASSIFICATION	 tasks can exist the delayed queue even after they have been scheduled trigger scheduling only the task still pending state 
WITHOUT_CLASSIFICATION	 starting from the startnodes add the children whose parents have been included the list 
WITHOUT_CLASSIFICATION	 partsfound 
WITHOUT_CLASSIFICATION	 test adding constraint 
WITHOUT_CLASSIFICATION	 print the value 
WITHOUT_CLASSIFICATION	 execute set command and retrieve values for the conf vars specified above 
WITHOUT_CLASSIFICATION	 dont reset anything are reusing column vectors 
WITHOUT_CLASSIFICATION	 provide instance the code doesnt try make real instance just want test that fail before trying make connector with null username 
WITHOUT_CLASSIFICATION	 specify them the rules link 
WITHOUT_CLASSIFICATION	 part the big table portion the join output result 
WITHOUT_CLASSIFICATION	 passing query spec column names and column types used part hive physical execution 
WITHOUT_CLASSIFICATION	 process reduce sink added hiveenforcesorting 
WITHOUT_CLASSIFICATION	 resultschema will null cbo disabled cbo enabled with ast return path whether succeeded not resultschema will reinitialized will only not null cbo enabled with new return path and succeeds 
WITHOUT_CLASSIFICATION	 after reading the batch reset the pointer beginning 
WITHOUT_CLASSIFICATION	 preserve existing return type behavior for division nondecimal division should return double 
WITHOUT_CLASSIFICATION	 clusters buckets 
WITHOUT_CLASSIFICATION	 optional string amhost 
WITHOUT_CLASSIFICATION	 false and hence not found should error out 
WITHOUT_CLASSIFICATION	 dphj disabled only attempt bmj mapjoin 
WITHOUT_CLASSIFICATION	 test zerodivide show results null 
WITHOUT_CLASSIFICATION	 when dag specific cleanup happens itll better link this dag though 
WITHOUT_CLASSIFICATION	 update should take place such with replication 
WITHOUT_CLASSIFICATION	 check ifofserde 
WITHOUT_CLASSIFICATION	 note not alter the projectedcolumns projectionsize the batches just the included columns partition columns for now need model the object inspector rows because there are still several vectorized operators that use them need continue model the object having null objects for not included columns until the following has been fixed when have output struct for avg switch row groupby operators some variations vectormapoperator use the row super class process rows 
WITHOUT_CLASSIFICATION	 total size each hash entry 
WITHOUT_CLASSIFICATION	 requestline 
WITHOUT_CLASSIFICATION	 the current plan can thrown away after being merged with the original plan 
WITHOUT_CLASSIFICATION	 ensure this explicitly since versions before read doesnt 
WITHOUT_CLASSIFICATION	 external table and custom root specified update the parent path 
WITHOUT_CLASSIFICATION	 remove the ddl time that gets refreshed 
WITHOUT_CLASSIFICATION	 base name varchar fully qualified name such varchar 
WITHOUT_CLASSIFICATION	 are putting join keys last part the spilled table 
WITHOUT_CLASSIFICATION	 the table bucketed partition column not valid for bucketing 
WITHOUT_CLASSIFICATION	 granularity column 
WITHOUT_CLASSIFICATION	 hence need not allow same event applied twice 
WITHOUT_CLASSIFICATION	 validate the first parameter which the expression compute over 
WITHOUT_CLASSIFICATION	 decide default directory selection 
WITHOUT_CLASSIFICATION	 committed basetxnid 
WITHOUT_CLASSIFICATION	 close the underlying stream own 
WITHOUT_CLASSIFICATION	 prior singleton range 
WITHOUT_CLASSIFICATION	 instead multiple times 
WITHOUT_CLASSIFICATION	 data columns 
WITHOUT_CLASSIFICATION	 parameter was array arrays make sure that the inner arrays contain primitive strings 
WITHOUT_CLASSIFICATION	 bytemaxvalue 
WITHOUT_CLASSIFICATION	 have selected port client port update clientportlist necessary not the list add the port 
WITHOUT_CLASSIFICATION	 calendargetinstance calculates the currenttime needlessly cache instance 
WITHOUT_CLASSIFICATION	 position distinct column aggregator list map gby before rewrite 
WITHOUT_CLASSIFICATION	 this can happen for truncate table case for nonmm tables 
WITHOUT_CLASSIFICATION	 create barrier task for dependency collection import tasks 
WITHOUT_CLASSIFICATION	 gather big and small table output result information from the mapjoindesc 
WITHOUT_CLASSIFICATION	 bitpacking disabled all register values takes bits and hence can more flexible with the threshold for entries sparse map can allowed 
WITHOUT_CLASSIFICATION	 other references this can throw out 
WITHOUT_CLASSIFICATION	 copy the files out the archive into the temporary directory 
WITHOUT_CLASSIFICATION	 unsupported types error 
WITHOUT_CLASSIFICATION	 save connectionmap can closed users convenience 
WITHOUT_CLASSIFICATION	 tables 
WITHOUT_CLASSIFICATION	 the default was decided the serde 
WITHOUT_CLASSIFICATION	 here the layout export metadata warehouse acidtbl acidtblpart nonacidnonbucket nonacidorctbl nonacidorctbl delta orcacidversion bucket delta orcacidversion bucket delta orcacidversion bucket timport directories files 
WITHOUT_CLASSIFICATION	 this assumes all struct cols immediately follow struct 
WITHOUT_CLASSIFICATION	 analyze and create tbl properties object 
WITHOUT_CLASSIFICATION	 not needed without semijoin reduction 
WITHOUT_CLASSIFICATION	 know need bail out 
WITHOUT_CLASSIFICATION	 create http client with retry mechanism when the server returns status code 
WITHOUT_CLASSIFICATION	 only need promote comptype existingtype for efficiency check existing exclusive which case need never promote comp exclusive sharedwrite which case can promote even though they may both shared write comp sharedread theres never need promote 
WITHOUT_CLASSIFICATION	 since cannot know what columns will needed ptf chain not prune columns ptfoperator for ptf chains 
WITHOUT_CLASSIFICATION	 copy limit 
WITHOUT_CLASSIFICATION	 tolerance check double equality 
WITHOUT_CLASSIFICATION	 set where derby logs 
WITHOUT_CLASSIFICATION	 create new table similar previous one 
WITHOUT_CLASSIFICATION	 didnt find any lock with extlockid but readcommitted there possibility that existed when above delete ran but didnt have the expected state 
WITHOUT_CLASSIFICATION	 set 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader 
WITHOUT_CLASSIFICATION	 for cases where the table external 
WITHOUT_CLASSIFICATION	 long add them list order preserved from now 
WITHOUT_CLASSIFICATION	 oracle doesnt exhibit this problem 
WITHOUT_CLASSIFICATION	 compare timestamp timestamp 
WITHOUT_CLASSIFICATION	 the bucketingversion not relevant here never used for smb look the parent tables bucketing versions and for 
WITHOUT_CLASSIFICATION	 week granularity 
WITHOUT_CLASSIFICATION	 create the object inspector and the lazy binary struct object 
WITHOUT_CLASSIFICATION	 check oozie has set hcat deleg token use 
WITHOUT_CLASSIFICATION	 move past separator 
WITHOUT_CLASSIFICATION	 indicated selectedinuse and the sel array 
WITHOUT_CLASSIFICATION	 will trigger cleanup 
WITHOUT_CLASSIFICATION	 simplify vector bruteforce flattening nonulls and isrepeating this can used reduce combinatorial explosion code paths vectorexpressions 
WITHOUT_CLASSIFICATION	 are viewfs dont want use tmp tmp dir since rename from tmp final userhivewarehouse will fail later instead pick tmp dir same namespace tbl dir 
WITHOUT_CLASSIFICATION	 get the named url from user specific config file present 
WITHOUT_CLASSIFICATION	 only used for testing 
WITHOUT_CLASSIFICATION	 cast string 
WITHOUT_CLASSIFICATION	 not filter when ptf reducer 
WITHOUT_CLASSIFICATION	 collection separator 
WITHOUT_CLASSIFICATION	 case operator need rewrite 
WITHOUT_CLASSIFICATION	 execute query druid 
WITHOUT_CLASSIFICATION	 compute count only the register values are updated else return the cached count 
WITHOUT_CLASSIFICATION	 partitions 
WITHOUT_CLASSIFICATION	 create gss context 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check table with the new name already exists 
WITHOUT_CLASSIFICATION	 invoked for test classes 
WITHOUT_CLASSIFICATION	 when adding support for new types should try use classes hive value system keep things more readable though functionally should not make difference 
WITHOUT_CLASSIFICATION	 maximum number decimal digits decimal long 
WITHOUT_CLASSIFICATION	 singlerow case theres next 
WITHOUT_CLASSIFICATION	 prefix for column names auto generated hive 
WITHOUT_CLASSIFICATION	 location not set utilize metastorewarehouse together with database name 
WITHOUT_CLASSIFICATION	 when negative when decimal zero when positive 
WITHOUT_CLASSIFICATION	 analyze table partition compute statistics the plan consists simple teztask followed statstask the tez task just simple tablescanoperator 
WITHOUT_CLASSIFICATION	 execute statement with the conf overlay 
WITHOUT_CLASSIFICATION	 that fractions query parallelism add etc 
WITHOUT_CLASSIFICATION	 whether have enforce sort anyway case deduplication 
WITHOUT_CLASSIFICATION	 more variations callbacks increase decrease and decrease increase the call coming before the message sent message should ever sent 
WITHOUT_CLASSIFICATION	 using metastore running existing cluster 
WITHOUT_CLASSIFICATION	 test normal drop should drop unconditionally 
WITHOUT_CLASSIFICATION	 then groups this arbitrary 
WITHOUT_CLASSIFICATION	 asserts the class invariant same types 
WITHOUT_CLASSIFICATION	 test andfilter operation 
WITHOUT_CLASSIFICATION	 where write our key and value pairs 
WITHOUT_CLASSIFICATION	 longmaxvalue 
WITHOUT_CLASSIFICATION	 will return back the original token which know insufficient 
WITHOUT_CLASSIFICATION	 covers both streaming api and post compaction style 
WITHOUT_CLASSIFICATION	 will unset and only these should updated and nothing for 
WITHOUT_CLASSIFICATION	 any destination partition present then throw semantic exception 
WITHOUT_CLASSIFICATION	 add only dynamic partition columns the temp table input data file 
WITHOUT_CLASSIFICATION	 this should fail 
WITHOUT_CLASSIFICATION	 sleep for 
WITHOUT_CLASSIFICATION	 set now can verify changed 
WITHOUT_CLASSIFICATION	 rare case buffer boundary unfortunately wed have copy some bytes 
WITHOUT_CLASSIFICATION	 this effectively the same the dense register impl 
WITHOUT_CLASSIFICATION	 sorts 
WITHOUT_CLASSIFICATION	 before the ownertype exists old hive schema user was the default type for owner lets set the default user keep backward compatibility 
WITHOUT_CLASSIFICATION	 test the inputformat execution path 
WITHOUT_CLASSIFICATION	 just fetch one blob weve serialized thrift objects final tasks 
WITHOUT_CLASSIFICATION	 llapcommon llaptez llapserver hiveexec hivecommon https deps jetty rewrite class registry 
WITHOUT_CLASSIFICATION	 select statement dynamic sql 
WITHOUT_CLASSIFICATION	 timeout minutes 
WITHOUT_CLASSIFICATION	 operator tree for processing row further optional 
WITHOUT_CLASSIFICATION	 test adding multiple partitions single partitionset atomically 
WITHOUT_CLASSIFICATION	 default the list empty operator wants add more counters should override this method and provide the new list counter names returned this method should wrapped counter names the strings should passed through 
WITHOUT_CLASSIFICATION	 add partitions the filesystem 
WITHOUT_CLASSIFICATION	 upload archive file hdfs 
WITHOUT_CLASSIFICATION	 otherwise wed create nullwritable and that isnt what want 
WITHOUT_CLASSIFICATION	 keep the hash multiset result for its spill information 
WITHOUT_CLASSIFICATION	 check giving invalid address causes retry connection attempt 
WITHOUT_CLASSIFICATION	 tag 
WITHOUT_CLASSIFICATION	 remain consistent need set input and output formats both 
WITHOUT_CLASSIFICATION	 start metrics for standalone remote mode 
WITHOUT_CLASSIFICATION	 success only all the commands were successful 
WITHOUT_CLASSIFICATION	 update max length new length greater than the ones seen far 
WITHOUT_CLASSIFICATION	 thread pool for actual execution work 
WITHOUT_CLASSIFICATION	 create join need check whether need update left case 
WITHOUT_CLASSIFICATION	 parition column case partition filter will evaluated partition pruner will not evaluate partition filter here 
WITHOUT_CLASSIFICATION	 give lot slack since low numbers consistent hashing very imprecise 
WITHOUT_CLASSIFICATION	 run aggregatejoin transpose cost based failed because missing stats continue with 
WITHOUT_CLASSIFICATION	 concurrent revocation and increase before the message sent 
WITHOUT_CLASSIFICATION	 localjobrunner does not work with mapreduce outputcommitter need use minimrcluster mapreduce 
WITHOUT_CLASSIFICATION	 distinct partitions created txnid because want txn used previous driverruninsert 
WITHOUT_CLASSIFICATION	 string range values empty strings 
WITHOUT_CLASSIFICATION	 call renamepartition without environment context 
WITHOUT_CLASSIFICATION	 run value expressions over original whole input batch 
WITHOUT_CLASSIFICATION	 create table scenarios 
WITHOUT_CLASSIFICATION	 nothing here 
WITHOUT_CLASSIFICATION	 remember the original parent list before start modifying 
WITHOUT_CLASSIFICATION	 initialize the execution engine based cluster type 
WITHOUT_CLASSIFICATION	 prefer numeric type arguments over other method signatures 
WITHOUT_CLASSIFICATION	 nearly from orcinputformat there are too many statics everywhere sort this out 
WITHOUT_CLASSIFICATION	 assume the index bad and full scan 
WITHOUT_CLASSIFICATION	 batch size from input and decaying factor 
WITHOUT_CLASSIFICATION	 indicates type was derived from the deserializer rather than hives metadata 
WITHOUT_CLASSIFICATION	 rxrx and and rand order limit 
WITHOUT_CLASSIFICATION	 find 
WITHOUT_CLASSIFICATION	 only last rows qualify 
WITHOUT_CLASSIFICATION	 delayed find local match 
WITHOUT_CLASSIFICATION	 this check isnt absolutely mandatory given the aborted check outside the 
WITHOUT_CLASSIFICATION	 run query loop that hit occasionally 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 find the bucket and switch buckets need 
WITHOUT_CLASSIFICATION	 just return stats gathering should not block the main query 
WITHOUT_CLASSIFICATION	 create session test the hook got fired checking the expected property 
WITHOUT_CLASSIFICATION	 sets the env variable value defined 
WITHOUT_CLASSIFICATION	 for java serialization only 
WITHOUT_CLASSIFICATION	 undone null false 
WITHOUT_CLASSIFICATION	 special case handling for multior and multiand 
WITHOUT_CLASSIFICATION	 setup dynamic partition pruning where possible 
WITHOUT_CLASSIFICATION	 need add filter create tableop filterdesc and set child top 
WITHOUT_CLASSIFICATION	 throwing would more appropriate but not change the api 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 skip from block block since only need the header 
WITHOUT_CLASSIFICATION	 filter operator has the same output columns its parent 
WITHOUT_CLASSIFICATION	 calculate unselected ones last evaluate 
WITHOUT_CLASSIFICATION	 will fail 
WITHOUT_CLASSIFICATION	 neither open nor opening 
WITHOUT_CLASSIFICATION	 authorize the revoke and get the set privileges revoked 
WITHOUT_CLASSIFICATION	 nothing registration involved 
WITHOUT_CLASSIFICATION	 serialize the keys and append the tag 
WITHOUT_CLASSIFICATION	 add all udtf columns 
WITHOUT_CLASSIFICATION	 build the new list 
WITHOUT_CLASSIFICATION	 optimize plan 
WITHOUT_CLASSIFICATION	 used for create mapjoindesc should order 
WITHOUT_CLASSIFICATION	 handle null return the type pca 
WITHOUT_CLASSIFICATION	 since have base there must least delta which must result acid write must immediate child the partition 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 parent opsselgbrsgbrs 
WITHOUT_CLASSIFICATION	 undone dont reuse for now 
WITHOUT_CLASSIFICATION	 otherwise value may too long convert appropriate value based params 
WITHOUT_CLASSIFICATION	 reduce sink group operator 
WITHOUT_CLASSIFICATION	 add user privileges 
WITHOUT_CLASSIFICATION	 check julian days between jan and jan 
WITHOUT_CLASSIFICATION	 first row determines isgroupresultnull and long firstvalue stream fill result repeated 
WITHOUT_CLASSIFICATION	 make insert into nonacid take shared lock 
WITHOUT_CLASSIFICATION	 null null 
WITHOUT_CLASSIFICATION	 handle null return the type pcb 
WITHOUT_CLASSIFICATION	 create delete delta that has rowids divisible but not this will produce 
WITHOUT_CLASSIFICATION	 mergecount 
WITHOUT_CLASSIFICATION	 optional bytes 
WITHOUT_CLASSIFICATION	 props new properties 
WITHOUT_CLASSIFICATION	 direct encoding 
WITHOUT_CLASSIFICATION	 begin write abort 
WITHOUT_CLASSIFICATION	 join with groupby having orderby 
WITHOUT_CLASSIFICATION	 try deserialize using deserializeread our writable row objects created serializewrite 
WITHOUT_CLASSIFICATION	 must called last 
WITHOUT_CLASSIFICATION	 get the last valid row the batch still available 
WITHOUT_CLASSIFICATION	 typename used however now the value dbnametablename make backward compatible take the tablename part typename 
WITHOUT_CLASSIFICATION	 set the java key provider for encrypted hdfs cluster 
WITHOUT_CLASSIFICATION	 safeguard against potential issues cbo rowresolver construction disable cbo for now 
WITHOUT_CLASSIFICATION	 input type date epochdays 
WITHOUT_CLASSIFICATION	 execute 
WITHOUT_CLASSIFICATION	 asserttrueve instanceof 
WITHOUT_CLASSIFICATION	 note wmfragmentcounters are created before tez counters are created 
WITHOUT_CLASSIFICATION	 for import statement require uri rwxowner privileges input uri and necessary privileges the output table and database note privileges are only checked the object that type marked part readentity writeentity table present import will mark table writeentity and well authorize for that and not present 
WITHOUT_CLASSIFICATION	 rounding results 
WITHOUT_CLASSIFICATION	 should have all the necessary information vectorize 
WITHOUT_CLASSIFICATION	 has also been adjusted point these buffers instead compressed data for the ranges 
WITHOUT_CLASSIFICATION	 thread reading the ats guid 
WITHOUT_CLASSIFICATION	 need iterate detect original directories that are supported but not acid 
WITHOUT_CLASSIFICATION	 partial partition spec supplied make sure this allowed 
WITHOUT_CLASSIFICATION	 column name not contained needed column list then partition column not need evaluate partition columns 
WITHOUT_CLASSIFICATION	 isforcedeactivate 
WITHOUT_CLASSIFICATION	 now generate insert statement 
WITHOUT_CLASSIFICATION	 variablelength arguments 
WITHOUT_CLASSIFICATION	 inheritdoc 
WITHOUT_CLASSIFICATION	 all hadoop versions 
WITHOUT_CLASSIFICATION	 this error should really produced hive ddltask 
WITHOUT_CLASSIFICATION	 reposition the begining 
WITHOUT_CLASSIFICATION	 gets lock 
WITHOUT_CLASSIFICATION	 possible all children have same expressions but not likely 
WITHOUT_CLASSIFICATION	 use old timestamp writable hash code for backwards compatibility 
WITHOUT_CLASSIFICATION	 binary transport settings 
WITHOUT_CLASSIFICATION	 didnt set the last repl due some failure 
WITHOUT_CLASSIFICATION	 adding these job properties will make them available the outputformat checkoutputspecs 
WITHOUT_CLASSIFICATION	 pass null for aliases here because filters aliases the children node context and need filter them the current joinoperators context 
WITHOUT_CLASSIFICATION	 create table object 
WITHOUT_CLASSIFICATION	 todo constant constant 
WITHOUT_CLASSIFICATION	 the table not external and might not subdirectory the database 
WITHOUT_CLASSIFICATION	 looks for hivesitexml from the classpath found this class parses the hivesitexml return set connection properties which can used construct the connection url for beeline connection 
WITHOUT_CLASSIFICATION	 common case the segment one buffer 
WITHOUT_CLASSIFICATION	 here after commitabort but before next currenttxnindex still 
WITHOUT_CLASSIFICATION	 when supported read field field number random access currently only supports this return return true when the field was not null and data put the appropriate current member otherwise false when the field null 
WITHOUT_CLASSIFICATION	 return the column numbers the bucketed columns 
WITHOUT_CLASSIFICATION	 pools 
WITHOUT_CLASSIFICATION	 ignore 
WITHOUT_CLASSIFICATION	 inform the shuffle handler 
WITHOUT_CLASSIFICATION	 used cleanup cache 
WITHOUT_CLASSIFICATION	 past the timeout 
WITHOUT_CLASSIFICATION	 tracks requests executing per node 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautillist 
WITHOUT_CLASSIFICATION	 process the actual join 
WITHOUT_CLASSIFICATION	 verify the configs are merged 
WITHOUT_CLASSIFICATION	 count characters forward 
WITHOUT_CLASSIFICATION	 referring job tracker and resource manager 
WITHOUT_CLASSIFICATION	 check select involves udtf 
WITHOUT_CLASSIFICATION	 overwrite the remote file already exists whether the file can added executor spark sparkfilesoverwrite 
WITHOUT_CLASSIFICATION	 global config vectorized input format enabled check these inputformats are excluded 
WITHOUT_CLASSIFICATION	 this input rel does produce the cor var referenced assume fieldaccess has the correct type info 
WITHOUT_CLASSIFICATION	 native vectorization supported 
WITHOUT_CLASSIFICATION	 make sure minihs the new leader 
WITHOUT_CLASSIFICATION	 allow implicit conversion from byte integer long float double 
WITHOUT_CLASSIFICATION	 startup may unable get number executors 
WITHOUT_CLASSIFICATION	 based actual timing 
WITHOUT_CLASSIFICATION	 reduce vertex 
WITHOUT_CLASSIFICATION	 checked semanticanalyzer should not happen 
WITHOUT_CLASSIFICATION	 write the first part the array 
WITHOUT_CLASSIFICATION	 test deprecated api 
WITHOUT_CLASSIFICATION	 all good 
WITHOUT_CLASSIFICATION	 analyzecreatetable uses thisast but dophase doesnt only reset here 
WITHOUT_CLASSIFICATION	 case were searching through especially large set data send heartbeat order avoid timeout 
WITHOUT_CLASSIFICATION	 case column stats hash aggregation grouping sets 
WITHOUT_CLASSIFICATION	 check removes the property 
WITHOUT_CLASSIFICATION	 the partition column were interested 
WITHOUT_CLASSIFICATION	 table non hive catalog 
WITHOUT_CLASSIFICATION	 run times present then set indicating values 
WITHOUT_CLASSIFICATION	 assume the existing vector always valid 
WITHOUT_CLASSIFICATION	 update update join key use 
WITHOUT_CLASSIFICATION	 first find first record for the key 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the join keys cannot transformed the subquery currently will only return the base table scan the join keys are constants column even simple cast the join keys will result null table scan operator case constant join keys they would 
WITHOUT_CLASSIFICATION	 nondeterministic functions well runtime constants are not materializable 
WITHOUT_CLASSIFICATION	 specialized class for native vectorized reduce sink that reducing uniform hash single long key column 
WITHOUT_CLASSIFICATION	 all but decimal 
WITHOUT_CLASSIFICATION	 via the properties 
WITHOUT_CLASSIFICATION	 roles grants 
WITHOUT_CLASSIFICATION	 now only string text int long byte and boolean comparisons are treated special cases for other types reuse 
WITHOUT_CLASSIFICATION	 null value with different type need introduce cast keep 
WITHOUT_CLASSIFICATION	 note this method must call before exiting 
WITHOUT_CLASSIFICATION	 any other queries running the session 
WITHOUT_CLASSIFICATION	 all getxxx needs tolowercase because they are directly called from semanticanalyzer all setxxx does not need because they are called from which already lowercases the aliases 
WITHOUT_CLASSIFICATION	 decimal debugging 
WITHOUT_CLASSIFICATION	 make sure compiles with both hadoop and hadoop 
WITHOUT_CLASSIFICATION	 just created this directory its not case precreation nuke 
WITHOUT_CLASSIFICATION	 test this restricts events generated those that match provided message format 
WITHOUT_CLASSIFICATION	 istezorspark 
WITHOUT_CLASSIFICATION	 make this client wait job trcker not behaving well 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 start blocking run one the tasks the main thread 
WITHOUT_CLASSIFICATION	 these are the columns the big and small table that are bytecolumnvector columns 
WITHOUT_CLASSIFICATION	 get both tablealias and column name from columnorigin 
WITHOUT_CLASSIFICATION	 try with extra base 
WITHOUT_CLASSIFICATION	 replication case export table tbl location for replication 
WITHOUT_CLASSIFICATION	 filesinkoperator knows how properly write 
WITHOUT_CLASSIFICATION	 reason retry the challenge ticket not valid 
WITHOUT_CLASSIFICATION	 valid inputs possible case 
WITHOUT_CLASSIFICATION	 note can really slow 
WITHOUT_CLASSIFICATION	 alter table the wrong catalog 
WITHOUT_CLASSIFICATION	 cmapstringstring ctinyint csmallint cfloat cbigint 
WITHOUT_CLASSIFICATION	 construct sortrel 
WITHOUT_CLASSIFICATION	 first stripe will satisfy the predicate and will single split last stripe will 
WITHOUT_CLASSIFICATION	 initialize string table lazy fashion 
WITHOUT_CLASSIFICATION	 they cancel each other 
WITHOUT_CLASSIFICATION	 hive doesnt support currency type 
WITHOUT_CLASSIFICATION	 walk through the join condition building ndv for selectivity 
WITHOUT_CLASSIFICATION	 first through and set all our values for datanucleus and javaxjdo parameters this 
WITHOUT_CLASSIFICATION	 store the mapping path bucket number this needed since for the maponly job any mapper can process any file for mapper processing the file corresponding bucket should also output the file corresponding bucket the output 
WITHOUT_CLASSIFICATION	 left semi join hash set 
WITHOUT_CLASSIFICATION	 break the client 
WITHOUT_CLASSIFICATION	 verify 
WITHOUT_CLASSIFICATION	 dirname uniquely identifies destination directory filesinkoperator more than one filesinkoperator write the same partition this dirname should different 
WITHOUT_CLASSIFICATION	 ignored 
WITHOUT_CLASSIFICATION	 make sure credential provider path points 
WITHOUT_CLASSIFICATION	 this methods main use help unit testing this class 
WITHOUT_CLASSIFICATION	 common one time setup native vectorized map join operators processop 
WITHOUT_CLASSIFICATION	 test select root from 
WITHOUT_CLASSIFICATION	 transaction batch size case 
WITHOUT_CLASSIFICATION	 the underlying database field varchar need compare numbers 
WITHOUT_CLASSIFICATION	 this the bit where make sure dont group across partition schema boundaries 
WITHOUT_CLASSIFICATION	 the set pruning sinks 
WITHOUT_CLASSIFICATION	 node now has free capacity task should allocated 
WITHOUT_CLASSIFICATION	 user can override value hive config true 
WITHOUT_CLASSIFICATION	 use the existing tableinfo object else create new one 
WITHOUT_CLASSIFICATION	 immutable causes copyobj return the original object 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 for comma 
WITHOUT_CLASSIFICATION	 assume the high watermark can used maximum transaction 
WITHOUT_CLASSIFICATION	 this case possible path not enabled 
WITHOUT_CLASSIFICATION	 codes and messages this should fixed 
WITHOUT_CLASSIFICATION	 compile internal query capture underlying table partition dependencies 
WITHOUT_CLASSIFICATION	 the authenticated user 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader int 
WITHOUT_CLASSIFICATION	 run selftest query doesnt work will selfdisable what pita 
WITHOUT_CLASSIFICATION	 immutablelist 
WITHOUT_CLASSIFICATION	 selectively used fetch formatter 
WITHOUT_CLASSIFICATION	 revert configs not affect other tests 
WITHOUT_CLASSIFICATION	 subquery was just one conjunct 
WITHOUT_CLASSIFICATION	 test singlethreaded implementation checker throws hiveexception when the there dummy directory present the nested level 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 let transaction aborted 
WITHOUT_CLASSIFICATION	 cancel given delegation token 
WITHOUT_CLASSIFICATION	 capacity force should allocate otherwise 
WITHOUT_CLASSIFICATION	 done 
WITHOUT_CLASSIFICATION	 the callable shouldnt null execute the thread pool also should configured execute requests 
WITHOUT_CLASSIFICATION	 loginfowriting value valueoffset length tailoffset valueoffset 
WITHOUT_CLASSIFICATION	 sans header row 
WITHOUT_CLASSIFICATION	 check the output fixacidkeyindex should indicate the index was valid 
WITHOUT_CLASSIFICATION	 table partitions statistics and table partitions column statistics are accurate not 
WITHOUT_CLASSIFICATION	 cannot performed maponly job 
WITHOUT_CLASSIFICATION	 number digits mask from the end 
WITHOUT_CLASSIFICATION	 switch from based start offset the hive end user convention based start offset the internal convention 
WITHOUT_CLASSIFICATION	 try allocate from targetsized free list maybe well get lucky 
WITHOUT_CLASSIFICATION	 put shuffle version into http header 
WITHOUT_CLASSIFICATION	 vertex mergejoin 
WITHOUT_CLASSIFICATION	 this means the exception was caused something other than race condition creating the partition since the partition still doesnt exist 
WITHOUT_CLASSIFICATION	 find the skew information corresponding the table 
WITHOUT_CLASSIFICATION	 boundary length will span two compression buffers 
WITHOUT_CLASSIFICATION	 requesting for the stats source will implicitly initialize 
WITHOUT_CLASSIFICATION	 simulate partition update 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that left semi join singlecolumn string using hash set 
WITHOUT_CLASSIFICATION	 mdonly table alter 
WITHOUT_CLASSIFICATION	 skip first child not involved the revert boolean the target type needs account for all operands 
WITHOUT_CLASSIFICATION	 need find the tables and data drop not part this dump 
WITHOUT_CLASSIFICATION	 handle the status change 
WITHOUT_CLASSIFICATION	 cause any problem the cleaner thread will remove this when this jar expires 
WITHOUT_CLASSIFICATION	 determine the the table from the job conf stored context the table properties are copied job conf therefore should able retrieve them here and determine appropriate behavior note that this will meaningless for nonacid tables will set null 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 return true retain item and false filter out 
WITHOUT_CLASSIFICATION	 thought creating template for each shims but couldnt generate proper mvn script 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 set the big table position both the reduce work and merge join operator should set with the same value 
WITHOUT_CLASSIFICATION	 create minimalistic table 
WITHOUT_CLASSIFICATION	 silent overflow 
WITHOUT_CLASSIFICATION	 boolean invert not expression left expression right expression 
WITHOUT_CLASSIFICATION	 add the new paths the znodes list well try for their removal well 
WITHOUT_CLASSIFICATION	 all the queries are maponly anyway the query most optimized 
WITHOUT_CLASSIFICATION	 inject properties from the main app that matches allowedprefix 
WITHOUT_CLASSIFICATION	 the name should not changed reload the with the original name 
WITHOUT_CLASSIFICATION	 end semijoinrulejava 
WITHOUT_CLASSIFICATION	 such base created compaction case nonacid acid table conversion definition there are open txns with 
WITHOUT_CLASSIFICATION	 verify that multi byte like expression doesnt match nonmatching string 
WITHOUT_CLASSIFICATION	 fail similarly when memory allocations fail 
WITHOUT_CLASSIFICATION	 this means have just created table and are specifying partition the load statement without precreating the partition which case lets use table input format class inherittablespecs defaults true when new partition created later will automatically inherit input format from table object 
WITHOUT_CLASSIFICATION	 remove countdistinct mapside gby 
WITHOUT_CLASSIFICATION	 create scratch dirs for this session 
WITHOUT_CLASSIFICATION	 depends this 
WITHOUT_CLASSIFICATION	 order sort clause 
WITHOUT_CLASSIFICATION	 extract the actual row from row batch 
WITHOUT_CLASSIFICATION	 nobucket table capture bucketid from streamedtable workaround hive bug that prevents joins two identically bucketed tables 
WITHOUT_CLASSIFICATION	 move past pair separator 
WITHOUT_CLASSIFICATION	 float 
WITHOUT_CLASSIFICATION	 bgenjjtree typestring 
WITHOUT_CLASSIFICATION	 todo clean exit 
WITHOUT_CLASSIFICATION	 check dead session get cleared 
WITHOUT_CLASSIFICATION	 see link nextnullwritable vectorizedrowbatch first and link when reading split original file and need decorate data with rowid this requires treating multiple files that are part the same bucket tranche for unbucketed tables single logical file number rowids consistently todo this logic executed per split every original file the computed result the same for every split form the same file this could optimized moving beforeduring split computation and passing the info the split hive 
WITHOUT_CLASSIFICATION	 not backward compatible 
WITHOUT_CLASSIFICATION	 handle remaining lower long word digits integer digits 
WITHOUT_CLASSIFICATION	 look for bean style accessors getfieldname and isfieldname 
WITHOUT_CLASSIFICATION	 https cannot done with zero copy 
WITHOUT_CLASSIFICATION	 subqueries 
WITHOUT_CLASSIFICATION	 and finally save the 
WITHOUT_CLASSIFICATION	 for now leave decimal precisionscale the name decimalcolumnvector scratch columns dont need their precisionscale adjusted 
WITHOUT_CLASSIFICATION	 task details fetch task tracker url 
WITHOUT_CLASSIFICATION	 this should eventually hang the delay code 
WITHOUT_CLASSIFICATION	 get http service port 
WITHOUT_CLASSIFICATION	 find the sourceinfo put values 
WITHOUT_CLASSIFICATION	 this case missing specification utf string storage 
WITHOUT_CLASSIFICATION	 getgenericudf actually clones the udf just call once and reuse 
WITHOUT_CLASSIFICATION	 new key 
WITHOUT_CLASSIFICATION	 return lowsurrogate 
WITHOUT_CLASSIFICATION	 user can override value for hive config false 
WITHOUT_CLASSIFICATION	 ignore the exception and fall through the default currentstateid 
WITHOUT_CLASSIFICATION	 empty string args 
WITHOUT_CLASSIFICATION	 note hadoop metric reporter does not support tags create single reporter for all metrics 
WITHOUT_CLASSIFICATION	 autosave then save 
WITHOUT_CLASSIFICATION	 cache rows guaranteed contain precedingspan rows before nextrowtoprocess 
WITHOUT_CLASSIFICATION	 expected 
WITHOUT_CLASSIFICATION	 stream variables 
WITHOUT_CLASSIFICATION	 calculation below consistent with also are capping the bloomfilter size below 
WITHOUT_CLASSIFICATION	 across process boundary normalized and stored type and not carried data for each row 
WITHOUT_CLASSIFICATION	 need generate limit 
WITHOUT_CLASSIFICATION	 leverage tez improve synchronization and the progress report behavior 
WITHOUT_CLASSIFICATION	 single threaded scheduler for tasks from wait queue executor threads 
WITHOUT_CLASSIFICATION	 the simd optimized form 
WITHOUT_CLASSIFICATION	 scale down does rounding 
WITHOUT_CLASSIFICATION	 with the sessions after thru all the concurrent user actions 
WITHOUT_CLASSIFICATION	 secure only set the registering service anyone can read the registrations 
WITHOUT_CLASSIFICATION	 replacing the right thing though since expect the kill all the fragments running the node via timeouts deallocate messages coming from the old node are sent the nodeinfo instance for the old node 
WITHOUT_CLASSIFICATION	 handled below 
WITHOUT_CLASSIFICATION	 use the serialization scale and create biginteger with trailing zeroes round the decimal necessary since are emulating old behavior and recommending the use instead just the slow way get the bigdecimalsetscale value and return the biginteger 
WITHOUT_CLASSIFICATION	 add the columns residual filters 
WITHOUT_CLASSIFICATION	 make copy since intend mutate sum 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 inherit the environment variables 
WITHOUT_CLASSIFICATION	 eventually enough small writes should result another buffer getting created 
WITHOUT_CLASSIFICATION	 get all locks 
WITHOUT_CLASSIFICATION	 admin check allows when set false when set true checks true and the logged user via pam spnego kerberos list 
WITHOUT_CLASSIFICATION	 where missing columns are nullfilled 
WITHOUT_CLASSIFICATION	 without extra structs 
WITHOUT_CLASSIFICATION	 parse out and havent already done and while were also parse out the precision factor the user has supplied one 
WITHOUT_CLASSIFICATION	 log not static make debugging easier being able identify which subclass 
WITHOUT_CLASSIFICATION	 null path unmanaged 
WITHOUT_CLASSIFICATION	 put accessed columns readentity 
WITHOUT_CLASSIFICATION	 requesting more partitions than allowed should throw exception 
WITHOUT_CLASSIFICATION	 helper setup default environment for task yarn 
WITHOUT_CLASSIFICATION	 add and verbose print verbose message 
WITHOUT_CLASSIFICATION	 timestamp 
WITHOUT_CLASSIFICATION	 querys session has compile lock timeout secs should 
WITHOUT_CLASSIFICATION	 first check will allow the user create table 
WITHOUT_CLASSIFICATION	 sortmerge join 
WITHOUT_CLASSIFICATION	 number variables and assignment expressions 
WITHOUT_CLASSIFICATION	 use session registry see 
WITHOUT_CLASSIFICATION	 are collapsing figure out this new row 
WITHOUT_CLASSIFICATION	 events are processed otherwise task metrics may get lost see hive 
WITHOUT_CLASSIFICATION	 spot check only nonstandard cases are checked for the same template another test 
WITHOUT_CLASSIFICATION	 are currently performing binary search the input dont forward the results currently this value set when query optimized using compact index the map reduce job responsible for scanning and filtering the index sets this value remains set throughout the binary search executed the until starting 
WITHOUT_CLASSIFICATION	 extracted from functionregistry 
WITHOUT_CLASSIFICATION	 partial partition spec has null parthandle 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 this means that the lock ready cleaned hence cannot 
WITHOUT_CLASSIFICATION	 cds are reused thry partition sds detach all cds from sds then remove unused cds 
WITHOUT_CLASSIFICATION	 this oid for kerberos gssapi mechanism 
WITHOUT_CLASSIFICATION	 advance the reader until reach the minimum key 
WITHOUT_CLASSIFICATION	 expect readreader return same key value objects common case 
WITHOUT_CLASSIFICATION	 well set for all others ensure determinism 
WITHOUT_CLASSIFICATION	 not sequential with previous 
WITHOUT_CLASSIFICATION	 add whether the row filtered not this value does not matter for the dummyobj because the join values are already null 
WITHOUT_CLASSIFICATION	 logger attempts 
WITHOUT_CLASSIFICATION	 doing select first less efficient but makes easier debug things 
WITHOUT_CLASSIFICATION	 uts there standalone hms running kick off compaction its done via runworker but normal usage concatenate blocking 
WITHOUT_CLASSIFICATION	 table properties 
WITHOUT_CLASSIFICATION	 metadatastore 
WITHOUT_CLASSIFICATION	 need input object inspector that for the row will extract out the 
WITHOUT_CLASSIFICATION	 noop for nontest mode for now 
WITHOUT_CLASSIFICATION	 flag that helps set the correct driver state finally block tracking the method has been returned error not 
WITHOUT_CLASSIFICATION	 rewrite the load launch insert job 
WITHOUT_CLASSIFICATION	 hive will always require user specify exact sizes for char varchar binary doesnt need any sizes decimal has the default 
WITHOUT_CLASSIFICATION	 reusing the tokenrewritestream map for views not overwrite the current tokenrewritestream 
WITHOUT_CLASSIFICATION	 most common scenario 
WITHOUT_CLASSIFICATION	 inside 
WITHOUT_CLASSIFICATION	 find which column contains the raw data size both partitioned and non partitioned 
WITHOUT_CLASSIFICATION	 varchar string length beyond max 
WITHOUT_CLASSIFICATION	 allocate new vectorization context reset the intermediate columns 
WITHOUT_CLASSIFICATION	 there should new directory base 
WITHOUT_CLASSIFICATION	 the parent same the then have cycle 
WITHOUT_CLASSIFICATION	 have get mtable again because datanucleus 
WITHOUT_CLASSIFICATION	 this for basic stats 
WITHOUT_CLASSIFICATION	 all key input columns are repeating generate key once lookup once 
WITHOUT_CLASSIFICATION	 dont need merge add the move job 
WITHOUT_CLASSIFICATION	 currently cannot handle nested complex types 
WITHOUT_CLASSIFICATION	 timestamp intervaldaytime intervaldaytime timestamp 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 hive removes liststatus from the code path there should only one read ops open after hive 
WITHOUT_CLASSIFICATION	 avro only allows maps with strings for keys only have worry about deserializing the values 
WITHOUT_CLASSIFICATION	 case need for the other case 
WITHOUT_CLASSIFICATION	 segment metadata query that retrieves all columns present the data source dimensions and metrics 
WITHOUT_CLASSIFICATION	 find all the indexes the sub byte 
WITHOUT_CLASSIFICATION	 undone for list and map yes for struct and union when field count different 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 path for split unqualified the split from being sampled serves more than one alias the alias serves not sampled serves different alias than another path for the same split 
WITHOUT_CLASSIFICATION	 update the partition col stats for table cache 
WITHOUT_CLASSIFICATION	 single long value hash map based the serialize the long key into binarysortable format into output buffer accepted 
WITHOUT_CLASSIFICATION	 executeupdate prepared statement 
WITHOUT_CLASSIFICATION	 convert byteswritable byte 
WITHOUT_CLASSIFICATION	 oldpath destf its subdir its should definitely deleted otherwise its existing content might result incorrect extra data but not sure why changed not delete the oldpath hive not the destf its subdir 
WITHOUT_CLASSIFICATION	 set the context attribute true which will interpreted the request interceptor 
WITHOUT_CLASSIFICATION	 connections guard rails 
WITHOUT_CLASSIFICATION	 for now disable the test attempts 
WITHOUT_CLASSIFICATION	 gather output works operators 
WITHOUT_CLASSIFICATION	 not currently supported 
WITHOUT_CLASSIFICATION	 assign values from the row local variables 
WITHOUT_CLASSIFICATION	 expressions for project operator 
WITHOUT_CLASSIFICATION	 expressions these comparisons are anded together 
WITHOUT_CLASSIFICATION	 even large say varchar columns most strings are small 
WITHOUT_CLASSIFICATION	 optional submissionstate 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 the last field the union field any 
WITHOUT_CLASSIFICATION	 the token file location comes after mainclass propval 
WITHOUT_CLASSIFICATION	 test integer 
WITHOUT_CLASSIFICATION	 outpath does not exist then means all paths within combine split are skipped they are incompatible for merge for example files without stripe stats those files will added incompatfileset 
WITHOUT_CLASSIFICATION	 stop nonexistent option 
WITHOUT_CLASSIFICATION	 first try temp table 
WITHOUT_CLASSIFICATION	 replication done need check new value set for existing property 
WITHOUT_CLASSIFICATION	 use hivevarchars internal text member read the value 
WITHOUT_CLASSIFICATION	 remove them 
WITHOUT_CLASSIFICATION	 add column info corresponding partition columns 
WITHOUT_CLASSIFICATION	 create map local operators 
WITHOUT_CLASSIFICATION	 counter for rows emitted 
WITHOUT_CLASSIFICATION	 share the code with recordreader 
WITHOUT_CLASSIFICATION	 for fullacid dont want delete any files even for overwrite see hivehive 
WITHOUT_CLASSIFICATION	 setup whitelist 
WITHOUT_CLASSIFICATION	 mapjoin and smbjoin not supported 
WITHOUT_CLASSIFICATION	 the plan consists statstask only 
WITHOUT_CLASSIFICATION	 may not own the table object create copy 
WITHOUT_CLASSIFICATION	 now get from cache 
WITHOUT_CLASSIFICATION	 find the table will working with 
WITHOUT_CLASSIFICATION	 wrap the current query string since can not add another inlist element value 
WITHOUT_CLASSIFICATION	 also minhistorylevel will have entry for the open txn 
WITHOUT_CLASSIFICATION	 proportion extra space provide when allocating more buffer space 
WITHOUT_CLASSIFICATION	 map from integer tag distinct aggrs 
WITHOUT_CLASSIFICATION	 initialize container use for storing tuples before emitting them 
WITHOUT_CLASSIFICATION	 otherwise create new condition 
WITHOUT_CLASSIFICATION	 source already path the checksum will always matching 
WITHOUT_CLASSIFICATION	 scale sumnulls based the number partitions 
WITHOUT_CLASSIFICATION	 check that partition keys have not changed except for virtual views 
WITHOUT_CLASSIFICATION	 use old value reference word expandandrehash key tablekey slot newslot newpairindex newpairindex empty slot 
WITHOUT_CLASSIFICATION	 compatibility mode need hook set and use 
WITHOUT_CLASSIFICATION	 first check all tables 
WITHOUT_CLASSIFICATION	 see serialization decimal for explanation below 
WITHOUT_CLASSIFICATION	 and any databases other than the default database 
WITHOUT_CLASSIFICATION	 update statistics based column statistics conditions keeps adding the stats independently this may result number rows getting more than the input rows 
WITHOUT_CLASSIFICATION	 create file system handle 
WITHOUT_CLASSIFICATION	 and load data into the same table which should now land delta 
WITHOUT_CLASSIFICATION	 remove any parents from mapjoin again 
WITHOUT_CLASSIFICATION	 determine the lock type acquire 
WITHOUT_CLASSIFICATION	 initial write small value 
WITHOUT_CLASSIFICATION	 input long set such without copying any modification the source will affect bloom filter 
WITHOUT_CLASSIFICATION	 value might have been changed because the normalization conversion 
WITHOUT_CLASSIFICATION	 set bootstrap dump location used 
WITHOUT_CLASSIFICATION	 already exists 
WITHOUT_CLASSIFICATION	 tests setting maxrows 
WITHOUT_CLASSIFICATION	 first calculate the length the output string 
WITHOUT_CLASSIFICATION	 since metavars are all different types use string for comparison 
WITHOUT_CLASSIFICATION	 count input and output are long just modes partial final 
WITHOUT_CLASSIFICATION	 check there are column stats available for these columns 
WITHOUT_CLASSIFICATION	 derived classes can set this different object needed 
WITHOUT_CLASSIFICATION	 the plan file should always local directory 
WITHOUT_CLASSIFICATION	 must iterate over all the delete records until find one record with 
WITHOUT_CLASSIFICATION	 get the valid write list for all the tables read the current txn 
WITHOUT_CLASSIFICATION	 dont user uber all mode everything can into llap which better than uber 
WITHOUT_CLASSIFICATION	 isoriginalmapjoin 
WITHOUT_CLASSIFICATION	 not analyze command and not column stats then not gatherstats 
WITHOUT_CLASSIFICATION	 converts negative byte positive index 
WITHOUT_CLASSIFICATION	 not possible expand since have more than one chunk with single segment this the case when user wants append segment with coarser granularity metadata storage already has segments for with granularity hour and segments append have day granularity druid shard specs does not support multiple partitions for same interval with different granularity 
WITHOUT_CLASSIFICATION	 this the last branch and always false assume alwaysfalse filter will get pushed down this branch wont read any data 
WITHOUT_CLASSIFICATION	 repeated iospecproto inputspecs 
WITHOUT_CLASSIFICATION	 get least splits 
WITHOUT_CLASSIFICATION	 integerminvalue 
WITHOUT_CLASSIFICATION	 finally make sure the file sink operators are set right 
WITHOUT_CLASSIFICATION	 now check that stats for partition didnt modify did not change 
WITHOUT_CLASSIFICATION	 need edit the configuration setup cmdline clone first 
WITHOUT_CLASSIFICATION	 note fix for sfnet bug working around issue jline see appending newline the end inputstream 
WITHOUT_CLASSIFICATION	 parameters the form keycolxtcoly 
WITHOUT_CLASSIFICATION	 need reset the monitor operation handle not available down stream ideally the monitor should associated with the operation handle 
WITHOUT_CLASSIFICATION	 usually this means weve already created the tables clean them and then try again 
WITHOUT_CLASSIFICATION	 use magic value indicating are writing the big value length 
WITHOUT_CLASSIFICATION	 there will data nodes there will task tracker nodes 
WITHOUT_CLASSIFICATION	 for ignore this for now but leave log message 
WITHOUT_CLASSIFICATION	 default implementation 
WITHOUT_CLASSIFICATION	 when fraction exactly and lowest new digit odd towards even 
WITHOUT_CLASSIFICATION	 get override compression properties via tblproperties clause set 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 not create predicate the leaf not the passed schema 
WITHOUT_CLASSIFICATION	 clear value arrays 
WITHOUT_CLASSIFICATION	 read all values 
WITHOUT_CLASSIFICATION	 map type contains schema the value element 
WITHOUT_CLASSIFICATION	 postgres specific parser 
WITHOUT_CLASSIFICATION	 push the context the end the serialized ngram estimation 
WITHOUT_CLASSIFICATION	 for this variation serialize the key without caring single long single string multikey etc 
WITHOUT_CLASSIFICATION	 that should ever change this will need reworking 
WITHOUT_CLASSIFICATION	 throw new type used without type params 
WITHOUT_CLASSIFICATION	 how many data columns the partition reader actually supplying 
WITHOUT_CLASSIFICATION	 checks for nonsecure hadoop installations 
WITHOUT_CLASSIFICATION	 set the jdbc connection pool 
WITHOUT_CLASSIFICATION	 assumed the caller have already allocated write for addingupdating data the acid tables however ddl operatons wont allocate write and hence this query may return empty result sets get the write allocated this txn for the given table writes 
WITHOUT_CLASSIFICATION	 report success for all other cases 
WITHOUT_CLASSIFICATION	 between and and firstname alan and substrxxxxx firstname and smith lastname and substrfirstname yyy 
WITHOUT_CLASSIFICATION	 intervaldaytime 
WITHOUT_CLASSIFICATION	 construct string column names based the number column types 
WITHOUT_CLASSIFICATION	 this actually alter table drop paritition statement 
WITHOUT_CLASSIFICATION	 serde null the input doesnt need spilled out 
WITHOUT_CLASSIFICATION	 ignore 
WITHOUT_CLASSIFICATION	 not able find thread execute the job request raise busy exception and client can retry the operation 
WITHOUT_CLASSIFICATION	 check the mere mortals 
WITHOUT_CLASSIFICATION	 can clear the global error when see that was set descendant node group expression because processgbyexpr returns exprnodedesc that effectively ignores its children although the error can set multiple times descendant nodes dfs traversal ensures that the error only needs cleared once also for case like select concatvalue concatvalue the logic still works the error only set with the first value all node processors quit early the global error set 
WITHOUT_CLASSIFICATION	 this does 
WITHOUT_CLASSIFICATION	 droptable event partitioned table 
WITHOUT_CLASSIFICATION	 reached the end the result file 
WITHOUT_CLASSIFICATION	 the data not escaped simply copy the data 
WITHOUT_CLASSIFICATION	 cache the values 
WITHOUT_CLASSIFICATION	 else the common code the end 
WITHOUT_CLASSIFICATION	 various final services configs etc 
WITHOUT_CLASSIFICATION	 previously assigned some rows with nonnull values the batch indices the unassigned row were tracked 
WITHOUT_CLASSIFICATION	 with hive hiverootlogger cannot have both logger name and log level still see split logger and level separately for hiverootlogger and hiveloglevel respectively 
WITHOUT_CLASSIFICATION	 failed dump the sidetable remove the partial file 
WITHOUT_CLASSIFICATION	 merge the two into the lateral view join the cols the merged result will the combination both the cols the udtf path and the cols the all path the internal names have changed avoid conflicts 
WITHOUT_CLASSIFICATION	 hadoopjobid 
WITHOUT_CLASSIFICATION	 used support where list struct that contains field called will return array that contains field all elements array 
WITHOUT_CLASSIFICATION	 rows match 
WITHOUT_CLASSIFICATION	 timeout for nodes larger than delay immediate allocation 
WITHOUT_CLASSIFICATION	 rename partition 
WITHOUT_CLASSIFICATION	 are working stripe over the min stripe size and crossed block boundary cut the input split here 
WITHOUT_CLASSIFICATION	 one the child conditions truefalse 
WITHOUT_CLASSIFICATION	 read the list 
WITHOUT_CLASSIFICATION	 initialize mapwork with smbmapjoin information 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream long 
WITHOUT_CLASSIFICATION	 pending update not done the task has terminated out date heartbeat 
WITHOUT_CLASSIFICATION	 setup our left semi join specific members 
WITHOUT_CLASSIFICATION	 methods summary 
WITHOUT_CLASSIFICATION	 current nodes the cache 
WITHOUT_CLASSIFICATION	 find all acid filesinkoperators 
WITHOUT_CLASSIFICATION	 union encountered for the first time 
WITHOUT_CLASSIFICATION	 there any partition column static partition dynamic 
WITHOUT_CLASSIFICATION	 this not new key well overwrite the key and hash bytes not needed anymore 
WITHOUT_CLASSIFICATION	 this should throw classcastexception 
WITHOUT_CLASSIFICATION	 need side file for this test create txn batch and test with only one 
WITHOUT_CLASSIFICATION	 returns set 
WITHOUT_CLASSIFICATION	 not support tables either this point could with some extra logic 
WITHOUT_CLASSIFICATION	 ensure that full qualified path most cases will since tblgetpath full qualified 
WITHOUT_CLASSIFICATION	 get locks that are relevant exclusive for insert overwrite 
WITHOUT_CLASSIFICATION	 all other primitive types are simple 
WITHOUT_CLASSIFICATION	 verify hwm properly set after repl load 
WITHOUT_CLASSIFICATION	 build tok from toksubquery the input subquery with correlation removed subqueryalias 
WITHOUT_CLASSIFICATION	 semantic analysis and plan generation 
WITHOUT_CLASSIFICATION	 standardlist uses arraylist store the row 
WITHOUT_CLASSIFICATION	 acid table this should fail 
WITHOUT_CLASSIFICATION	 row resolve once more because the columninfo row resolver already removed 
WITHOUT_CLASSIFICATION	 replicate only insert into operations 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 where listpartitions already provided where want fetch partitions lazily when theyre needed 
WITHOUT_CLASSIFICATION	 create source table 
WITHOUT_CLASSIFICATION	 get current mapred work and its local work 
WITHOUT_CLASSIFICATION	 test with remote metastore service 
WITHOUT_CLASSIFICATION	 note this tableexport actually never used other than for auth and another one 
WITHOUT_CLASSIFICATION	 not efficient but dont expect this called frequently 
WITHOUT_CLASSIFICATION	 propagate this value from dont allow users set initconf will set wont set otherwise noone calls setuppool 
WITHOUT_CLASSIFICATION	 are doing acid operation they will always all true recordupdaters always collect stats 
WITHOUT_CLASSIFICATION	 misc ddl 
WITHOUT_CLASSIFICATION	 are waiting for next block either will get told are done 
WITHOUT_CLASSIFICATION	 validate input reducework 
WITHOUT_CLASSIFICATION	 push not through between 
WITHOUT_CLASSIFICATION	 which not tracked directly but available jobsid node via mtime stat 
WITHOUT_CLASSIFICATION	 there extra dependency metricsregistry for snapshot 
WITHOUT_CLASSIFICATION	 there was parallel deallocate didnt account for the memory 
WITHOUT_CLASSIFICATION	 check tablepartition doesnt have ckpt property 
WITHOUT_CLASSIFICATION	 the alias 
WITHOUT_CLASSIFICATION	 set database specific parameters 
WITHOUT_CLASSIFICATION	 add the map 
WITHOUT_CLASSIFICATION	 irrelevant 
WITHOUT_CLASSIFICATION	 also populate with 
WITHOUT_CLASSIFICATION	 print out the location the log file for the user 
WITHOUT_CLASSIFICATION	 create the configuration hadoopsitexml file 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 wait for all invocations complete 
WITHOUT_CLASSIFICATION	 gobble the exception message delivery best effort 
WITHOUT_CLASSIFICATION	 alias 
WITHOUT_CLASSIFICATION	 now expand the view definition with extras such explicit column references this expanded form what well reparse when the view 
WITHOUT_CLASSIFICATION	 the split from something other than the file the logical bucket compute offset 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 perform data operation 
WITHOUT_CLASSIFICATION	 check has expected version marker 
WITHOUT_CLASSIFICATION	 there should really only one line with script failed 
WITHOUT_CLASSIFICATION	 unequal strings 
WITHOUT_CLASSIFICATION	 bootstrap dumpload 
WITHOUT_CLASSIFICATION	 select first and last rows 
WITHOUT_CLASSIFICATION	 reduce side work 
WITHOUT_CLASSIFICATION	 executed too small number reducers 
WITHOUT_CLASSIFICATION	 use dfs traverse all the branches until dpp hit 
WITHOUT_CLASSIFICATION	 first write table will allocate write and rest the writes should reuse 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 not need zookeeper the moment 
WITHOUT_CLASSIFICATION	 this has done synchronously avoid the caller getting this session again ideally wed get rid this threadlocal nonsense 
WITHOUT_CLASSIFICATION	 initialize stats publishing table 
WITHOUT_CLASSIFICATION	 last item 
WITHOUT_CLASSIFICATION	 operator 
WITHOUT_CLASSIFICATION	 for the case implicit type conversion varchar and varchar pick the common type for all the keys since during runtime same key type assumed 
WITHOUT_CLASSIFICATION	 lets say that passing null will not any filtering 
WITHOUT_CLASSIFICATION	 has the table changed since the query was cached for transactional tables can compare the table writeids the currentcached query 
WITHOUT_CLASSIFICATION	 assume not temp table try underlying client 
WITHOUT_CLASSIFICATION	 bloom filter uses binary 
WITHOUT_CLASSIFICATION	 should have some query and also its parent because supposition are subq 
WITHOUT_CLASSIFICATION	 skip for tests not present 
WITHOUT_CLASSIFICATION	 driverruninsert overwrite table select from inpy 
WITHOUT_CLASSIFICATION	 ensure have stripe metadata might have read before for filtering 
WITHOUT_CLASSIFICATION	 column family become map 
WITHOUT_CLASSIFICATION	 bgenjjtree xception 
WITHOUT_CLASSIFICATION	 should never come here 
WITHOUT_CLASSIFICATION	 plan using dummypartition can only lock the table unfortunately 
WITHOUT_CLASSIFICATION	 this catch all state when containers have not started yet llap has not started yet 
WITHOUT_CLASSIFICATION	 signed comparison longminvalue decimal 
WITHOUT_CLASSIFICATION	 cache columnlist from thissd 
WITHOUT_CLASSIFICATION	 the hostname doesnt contain port add the configured port hostname 
WITHOUT_CLASSIFICATION	 binary 
WITHOUT_CLASSIFICATION	 null out final members 
WITHOUT_CLASSIFICATION	 beware any implementation whose hashcode mutable reference inserting into map and then changing the hashcode can make disappear out the map during lookups 
WITHOUT_CLASSIFICATION	 this simulates the completion txnid 
WITHOUT_CLASSIFICATION	 constructor useful making projection vectorization context use with and addprojectioncolumn 
WITHOUT_CLASSIFICATION	 class iterator 
WITHOUT_CLASSIFICATION	 rootnotmodified false then startindx and endindx will stale 
WITHOUT_CLASSIFICATION	 missing class setting field 
WITHOUT_CLASSIFICATION	 the table should also considered part inputs even the table partitioned table and whether any partition selected not 
WITHOUT_CLASSIFICATION	 add fake partition dir 
WITHOUT_CLASSIFICATION	 rank functions type intdouble 
WITHOUT_CLASSIFICATION	 add the path alias mapping 
WITHOUT_CLASSIFICATION	 return array fields where the last field has the actual data 
WITHOUT_CLASSIFICATION	 output column the reducesink operator 
WITHOUT_CLASSIFICATION	 write totalseconds nanos dataoutput 
WITHOUT_CLASSIFICATION	 serialize context 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 requestmanager will catch this and handle like any other error 
WITHOUT_CLASSIFICATION	 client requesting fetchfromstart and its not the first time reading from this operation then reset the fetch position beginning 
WITHOUT_CLASSIFICATION	 necessary compare against hiveconf defaults hivesitexml not available task nodes like 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 add needed columns 
WITHOUT_CLASSIFICATION	 kryo setter 
WITHOUT_CLASSIFICATION	 this dealing with tasks from different submission and cause the kill out before the previous submissions has completed handled the 
WITHOUT_CLASSIFICATION	 strict mode the presence order limit must specified 
WITHOUT_CLASSIFICATION	 extract join type 
WITHOUT_CLASSIFICATION	 single call get all column stats for all partitions 
WITHOUT_CLASSIFICATION	 ignore the predicate case not sampling predicate 
WITHOUT_CLASSIFICATION	 duplicates logic 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 before any activity the table open ids 
WITHOUT_CLASSIFICATION	 nonjavadoc this provides lazylong like class which can initialized from data stored binary format see int int 
WITHOUT_CLASSIFICATION	 the first group 
WITHOUT_CLASSIFICATION	 start all the outputs 
WITHOUT_CLASSIFICATION	 open txn with writes 
WITHOUT_CLASSIFICATION	 set hivesitexml default hivesitexml that has embedded metastore 
WITHOUT_CLASSIFICATION	 note here should use the new partition predicate pushdown api get list pruned list 
WITHOUT_CLASSIFICATION	 the max size memory for buffering records before writes them out 
WITHOUT_CLASSIFICATION	 verify schema 
WITHOUT_CLASSIFICATION	 decimaltotimestamp should consistent with doubletotimestamp for this level precision 
WITHOUT_CLASSIFICATION	 has reached the end the current batch lets fetch the next batch 
WITHOUT_CLASSIFICATION	 sets the sticky bit stickybitdir now removing file kvtxt from stickybitdir unprivileged user will result dfs error 
WITHOUT_CLASSIFICATION	 acid tables have complex directory layout and require merging delta files read thus should not try read bucket files directly 
WITHOUT_CLASSIFICATION	 exprnodecolumndesc etc not have leadlag inside 
WITHOUT_CLASSIFICATION	 not insert not need anything 
WITHOUT_CLASSIFICATION	 capabilities 
WITHOUT_CLASSIFICATION	 existsordering and existspartitioning should false 
WITHOUT_CLASSIFICATION	 logdebugclassname logical logical batchindex batchindex null 
WITHOUT_CLASSIFICATION	 followed each key and then each value 
WITHOUT_CLASSIFICATION	 append task specific info stagingpathname instead creating subdirectory this way dont have worry about deleting the stagingpathname separately end query execution 
WITHOUT_CLASSIFICATION	 because divisor negative quotient most remainder must dividend itself quotient dividend divisor 
WITHOUT_CLASSIFICATION	 always 
WITHOUT_CLASSIFICATION	 unknown unknown 
WITHOUT_CLASSIFICATION	 empty value too 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 cache delegation tokens when link submits job that requires metastore access and this access should secure tcj will add delegation token the submitted job when the job completes need cancel the token since default the token lives for days and over time can cause oom not cancelled cancelling from mapper via custom outputcommitter for example requires the jar containing hivemetastoreclient and any dependent jars available the node running launchmapper specifying transitive closure the necessary jars headache for each release caching the token means cancellation done from webhcat server and thus has hive jars the classpath while its possible that webhcat crashes and looses this inmemory state but this would exceptional condition and since tokens will automatically cancelled after days the fact that this info not persisted persisting also complicates things because that needs done securely see 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlnclob 
WITHOUT_CLASSIFICATION	 search for the key 
WITHOUT_CLASSIFICATION	 renametable event unpartitioned table 
WITHOUT_CLASSIFICATION	 bgenjjtree service 
WITHOUT_CLASSIFICATION	 first find out any the jobs needs run nonlocally 
WITHOUT_CLASSIFICATION	 sort for readability 
WITHOUT_CLASSIFICATION	 matters only for permanent functions 
WITHOUT_CLASSIFICATION	 keep mapping from tag the fetch operator alias 
WITHOUT_CLASSIFICATION	 colnames 
WITHOUT_CLASSIFICATION	 storage table could any storage system hbase cassandra etc 
WITHOUT_CLASSIFICATION	 the results this query execution might cacheable add placeholder entry the cache other queries know this result pending 
WITHOUT_CLASSIFICATION	 this task contains join can converted mapjoin task this operator present the mapper for sortmerge join operator present followed regular join cannot converted auto mapjoin 
WITHOUT_CLASSIFICATION	 and those from follow 
WITHOUT_CLASSIFICATION	 undone inner count 
WITHOUT_CLASSIFICATION	 multiparameter aggregations supported 
WITHOUT_CLASSIFICATION	 attempt delete temp file this fails not much can done about 
WITHOUT_CLASSIFICATION	 check this because will 
WITHOUT_CLASSIFICATION	 inner join hash map 
WITHOUT_CLASSIFICATION	 allocate the source conversion related arrays optional 
WITHOUT_CLASSIFICATION	 test regular outputformat 
WITHOUT_CLASSIFICATION	 create join rel 
WITHOUT_CLASSIFICATION	 merge should update registers and hence the count 
WITHOUT_CLASSIFICATION	 hive conf 
WITHOUT_CLASSIFICATION	 not check the state this coming from the updater under epic lock 
WITHOUT_CLASSIFICATION	 preallocated member for storing the physical batch index matching row single 
WITHOUT_CLASSIFICATION	 first determine whether rounding necessary based rounding point which inside integer part and get rid any fractional digits the result scale will 
WITHOUT_CLASSIFICATION	 now check more detail canhandleqbforcbo returns null query can 
WITHOUT_CLASSIFICATION	 error occurred retry 
WITHOUT_CLASSIFICATION	 todo the only reason this done this way because want unique subjects that the fsget gives different objects different fragments 
WITHOUT_CLASSIFICATION	 todo make these like operationtype and remove above char constatns 
WITHOUT_CLASSIFICATION	 recursively create the exprnodedesc base cases when encounter column ref convert that into exprnodecolumndesc when encounter constant convert that into for others just build the exprnodefuncdesc with recursively built children 
WITHOUT_CLASSIFICATION	 verify when second argument repeating 
WITHOUT_CLASSIFICATION	 exception should thrown 
WITHOUT_CLASSIFICATION	 provide faster way write hive interval year month without object 
WITHOUT_CLASSIFICATION	 strip off the stop marker which may left all the fields were the serialization 
WITHOUT_CLASSIFICATION	 have different settings from those hiveserver 
WITHOUT_CLASSIFICATION	 prepare prefix and suffix 
WITHOUT_CLASSIFICATION	 all table column names 
WITHOUT_CLASSIFICATION	 uri added later 
WITHOUT_CLASSIFICATION	 assume the caller will handle extra columns default with nulls etc 
WITHOUT_CLASSIFICATION	 this only useful for the daemons know themselves 
WITHOUT_CLASSIFICATION	 todo check defaults maxtimeout keepalive maxbodysize bodyrecieveduration etc 
WITHOUT_CLASSIFICATION	 currently support lazysimple deserialization and 
WITHOUT_CLASSIFICATION	 set permissions for current user dag 
WITHOUT_CLASSIFICATION	 encountering dot attempt resolve the leftmost name the parent query unqualified name assumed subquery reference dont attempt resolve this the parent because require all parent column references qualified all other expressions have type based their children expr children assumed refer neither 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 update table stats for partitioned table update stats alterpartition 
WITHOUT_CLASSIFICATION	 running queued running running running queued 
WITHOUT_CLASSIFICATION	 verify null output entry correct 
WITHOUT_CLASSIFICATION	 the jdbc spec says when you have duplicate column names the first one should returned 
WITHOUT_CLASSIFICATION	 leftfast leftfast 
WITHOUT_CLASSIFICATION	 try find the file the include path 
WITHOUT_CLASSIFICATION	 find the buddy the header list level dont know what list actually 
WITHOUT_CLASSIFICATION	 are stack trace 
WITHOUT_CLASSIFICATION	 move all the partition columns the end table columns 
WITHOUT_CLASSIFICATION	 errors are tolerated 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 theres delegation token available then use token based connection 
WITHOUT_CLASSIFICATION	 use that the current user 
WITHOUT_CLASSIFICATION	 start third batch aborttransaction everything dont properly close 
WITHOUT_CLASSIFICATION	 call copyfromlocal below basically assume src local file 
WITHOUT_CLASSIFICATION	 cannot push limit bail out 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this tablescandesc flag strictly set the vectorizer class for vectorized mapwork vertices 
WITHOUT_CLASSIFICATION	 most likely this value should not exist 
WITHOUT_CLASSIFICATION	 clear away any residue from our optimizations 
WITHOUT_CLASSIFICATION	 inputs are not equal could zip till here 
WITHOUT_CLASSIFICATION	 void can anything 
WITHOUT_CLASSIFICATION	 utc epoch for instant 
WITHOUT_CLASSIFICATION	 configure getpassword fall back conf credential doesnt have entry 
WITHOUT_CLASSIFICATION	 batch full and have least more row 
WITHOUT_CLASSIFICATION	 change value metavar config param new hive conf 
WITHOUT_CLASSIFICATION	 else create new one 
WITHOUT_CLASSIFICATION	 read logs 
WITHOUT_CLASSIFICATION	 valid inputs 
WITHOUT_CLASSIFICATION	 start delegation token manager 
WITHOUT_CLASSIFICATION	 for reasons that are completely incomprehensible the semantic analyzers often ask for multiple locks the same entity for example sharedread and exlcusive lock the locking system gets confused this and dead locks resolve that well make sure the request that multiple locks are coalesced and promoted the higher level locking this put all locks components trie based dbname tablename partition name and handle the promotion new requests come this structure depends the fact that null valid key linkedhashmap database lock will map dbname null null 
WITHOUT_CLASSIFICATION	 nonempty java opts without xmx specified 
WITHOUT_CLASSIFICATION	 start exclusive infinity 
WITHOUT_CLASSIFICATION	 the clientugi 
WITHOUT_CLASSIFICATION	 this command has terminated 
WITHOUT_CLASSIFICATION	 iterate over all expression after select 
WITHOUT_CLASSIFICATION	 delete the data the table 
WITHOUT_CLASSIFICATION	 server thread pool start with minworkerthreads expand till maxworkerthreads and reject 
WITHOUT_CLASSIFICATION	 its the when matched 
WITHOUT_CLASSIFICATION	 tag union field the first byte parsed 
WITHOUT_CLASSIFICATION	 update our counts for the last key 
WITHOUT_CLASSIFICATION	 initialize the lazy object 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 that will propagate the inputs the join 
WITHOUT_CLASSIFICATION	 select from table situations nonmr can add things the job its safe add this the job since its not actually mapred job 
WITHOUT_CLASSIFICATION	 protect against bad location being requested 
WITHOUT_CLASSIFICATION	 new method that distributes the select query creating splits containing information about different druid nodes that have the data for the given query 
WITHOUT_CLASSIFICATION	 arrayentry 
WITHOUT_CLASSIFICATION	 step explain the query and provide the runtime rows collected 
WITHOUT_CLASSIFICATION	 now try find the file based sha and name currently require exact name match 
WITHOUT_CLASSIFICATION	 dummy ops need updated the cloned ones 
WITHOUT_CLASSIFICATION	 tablescan and join operators 
WITHOUT_CLASSIFICATION	 any more left 
WITHOUT_CLASSIFICATION	 mapentry 
WITHOUT_CLASSIFICATION	 when overwriting just start with empty timeline 
WITHOUT_CLASSIFICATION	 expand the array 
WITHOUT_CLASSIFICATION	 when set true use the overflow checked vector expressions 
WITHOUT_CLASSIFICATION	 doesnt support creating vrbs 
WITHOUT_CLASSIFICATION	 generate result within big table batch itself 
WITHOUT_CLASSIFICATION	 dummy alias just use the input path 
WITHOUT_CLASSIFICATION	 weve found something that matches what were trying lock 
WITHOUT_CLASSIFICATION	 numpartitions could not obtained from orm filters then get number partitions names and count them 
WITHOUT_CLASSIFICATION	 yyyyyyymm should more than enough 
WITHOUT_CLASSIFICATION	 gather physical pipeline based user config grping sets size 
WITHOUT_CLASSIFICATION	 read friendly string 
WITHOUT_CLASSIFICATION	 set hadoopusername env variable for child process that also runs with hadoop permissions for the user the job running 
WITHOUT_CLASSIFICATION	 add and dryrun generate list only 
WITHOUT_CLASSIFICATION	 nonjavadoc should ideally not modify the tree traverse however since need walk the tree any time when modify the operator might well here 
WITHOUT_CLASSIFICATION	 this function should overriden every sub class and the sub class should call superinitm parameters get mode set 
WITHOUT_CLASSIFICATION	 failed submit after retrying destroy session and bail 
WITHOUT_CLASSIFICATION	 negative power with range adjust the scale 
WITHOUT_CLASSIFICATION	 should copy properties first 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 converted sortmerge join 
WITHOUT_CLASSIFICATION	 rolegrantslist 
WITHOUT_CLASSIFICATION	 second incremental dump 
WITHOUT_CLASSIFICATION	 querydirectory should not null 
WITHOUT_CLASSIFICATION	 over all the destination structures and populate the related 
WITHOUT_CLASSIFICATION	 append prefix 
WITHOUT_CLASSIFICATION	 exhausted the batch longer have heartbeat for current txn batch 
WITHOUT_CLASSIFICATION	 involving constant truefalse values 
WITHOUT_CLASSIFICATION	 replace the commar finish clause string 
WITHOUT_CLASSIFICATION	 there are nulls 
WITHOUT_CLASSIFICATION	 any redirect handlers need added first 
WITHOUT_CLASSIFICATION	 avoid reading the footer twice will cache first and then read from cache parquet calls protobuf methods directly the stream and cant get bytes after the fact 
WITHOUT_CLASSIFICATION	 create failed compactions 
WITHOUT_CLASSIFICATION	 gby without distinct keys not prepared process distinct key structured rows 
WITHOUT_CLASSIFICATION	 column columnfamily with columnqualifier 
WITHOUT_CLASSIFICATION	 evaluate the keys 
WITHOUT_CLASSIFICATION	 replicate all the events except drop 
WITHOUT_CLASSIFICATION	 todo maybe throw 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 otherwise notify about spark jobs after the state notification 
WITHOUT_CLASSIFICATION	 grouping sets members 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 this code doesnt propagate msg cprgeterrorcode 
WITHOUT_CLASSIFICATION	 note its rather important that this and other methods catch exception not throwable combination with code perhaps unintentionally used also catch all errors and now allows ooms only propagate 
WITHOUT_CLASSIFICATION	 not event dump not table dump thus dump 
WITHOUT_CLASSIFICATION	 last try try parse date and transform 
WITHOUT_CLASSIFICATION	 walk 
WITHOUT_CLASSIFICATION	 resetting the queue config old hack that may remove future 
WITHOUT_CLASSIFICATION	 trigger vectorforward 
WITHOUT_CLASSIFICATION	 nonverbose pattern look for 
WITHOUT_CLASSIFICATION	 make sure result precisionscale matches the input precscale 
WITHOUT_CLASSIFICATION	 optimize this newwork given the big table position 
WITHOUT_CLASSIFICATION	 someone allocating this arena wait bit and recheck 
WITHOUT_CLASSIFICATION	 disk for the hybrid grace hash partitioning 
WITHOUT_CLASSIFICATION	 will trigger spills 
WITHOUT_CLASSIFICATION	 the child tasks may null case select 
WITHOUT_CLASSIFICATION	 going through file list and make the retry list 
WITHOUT_CLASSIFICATION	 verifying that method supported 
WITHOUT_CLASSIFICATION	 create the new filter that might pushed down 
WITHOUT_CLASSIFICATION	 dummy operator for not increasing seqid 
WITHOUT_CLASSIFICATION	 join keys dont match the bucketing keys 
WITHOUT_CLASSIFICATION	 get current input file name 
WITHOUT_CLASSIFICATION	 nothing needed here default 
WITHOUT_CLASSIFICATION	 set yarn queue name 
WITHOUT_CLASSIFICATION	 into the doas below 
WITHOUT_CLASSIFICATION	 conversion 
WITHOUT_CLASSIFICATION	 hivetmptablespace hivehdfssessionpath and are respectively saved hdfstmptablespace hdfssessionpath and localsessionpath saving them conf variables useful expose them end users but end users shouldnt change them 
WITHOUT_CLASSIFICATION	 test new api 
WITHOUT_CLASSIFICATION	 chooses representative alias and index use the string the first used because set the constructor 
WITHOUT_CLASSIFICATION	 this wont into checkandsend 
WITHOUT_CLASSIFICATION	 statement should open even after resultsetclose 
WITHOUT_CLASSIFICATION	 cpu cost sorting cost 
WITHOUT_CLASSIFICATION	 last batch can sometimes have less number elements 
WITHOUT_CLASSIFICATION	 spot check correctness decimal scalar multiply decimal column the case for addition checks all the cases for the template dont that redundantly here 
WITHOUT_CLASSIFICATION	 create conditional task and insert conditional task into task tree 
WITHOUT_CLASSIFICATION	 trimblanks 
WITHOUT_CLASSIFICATION	 add mapping from the table scan operator table 
WITHOUT_CLASSIFICATION	 relying watchserviceclose clean all pending watches 
WITHOUT_CLASSIFICATION	 bypass for explain queries for now 
WITHOUT_CLASSIFICATION	 exclude all standard table properties 
WITHOUT_CLASSIFICATION	 now the most important check when query this record for its schema 
WITHOUT_CLASSIFICATION	 these schemata are used other tests 
WITHOUT_CLASSIFICATION	 get not between filter expression this treated special case because the not actually specified the expression tree the first argument and dont want any runtime cost for that creating the vectorexpression needs done differently than the standard way where all arguments are passed the vectorexpression constructor 
WITHOUT_CLASSIFICATION	 when selectedinuse set false everything the batch selected 
WITHOUT_CLASSIFICATION	 this effectively dag completed and can used reset statistics being tracked 
WITHOUT_CLASSIFICATION	 get the object inspector for myrow 
WITHOUT_CLASSIFICATION	 note enhance showresourceplan display all the pools triggers and mappings 
WITHOUT_CLASSIFICATION	 not hiveinputformat custom vertexmanager will take care grouping splits 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 the query was the result analyze table column compute statistics rewrite create column stats task instead fetch task persist stats the metastore per hive will also collect table stats when user computes column stats that means iscstats need collect table stats iscstats need include basic stats task else which should have move task with stats task already 
WITHOUT_CLASSIFICATION	 that follows this used for connecting them later 
WITHOUT_CLASSIFICATION	 allow 
WITHOUT_CLASSIFICATION	 verify cmrecycledb table part api moves file cmroot dir 
WITHOUT_CLASSIFICATION	 this means partition exists for the given partition key value pairs thrift cannot handle null return values hence getpartition throws indicate null partition 
WITHOUT_CLASSIFICATION	 try read the default named url from the connection configuration file 
WITHOUT_CLASSIFICATION	 already retrieved the incoming info check without ugi 
WITHOUT_CLASSIFICATION	 need track this some listeners pass through our config and need honor 
WITHOUT_CLASSIFICATION	 work 
WITHOUT_CLASSIFICATION	 notification generated for newly created partitions only the subset partitions 
WITHOUT_CLASSIFICATION	 dblevel repl loads testing done now moving table level repl loads each these cases the tablelevel repllastid must move forward but the dblevel lastreplid must not 
WITHOUT_CLASSIFICATION	 may flood the log 
WITHOUT_CLASSIFICATION	 hive should remove this code 
WITHOUT_CLASSIFICATION	 this creates orc data file with correct schema under table root 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 check aggoutputproject projects only one expression 
WITHOUT_CLASSIFICATION	 nonjavadoc see int int int 
WITHOUT_CLASSIFICATION	 wait for all threads ready 
WITHOUT_CLASSIFICATION	 log warning incase reporters were successfully added 
WITHOUT_CLASSIFICATION	 this true drop table 
WITHOUT_CLASSIFICATION	 the left was the left side right outer join 
WITHOUT_CLASSIFICATION	 metastore schema only allows maximum for constraint value column 
WITHOUT_CLASSIFICATION	 different duplicate some other function 
WITHOUT_CLASSIFICATION	 logicalproject maps set rows different set without knowledge the mapping functionwhether preserves uniqueness only safe derive uniqueness info from the child project when the mapping further more the unique bitset coming from the child needs mapped match the output the project 
WITHOUT_CLASSIFICATION	 test third argument repeating 
WITHOUT_CLASSIFICATION	 this point dont have anything special just run through the regular paces creating new task 
WITHOUT_CLASSIFICATION	 partition not found describe table partition 
WITHOUT_CLASSIFICATION	 customization this api done for most authorization implementations meant used for special cases apache sentry incubating null returned when customization needed for the translator see javadoc interface for details 
WITHOUT_CLASSIFICATION	 create operationlog object with above log file 
WITHOUT_CLASSIFICATION	 test longstring version 
WITHOUT_CLASSIFICATION	 this input rel does not produce the cor var needed 
WITHOUT_CLASSIFICATION	 inject behavior where repl load failed when try load table fails 
WITHOUT_CLASSIFICATION	 for set role all reset roles default roles 
WITHOUT_CLASSIFICATION	 num executors less than max executors per query which not expected case default executors will 
WITHOUT_CLASSIFICATION	 make sure that they have the same type 
WITHOUT_CLASSIFICATION	 when selectedinuse true start with every bit set false and selectively set certain bits true based the selected vector 
WITHOUT_CLASSIFICATION	 list 
WITHOUT_CLASSIFICATION	 fits one longword 
WITHOUT_CLASSIFICATION	 decimal maximum digits lower longs digits here 
WITHOUT_CLASSIFICATION	 islastgroupbatch 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 are going serialize using the basic types 
WITHOUT_CLASSIFICATION	 the first shot fails then log the waiting messages 
WITHOUT_CLASSIFICATION	 now filter 
WITHOUT_CLASSIFICATION	 now serialize 
WITHOUT_CLASSIFICATION	 note that cache each slice separately could cache them together the end but then wont able pass them users without increfing explicitly 
WITHOUT_CLASSIFICATION	 walk through exprs and extract field collations and additional 
WITHOUT_CLASSIFICATION	 start only serializing primitives asis 
WITHOUT_CLASSIFICATION	 join then there should only either subquery 
WITHOUT_CLASSIFICATION	 various errors when creating spark client 
WITHOUT_CLASSIFICATION	 its childrens parents lists also see childoperatorstag operator here 
WITHOUT_CLASSIFICATION	 expecting not change the size internal structures 
WITHOUT_CLASSIFICATION	 initialize destination tablepartition 
WITHOUT_CLASSIFICATION	 set the bit key not null 
WITHOUT_CLASSIFICATION	 there another batch buffer 
WITHOUT_CLASSIFICATION	 checkconstraints 
WITHOUT_CLASSIFICATION	 the first child toktableorcol and nodeoutput null 
WITHOUT_CLASSIFICATION	 friday august 
WITHOUT_CLASSIFICATION	 this optimizer will serialize all filters that made the table scan operator avoid having multiple times the backend you have physical optimization that changes table scans filters you have invoke before this one 
WITHOUT_CLASSIFICATION	 does not end with then line continuation 
WITHOUT_CLASSIFICATION	 tbl location available use else derive the tbl location from database location 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 mystringstringmap 
WITHOUT_CLASSIFICATION	 database name pattern 
WITHOUT_CLASSIFICATION	 this shouldnt happen the parser should have converted the union contained subquery just case keep the error fallback 
WITHOUT_CLASSIFICATION	 column list 
WITHOUT_CLASSIFICATION	 update cached aggregate stats for all partitions table and for all 
WITHOUT_CLASSIFICATION	 prefix for window functions discern leadlag udfs from window functions with the same name 
WITHOUT_CLASSIFICATION	 its either file glob 
WITHOUT_CLASSIFICATION	 wantwritable 
WITHOUT_CLASSIFICATION	 these anyway 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 exclude constants aggregatetrue occurs because aggregate would generate row even when applied empty table 
WITHOUT_CLASSIFICATION	 required required 
WITHOUT_CLASSIFICATION	 verify the writeid this committed txn should invalid for test txn 
WITHOUT_CLASSIFICATION	 data size still then get file size 
WITHOUT_CLASSIFICATION	 select from the new table should pass 
WITHOUT_CLASSIFICATION	 for pfile calculate the checksum for use testing 
WITHOUT_CLASSIFICATION	 optimize whole decimal fits two binary words 
WITHOUT_CLASSIFICATION	 semijoin keys and keys completely unrelated the cardinality both sets could obtained adding both cardinalities would there average case 
WITHOUT_CLASSIFICATION	 selpairgetkey the operator right before selpairgetvalue which only contains columns needed result set extra columns needed order will absent from 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 validation 
WITHOUT_CLASSIFICATION	 remember which mapjoin operator links with which work 
WITHOUT_CLASSIFICATION	 deal with static partition columns 
WITHOUT_CLASSIFICATION	 check all the operators the stack currently only selects and filters 
WITHOUT_CLASSIFICATION	 examine all digits being thrown away determine result 
WITHOUT_CLASSIFICATION	 let cleaner delete obsolete filesdirs 
WITHOUT_CLASSIFICATION	 generate the local work for the big table alias 
WITHOUT_CLASSIFICATION	 the absence column statistics compute data size based based average row size 
WITHOUT_CLASSIFICATION	 project the columns the group plus the arguments the agg function 
WITHOUT_CLASSIFICATION	 calcite expects the grouping sets sorted and without duplicates 
WITHOUT_CLASSIFICATION	 group grants 
WITHOUT_CLASSIFICATION	 lfu extreme order accesses should ignored only frequency matters touch first elements later but less times they will evicted first 
WITHOUT_CLASSIFICATION	 since setstructfielddata and create return list getstructfielddata should able handle list data this required when table serde parquethiveserde and partition serde something else 
WITHOUT_CLASSIFICATION	 bootstrap repl 
WITHOUT_CLASSIFICATION	 handle three types scenarios with special case handling level metadata handling subsequent loadtask which will start running from the previous replicationstate other events these can only either table function metadata 
WITHOUT_CLASSIFICATION	 nway join has been calculated hashtableloader earlier just need retrieve that number 
WITHOUT_CLASSIFICATION	 partitioned input not sorted 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 assume millislocal midnight some date what are basically trying here from localmidnight utcmidnight whatever time that happens 
WITHOUT_CLASSIFICATION	 read the configuration parameters 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 grant 
WITHOUT_CLASSIFICATION	 not operator bail out 
WITHOUT_CLASSIFICATION	 required required required required required required required required required required required required required required optional 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 remap arguments 
WITHOUT_CLASSIFICATION	 static partition specified 
WITHOUT_CLASSIFICATION	 the bottom aggregate has converted the distinct aggregate group clause 
WITHOUT_CLASSIFICATION	 mapping bucket number required tasks run 
WITHOUT_CLASSIFICATION	 using different code blocks that jdbc variables are not accidently reused between the actions different connectionstatement object should used for each action 
WITHOUT_CLASSIFICATION	 future only takes final seemingly final values make final copy taskid 
WITHOUT_CLASSIFICATION	 the equality implemented fully the implementation sorts the maps their keys provide transitive compare 
WITHOUT_CLASSIFICATION	 from the map jobs 
WITHOUT_CLASSIFICATION	 grandparent works need set these the parents the cloned works 
WITHOUT_CLASSIFICATION	 save last longword 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 nothing update everything the same 
WITHOUT_CLASSIFICATION	 sequence tablescan operators walked 
WITHOUT_CLASSIFICATION	 resfile pctx roottasks fetchtask analyzer explainconfig cboinfo 
WITHOUT_CLASSIFICATION	 update table schema add the newly added columns 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 delete the data 
WITHOUT_CLASSIFICATION	 add the setrcols the input list 
WITHOUT_CLASSIFICATION	 dont eat and wrap runtimeexceptions because the objectbufferwrite handles specifically resizing the buffer 
WITHOUT_CLASSIFICATION	 that something blocking that would not block read 
WITHOUT_CLASSIFICATION	 rethrow without losing original stack trace 
WITHOUT_CLASSIFICATION	 create the root the operator tree 
WITHOUT_CLASSIFICATION	 flip the boolean variable 
WITHOUT_CLASSIFICATION	 copy the column values from the input row dpvals 
WITHOUT_CLASSIFICATION	 template expansion logic the same for both columnscalar and scalarcolumn cases 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 sparkwork dependency graph from sparkwork with operators all 
WITHOUT_CLASSIFICATION	 dont bother cleaning from the txns table separate call will that dont know here which txns still have components from other tables partitions the table dont know which ones can and cannot clean 
WITHOUT_CLASSIFICATION	 record this change the metastore 
WITHOUT_CLASSIFICATION	 the testxml was not generated was corrupt let someone know 
WITHOUT_CLASSIFICATION	 assert false 
WITHOUT_CLASSIFICATION	 get delegation token for user from filesystem and write the token along with metastore tokens into file 
WITHOUT_CLASSIFICATION	 pass job initialize metastore conf overrides 
WITHOUT_CLASSIFICATION	 precision 
WITHOUT_CLASSIFICATION	 can process this batch immediately 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 these calls are see how much data there the setfrombytes call below will the same readvint reads but actually unpack the decimal 
WITHOUT_CLASSIFICATION	 const first argument then evaluate the result 
WITHOUT_CLASSIFICATION	 note could use lock allow concurrent calls for different sessions however all those calls add elements lists and maps and wed need sync those separately separately plus have object notify because lock does not support conditions 
WITHOUT_CLASSIFICATION	 not merge the mapredwork mapjoin has multiple input aliases 
WITHOUT_CLASSIFICATION	 the client requested that extra mapreduce step performed 
WITHOUT_CLASSIFICATION	 the code inside the attribute getter threw exception log and skip outputting the attribute 
WITHOUT_CLASSIFICATION	 get our multikey hash multiset information for this specialized class 
WITHOUT_CLASSIFICATION	 trigger lazy read metadata make sure serialized data not corrupted and readable 
WITHOUT_CLASSIFICATION	 blindly add this integer list should sufficient for the test case use the nonsettable list object inspector 
WITHOUT_CLASSIFICATION	 thread being interrupted 
WITHOUT_CLASSIFICATION	 add hbase properties 
WITHOUT_CLASSIFICATION	 want wait for the iteration finish and set the cluster fraction 
WITHOUT_CLASSIFICATION	 end the root object 
WITHOUT_CLASSIFICATION	 consistent with the behavior listpartitionnames the table does not exist return empty list 
WITHOUT_CLASSIFICATION	 char length available copy whole string value here 
WITHOUT_CLASSIFICATION	 after recovery there shouldntable any flushlength files 
WITHOUT_CLASSIFICATION	 captures how the input should ordered this captured list astnodes that are the expressions the sort clause ptf invocation 
WITHOUT_CLASSIFICATION	 operators with 
WITHOUT_CLASSIFICATION	 decimal 
WITHOUT_CLASSIFICATION	 initialize the array 
WITHOUT_CLASSIFICATION	 initialize aliastowork 
WITHOUT_CLASSIFICATION	 spot check decimal columncolumn subtract 
WITHOUT_CLASSIFICATION	 did not get token set oozie lets get them ourselves here essentially get token per unique output hcattableinfo this done because through pig setoutput method called multiple times want only get the token once per unique output hcattableinfo cannot just get one token since multiquery case store job the case when single pig script results jobs the single token will get cancelled the output committer and the subsequent stores will fail tying the token with the concatenation dbname tablename and partition keyvalues the output tableinfo can have many tokens there are stores and the tokenselector will correctly pick the right tokens which the committer will use and 
WITHOUT_CLASSIFICATION	 first check the table dir exists could have been deleted for some reason precommit tests 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautillist 
WITHOUT_CLASSIFICATION	 look for interfaces both the class and all base classes 
WITHOUT_CLASSIFICATION	 table creation with long table name causes 
WITHOUT_CLASSIFICATION	 try recursive folding 
WITHOUT_CLASSIFICATION	 this method inserts the right profiles into profiles cbo depending the query characteristics 
WITHOUT_CLASSIFICATION	 scratch arrays used fastbigintegerbytes calls for better performance 
WITHOUT_CLASSIFICATION	 kerberos security enabled and doas enabled then additional params need set that the command run intended user 
WITHOUT_CLASSIFICATION	 rewrite value index for mapjoin 
WITHOUT_CLASSIFICATION	 initially all deltas and rcs are empty list starts there are objects take 
WITHOUT_CLASSIFICATION	 there are previous nodes then and the current node with the previous one 
WITHOUT_CLASSIFICATION	 smallint 
WITHOUT_CLASSIFICATION	 input key bigger than any keys hash 
WITHOUT_CLASSIFICATION	 run queries 
WITHOUT_CLASSIFICATION	 number reducers 
WITHOUT_CLASSIFICATION	 adjacencytype 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangobject 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 first see there direct match 
WITHOUT_CLASSIFICATION	 boundary specifies how many rows backforward windowframe extends from the current row boundary specified range boundary the number rows forward back from the current row current row which implies the boundary the current row value boundary which specified the amount the value expression must decreaseincrease 
WITHOUT_CLASSIFICATION	 the command has schema make sure nothing printed 
WITHOUT_CLASSIFICATION	 completion txnididtxnupdate 
WITHOUT_CLASSIFICATION	 return vector expression for custom not builtin udf 
WITHOUT_CLASSIFICATION	 strip trailing carriage return input ignore changes whose lines are all blank 
WITHOUT_CLASSIFICATION	 prspjoincrscgby 
WITHOUT_CLASSIFICATION	 dynamic partition pruning pipeline doesnt have multiple children 
WITHOUT_CLASSIFICATION	 rqst 
WITHOUT_CLASSIFICATION	 need copy standard object otherwise deserializer overwrites the values 
WITHOUT_CLASSIFICATION	 have storage specification for map column type 
WITHOUT_CLASSIFICATION	 because txn may include different partitionstables even auto commit mode 
WITHOUT_CLASSIFICATION	 optional int dagstarttime 
WITHOUT_CLASSIFICATION	 try resolve qualified name column reference the parent querys rowresolver apply this logic the leftmostfirst dot ast tree 
WITHOUT_CLASSIFICATION	 recall that the sequence must prsselcrs 
WITHOUT_CLASSIFICATION	 set the dynamic values the childwork 
WITHOUT_CLASSIFICATION	 key 
WITHOUT_CLASSIFICATION	 finalize the last record 
WITHOUT_CLASSIFICATION	 return proper response 
WITHOUT_CLASSIFICATION	 setup 
WITHOUT_CLASSIFICATION	 calculate the variance result when count public vectorization code can use etc 
WITHOUT_CLASSIFICATION	 min needed the case that entire string whitespace 
WITHOUT_CLASSIFICATION	 finally not reduce the input size bail out 
WITHOUT_CLASSIFICATION	 find operators with partition pruning enabled plan because these may potentially read different data for different pipeline these can with dpp with semijoin dpp 
WITHOUT_CLASSIFICATION	 expand all privileges any 
WITHOUT_CLASSIFICATION	 convert semijoin gby semijoin 
WITHOUT_CLASSIFICATION	 initialize the task and execute 
WITHOUT_CLASSIFICATION	 set the leftmost header the base and its buddy that are now being merged 
WITHOUT_CLASSIFICATION	 figure out there group 
WITHOUT_CLASSIFICATION	 find out cpu msecs the case that cant find out this number just skip the step print 
WITHOUT_CLASSIFICATION	 the hash table slots for bytes key hash table each slot longs and the array sized the slot triple nonzero reference word the key bytes the key hash code and nonzero reference word the first value bytes 
WITHOUT_CLASSIFICATION	 capacity check element needs evicted 
WITHOUT_CLASSIFICATION	 else this means pigs optimizer never invoked the pushprojection method need all fields and hence should not call the setoutputschema hcatinputformat 
WITHOUT_CLASSIFICATION	 external table should also check the underlying file size 
WITHOUT_CLASSIFICATION	 create the timeline for the existing and new segments 
WITHOUT_CLASSIFICATION	 completion txnididtxnupdate 
WITHOUT_CLASSIFICATION	 the pending query were waiting failed but there might still another pending completed entry the cache that can satisfy this query lookup again 
WITHOUT_CLASSIFICATION	 find the privrequirements that match iotype actiontype and add the privilege required reqprivs 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 todo either have kill the nonactor model would implicitly hope for the best and continue other threads the latter for now 
WITHOUT_CLASSIFICATION	 always bit insure the key reference nonzero 
WITHOUT_CLASSIFICATION	 exponent 
WITHOUT_CLASSIFICATION	 llapdaemonmxbean methods will exposed via jmx 
WITHOUT_CLASSIFICATION	 the size deserialized partition shouldnt exceed half memory limit 
WITHOUT_CLASSIFICATION	 binary tcp mode 
WITHOUT_CLASSIFICATION	 the reason this guard because when not have good way initializing the config the handlers thread local config until this call then once done though need not repeat this linking simply call setmetastorehandler and let the and what they want 
WITHOUT_CLASSIFICATION	 when using command always single line 
WITHOUT_CLASSIFICATION	 add the default sql completions 
WITHOUT_CLASSIFICATION	 configured warehouse local dont need bother checking 
WITHOUT_CLASSIFICATION	 must held same thread 
WITHOUT_CLASSIFICATION	 cascadetrue then need authorize the drop table action well 
WITHOUT_CLASSIFICATION	 ignore the exception because are not comparing long string here there should never exception 
WITHOUT_CLASSIFICATION	 introduce exchange operators below joinmultijoin operators 
WITHOUT_CLASSIFICATION	 start inclusive end inclusive 
WITHOUT_CLASSIFICATION	 trim off lower fractional digits but with rounding 
WITHOUT_CLASSIFICATION	 use tostring which will have exponents instead toplainstring 
WITHOUT_CLASSIFICATION	 same mapredtaskid figure 
WITHOUT_CLASSIFICATION	 test that when transaction aborted the heartbeat fails 
WITHOUT_CLASSIFICATION	 prevent hive configurations from being visible spark 
WITHOUT_CLASSIFICATION	 the uses the prior read type flush the prior deserializerbatch set here none 
WITHOUT_CLASSIFICATION	 its nondeterministic 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite with new exclusive coalesces 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader 
WITHOUT_CLASSIFICATION	 make everything qualify and ensure selected not use 
WITHOUT_CLASSIFICATION	 commit has succeeded since exceptions have been thrown 
WITHOUT_CLASSIFICATION	 sparksubmit will take care that for 
WITHOUT_CLASSIFICATION	 get configuration parameters 
WITHOUT_CLASSIFICATION	 hence true only map side 
WITHOUT_CLASSIFICATION	 are columns used this select operator 
WITHOUT_CLASSIFICATION	 set client port have already had list valid ports use 
WITHOUT_CLASSIFICATION	 then null else null unusual case but possible 
WITHOUT_CLASSIFICATION	 since after various rules original relnode could have different corref might not have all need traverse the new node figure out new cor refs and put that into map 
WITHOUT_CLASSIFICATION	 test with table name which does not exists the given database 
WITHOUT_CLASSIFICATION	 will called before closing the orc file stop writing any additional information the acid key index 
WITHOUT_CLASSIFICATION	 turn the tree set into array can move back and forth easily 
WITHOUT_CLASSIFICATION	 for conditional expressions 
WITHOUT_CLASSIFICATION	 all partitions have been statically removed 
WITHOUT_CLASSIFICATION	 verify extension values the array 
WITHOUT_CLASSIFICATION	 special handling for set role statement 
WITHOUT_CLASSIFICATION	 make the table acid 
WITHOUT_CLASSIFICATION	 checking state per node for future failure handling scenarios where update 
WITHOUT_CLASSIFICATION	 create thread pool with poolsize threads threads terminate when they are idle for more than the keepalivetime bounded blocking queue used queue incoming operations operations poolsize 
WITHOUT_CLASSIFICATION	 try with decimal input and decimal output 
WITHOUT_CLASSIFICATION	 the row consists some string columns some arrayint columns 
WITHOUT_CLASSIFICATION	 analyze command 
WITHOUT_CLASSIFICATION	 for table only want delete delta dirs for aborted txns 
WITHOUT_CLASSIFICATION	 hiveserver metadata api types start here these corresponds various calls 
WITHOUT_CLASSIFICATION	 insert reduceside 
WITHOUT_CLASSIFICATION	 largest possible base exponent any exponent larger than this will already produce underflow overflow theres need worry about additional digits 
WITHOUT_CLASSIFICATION	 into llapnodeid get node info from registry that should can include 
WITHOUT_CLASSIFICATION	 float loses some precisions 
WITHOUT_CLASSIFICATION	 reuse old object prevent needless expr cloning 
WITHOUT_CLASSIFICATION	 support names like colxcoly 
WITHOUT_CLASSIFICATION	 the following tests spotcheck that vectorized functions with signature double funcdouble that came from template columnunaryfunctxt get the right result null propagation isrepeating propagation will checked once for single expansion the template for 
WITHOUT_CLASSIFICATION	 but count inner side before that make sure generates atmost row 
WITHOUT_CLASSIFICATION	 separate split 
WITHOUT_CLASSIFICATION	 reverse this 
WITHOUT_CLASSIFICATION	 basic algorithm determine rounding digit for rounding scale away fractional digits present rounding clear integer rounding portion and add 
WITHOUT_CLASSIFICATION	 txn there one started not finished 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 get tokens for default not all fss support delegation tokens wasb 
WITHOUT_CLASSIFICATION	 convert the group mapside group 
WITHOUT_CLASSIFICATION	 silly little pager 
WITHOUT_CLASSIFICATION	 test backward scan 
WITHOUT_CLASSIFICATION	 serialization the option selected 
WITHOUT_CLASSIFICATION	 create the thread pool for the web server handle http requests 
WITHOUT_CLASSIFICATION	 bootstrap case 
WITHOUT_CLASSIFICATION	 table inside view not care about its authorization 
WITHOUT_CLASSIFICATION	 druid json timestamp column name 
WITHOUT_CLASSIFICATION	 required required optional required required optional optional optional optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 round without digits 
WITHOUT_CLASSIFICATION	 equal key series checking 
WITHOUT_CLASSIFICATION	 result privilege 
WITHOUT_CLASSIFICATION	 bgenjjtree fieldtype 
WITHOUT_CLASSIFICATION	 dynamic partition pruning enabled some all cases either true true 
WITHOUT_CLASSIFICATION	 auth specific confs 
WITHOUT_CLASSIFICATION	 test notequals operator for strings and integers 
WITHOUT_CLASSIFICATION	 get total number rows from all memory partitions 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader 
WITHOUT_CLASSIFICATION	 run the last combined strategy any 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 because use parentheses addition whitespace keyword delimiter need define new argumentdelimiter 
WITHOUT_CLASSIFICATION	 float 
WITHOUT_CLASSIFICATION	 slide the column names down for the name array 
WITHOUT_CLASSIFICATION	 just case deserialize decimal with trailing zeroes 
WITHOUT_CLASSIFICATION	 build map hive column names exprnodecolumndesc name the positions those projections the input 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 make sure struct record 
WITHOUT_CLASSIFICATION	 test for publish with invalid partition key name 
WITHOUT_CLASSIFICATION	 incomplete message buffer 
WITHOUT_CLASSIFICATION	 avoid double casting preserve original string representation constant 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 should have printed out the header for the field schema 
WITHOUT_CLASSIFICATION	 for table explicitly try load partitions there separate partitions events 
WITHOUT_CLASSIFICATION	 handle countsumavg function for the case count and countdistinct 
WITHOUT_CLASSIFICATION	 calculate the function result for row the batch and set the output column vector entry the result 
WITHOUT_CLASSIFICATION	 but that set immutable 
WITHOUT_CLASSIFICATION	 note per our current constraints the behavior two parallel activates undefined although only one will succeed and the other will receive exception need proper semitransactional modifications support this without hacks 
WITHOUT_CLASSIFICATION	 are asked start from begining clear the current fetched resultset 
WITHOUT_CLASSIFICATION	 remove from all its parents child list 
WITHOUT_CLASSIFICATION	 break out and try executing 
WITHOUT_CLASSIFICATION	 each newinput 
WITHOUT_CLASSIFICATION	 null qualifier would mean all qualifiers that family want empty qualifier 
WITHOUT_CLASSIFICATION	 have use the length instead the actual prefix because the prefix location dependent byte hash for the path separator can less than due unicode characters 
WITHOUT_CLASSIFICATION	 now abort compact and 
WITHOUT_CLASSIFICATION	 need full scan 
WITHOUT_CLASSIFICATION	 join need update the state information accordingly 
WITHOUT_CLASSIFICATION	 row offsets will determined from the reader could set the first from last 
WITHOUT_CLASSIFICATION	 cababc 
WITHOUT_CLASSIFICATION	 check list element and value are same type 
WITHOUT_CLASSIFICATION	 should have been replaced 
WITHOUT_CLASSIFICATION	 connecting foo 
WITHOUT_CLASSIFICATION	 generate the columns according the column mapping provided 
WITHOUT_CLASSIFICATION	 this should never happen 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 tablefunction may able accept its input stream this case the contract startpartition must invoked give the ptf chance initialize stream processing each input row passed via processrowor processrows invocation processrow can return more rows finishpartition invoked give the ptf chance finish processing and return any remaining rows 
WITHOUT_CLASSIFICATION	 cannot lock remove this from cache and continue 
WITHOUT_CLASSIFICATION	 look see how this filter created 
WITHOUT_CLASSIFICATION	 subscriber accept the feed and something depending the task type 
WITHOUT_CLASSIFICATION	 return remaining records any from last processed input record 
WITHOUT_CLASSIFICATION	 duration estimate the size the map changes can very different 
WITHOUT_CLASSIFICATION	 add all 
WITHOUT_CLASSIFICATION	 tailing zeroes difference 
WITHOUT_CLASSIFICATION	 connection however retry one more time 
WITHOUT_CLASSIFICATION	 process join 
WITHOUT_CLASSIFICATION	 root task cannot depend any other task therefore childtask cannot 
WITHOUT_CLASSIFICATION	 virtual columns 
WITHOUT_CLASSIFICATION	 select query 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 mysql returns the string not wellformed numeric value return longwritablevalueof but decided return null instead which more conservative 
WITHOUT_CLASSIFICATION	 initialize buffer read the entire stripe 
WITHOUT_CLASSIFICATION	 groupb user 
WITHOUT_CLASSIFICATION	 keep the parent correct 
WITHOUT_CLASSIFICATION	 grantrequest 
WITHOUT_CLASSIFICATION	 repeating expression 
WITHOUT_CLASSIFICATION	 remove all parameters that are tested the parameter tested part 
WITHOUT_CLASSIFICATION	 loop while you either have tasks running tasks queued 
WITHOUT_CLASSIFICATION	 multiply the same power ten shift the decimal point back the original place places the right the decimal will zero 
WITHOUT_CLASSIFICATION	 test string column string literal comparison 
WITHOUT_CLASSIFICATION	 base has rows splits delta has rows split and delta has splits 
WITHOUT_CLASSIFICATION	 get stored the udfcontext would have been stored there earlier call setpartitionfilter call setinput hcatinputformat only the frontend because internally makes calls the hcat server dont want these happen the backend the hadoop front end mapredtaskid property will not set 
WITHOUT_CLASSIFICATION	 compute product distinct values grouping columns 
WITHOUT_CLASSIFICATION	 someone else also trying append 
WITHOUT_CLASSIFICATION	 columns 
WITHOUT_CLASSIFICATION	 add this dummy the dummp operator list 
WITHOUT_CLASSIFICATION	 splice the section that have evicted out the list have already updated the state above need that again 
WITHOUT_CLASSIFICATION	 unregister the functions well 
WITHOUT_CLASSIFICATION	 after processing subqueries and source tables process partitioned table functions 
WITHOUT_CLASSIFICATION	 attempt cleanup stack trace elements that vary 
WITHOUT_CLASSIFICATION	 grape expects excludes key args map 
WITHOUT_CLASSIFICATION	 its imperative that code acquirelocks called for all commands that hivetxnmanager can transition its state machine correctly 
WITHOUT_CLASSIFICATION	 delegate updates over the source state tracker 
WITHOUT_CLASSIFICATION	 bootstrap load replica 
WITHOUT_CLASSIFICATION	 know merge will triggered execution time 
WITHOUT_CLASSIFICATION	 before each test 
WITHOUT_CLASSIFICATION	 create znode under the rootnamespace parent for this instance the server 
WITHOUT_CLASSIFICATION	 all rowids are unique read after conversion acid rowids are exactly the same before and after compaction also check the file name only after compaction for completeness note order rows file ends being the reverse order values clause why 
WITHOUT_CLASSIFICATION	 the url passed valid url with protocol use asis otherwise assume name parameter that have get the url from 
WITHOUT_CLASSIFICATION	 create our vectorized copy row and deserialize row helper objects 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that inner join singlecolumn long and only big table columns appear the join result hash multiset used 
WITHOUT_CLASSIFICATION	 adjust counters and buffer limit 
WITHOUT_CLASSIFICATION	 type system for this 
WITHOUT_CLASSIFICATION	 adding the reducers run time statistics for the job the queryplan 
WITHOUT_CLASSIFICATION	 validate the operation renaming column name 
WITHOUT_CLASSIFICATION	 called the subquery agg and correlated doesnt have groupby added the ast 
WITHOUT_CLASSIFICATION	 insert event found then return null hence event dumped 
WITHOUT_CLASSIFICATION	 loop until the value correct run out tries 
WITHOUT_CLASSIFICATION	 outer joins with postfiltering conditions cannot merged 
WITHOUT_CLASSIFICATION	 load each incremental dump from the list each dump have only one operation 
WITHOUT_CLASSIFICATION	 coming from below 
WITHOUT_CLASSIFICATION	 create the table 
WITHOUT_CLASSIFICATION	 enable the hook check after the server startup 
WITHOUT_CLASSIFICATION	 from the above checks know fast zero 
WITHOUT_CLASSIFICATION	 get optional read variations for fields 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 underflow 
WITHOUT_CLASSIFICATION	 change lock manager embedded mode 
WITHOUT_CLASSIFICATION	 uncompressedoffset middle integer encoding runs rle delta etc consume 
WITHOUT_CLASSIFICATION	 handle table schema 
WITHOUT_CLASSIFICATION	 this could expensive when there are lot compactions 
WITHOUT_CLASSIFICATION	 expressions 
WITHOUT_CLASSIFICATION	 the current split should use the preceding splits footerbuffer order skip footer correctly 
WITHOUT_CLASSIFICATION	 make one partitioned 
WITHOUT_CLASSIFICATION	 fastscale abspower hivedecimalmaxscale 
WITHOUT_CLASSIFICATION	 collect 
WITHOUT_CLASSIFICATION	 select query 
WITHOUT_CLASSIFICATION	 list overhead configured number element list size element 
WITHOUT_CLASSIFICATION	 short 
WITHOUT_CLASSIFICATION	 right full outer join need iterate through the row container that contains all the right records that did not produce results then for each those records replace the left side with null values and produce the records observe that only enter this block when have finished iterating through all the left and right records aliasnum numaliases and thus have tried evaluate the postfilter condition every possible combination note the left records that not produce results for left full outer join will always caught the genobject method 
WITHOUT_CLASSIFICATION	 can offer ecb even with some streams not discarded reset will clear the arrays 
WITHOUT_CLASSIFICATION	 note this work around hive calcite limitations wrt calcite can not accept expressions instead needs expressed input select hive can not preserve ordering through select boundaries this map used for outermost migrate the corresponding expressions from input select this used astconverter after are done with calcite planning 
WITHOUT_CLASSIFICATION	 scratch cols are 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 updates before its running 
WITHOUT_CLASSIFICATION	 make sure the referenced schema exists 
WITHOUT_CLASSIFICATION	 sort columns specified table 
WITHOUT_CLASSIFICATION	 construct new row resolver with everything from below 
WITHOUT_CLASSIFICATION	 byte bytes outputgetlength 
WITHOUT_CLASSIFICATION	 this create and publish the segment overwritten 
WITHOUT_CLASSIFICATION	 get the local path downloaded jars 
WITHOUT_CLASSIFICATION	 update key with assigned identifier 
WITHOUT_CLASSIFICATION	 useminmax minmaxenabled 
WITHOUT_CLASSIFICATION	 optional int dagindex 
WITHOUT_CLASSIFICATION	 timestamp strings should parse 
WITHOUT_CLASSIFICATION	 compactor should only schedule compaction for ttp deltanumthreshold not ttp 
WITHOUT_CLASSIFICATION	 additions the group clause 
WITHOUT_CLASSIFICATION	 adding columns and limited integer type promotion supported for orc schema evolution 
WITHOUT_CLASSIFICATION	 consider for now recompute integerdigitcount 
WITHOUT_CLASSIFICATION	 temp hdfs path for spark hashtable sink 
WITHOUT_CLASSIFICATION	 responsive again recovery 
WITHOUT_CLASSIFICATION	 somewhere like try randomize bit for now 
WITHOUT_CLASSIFICATION	 schedule task cleanup dangling scratch dir periodically initial wait for random time between min 
WITHOUT_CLASSIFICATION	 use table descriptor for columns 
WITHOUT_CLASSIFICATION	 same comment applies here 
WITHOUT_CLASSIFICATION	 and hash with mask out sign bit make sure its positive then know taking the result mod the range 
WITHOUT_CLASSIFICATION	 original tostring takes too much space 
WITHOUT_CLASSIFICATION	 create new mapwork 
WITHOUT_CLASSIFICATION	 test that the values added are there 
WITHOUT_CLASSIFICATION	 this global allows various validation methods set the not vectorized reason 
WITHOUT_CLASSIFICATION	 the table location already exists and may contain data 
WITHOUT_CLASSIFICATION	 search for any the sparktask 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 generate reducesinkoperator 
WITHOUT_CLASSIFICATION	 rewrite only analyze table column compute statistics dont rewrite analyze table command table stats are collected the table scan operator and not rewritten aggregation 
WITHOUT_CLASSIFICATION	 add the task the delayed task queue does not already exist 
WITHOUT_CLASSIFICATION	 check column type 
WITHOUT_CLASSIFICATION	 now prepare partnames with partitions tabparttabpart which are contained the 
WITHOUT_CLASSIFICATION	 tezllap requires rpc query plan 
WITHOUT_CLASSIFICATION	 waits for lock from fifer 
WITHOUT_CLASSIFICATION	 all children expression should resolved 
WITHOUT_CLASSIFICATION	 this call sets the default ssl params including the correct keystore the server config 
WITHOUT_CLASSIFICATION	 current key objectinspectors are standard objectinspectors 
WITHOUT_CLASSIFICATION	 check the file exists 
WITHOUT_CLASSIFICATION	 make sure the schema mapping right 
WITHOUT_CLASSIFICATION	 this will break the iterator however this the last task can add the way this currently runs only one duck distributed when failedupdate present that should 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 currently only functions columns and scalars supported 
WITHOUT_CLASSIFICATION	 pass the row rather than recordvalue 
WITHOUT_CLASSIFICATION	 prefer methods with closer signature based the primitive grouping each argument score each method based its similarity the passed argument types 
WITHOUT_CLASSIFICATION	 getset methods 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 test outputformat with compression 
WITHOUT_CLASSIFICATION	 this vectorized aware evaluator 
WITHOUT_CLASSIFICATION	 single column unnamed primary key default catalog and database 
WITHOUT_CLASSIFICATION	 check whether merging the works would cause the size the data memory grow too large 
WITHOUT_CLASSIFICATION	 may get treated base splitupdate enabled for acid see hive for details 
WITHOUT_CLASSIFICATION	 called transform tasks into local tasks where possibledesirable 
WITHOUT_CLASSIFICATION	 warning note not threadunique getconf 
WITHOUT_CLASSIFICATION	 test the validation incorrect null values the tables throws exception 
WITHOUT_CLASSIFICATION	 this method parses the custom dynamic path and replaces each occurrence 
WITHOUT_CLASSIFICATION	 could not find allowed path table scan operator hence are done 
WITHOUT_CLASSIFICATION	 create the operator tree 
WITHOUT_CLASSIFICATION	 must deterministic order map for consistent qtest output across java versions 
WITHOUT_CLASSIFICATION	 tests for droppartitionstring dbname string tblname string name boolean deletedata method 
WITHOUT_CLASSIFICATION	 validate the second parameter which should integer 
WITHOUT_CLASSIFICATION	 should never happen 
WITHOUT_CLASSIFICATION	 scaling definitely larger 
WITHOUT_CLASSIFICATION	 all the split strategies are done must safe access splitfutures 
WITHOUT_CLASSIFICATION	 not force script execution abort when failure occurs 
WITHOUT_CLASSIFICATION	 methods that create relational expressions 
WITHOUT_CLASSIFICATION	 input file name big bucket number 
WITHOUT_CLASSIFICATION	 use this constructor when there output column 
WITHOUT_CLASSIFICATION	 this value should get overwritten with correct value ditto 
WITHOUT_CLASSIFICATION	 handles the case like line show tables test comment 
WITHOUT_CLASSIFICATION	 there should only one parent 
WITHOUT_CLASSIFICATION	 make sure itereated through all possible connparams 
WITHOUT_CLASSIFICATION	 todo hive creationmetadata tablenames tablenames 
WITHOUT_CLASSIFICATION	 set remaining fractional portion nanos 
WITHOUT_CLASSIFICATION	 shall have enough time synchronize privileges during loading information schema 
WITHOUT_CLASSIFICATION	 create filesink operator 
WITHOUT_CLASSIFICATION	 return directly last value null 
WITHOUT_CLASSIFICATION	 must least able return back 
WITHOUT_CLASSIFICATION	 conversion 
WITHOUT_CLASSIFICATION	 helper methods 
WITHOUT_CLASSIFICATION	 are consuming too much memory 
WITHOUT_CLASSIFICATION	 backtrack can null when input script operator 
WITHOUT_CLASSIFICATION	 want use metricsdir the same directory the destination file support atomic move temp file the destination metrics file 
WITHOUT_CLASSIFICATION	 need compare partition name with requested name since some dbs like mysql derby considers whereas others like postgres 
WITHOUT_CLASSIFICATION	 copy fails fall through the retry logic 
WITHOUT_CLASSIFICATION	 lock the lowest priority buffer try evict well evict some other buffer 
WITHOUT_CLASSIFICATION	 logger the callstack from which the error has been set 
WITHOUT_CLASSIFICATION	 only 
WITHOUT_CLASSIFICATION	 generate mapredlocalworks for and hts 
WITHOUT_CLASSIFICATION	 hive pending rename afterclass 
WITHOUT_CLASSIFICATION	 conjunctive predicate elements are more than one then walk through them one one compute cross product ndv cross product computed multiplying the largest ndv all the conjunctive predicate elements with degraded ndv rest the conjunctive predicate elements ndv degraded using log functionfinally the ndvcrossproduct fenced the join cross product ensure that ndv can not exceed worst case join cardinalitybr ndv conjunctive predicate element the max ndv all arguments lhs rhs expressions ndvjoincondition min left cardinality right cardinality where pex the predicate element join condition with max ndv ndvpe maxndvleftexpr ndvrightexpr 
WITHOUT_CLASSIFICATION	 its not likely there bug but case happens must have found wrong filter operator skip the optimization then 
WITHOUT_CLASSIFICATION	 process join 
WITHOUT_CLASSIFICATION	 run major compaction 
WITHOUT_CLASSIFICATION	 constant add them coltoconstants halfdeterministic columns 
WITHOUT_CLASSIFICATION	 native vector map join hash table setup 
WITHOUT_CLASSIFICATION	 the cancel case where the driver state interrupted destroy will deferred the query process 
WITHOUT_CLASSIFICATION	 mapreduce api catch the error log debug message and just keep going 
WITHOUT_CLASSIFICATION	 todo rename files case 
WITHOUT_CLASSIFICATION	 reconnect only supported for and streaming jobs this time 
WITHOUT_CLASSIFICATION	 where the highest longword middle longword etc 
WITHOUT_CLASSIFICATION	 fill colstatus 
WITHOUT_CLASSIFICATION	 errors are handled the way over failsuccess informed via regular heartbeats killed via kill message when task kill requested the daemon 
WITHOUT_CLASSIFICATION	 now make sure delete deltas are present 
WITHOUT_CLASSIFICATION	 for demo purposes 
WITHOUT_CLASSIFICATION	 this hook verifies that the location every partition the inputs and outputs starts with the location the table very simple check make sure subdirectory 
WITHOUT_CLASSIFICATION	 there are modes reading for vectorization one for the vectorized input file format which returns vectorizedrowbatch the row one for using deserialize each row into the vectorizedrowbatch currently these input file formats textfile sequencefile and one using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow this picks input file format not supported the other two 
WITHOUT_CLASSIFICATION	 exponents into floatingpoint numbers 
WITHOUT_CLASSIFICATION	 import statement specified external 
WITHOUT_CLASSIFICATION	 state that the driver enters after close has been called clean the query results and release the resources after the query has been executed 
WITHOUT_CLASSIFICATION	 periodically report progress the context object prevent tasktracker from killing the templeton controller task 
WITHOUT_CLASSIFICATION	 llap cluster info does not need admin privilege since read only assigning privilege same 
WITHOUT_CLASSIFICATION	 get key columns 
WITHOUT_CLASSIFICATION	 convert byteswritable byte 
WITHOUT_CLASSIFICATION	 lazysimpleserde can convert any types string type using jsonformat however may add more operators thus still keep the conversion 
WITHOUT_CLASSIFICATION	 add interceptor add xsrf header 
WITHOUT_CLASSIFICATION	 come ride the api rollercoaster 
WITHOUT_CLASSIFICATION	 add map 
WITHOUT_CLASSIFICATION	 the source table now has partitions one textfile the other orc test adding these partitions the targettable without replicating the tablechange 
WITHOUT_CLASSIFICATION	 must deterministic order map for consistent test output across java versions 
WITHOUT_CLASSIFICATION	 first try split the task 
WITHOUT_CLASSIFICATION	 this should never happen provide good error message case theres bug 
WITHOUT_CLASSIFICATION	 select select transform 
WITHOUT_CLASSIFICATION	 there failure from here until when the metadata changed the partition will empty throw errors read 
WITHOUT_CLASSIFICATION	 expression splits each part the partition 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 make sure they are not public 
WITHOUT_CLASSIFICATION	 verify cleanup functionality open new session since this case needs close the session the end 
WITHOUT_CLASSIFICATION	 found some old value but couldnt incref remove 
WITHOUT_CLASSIFICATION	 location specified set partition 
WITHOUT_CLASSIFICATION	 partition cols just distribute the data uniformly provide better load balance the requirement have single reducer should set the number reducers use constant seed make the code deterministic 
WITHOUT_CLASSIFICATION	 link import tasks the barrier task which will inturn linked with repl state update tasks 
WITHOUT_CLASSIFICATION	 actualbatchsize 
WITHOUT_CLASSIFICATION	 sum and count are rolled sum hence sum represents both here 
WITHOUT_CLASSIFICATION	 use internal text member read value 
WITHOUT_CLASSIFICATION	 because tablenoautocompact was originally assumed noautocompact and then was moved 
WITHOUT_CLASSIFICATION	 join filter does not change the old input ordering all input fields from newleftinputie the original input the old 
WITHOUT_CLASSIFICATION	 the unit caching for orc column see orcbatchkey 
WITHOUT_CLASSIFICATION	 nulls not repeating 
WITHOUT_CLASSIFICATION	 check interrupt the last moment case get cancelled quickly 
WITHOUT_CLASSIFICATION	 jar file the hdfs should downloaded first 
WITHOUT_CLASSIFICATION	 reducers from the parent operators 
WITHOUT_CLASSIFICATION	 direct access interfaces 
WITHOUT_CLASSIFICATION	 the syntax should not allow these fields null but lets verify 
WITHOUT_CLASSIFICATION	 username 
WITHOUT_CLASSIFICATION	 first remove all the membership the membership that this role has been granted 
WITHOUT_CLASSIFICATION	 this returns the source corvar rel which produces cor var 
WITHOUT_CLASSIFICATION	 show tracking url for remotely running jobs 
WITHOUT_CLASSIFICATION	 make sure driver returns all results drop and recreate the necessary databases and tables 
WITHOUT_CLASSIFICATION	 doublestats 
WITHOUT_CLASSIFICATION	 inner join specific members 
WITHOUT_CLASSIFICATION	 find table which name contains tofind the dummy database 
WITHOUT_CLASSIFICATION	 the rowresolver setup for select drops table associations setup astnode unqualified name 
WITHOUT_CLASSIFICATION	 the spark job finishes before this listener called the queued status will not set 
WITHOUT_CLASSIFICATION	 accumulo like the above rangeinputsplit should have the table name 
WITHOUT_CLASSIFICATION	 static pattern regexrid patterncompilexidaf static simpledateformat dateparser new zzzzz 
WITHOUT_CLASSIFICATION	 set state close long all parents are closed 
WITHOUT_CLASSIFICATION	 try readlock the candidatelist timeout after maxreaderwaittime 
WITHOUT_CLASSIFICATION	 implies all properties needs inherited 
WITHOUT_CLASSIFICATION	 initialize schema 
WITHOUT_CLASSIFICATION	 argument descriptors 
WITHOUT_CLASSIFICATION	 the input sorted alias are already the last join operand can emit some results now note this has done before adding the current row the storage preserve the correctness for outer joins 
WITHOUT_CLASSIFICATION	 each items keyvalue format 
WITHOUT_CLASSIFICATION	 get column names 
WITHOUT_CLASSIFICATION	 get the rewritten ast 
WITHOUT_CLASSIFICATION	 test setup append next hivereplrootdir and use that the dump location 
WITHOUT_CLASSIFICATION	 there were exception batchsize doesnt change until there exception 
WITHOUT_CLASSIFICATION	 match unknown 
WITHOUT_CLASSIFICATION	 decimal string 
WITHOUT_CLASSIFICATION	 optional optional required required optional 
WITHOUT_CLASSIFICATION	 float types require conversion use noop 
WITHOUT_CLASSIFICATION	 build operator 
WITHOUT_CLASSIFICATION	 found files under currentpath add them the queue directory 
WITHOUT_CLASSIFICATION	 full test 
WITHOUT_CLASSIFICATION	 update location 
WITHOUT_CLASSIFICATION	 there could several big table input files mapping the same small input file find that one with the lowest bucket 
WITHOUT_CLASSIFICATION	 need evaluate just forward 
WITHOUT_CLASSIFICATION	 this class captures the information about conjunct the where clause the subquery for equality predicate capture for each side the ast the type expression basically what columns are referenced for expressions that refer the parent captures the parents columninfo case outer aggregation expressions need this introduce new mapping the outerquery rowresolver join condition must use qualified column references generate new name for the aggr expression and use the joining condition for having exists select from where minrz where the expression minrz from the outer query give this expression new name like rgbysqcol and use the join condition rgbysqcol 
WITHOUT_CLASSIFICATION	 format 
WITHOUT_CLASSIFICATION	 the form partition 
WITHOUT_CLASSIFICATION	 there are async requests satisfy them first 
WITHOUT_CLASSIFICATION	 order 
WITHOUT_CLASSIFICATION	 expect the correct ois 
WITHOUT_CLASSIFICATION	 assign repeated value index over and over 
WITHOUT_CLASSIFICATION	 hasresultset 
WITHOUT_CLASSIFICATION	 only copy data values entry not null the string value position undefined the position value null 
WITHOUT_CLASSIFICATION	 clear out any parents reducer the root 
WITHOUT_CLASSIFICATION	 turn off skew additional job required anyway for grouping sets 
WITHOUT_CLASSIFICATION	 with hive this should use static parts and thus not need 
WITHOUT_CLASSIFICATION	 the call ats appears block indefinitely blocking the ats thread while the hook continues submit work the executorservice with each query over time the queued items can cause oom the hookcontext seems contain some items which use lot memory prevent this situation creating executor with bounded capacity 
WITHOUT_CLASSIFICATION	 only ask for the views 
WITHOUT_CLASSIFICATION	 through each target column generate the lineage edges 
WITHOUT_CLASSIFICATION	 necessary copy the big table key into the overflow batchs small table result area 
WITHOUT_CLASSIFICATION	 always include headers since they contain nonvectorized objects too 
WITHOUT_CLASSIFICATION	 collect information about cte there any the base table cte should masked the cte itself should not masked the references the following main query 
WITHOUT_CLASSIFICATION	 set one the partitions skipped that command created for every other one 
WITHOUT_CLASSIFICATION	 may have missed the start the vertex due the seconds interval 
WITHOUT_CLASSIFICATION	 register not fire the rule again 
WITHOUT_CLASSIFICATION	 each qfile may include most one include exclude directive qfile contains include directive and hadoopver does not appear the list versions include then the qfile skipped qfile contains exclude directive and hadoopver listed the list versions exclude then the qfile skipped otherwise the qfile included 
WITHOUT_CLASSIFICATION	 stub actions 
WITHOUT_CLASSIFICATION	 the row matches skewed column names 
WITHOUT_CLASSIFICATION	 translate grouping set col bitset 
WITHOUT_CLASSIFICATION	 append the trailing path string any 
WITHOUT_CLASSIFICATION	 this function not deterministic function but runtime constant the return value constant within query but can different between queries 
WITHOUT_CLASSIFICATION	 the length tblprops only keep the rest 
WITHOUT_CLASSIFICATION	 aggregate operator 
WITHOUT_CLASSIFICATION	 windows that are unbounded following dont benefit from streaming 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 estimation larger than max 
WITHOUT_CLASSIFICATION	 lintstring 
WITHOUT_CLASSIFICATION	 all partitions with should have columns 
WITHOUT_CLASSIFICATION	 only contain multisourced because multisourced cannot hashed direct readable 
WITHOUT_CLASSIFICATION	 phaseresult false return 
WITHOUT_CLASSIFICATION	 the partition directory 
WITHOUT_CLASSIFICATION	 otherwise have wait until after the maskingfiltering step 
WITHOUT_CLASSIFICATION	 get map operator and initialize 
WITHOUT_CLASSIFICATION	 set appropriate acid readerswriters based the table properties 
WITHOUT_CLASSIFICATION	 add list 
WITHOUT_CLASSIFICATION	 not then create set hanging readers that sortmerge find the next smallest delete event ondemand caps the memory consumption someconst readers 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 triggers 
WITHOUT_CLASSIFICATION	 todo hive case abort request throw 
WITHOUT_CLASSIFICATION	 delete jar added using query 
WITHOUT_CLASSIFICATION	 aliases for java class names 
WITHOUT_CLASSIFICATION	 the union already initialized however the union walked from another input initunionplan idempotent 
WITHOUT_CLASSIFICATION	 future could check argcolvectornonulls and optimize these loops 
WITHOUT_CLASSIFICATION	 first violation for the session 
WITHOUT_CLASSIFICATION	 get custom path string 
WITHOUT_CLASSIFICATION	 value 
WITHOUT_CLASSIFICATION	 the basic idea similar unparsetranslator 
WITHOUT_CLASSIFICATION	 can ref cursor variable 
WITHOUT_CLASSIFICATION	 when txnid the lock 
WITHOUT_CLASSIFICATION	 the task cannot finish and slots are available then dont schedule also dont wait have task and just killed something schedule 
WITHOUT_CLASSIFICATION	 resolved task 
WITHOUT_CLASSIFICATION	 ignore safe does not exist 
WITHOUT_CLASSIFICATION	 check the character array has the character 
WITHOUT_CLASSIFICATION	 the head means the number bytes for register 
WITHOUT_CLASSIFICATION	 default executor when option specified 
WITHOUT_CLASSIFICATION	 step remove any tmp file doublecommitted output files 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 implement reloptrule 
WITHOUT_CLASSIFICATION	 invalidate cached aggregate stats 
WITHOUT_CLASSIFICATION	 currently partition spec can only static partition 
WITHOUT_CLASSIFICATION	 load using same dump with table should fail not empty 
WITHOUT_CLASSIFICATION	 case when need check that children not contain stateful functions they are not allowed 
WITHOUT_CLASSIFICATION	 regular insert export some deltas then import into new table 
WITHOUT_CLASSIFICATION	 note ptndesc can null empty for nonptn table 
WITHOUT_CLASSIFICATION	 construct sorted map partition dir partition descriptor ordering based patition dir map key assumption there mapping between partition dir and partition descriptor lists 
WITHOUT_CLASSIFICATION	 iterate thru the cols and load the batch 
WITHOUT_CLASSIFICATION	 there authorization anybody has administrator access 
WITHOUT_CLASSIFICATION	 this node was parsed while loading the definition another view being referenced the one being created and dont want track any expansions for the underlying view 
WITHOUT_CLASSIFICATION	 remove expression node descriptor and children for given predicate from mapping its already keys 
WITHOUT_CLASSIFICATION	 noop authentication 
WITHOUT_CLASSIFICATION	 now compact and see compaction still preserves the data correctness 
WITHOUT_CLASSIFICATION	 convert millis 
WITHOUT_CLASSIFICATION	 now look the current hive config value again avoiding getting defaults 
WITHOUT_CLASSIFICATION	 the new numreducer less than minreducer will not consider reducesinkoperator with this newnumreducer correlated reducesinkoperator 
WITHOUT_CLASSIFICATION	 add dummy aggregate stats object for the above parts partpart tab for col 
WITHOUT_CLASSIFICATION	 same also emit extra records from separate thread 
WITHOUT_CLASSIFICATION	 mode all just run 
WITHOUT_CLASSIFICATION	 case test with originals compactedbase insertdeltas deletedeltas exhaustive test 
WITHOUT_CLASSIFICATION	 check 
WITHOUT_CLASSIFICATION	 since were passing the object output the udtf directly the next 
WITHOUT_CLASSIFICATION	 which the minimum non value this case 
WITHOUT_CLASSIFICATION	 set pending false since scheduling about run any triggers this point will handled the next run new request may come right after this set false but before the actual scheduling this will handled this run but will cause immediate run after which harmless this mainly handle tryschedue request while the middle run since the event which triggered may not processed for all tasks the run 
WITHOUT_CLASSIFICATION	 handle sessions that are being destroyed users destroy implies return 
WITHOUT_CLASSIFICATION	 unless already installed all the cluster nodes well have 
WITHOUT_CLASSIFICATION	 repeated null permutations 
WITHOUT_CLASSIFICATION	 will also read the last row 
WITHOUT_CLASSIFICATION	 hivedefaultnull the system default value for null and empty string todo should allow user specify default partition hdfs file location 
WITHOUT_CLASSIFICATION	 wrap big catch throwable statement 
WITHOUT_CLASSIFICATION	 for inner joins push not null predicate the join sources for every non nullsafe predicate 
WITHOUT_CLASSIFICATION	 the default equals provided thrift compares the comments too for equality thus need compare the relevant fields here 
WITHOUT_CLASSIFICATION	 return node 
WITHOUT_CLASSIFICATION	 loginforead hash code for length 
WITHOUT_CLASSIFICATION	 need set the merge work that has been created part the dummy store walk merge work already exists for this merge join operator add the dummy store work the 
WITHOUT_CLASSIFICATION	 may produce filesplit that not orcsplit 
WITHOUT_CLASSIFICATION	 string can used write char and varchar when the caller takes responsibility for truncationpadding issues 
WITHOUT_CLASSIFICATION	 lead the whole partition not the iterator range 
WITHOUT_CLASSIFICATION	 for fullacid tables dont delete files for commands with overwrite create new basex there insert overwrite and load data overwrite 
WITHOUT_CLASSIFICATION	 add the privileges supported authorization mode 
WITHOUT_CLASSIFICATION	 construct the list columns that need projected 
WITHOUT_CLASSIFICATION	 this guaranteed positive because types only have children ids greater than their own 
WITHOUT_CLASSIFICATION	 properties file used configure logj 
WITHOUT_CLASSIFICATION	 srswwait lock are examining waiting this case keep looking its possible that something front blocking that the other locker hasnt checked yet and could lock well 
WITHOUT_CLASSIFICATION	 weve already locked the table the input dont relock the output 
WITHOUT_CLASSIFICATION	 the event already replayed then need replay again 
WITHOUT_CLASSIFICATION	 constant byte arrays 
WITHOUT_CLASSIFICATION	 the partition location already existed and may contain data lets try populate those statistics that dont require full scan the data 
WITHOUT_CLASSIFICATION	 earlier implementation have quoted boolean valuesso the new implementation should preserve this 
WITHOUT_CLASSIFICATION	 translated includes may superset writer includes due cache 
WITHOUT_CLASSIFICATION	 create table database specific location 
WITHOUT_CLASSIFICATION	 this the small table side 
WITHOUT_CLASSIFICATION	 verify that partitioned table partition property set worked 
WITHOUT_CLASSIFICATION	 check edge case where the does not allow one clause with single value 
WITHOUT_CLASSIFICATION	 this parent does not contain constant this position continue look other positions 
WITHOUT_CLASSIFICATION	 construct kerberostoken relies proxyuser configuration will the client making the request top the hss user accumulo will require proper proxyuser auth configs 
WITHOUT_CLASSIFICATION	 orc table restrict changing the file format can break schema evolution 
WITHOUT_CLASSIFICATION	 null last 
WITHOUT_CLASSIFICATION	 must the server uri added older version 
WITHOUT_CLASSIFICATION	 add columnstatistics for tbl metastore via objectstore 
WITHOUT_CLASSIFICATION	 add vector partition descriptor partition descriptor removing duplicate object the same vector partition descriptor has already been allocated share that object 
WITHOUT_CLASSIFICATION	 avoid processing the same config multiple times check marker 
WITHOUT_CLASSIFICATION	 for reads whatever sarg maybe applicable base its not applicable deletedelta since has user columns for compaction there never sarg 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqltime javautilcalendar 
WITHOUT_CLASSIFICATION	 singlecolumn long specific imports 
WITHOUT_CLASSIFICATION	 find the location the table 
WITHOUT_CLASSIFICATION	 sindouble 
WITHOUT_CLASSIFICATION	 pretend are 
WITHOUT_CLASSIFICATION	 logging disabled deny everything 
WITHOUT_CLASSIFICATION	 started with single drl assume there will consecutive missing blocks after the cache has inserted cache data also assume all the missing parts will represent one several column chunks since always cache column chunk boundaries 
WITHOUT_CLASSIFICATION	 order for this work hivesitexml must the classpath 
WITHOUT_CLASSIFICATION	 branch hit 
WITHOUT_CLASSIFICATION	 set 
WITHOUT_CLASSIFICATION	 over predicate only contains columns columnsmapped construct new predicate based mapping 
WITHOUT_CLASSIFICATION	 init keyfields 
WITHOUT_CLASSIFICATION	 will iterate through the children insert will traverse 
WITHOUT_CLASSIFICATION	 use byteswritable because supports comparable for our treemap 
WITHOUT_CLASSIFICATION	 renew the metastore since the cluster type unencrypted 
WITHOUT_CLASSIFICATION	 msb set then next qprime msb bits contains the value number zeroes msb set then number zeroes contained within pprime bits 
WITHOUT_CLASSIFICATION	 get the ndv 
WITHOUT_CLASSIFICATION	 the patterns orandeqop oreqop are not matched bail out 
WITHOUT_CLASSIFICATION	 todo shut down 
WITHOUT_CLASSIFICATION	 why cannot just use the exprnodeevaluator the column because the reduceside initialized based the rowoi the hivetable and not the the parent this operator the reduceside 
WITHOUT_CLASSIFICATION	 shut down all the servers 
WITHOUT_CLASSIFICATION	 multiple udfs with the same max type unless find lower one well give 
WITHOUT_CLASSIFICATION	 type intervalyearmonth longcolumnvector storing months 
WITHOUT_CLASSIFICATION	 create partitioned table event 
WITHOUT_CLASSIFICATION	 parse command line 
WITHOUT_CLASSIFICATION	 schema the mapreduce value object this heterogeneous 
WITHOUT_CLASSIFICATION	 that guaranteed fit any maximum allocation 
WITHOUT_CLASSIFICATION	 sort ascending resource nulls first 
WITHOUT_CLASSIFICATION	 ngram estimator object 
WITHOUT_CLASSIFICATION	 convert children aggparameters 
WITHOUT_CLASSIFICATION	 correlgetcondition was here however correlate was updated 
WITHOUT_CLASSIFICATION	 resourcetype 
WITHOUT_CLASSIFICATION	 lastanalyzed stored per column but thrift object has per multiple columns luckily nobody actually uses will set lowest value all columns for now 
WITHOUT_CLASSIFICATION	 copy clonetowork ensure rdd cache still works 
WITHOUT_CLASSIFICATION	 was false 
WITHOUT_CLASSIFICATION	 the output files filesink can merged they are either not being written table are being written table which not bucketed 
WITHOUT_CLASSIFICATION	 test need groupby shuffle 
WITHOUT_CLASSIFICATION	 pending task which not finishable 
WITHOUT_CLASSIFICATION	 dummy vertex treated branch join operator 
WITHOUT_CLASSIFICATION	 try parse where there millisecond part input expected return 
WITHOUT_CLASSIFICATION	 checks resource has uploaded hdfs for yarncluster mode 
WITHOUT_CLASSIFICATION	 case find rows which belong write ids that are not valid 
WITHOUT_CLASSIFICATION	 total size the inputs 
WITHOUT_CLASSIFICATION	 connection properties 
WITHOUT_CLASSIFICATION	 creating new connection expensive well reuse this object 
WITHOUT_CLASSIFICATION	 test both inputs repeating 
WITHOUT_CLASSIFICATION	 add back the nonexpired session need notify are the only ones waiting 
WITHOUT_CLASSIFICATION	 the state has changed between this and previous check within this method the failed update was rendered irrelevant just exit 
WITHOUT_CLASSIFICATION	 should ignore the failure 
WITHOUT_CLASSIFICATION	 selfdescribing input format will convert its data the table schema there will vectormapoperator conversion needed 
WITHOUT_CLASSIFICATION	 then scale back 
WITHOUT_CLASSIFICATION	 convert rexnode exprnodedesc 
WITHOUT_CLASSIFICATION	 use the object pool rather than creating new object 
WITHOUT_CLASSIFICATION	 the response will have one entry per table and hence get only one validwriteidlist 
WITHOUT_CLASSIFICATION	 can only happens wzcr for single input buffer 
WITHOUT_CLASSIFICATION	 these members have information for assigning row column objects into the vectorizedrowbatch columns say target because when there conversion the data type being converted the source 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 invalid inflation factor 
WITHOUT_CLASSIFICATION	 compare the field names using ignorecase semantics 
WITHOUT_CLASSIFICATION	 run with checkindex and save the output file can checked 
WITHOUT_CLASSIFICATION	 txntowriteids 
WITHOUT_CLASSIFICATION	 static class iterator 
WITHOUT_CLASSIFICATION	 for efficiency alpha multiplied 
WITHOUT_CLASSIFICATION	 there are more cores use the number cores 
WITHOUT_CLASSIFICATION	 data needs deletion check trash may skipped trash may skipped iff deletedata true obviously tbl external either user has specified purge from the commandline and not user has set the table autopurge 
WITHOUT_CLASSIFICATION	 determine who run 
WITHOUT_CLASSIFICATION	 upgrade from schema and revalidate 
WITHOUT_CLASSIFICATION	 value based compare remove first 
WITHOUT_CLASSIFICATION	 make sure both buckets are not empty 
WITHOUT_CLASSIFICATION	 field contains fieldtype which turn contains type 
WITHOUT_CLASSIFICATION	 bootstrap dump shouldnt fail the table droppedrenamed while dumping just log debug message and skip 
WITHOUT_CLASSIFICATION	 tables and tables inside view otherwise calcite will treat them the same 
WITHOUT_CLASSIFICATION	 output bucketed 
WITHOUT_CLASSIFICATION	 present only the qltest directory 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 sql indexed druid indexed 
WITHOUT_CLASSIFICATION	 use the tez grouper combine splits once per bucket 
WITHOUT_CLASSIFICATION	 matter statsgenerated user task all need recalculate the stats user alter table update statistics task from some sql operation which could collect and compute stats 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 missing fields 
WITHOUT_CLASSIFICATION	 pushes node the stack 
WITHOUT_CLASSIFICATION	 ensure metatore sitexml does not get override this 
WITHOUT_CLASSIFICATION	 create delta directory 
WITHOUT_CLASSIFICATION	 heartbeats can only sent for open transactions there race between committingaborting transaction and heartbeat example heartbeat sent for committed txn exception will thrown similarly dont send heartbeat metastore server might abort txn for missed heartbeat right before commit txn call 
WITHOUT_CLASSIFICATION	 string representation folding constant 
WITHOUT_CLASSIFICATION	 the length the scratch byte array that needs passed bigintegerbytes etc 
WITHOUT_CLASSIFICATION	 nothing its not partitioned table 
WITHOUT_CLASSIFICATION	 division 
WITHOUT_CLASSIFICATION	 changes the owner user and verify the change 
WITHOUT_CLASSIFICATION	 startrowoffset 
WITHOUT_CLASSIFICATION	 the conf does not define any transactional properties the parseint should receive value which will set default type and return that 
WITHOUT_CLASSIFICATION	 construct object above type 
WITHOUT_CLASSIFICATION	 create the httphttps url jdbc driver will set https url ssl enabled otherwise http 
WITHOUT_CLASSIFICATION	 the split ends within and would read the last row this slice exact match 
WITHOUT_CLASSIFICATION	 spilled batchindex batchindex length length 
WITHOUT_CLASSIFICATION	 for whatever reason reserve failed 
WITHOUT_CLASSIFICATION	 error couldnt find the task lastsetguaranteed does not change the logic here does not account for one special case have updated the task but the response was lost and have received network error the state could inconsistent making 
WITHOUT_CLASSIFICATION	 output format string not supported anymore warn user deprecation 
WITHOUT_CLASSIFICATION	 this test the parameter value denotes the method which needs throw error 
WITHOUT_CLASSIFICATION	 one call init one called here 
WITHOUT_CLASSIFICATION	 cint cboolean cdouble cstring carrayint 
WITHOUT_CLASSIFICATION	 nothing handle rud add another status 
WITHOUT_CLASSIFICATION	 try fixing this should result new fixed file 
WITHOUT_CLASSIFICATION	 for permanent functions check for any resources from local filesystem 
WITHOUT_CLASSIFICATION	 check ckpt property set tablepartition after bootstrap load 
WITHOUT_CLASSIFICATION	 call the different round flavor 
WITHOUT_CLASSIFICATION	 test the idempotent behavior create function 
WITHOUT_CLASSIFICATION	 setting success false make sure that the listener fails rollback happens 
WITHOUT_CLASSIFICATION	 only create mapjoin the user explicitly gave join without mapjoin hint 
WITHOUT_CLASSIFICATION	 context for list bucketing 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader int 
WITHOUT_CLASSIFICATION	 this only responds 
WITHOUT_CLASSIFICATION	 verify the column name 
WITHOUT_CLASSIFICATION	 plumb the kryomessagecodec instance through the constructors 
WITHOUT_CLASSIFICATION	 range starts here 
WITHOUT_CLASSIFICATION	 the lateral view forward operator has children select and selectcols for the udtf operator the child index the select because thats the way that the dag was constructed only want get the predicates from the select 
WITHOUT_CLASSIFICATION	 materializationtime 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 throw hiveexception for other than rcfile and orcfile 
WITHOUT_CLASSIFICATION	 unparsetranslator 
WITHOUT_CLASSIFICATION	 and return the ones which have marked column 
WITHOUT_CLASSIFICATION	 this project will what the old input maps 
WITHOUT_CLASSIFICATION	 return defaultname selexpr not simple xxyyzz 
WITHOUT_CLASSIFICATION	 table cache not yet prewarmed add this set which the prewarm thread can check that the prewarm thread does not add back 
WITHOUT_CLASSIFICATION	 joda parsing only supports millisecond precision 
WITHOUT_CLASSIFICATION	 use exact byte array which might generate array out bounds 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 check the other side the join using the dynamiclistcontext 
WITHOUT_CLASSIFICATION	 assume this hashtable loaded only when tez enabled 
WITHOUT_CLASSIFICATION	 first comparison unsigned 
WITHOUT_CLASSIFICATION	 then master commits everything goes well 
WITHOUT_CLASSIFICATION	 join groupby distinct lateral view subq ctas insert not analyze command and single sourced 
WITHOUT_CLASSIFICATION	 dont cache the filesystem object for now tez closes and cache will fix all that 
WITHOUT_CLASSIFICATION	 check mapred 
WITHOUT_CLASSIFICATION	 test basic case 
WITHOUT_CLASSIFICATION	 make sure the functioninfo listed persistent rather than temporary 
WITHOUT_CLASSIFICATION	 started initialize context for new batch 
WITHOUT_CLASSIFICATION	 operation 
WITHOUT_CLASSIFICATION	 dont need add this new entry since theres already overlapping one 
WITHOUT_CLASSIFICATION	 there are several hive relnode types which not have their own visit method defined the hiverelshuttle interface which need handled appropriately here per jcamachorodriguez should not encounter during these checks need add those here 
WITHOUT_CLASSIFICATION	 todo make expr traversal recursive extend traverse inside elements dnfcnf extract more deterministic pieces out 
WITHOUT_CLASSIFICATION	 utility methods used store pairs ints long 
WITHOUT_CLASSIFICATION	 testspecific 
WITHOUT_CLASSIFICATION	 instantiate the valueprocessor based the input type 
WITHOUT_CLASSIFICATION	 myenumset 
WITHOUT_CLASSIFICATION	 test dependent getting new buffer within 
WITHOUT_CLASSIFICATION	 key full table name string format dbnametablename 
WITHOUT_CLASSIFICATION	 the rest optimizations 
WITHOUT_CLASSIFICATION	 only used for materialized views only used for materialized views only used for materialized views only used for materialized views only used for materialized views 
WITHOUT_CLASSIFICATION	 with bucketed target table union all not removed ekoifman tree ext orcacidversion delta bucket orcacidversion delta bucket directories files 
WITHOUT_CLASSIFICATION	 interfere with the view creation skip the rest this method 
WITHOUT_CLASSIFICATION	 check all the arguments 
WITHOUT_CLASSIFICATION	 field find record identifier field bucket record for inspecting record for inspecting bucket 
WITHOUT_CLASSIFICATION	 only first second operator contains dpp pruning 
WITHOUT_CLASSIFICATION	 closing the chunked output stream early gives error 
WITHOUT_CLASSIFICATION	 since dont clone jobconf per alias 
WITHOUT_CLASSIFICATION	 todo allocate work remove the temporary files and make that dependent the redtask 
WITHOUT_CLASSIFICATION	 evaluate 
WITHOUT_CLASSIFICATION	 cannot have scalar scalar 
WITHOUT_CLASSIFICATION	 common repeated join result processing 
WITHOUT_CLASSIFICATION	 may have been created 
WITHOUT_CLASSIFICATION	 check for this pattern the pattern matching could simplified rules can applied during decorrelation correlaterelleft correlation condition true leftinputrel aggregate groupby singlevalue projecta may reference covar 
WITHOUT_CLASSIFICATION	 not test mode then create the appender 
WITHOUT_CLASSIFICATION	 always validate acls 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 for backward compatibility fieldnames can also integer strings 
WITHOUT_CLASSIFICATION	 replace void type with string when the output temp table local files void type can generated under the query select null from insert overwrite local directory abc select null from where there column type which the null value should converted 
WITHOUT_CLASSIFICATION	 string pathnames the path separator the file separator directory 
WITHOUT_CLASSIFICATION	 test null propagation 
WITHOUT_CLASSIFICATION	 droptabledesc todo this currently used for both drop table and drop partitions 
WITHOUT_CLASSIFICATION	 make sure reduce task environment points 
WITHOUT_CLASSIFICATION	 need some initial values case user dont call initialize 
WITHOUT_CLASSIFICATION	 config settings 
WITHOUT_CLASSIFICATION	 such that the aggregation expressions need handled the windowing ptf invoke this function clear the aggexprs the dest 
WITHOUT_CLASSIFICATION	 add the path the list input paths 
WITHOUT_CLASSIFICATION	 get all the stuff for dont emptylist check expect partitions have sds 
WITHOUT_CLASSIFICATION	 see need fetch default constraints from metastore 
WITHOUT_CLASSIFICATION	 the alias modified subqa and subqa from identify the right subquery 
WITHOUT_CLASSIFICATION	 scale downup the column statistics based the changes number rows from each parent for there are parents for join operator with parent having rows and parent having rows now the new number rows after applying join rule then the column stats for columns from parent should scaled down and stats for columns from parent should scaled down 
WITHOUT_CLASSIFICATION	 check the stats 
WITHOUT_CLASSIFICATION	 any tablepartition updated then update repl state object 
WITHOUT_CLASSIFICATION	 make sure the schemas both sides are the same 
WITHOUT_CLASSIFICATION	 verify create table only table should created retry 
WITHOUT_CLASSIFICATION	 signature for wrapped loader see comments 
WITHOUT_CLASSIFICATION	 propertiesfile contains the spark keys that are meant for sparkconf object 
WITHOUT_CLASSIFICATION	 check this user has necessary privileges reqprivs this object 
WITHOUT_CLASSIFICATION	 will requeue and not kill the queries that are not running yet 
WITHOUT_CLASSIFICATION	 update the create table descriptor with the resulting schema 
WITHOUT_CLASSIFICATION	 optimize inner join keys small table results 
WITHOUT_CLASSIFICATION	 the middle and lowest longwords highest digit number 
WITHOUT_CLASSIFICATION	 note this code would invalid for transactional tables any kind 
WITHOUT_CLASSIFICATION	 otherwise fall through and process the what saw before possible trailing blanks 
WITHOUT_CLASSIFICATION	 use hive type name 
WITHOUT_CLASSIFICATION	 whether session running silent mode not 
WITHOUT_CLASSIFICATION	 optional int purgedmemorybytes 
WITHOUT_CLASSIFICATION	 partition and order 
WITHOUT_CLASSIFICATION	 mixed source all types 
WITHOUT_CLASSIFICATION	 not boolean column return half the number rows 
WITHOUT_CLASSIFICATION	 fill coltype 
WITHOUT_CLASSIFICATION	 the column positions the operator should like this nonpartition columnsstatic partition columnsdynamic partition columns exprnodecolumndesc exprnodecolumndesc from input generate itself from input 
WITHOUT_CLASSIFICATION	 keep alive enabled not close the connection 
WITHOUT_CLASSIFICATION	 this our row test expressions 
WITHOUT_CLASSIFICATION	 buffer the top the heap 
WITHOUT_CLASSIFICATION	 make the new join rel 
WITHOUT_CLASSIFICATION	 annotate the aggregate operator with this info 
WITHOUT_CLASSIFICATION	 this table needs converted crud acid 
WITHOUT_CLASSIFICATION	 should make inf 
WITHOUT_CLASSIFICATION	 get create context object create have clean later well 
WITHOUT_CLASSIFICATION	 disable feature 
WITHOUT_CLASSIFICATION	 set true default only actively set the multiple key case support outer join 
WITHOUT_CLASSIFICATION	 add new entry for this table 
WITHOUT_CLASSIFICATION	 flush current group batch last batch group 
WITHOUT_CLASSIFICATION	 infer any column can primary key based column statistics 
WITHOUT_CLASSIFICATION	 value 
WITHOUT_CLASSIFICATION	 first the cross join 
WITHOUT_CLASSIFICATION	 keys 
WITHOUT_CLASSIFICATION	 partition cant have this name 
WITHOUT_CLASSIFICATION	 this covers backward compat cases where this prop may have been set already 
WITHOUT_CLASSIFICATION	 this the first time the table being initialized transactionaltrue any valid value can set for the 
WITHOUT_CLASSIFICATION	 expand all supported privileges 
WITHOUT_CLASSIFICATION	 normal column also string 
WITHOUT_CLASSIFICATION	 charxvarcharx types 
WITHOUT_CLASSIFICATION	 merge and sort result 
WITHOUT_CLASSIFICATION	 check isshutdown opportunistically its never unset 
WITHOUT_CLASSIFICATION	 query per mbean attribute 
WITHOUT_CLASSIFICATION	 doesnt make sense have both and make sure one matches the other 
WITHOUT_CLASSIFICATION	 build regular expression for operator rule 
WITHOUT_CLASSIFICATION	 first check should not have repeats results 
WITHOUT_CLASSIFICATION	 finally connect the union work with work 
WITHOUT_CLASSIFICATION	 get compatible taskid for bucketname 
WITHOUT_CLASSIFICATION	 have the optional fourth parameter make sure its also integer 
WITHOUT_CLASSIFICATION	 empty value 
WITHOUT_CLASSIFICATION	 base this hiveoperation instead this and ddlnolock peppered all over the code seems much cleaner each stmt identified particular hiveoperation which think makes sense everywhere this however would problematic for merge 
WITHOUT_CLASSIFICATION	 not flattened struct need unflatten 
WITHOUT_CLASSIFICATION	 redo createtableview analysis because its not part dophase 
WITHOUT_CLASSIFICATION	 explain type 
WITHOUT_CLASSIFICATION	 want make sure this runs low priority the background 
WITHOUT_CLASSIFICATION	 set input the join that should omitted the output 
WITHOUT_CLASSIFICATION	 for the tab case add all the columns the fieldlist from the input schema 
WITHOUT_CLASSIFICATION	 note hadoop onwards includes class that usable asis however since have work multitude hadoop versions including very old ones either duplicate their code here not support xsrffilter older versions hadoop duplicate minimize evilugh see hadoop for details what this doing this method should never called hadoop available 
WITHOUT_CLASSIFICATION	 more rows 
WITHOUT_CLASSIFICATION	 found dynamic partition pruning operator 
WITHOUT_CLASSIFICATION	 light results from union queries need aware that subdirectories can exist the partition directory want ignore these subdirectories and promote merged files the partition directory 
WITHOUT_CLASSIFICATION	 production bool 
WITHOUT_CLASSIFICATION	 template classname valuetype varianceformula descriptionname 
WITHOUT_CLASSIFICATION	 time part the timestamp should not skipped 
WITHOUT_CLASSIFICATION	 orcinputformat will get mock from filesystemget add global files 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the options principalkeypad not work with proxyuser sparksubmitsh see hive spark spark hive could only support doas delegation token renewal but not both since doas more common case both are needed choose favor doas when doas enabled use kinit command otherwise pass the principalkeypad spark support the token renewal for 
WITHOUT_CLASSIFICATION	 form truncated boolean include array for our vectorrow deserializers 
WITHOUT_CLASSIFICATION	 this not hash groupby return 
WITHOUT_CLASSIFICATION	 the parents have already been created create the last child only 
WITHOUT_CLASSIFICATION	 check that the hcat result valid and has valid json 
WITHOUT_CLASSIFICATION	 lock response acquired can create the heartbeater 
WITHOUT_CLASSIFICATION	 has parents guaranteed 
WITHOUT_CLASSIFICATION	 randomuuid slow since its cryptographically secure only first query will take time 
WITHOUT_CLASSIFICATION	 drop table after dump 
WITHOUT_CLASSIFICATION	 get the sorted children expr strings 
WITHOUT_CLASSIFICATION	 disallow update and delete nonacid tables 
WITHOUT_CLASSIFICATION	 rest the data serialized long values for the bitset which are supposed bitwiseored 
WITHOUT_CLASSIFICATION	 for vertex group all outputs use the same keyclass valclass and partitioner pick any one source vertex figure out the edge configuration 
WITHOUT_CLASSIFICATION	 allow use external byte for efficiency 
WITHOUT_CLASSIFICATION	 the one join column for this specialized class 
WITHOUT_CLASSIFICATION	 use fsshell change group permissions and extended acls recursively 
WITHOUT_CLASSIFICATION	 need convert the jobcontext into jobconf jobconf hive jobcontext hcat 
WITHOUT_CLASSIFICATION	 next partition next partition nothing 
WITHOUT_CLASSIFICATION	 optimize the query select countdistinct keys from where bucketized and sorted partial aggregation can done the mappers this scenario 
WITHOUT_CLASSIFICATION	 check the properties expected hive client without metastore 
WITHOUT_CLASSIFICATION	 test with empty array 
WITHOUT_CLASSIFICATION	 unknown 
WITHOUT_CLASSIFICATION	 for now use calcite default formulas for propagating ndvs the query tree 
WITHOUT_CLASSIFICATION	 the output stream serialized objects 
WITHOUT_CLASSIFICATION	 this avoid getting notified low memory too often and flushing too often 
WITHOUT_CLASSIFICATION	 store the byte every eight elements 
WITHOUT_CLASSIFICATION	 create data buffers for these columns can copy strings into those columns value 
WITHOUT_CLASSIFICATION	 make sure pad the right amount spaces vallength terms code points while stringutilsrpad based the number java chars 
WITHOUT_CLASSIFICATION	 nothing there user connection configuration file beelinesitexml the path 
WITHOUT_CLASSIFICATION	 also convert tofrom binarysortable representation 
WITHOUT_CLASSIFICATION	 there are nulls either input vector 
WITHOUT_CLASSIFICATION	 note udaf not included exprcolmap 
WITHOUT_CLASSIFICATION	 pairwise columnhasnulls columnisrepeating columnhasnulls columnisrepeating 
WITHOUT_CLASSIFICATION	 this can happen for numbers less than for bdprecision bdscale this case well set the type have the same precision the scale 
WITHOUT_CLASSIFICATION	 same query within dag priority lower values indicate higher priority 
WITHOUT_CLASSIFICATION	 update twice accurately detect cache dirty not 
WITHOUT_CLASSIFICATION	 add constant struct field names references overhead 
WITHOUT_CLASSIFICATION	 divide that can have more reducers 
WITHOUT_CLASSIFICATION	 parse until key separator currentlevel 
WITHOUT_CLASSIFICATION	 rethrow the sqlexception 
WITHOUT_CLASSIFICATION	 create nonexistent path for row results 
WITHOUT_CLASSIFICATION	 introduce and before 
WITHOUT_CLASSIFICATION	 sanity check make sure there alias conflict after merge 
WITHOUT_CLASSIFICATION	 quote the database requires 
WITHOUT_CLASSIFICATION	 left side 
WITHOUT_CLASSIFICATION	 fail heartbeater that can get runtimeexception from the query more specifically its the original ioexception thrown either mrs tezs progress monitoring loop 
WITHOUT_CLASSIFICATION	 print only the errors the operation log and the query results 
WITHOUT_CLASSIFICATION	 use the input tablescanoperator case there mapside reshape input the parent reducesinkoperator ptfoperator use its output 
WITHOUT_CLASSIFICATION	 hive 
WITHOUT_CLASSIFICATION	 the threshold should less than bytes for the reason divide sparse mode after serialization the entriesin sparse map are compressed and delta encoded varints the worst case size varints are bytes hence entries 
WITHOUT_CLASSIFICATION	 this called from replication task the user the user who has fired the repl command this required for standalone metastore authentication 
WITHOUT_CLASSIFICATION	 columnscalar 
WITHOUT_CLASSIFICATION	 mapping from column name default value 
WITHOUT_CLASSIFICATION	 only expect here because well get whichever the partitions published its stats last 
WITHOUT_CLASSIFICATION	 each validwriteidlist separated with and each one maps one table 
WITHOUT_CLASSIFICATION	 first set basic stats true 
WITHOUT_CLASSIFICATION	 memory pressure 
WITHOUT_CLASSIFICATION	 with uncompressed streams know are done earlier 
WITHOUT_CLASSIFICATION	 return the current blocks key length 
WITHOUT_CLASSIFICATION	 always set the semijoin optimization victim 
WITHOUT_CLASSIFICATION	 locations for each the storage types 
WITHOUT_CLASSIFICATION	 case when there are changes multiple table properties 
WITHOUT_CLASSIFICATION	 the default pool not disabled override the size with the specified parallelism 
WITHOUT_CLASSIFICATION	 always keep transactional tables managed tables 
WITHOUT_CLASSIFICATION	 basic algorithm determine rounding part meets bankers rounding rules for rounding scale away fractional digits present rounding clear integer rounding portion and add 
WITHOUT_CLASSIFICATION	 test http mode 
WITHOUT_CLASSIFICATION	 add self 
WITHOUT_CLASSIFICATION	 write out the buffer into file add beeline commands for autocommit and close 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 set bitvectorindex 
WITHOUT_CLASSIFICATION	 reset the driver 
WITHOUT_CLASSIFICATION	 use the defalut methods for next the child class 
WITHOUT_CLASSIFICATION	 creationmetadata 
WITHOUT_CLASSIFICATION	 use its conversion ability 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlsqlxml 
WITHOUT_CLASSIFICATION	 add new token shared store need persist expiration along with password 
WITHOUT_CLASSIFICATION	 now generate the matchs single small table values will put into the big table batch and come back matchs any multiple small table value results will into 
WITHOUT_CLASSIFICATION	 the only time this condition should false the case dynamic partitioning 
WITHOUT_CLASSIFICATION	 test null left 
WITHOUT_CLASSIFICATION	 sparsesparse overload dense 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 verify batch size 
WITHOUT_CLASSIFICATION	 the new table rhs table 
WITHOUT_CLASSIFICATION	 restrict instantiation 
WITHOUT_CLASSIFICATION	 the task hasnt started inform about fragment completion immediately its possible for the callable never run 
WITHOUT_CLASSIFICATION	 this column that dont want not included are done 
WITHOUT_CLASSIFICATION	 hivehistory object 
WITHOUT_CLASSIFICATION	 dont support using multiple chars delimiters within complex types 
WITHOUT_CLASSIFICATION	 the ctor should throw the error 
WITHOUT_CLASSIFICATION	 find the positionsorder the sorted columns the table corresponding 
WITHOUT_CLASSIFICATION	 iterate through the line and invoke the addcmdpart method whenever the delimiter seen that not inside 
WITHOUT_CLASSIFICATION	 the number integer digits the decimal when the integer portion zero this 
WITHOUT_CLASSIFICATION	 cancel existing watches 
WITHOUT_CLASSIFICATION	 check compaction set for this table 
WITHOUT_CLASSIFICATION	 add the value the arraylist 
WITHOUT_CLASSIFICATION	 include state for cached columns 
WITHOUT_CLASSIFICATION	 note keep the typeinfo and arrays 
WITHOUT_CLASSIFICATION	 database url 
WITHOUT_CLASSIFICATION	 but note that any sql error will also result return false 
WITHOUT_CLASSIFICATION	 the most correct behavior throw only the request tries enable the readonly mode 
WITHOUT_CLASSIFICATION	 start third batch but dont close 
WITHOUT_CLASSIFICATION	 should generate inf 
WITHOUT_CLASSIFICATION	 indicate whether the pages should thrown away not 
WITHOUT_CLASSIFICATION	 blobstore section 
WITHOUT_CLASSIFICATION	 passing charvarchar arguments should prefer the version evaluate with text args 
WITHOUT_CLASSIFICATION	 for predicate check candidate for pushing down limit optimization the expression must the form rankfn constant 
WITHOUT_CLASSIFICATION	 when doing updates and deletes always want sort the rowid because the acid reader will expect this sort order when doing reads ignore whatever comes from the table and enforce this sort order instead 
WITHOUT_CLASSIFICATION	 stats from the record writer and store the previous fsp that cached 
WITHOUT_CLASSIFICATION	 keep track subqueries which are scalar correlated and contains aggregate subquery expression this will later special cased subquery remove rule for correlated scalar queries with aggregate have take care the case where inner aggregate happens empty result 
WITHOUT_CLASSIFICATION	 not need evaluate the input row for this parent can just forward the child this muxoperator 
WITHOUT_CLASSIFICATION	 general case can have unlimited branches currently only handle either branch 
WITHOUT_CLASSIFICATION	 check column order and types 
WITHOUT_CLASSIFICATION	 inputgeti since inputget the schema 
WITHOUT_CLASSIFICATION	 dynamicserde always writes out byteswritable 
WITHOUT_CLASSIFICATION	 nonjavadoc see int 
WITHOUT_CLASSIFICATION	 use destination tables location 
WITHOUT_CLASSIFICATION	 other format pattern should also work 
WITHOUT_CLASSIFICATION	 pass null complete batch 
WITHOUT_CLASSIFICATION	 partcol and nonpartcol replaced with partcol and true which will folded partcol this cannot done also for 
WITHOUT_CLASSIFICATION	 maybe someone removed the field probably ignore 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqltimestamp 
WITHOUT_CLASSIFICATION	 optional parameter 
WITHOUT_CLASSIFICATION	 read the template into string 
WITHOUT_CLASSIFICATION	 optional int timestamp 
WITHOUT_CLASSIFICATION	 get the input expression 
WITHOUT_CLASSIFICATION	 write with additional uncommitted ids should match 
WITHOUT_CLASSIFICATION	 join keys have difference sizes 
WITHOUT_CLASSIFICATION	 setup vectorized deserialization for the key and value 
WITHOUT_CLASSIFICATION	 fourth group 
WITHOUT_CLASSIFICATION	 partitioned table 
WITHOUT_CLASSIFICATION	 the deadline exception needs retry and thrown immediately 
WITHOUT_CLASSIFICATION	 check path conforms hives file name convention hive expects filenames specific format like but load data commands can let you add any files any partitionstables without renaming this can cause movetask remove files some cases where movetask assumes the files are are generated speculatively executed tasks example movetask thinks the following files are same partm partm assumes taskid and retains only large file supposedly generated speculative execution this can result data loss case concatenatemerging filter out files that does not match hives filename convention 
WITHOUT_CLASSIFICATION	 serde info 
WITHOUT_CLASSIFICATION	 was merge task local map reduce task nothing can inferred 
WITHOUT_CLASSIFICATION	 hang onto byte array for holding smaller byte values 
WITHOUT_CLASSIFICATION	 this particular test doesnt care which the lower pri tasks gets the duck 
WITHOUT_CLASSIFICATION	 have this reverse order that drop the materialized view first 
WITHOUT_CLASSIFICATION	 hivespark keys are passed down the remotedriver via conf 
WITHOUT_CLASSIFICATION	 private methods should never catch sqlexception and then throw metaexception the public methods depend sqlexception coming back they can detect and handle deadlocks private methods should only throw metaexception when they explicitly know theres logic error and they want throw past the public methods all public methods that write the database have check for deadlocks when sqlexception comes back and handle they see one this has done with the connection pooling mind this they should call checkretryable after rolling back the transaction and then they should catch retryexception and call themselves recursively see committxn for example 
WITHOUT_CLASSIFICATION	 override this for using extended fieldobject 
WITHOUT_CLASSIFICATION	 run given query and validate expecated result 
WITHOUT_CLASSIFICATION	 only update someone waiting for info have the info 
WITHOUT_CLASSIFICATION	 were going wait for the session abandoned 
WITHOUT_CLASSIFICATION	 inform the scheduler that this fragment has been killed the kill failed that means the task has already hit final condition 
WITHOUT_CLASSIFICATION	 sdsetbucketcolsnew arrayliststring 
WITHOUT_CLASSIFICATION	 found 
WITHOUT_CLASSIFICATION	 tokdropfunction identifier ifexists temp 
WITHOUT_CLASSIFICATION	 check have ckpt property set bootstrap dump location used and missing for tablepartition 
WITHOUT_CLASSIFICATION	 set values reference copy the data out and verify equality 
WITHOUT_CLASSIFICATION	 find free port 
WITHOUT_CLASSIFICATION	 move the query results the query cache directory 
WITHOUT_CLASSIFICATION	 partitioned table need get the old stats the partition and update the table stats based the old and new stats 
WITHOUT_CLASSIFICATION	 query column stats for column whose stats were updated the previous call 
WITHOUT_CLASSIFICATION	 move past key separator 
WITHOUT_CLASSIFICATION	 split for delta which filtered out entirely txns 
WITHOUT_CLASSIFICATION	 jump the end current line when multiple line query executed with parameter passed one line string separated with 
WITHOUT_CLASSIFICATION	 nothing done 
WITHOUT_CLASSIFICATION	 case analyze command 
WITHOUT_CLASSIFICATION	 this should never happen however want make sure propagate the exception 
WITHOUT_CLASSIFICATION	 verify table for key row and byte hash table hashmap 
WITHOUT_CLASSIFICATION	 list partitions that are required populated from processing each event 
WITHOUT_CLASSIFICATION	 buffer since that what used vectorserializerow serializewrite periodically flush this buffer disk 
WITHOUT_CLASSIFICATION	 lastevent 
WITHOUT_CLASSIFICATION	 udf which sleeps for some number simulate long running query 
WITHOUT_CLASSIFICATION	 there are paths from the operator previous lvj operator the same selectoperator selectoperator gets cols for udtf udtfoperator 
WITHOUT_CLASSIFICATION	 source file system list source paths 
WITHOUT_CLASSIFICATION	 nothing prune for this mapwork 
WITHOUT_CLASSIFICATION	 escaping happened are already done 
WITHOUT_CLASSIFICATION	 small table values are set null 
WITHOUT_CLASSIFICATION	 verify table for key long hash table hashset 
WITHOUT_CLASSIFICATION	 error the script itself likely caused incompatible change new functionality states added 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 not setting payload since the mrinput payload the same and can accessed 
WITHOUT_CLASSIFICATION	 low memory canary set and records after set canary exceeds threshold trigger flush 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 initialize all the dummy ops 
WITHOUT_CLASSIFICATION	 used datumwriter applications should not call 
WITHOUT_CLASSIFICATION	 case udtf selectoutputrr may null 
WITHOUT_CLASSIFICATION	 constructors 
WITHOUT_CLASSIFICATION	 tiemestamp 
WITHOUT_CLASSIFICATION	 maxtables 
WITHOUT_CLASSIFICATION	 keep track the principals which privileges have been checked for this object 
WITHOUT_CLASSIFICATION	 countdistinct countdistinct find the correct mapping 
WITHOUT_CLASSIFICATION	 query per mbean 
WITHOUT_CLASSIFICATION	 metalisteners 
WITHOUT_CLASSIFICATION	 not mess with table instance 
WITHOUT_CLASSIFICATION	 retrieve the tables from the metastore batches some databases like 
WITHOUT_CLASSIFICATION	 not assign the input value object the timestampvalues array element always copy value using set methods 
WITHOUT_CLASSIFICATION	 reduced 
WITHOUT_CLASSIFICATION	 map precision the number bytes needed for binary conversion 
WITHOUT_CLASSIFICATION	 hive cant handle select rank overorder sumcsumc from group but can handle select rank over order from select sumcsumc from group introduce project top this gby 
WITHOUT_CLASSIFICATION	 get the input table and make sure the keys match 
WITHOUT_CLASSIFICATION	 harprocessor regular operation 
WITHOUT_CLASSIFICATION	 creates job request object and sets execution environment creates thread pool execute job requests param requesttype job request type param config name used extract number concurrent requests serviced param config name used extract maximum time task can execute request param enablecanceltask flag indicate whether cancel the task when exception timeoutexception raised 
WITHOUT_CLASSIFICATION	 llapinputformat needs know the file schema decide schema evolution supported 
WITHOUT_CLASSIFICATION	 this required 
WITHOUT_CLASSIFICATION	 specific the multi group optimization 
WITHOUT_CLASSIFICATION	 not retrying when user explicitly stops the test 
WITHOUT_CLASSIFICATION	 event loads will behave similar table loads with one crucial difference precursor order strict and each event must processed after the previous one the way handle this strict order follows first start with taskchaintail which dummy noop task the head our event chain for each event process tell analyzetableload create tasks that use the taskchaintail dependency then collect all those tasks and introduce new barrier taskalso which depends all these tasks then this barrier task becomes our new taskchaintail thus get set tasks follows evtask evtask evtaskrootevtask evtask once this entire chain generated add evtaskroot roottasks execute the entire chain 
WITHOUT_CLASSIFICATION	 for tostring the time does not matter 
WITHOUT_CLASSIFICATION	 this only used the error code path 
WITHOUT_CLASSIFICATION	 open txn doesnt allocate writeid the entries for aborted and committed should retained 
WITHOUT_CLASSIFICATION	 repair metadata hms 
WITHOUT_CLASSIFICATION	 the time currentcompactions generated and now 
WITHOUT_CLASSIFICATION	 throw new 
WITHOUT_CLASSIFICATION	 instantiate the underlying output format since there way parametrize instance class 
WITHOUT_CLASSIFICATION	 writing delta dirs need make clone original options avoid polluting for 
WITHOUT_CLASSIFICATION	 latin small letter turned bytes 
WITHOUT_CLASSIFICATION	 connect using the jdbc url and userpass from conf 
WITHOUT_CLASSIFICATION	 convert logs rowset 
WITHOUT_CLASSIFICATION	 trigger scheduling run case theres some task which was waiting for this node 
WITHOUT_CLASSIFICATION	 the cookie based authentication not enabled the request does not have valid cookie use the kerberos password based authentication 
WITHOUT_CLASSIFICATION	 excluded overrides included 
WITHOUT_CLASSIFICATION	 the selected vector represents selected rows clone the selected vector 
WITHOUT_CLASSIFICATION	 not final since may change later due decimal 
WITHOUT_CLASSIFICATION	 reserialize the splits after grouping 
WITHOUT_CLASSIFICATION	 get all data out 
WITHOUT_CLASSIFICATION	 concern currently differentiate decimal columns their precision and scale 
WITHOUT_CLASSIFICATION	 first use tgetparameters prune the stats 
WITHOUT_CLASSIFICATION	 that case just pick one and spill 
WITHOUT_CLASSIFICATION	 this work now moved the parentwork thus should 
WITHOUT_CLASSIFICATION	 rrr 
WITHOUT_CLASSIFICATION	 add udfdata udf necessary 
WITHOUT_CLASSIFICATION	 kids reduce sink operator mapjoin operators merged into root task 
WITHOUT_CLASSIFICATION	 set filter expression 
WITHOUT_CLASSIFICATION	 catch throwables besteffort report job status back the client its rethrown that the executor can destroy the affected thread the jvm can die whatever would happen the throwable bubbled 
WITHOUT_CLASSIFICATION	 ignore this particular error expected 
WITHOUT_CLASSIFICATION	 null null 
WITHOUT_CLASSIFICATION	 merge with children predicates 
WITHOUT_CLASSIFICATION	 opereations none them are enforced hive right now 
WITHOUT_CLASSIFICATION	 statistics track allocations 
WITHOUT_CLASSIFICATION	 trigger bootstrap replication 
WITHOUT_CLASSIFICATION	 keytypeptr 
WITHOUT_CLASSIFICATION	 high word multiplier digit commad 
WITHOUT_CLASSIFICATION	 normally import trying create table partition that does not yet exist error condition however the case repl load possible that are trying create tasks create table inside that asofnow does not exist but there precursor task waiting that will create before this encountered thus instantiate defaults and not error out that case the above will change now since are going split replication load multiple execution tasks and hence could have created the database earlier which case the waitonprecursor will 
WITHOUT_CLASSIFICATION	 encode 
WITHOUT_CLASSIFICATION	 fix the nonprintable chars 
WITHOUT_CLASSIFICATION	 put the filter skewed column skewed keys 
WITHOUT_CLASSIFICATION	 pull the output schema out the taskattemptcontext 
WITHOUT_CLASSIFICATION	 generate backtrack select operator 
WITHOUT_CLASSIFICATION	 verify the filter correct and returns rows and contains columns and the contents match 
WITHOUT_CLASSIFICATION	 does the move task involve moving local file system 
WITHOUT_CLASSIFICATION	 llap only mode grace hash join will disabled later the llapdispatcher anyway since the presence grace hash join disables some native vectorization optimizations will disable the grace hash join now before vectorization done 
WITHOUT_CLASSIFICATION	 compile being called multiple times clear the old shutdownhook 
WITHOUT_CLASSIFICATION	 terminate 
WITHOUT_CLASSIFICATION	 attach the original predicate the table scan operator for index optimizations that require the pushed predicate before pcr later optimizations are applied 
WITHOUT_CLASSIFICATION	 retrieve all partitions from the table 
WITHOUT_CLASSIFICATION	 recordreader readerrows 
WITHOUT_CLASSIFICATION	 reassign new port just case one the services grabbed the last one 
WITHOUT_CLASSIFICATION	 test select rootcola from 
WITHOUT_CLASSIFICATION	 should have more rows 
WITHOUT_CLASSIFICATION	 not call startgroup operators below because are batching rows output batch and the semantics will not work superstartgroup 
WITHOUT_CLASSIFICATION	 listofneedfetchnext contains all tables that have joined data their candidatestorage and need clear candidate storage and promote their nextgroupstorage candidatestorage and fetch data until reach new group 
WITHOUT_CLASSIFICATION	 set the newlyconstructed ranges the current state 
WITHOUT_CLASSIFICATION	 make sure dont get stuck time however unlikely that 
WITHOUT_CLASSIFICATION	 however the above query contains dynamic partitions subq and subq have write directories and respectively the movetask that follows subq and subq tasks still moves the directory parent 
WITHOUT_CLASSIFICATION	 now update this record being worked this worker 
WITHOUT_CLASSIFICATION	 the numerator the tablesample clause 
WITHOUT_CLASSIFICATION	 also dynamic partitioning being used want set appropriate list columns for the columns dynamically specified these would partition keys too would also need removed from output schema and partcols 
WITHOUT_CLASSIFICATION	 newfunc 
WITHOUT_CLASSIFICATION	 check batch 
WITHOUT_CLASSIFICATION	 are unique keys for left 
WITHOUT_CLASSIFICATION	 acid off post upgrade you cant make any tables acid will throw 
WITHOUT_CLASSIFICATION	 may processed thread which ends executing before task 
WITHOUT_CLASSIFICATION	 replicate the incremental drops 
WITHOUT_CLASSIFICATION	 copy string value asis 
WITHOUT_CLASSIFICATION	 kerberos not enabled 
WITHOUT_CLASSIFICATION	 constant char projection 
WITHOUT_CLASSIFICATION	 only selects and filters are allowed 
WITHOUT_CLASSIFICATION	 are skipping calling checkoutputspecs for each partition can throw when more than one mapper writing partition see hcatalog also avoid contacting the namenode for each new fileoutputformat instance general this should for most fileoutputformat implementations but may become issue for cases when the method used perform other setup tasks 
WITHOUT_CLASSIFICATION	 use large capacity that doesnt require expansion yet 
WITHOUT_CLASSIFICATION	 destf existing directory replace true delete followed renamemv equivalent replace replace false rename actually move the src under dest dir destf existing file rename actually replace and not need delete the file first 
WITHOUT_CLASSIFICATION	 classloader parent can pollute the session see hive 
WITHOUT_CLASSIFICATION	 try reading table user should succeed 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 columnstatsaccurate params set correct value 
WITHOUT_CLASSIFICATION	 exceeds maxvalue 
WITHOUT_CLASSIFICATION	 ignore and use uuid instead 
WITHOUT_CLASSIFICATION	 its nondeterministic udf set unknown true 
WITHOUT_CLASSIFICATION	 ignore this expected 
WITHOUT_CLASSIFICATION	 need further down for select with all file sink script child since all cols are needed for these ops however one the children not file sink script still down 
WITHOUT_CLASSIFICATION	 need run cbo table ref virtual table not supported 
WITHOUT_CLASSIFICATION	 ignore the exception 
WITHOUT_CLASSIFICATION	 revert the projected columns back because batch can reused our parent operators 
WITHOUT_CLASSIFICATION	 start the protocol server after properly authenticating with daemon keytab 
WITHOUT_CLASSIFICATION	 create empty priv set 
WITHOUT_CLASSIFICATION	 make sure dont give out more than allowed due doublerounding artifacts 
WITHOUT_CLASSIFICATION	 the small table hash table for the native vectorized map join operator 
WITHOUT_CLASSIFICATION	 common leftsemi join result processing 
WITHOUT_CLASSIFICATION	 dont need this anymore 
WITHOUT_CLASSIFICATION	 modifier letter small bytes 
WITHOUT_CLASSIFICATION	 retries will done till decaying factor reduces decaying factor log base batchsize plus most significant bit batchsize plus will give the number expected calls 
WITHOUT_CLASSIFICATION	 nonjavadoc see int javalangstring 
WITHOUT_CLASSIFICATION	 matter loc the table location part location must directory 
WITHOUT_CLASSIFICATION	 traverse path filter projects get the tablescan case unique keys stop you reach project will handled the invocation the project case getting the base rowcount path keep going past project 
WITHOUT_CLASSIFICATION	 for partial and final objectinspectors for partial aggregations 
WITHOUT_CLASSIFICATION	 whole value copied including spaces 
WITHOUT_CLASSIFICATION	 the table not yet loaded cache 
WITHOUT_CLASSIFICATION	 this insert originalwriteid should set this transaction not will reset the following anyway 
WITHOUT_CLASSIFICATION	 varchar tests 
WITHOUT_CLASSIFICATION	 memoize decorator for 
WITHOUT_CLASSIFICATION	 traverse the aborted txns list starting first aborted txn index 
WITHOUT_CLASSIFICATION	 create thread pool with queue wait time execute the operation this will ensure that job requests are rejected there are already maximum number threads busy 
WITHOUT_CLASSIFICATION	 some parts the system cant handle rows with zero fields 
WITHOUT_CLASSIFICATION	 template classnameprefix returntype operandtype funcname operandcast resultcast 
WITHOUT_CLASSIFICATION	 this delimiter directive reset our delimiter 
WITHOUT_CLASSIFICATION	 preventing empty ctor from being callable 
WITHOUT_CLASSIFICATION	 may caused for other different errors but take our path readonly 
WITHOUT_CLASSIFICATION	 all qfiles handled via this 
WITHOUT_CLASSIFICATION	 wed like move this hivemetastore for any nonnative table but first need support storing null for location table 
WITHOUT_CLASSIFICATION	 initialize the cache 
WITHOUT_CLASSIFICATION	 nosasl 
WITHOUT_CLASSIFICATION	 fastislong returns false 
WITHOUT_CLASSIFICATION	 nothing here will throw exception the following block 
WITHOUT_CLASSIFICATION	 with many arguments 
WITHOUT_CLASSIFICATION	 try getobjectinspector 
WITHOUT_CLASSIFICATION	 the curraliasid and currtopop not valid any more 
WITHOUT_CLASSIFICATION	 see the structexp exp has atleast one expression with partition column with single table alias not bail out might have expressions containing only partitioning columns say where and are both partitioning columns 
WITHOUT_CLASSIFICATION	 with filesinks and buckets should see since data sorted rowid where tnxid the first component should see 
WITHOUT_CLASSIFICATION	 string classname thisgetclassgetname 
WITHOUT_CLASSIFICATION	 binary search find the closest bucket that should into bin should interpreted the bin shift right order accomodate result bin the range where means that the value greater than all the bins currently the histogram also possible that 
WITHOUT_CLASSIFICATION	 generate dummy preupgrade scripts with valid sql 
WITHOUT_CLASSIFICATION	 groupingsets are known mapreducer side but have real processing 
WITHOUT_CLASSIFICATION	 report size the extent possible 
WITHOUT_CLASSIFICATION	 comparing time not sufficient since two may created the same time which case inserting into treesetmap would break 
WITHOUT_CLASSIFICATION	 determine class 
WITHOUT_CLASSIFICATION	 descending sort based split size followed file name followed startposition 
WITHOUT_CLASSIFICATION	 optional string connectedvertexname 
WITHOUT_CLASSIFICATION	 the following data should changed 
WITHOUT_CLASSIFICATION	 ship additional artifacts for example for tez 
WITHOUT_CLASSIFICATION	 query has run capture the time 
WITHOUT_CLASSIFICATION	 hive pending rename after 
WITHOUT_CLASSIFICATION	 create fetch operator 
WITHOUT_CLASSIFICATION	 this value should into smallbuffer 
WITHOUT_CLASSIFICATION	 taskinfo instances for two different tasks will not the same only single instance should 
WITHOUT_CLASSIFICATION	 test that write can acquire after read 
WITHOUT_CLASSIFICATION	 make both scaling follows unscaledvalue significand scale scaletwoscaledown 
WITHOUT_CLASSIFICATION	 task requested host got host 
WITHOUT_CLASSIFICATION	 check data distribution buckets 
WITHOUT_CLASSIFICATION	 this class provides and implementation for case insensitive token checker for the lexical analysis part antlr converting the token stream into upper case the time when lexical rules are checked this class ensures that the lexical rules need just match the token with upper case letters opposed combination upper case and lower case characteres this purely used for matching lexical rules the actual token text stored the same way the user input without actually converting into upper case the token values are generated the consume function the super class antlrstringstream the function the lookahead funtion and purely used for matching lexical rules this also means that the grammar will only accept capitalized tokens case run from other tools like antlrworks which not have the implementation 
WITHOUT_CLASSIFICATION	 tablespec got from the query user specified which means the user didnt specify partitions their query 
WITHOUT_CLASSIFICATION	 this map should map columninfo 
WITHOUT_CLASSIFICATION	 couldnt find asterisk its not prefix 
WITHOUT_CLASSIFICATION	 state byte the first record 
WITHOUT_CLASSIFICATION	 encode schema with write out 
WITHOUT_CLASSIFICATION	 dont remove the operator for distincts 
WITHOUT_CLASSIFICATION	 and where the transaction has already been committed per snapshot taken 
WITHOUT_CLASSIFICATION	 now create delete delta that has rowids divisible but not this will produce 
WITHOUT_CLASSIFICATION	 write the null bytes 
WITHOUT_CLASSIFICATION	 for all other data types use int conversion some point should have all conversions make sure the value fits 
WITHOUT_CLASSIFICATION	 conversions 
WITHOUT_CLASSIFICATION	 remember the dummy ops created 
WITHOUT_CLASSIFICATION	 this indicates logged inconsistency from our pointofview and will not make this 
WITHOUT_CLASSIFICATION	 optional userpayloadproto userpayload 
WITHOUT_CLASSIFICATION	 createtime 
WITHOUT_CLASSIFICATION	 alias stage local task 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 should allocate 
WITHOUT_CLASSIFICATION	 note that this method assumes you have already decided this acid table cannot 
WITHOUT_CLASSIFICATION	 due reflection the jdo exception wrapped 
WITHOUT_CLASSIFICATION	 test dryrun schema upgrade 
WITHOUT_CLASSIFICATION	 handled 
WITHOUT_CLASSIFICATION	 this could due either uri syntax error file constructor illegal arg dont really care which one 
WITHOUT_CLASSIFICATION	 this fast path for query optimizations can find this info quickly using directsql point failing back slow path here 
WITHOUT_CLASSIFICATION	 currently evaluate the constants special ways 
WITHOUT_CLASSIFICATION	 note the row mapping not relevant when 
WITHOUT_CLASSIFICATION	 compare they are the same constant 
WITHOUT_CLASSIFICATION	 groupby query results records types defined metastore 
WITHOUT_CLASSIFICATION	 the driver registered the driver manager get the connection via the driver manager 
WITHOUT_CLASSIFICATION	 prevent equals from being overridden subclasses always use you need any other equality than objectequals 
WITHOUT_CLASSIFICATION	 check empty 
WITHOUT_CLASSIFICATION	 get the string value and convert date value 
WITHOUT_CLASSIFICATION	 verify that multi byte like expression matches matching string 
WITHOUT_CLASSIFICATION	 even the data not deleted 
WITHOUT_CLASSIFICATION	 start index means give the last characters the string 
WITHOUT_CLASSIFICATION	 primarykeys 
WITHOUT_CLASSIFICATION	 elapsedms 
WITHOUT_CLASSIFICATION	 this pluggable policy chose the candidate mapjoin table for converting join sort merge join the leftmost table chosen the join table 
WITHOUT_CLASSIFICATION	 create structs 
WITHOUT_CLASSIFICATION	 can get the number rows from the first vector 
WITHOUT_CLASSIFICATION	 timestampdate higher precedence than stringgroup 
WITHOUT_CLASSIFICATION	 clauses need combined keeping all elements 
WITHOUT_CLASSIFICATION	 bgenjjtree senumdeflist 
WITHOUT_CLASSIFICATION	 have fired already once return and the test will fail 
WITHOUT_CLASSIFICATION	 workgettablespecs null means not analyze command and then not followed column stats should clean column stats 
WITHOUT_CLASSIFICATION	 nice message 
WITHOUT_CLASSIFICATION	 not making this configurable best effort 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 delete the data the tables which have other locations 
WITHOUT_CLASSIFICATION	 for some reason even with mbeanexception available them runtime exceptions can still find their way through treat them the same mbeanexception 
WITHOUT_CLASSIFICATION	 reopen essentially just destroy get new session for session use 
WITHOUT_CLASSIFICATION	 were only considering the first element the list for the type 
WITHOUT_CLASSIFICATION	 use our helper from byte string 
WITHOUT_CLASSIFICATION	 some places fileinputformat this blocklocation used figure out sizesoffsets and completely blank one will not work 
WITHOUT_CLASSIFICATION	 dont fetch the footer ppd happens 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 now only string text int long byte and boolean comparisons are treated special cases for other types reuse 
WITHOUT_CLASSIFICATION	 writing instrumentation agent for object size estimation 
WITHOUT_CLASSIFICATION	 nothing fall through and verify the data 
WITHOUT_CLASSIFICATION	 exclusives can never pass 
WITHOUT_CLASSIFICATION	 this denotes listing any directories where during replication want take care level operations first namely our case its only during creation the replica warehouse 
WITHOUT_CLASSIFICATION	 char binary conversion include trailing spaces 
WITHOUT_CLASSIFICATION	 add the column only the family has not already been added 
WITHOUT_CLASSIFICATION	 make new client since transport was closed for the last one 
WITHOUT_CLASSIFICATION	 create some acid tables unpartitioned table partitioned table 
WITHOUT_CLASSIFICATION	 test various scenarios 
WITHOUT_CLASSIFICATION	 ignoring and continuing watch for additional elements the dir 
WITHOUT_CLASSIFICATION	 leading spaces are significant 
WITHOUT_CLASSIFICATION	 recreate table external drop partition and should 
WITHOUT_CLASSIFICATION	 test hiveconf property variable substitution hivesitexml 
WITHOUT_CLASSIFICATION	 computes the temporal run time statistics the reducers for specific jobid 
WITHOUT_CLASSIFICATION	 stores the temporal statistics milliseconds for reducers specific job 
WITHOUT_CLASSIFICATION	 complete txn 
WITHOUT_CLASSIFICATION	 bail mux operator because currently the mux operator masks the emit keys the constituent reduce sinks 
WITHOUT_CLASSIFICATION	 initqueryfile 
WITHOUT_CLASSIFICATION	 tbl 
WITHOUT_CLASSIFICATION	 initialize using target data type names projection the column range typessize 
WITHOUT_CLASSIFICATION	 check there data the resultset 
WITHOUT_CLASSIFICATION	 duplicate function with possibly replaced children 
WITHOUT_CLASSIFICATION	 find context for current input file 
WITHOUT_CLASSIFICATION	 yay shortcircuited skip everything remaining the batch and return 
WITHOUT_CLASSIFICATION	 fetch the bucketing version from table scan operator 
WITHOUT_CLASSIFICATION	 track the input columninfos that are added the output columninfo has multiple mappings then add the column only once but carry the mappings forward 
WITHOUT_CLASSIFICATION	 there should original bucket files and plus new delta directory 
WITHOUT_CLASSIFICATION	 for dynamic partitioned hash join rule may not get run for all the reducesink parents because the parents the mapjoin operator get removed later this method keep track the parent mapjoin mapping 
WITHOUT_CLASSIFICATION	 return immediately batch empty 
WITHOUT_CLASSIFICATION	 insert event unpartitioned table 
WITHOUT_CLASSIFICATION	 are here means user requesting role doesnt belong 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 move data file backup path 
WITHOUT_CLASSIFICATION	 the error codes are hivespecific and partitioned into the following ranges errors occurring during semantic analysis and compilation the query runtime errors where hive believes that retries are unlikely succeed runtime errors which hive thinks may transient and retrying may succeed errors where hive unable advise about retries addition the error code errormsg also has sqlstate field sqlstates are taken from section iso see most will just rollup the generic syntax error state but specific errors can override the that state see this page for how mysql uses sqlstate codes 
WITHOUT_CLASSIFICATION	 when start minihs will leader sequential start 
WITHOUT_CLASSIFICATION	 repeated nonnull permutations 
WITHOUT_CLASSIFICATION	 accumuloinputformat expects 
WITHOUT_CLASSIFICATION	 anything else indicates failure 
WITHOUT_CLASSIFICATION	 dont close the socket the stream already does that needed 
WITHOUT_CLASSIFICATION	 load all the default config values from hiveconf 
WITHOUT_CLASSIFICATION	 finder finds the first index its pattern given byte array its threadsafety depends its implementation 
WITHOUT_CLASSIFICATION	 mapping from constraint name list not null columns 
WITHOUT_CLASSIFICATION	 outputformat 
WITHOUT_CLASSIFICATION	 executionresult will errnook only all initfiles execute successfully 
WITHOUT_CLASSIFICATION	 find the base files original new style 
WITHOUT_CLASSIFICATION	 get the all path making select 
WITHOUT_CLASSIFICATION	 extrapolation not needed for this column extrapolation not possible for this column countpartitionname 
WITHOUT_CLASSIFICATION	 note not actually used for pool sessions verify some things like doas are not set 
WITHOUT_CLASSIFICATION	 create dummy select this select needed the walker split the mapjoin later 
WITHOUT_CLASSIFICATION	 partitions are not added write entries drop partitions hive 
WITHOUT_CLASSIFICATION	 ignore all other chars outside the enclosure 
WITHOUT_CLASSIFICATION	 lazyobject can only binary when its not string well return 
WITHOUT_CLASSIFICATION	 build the supported formats list 
WITHOUT_CLASSIFICATION	 mostly dup genincludedcolumns 
WITHOUT_CLASSIFICATION	 use bigint 
WITHOUT_CLASSIFICATION	 require all the directories present with some values 
WITHOUT_CLASSIFICATION	 the order processing follows wed reclaim kill all the sessions that can reclaim from various user actions and errors then apply the new plan any then give out all can give out restart get and reopen callers and rebalance the resource allocations all the affected pools for every session wed check all the concurrent things happening 
WITHOUT_CLASSIFICATION	 return all 
WITHOUT_CLASSIFICATION	 validate the value 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 todo fucntioncache 
WITHOUT_CLASSIFICATION	 then all output will null 
WITHOUT_CLASSIFICATION	 cant much outputtypeinfo not set 
WITHOUT_CLASSIFICATION	 let driver strip comments using sql parser 
WITHOUT_CLASSIFICATION	 working the assumption that the user here will the hive user doas false well make past this false check 
WITHOUT_CLASSIFICATION	 this method 
WITHOUT_CLASSIFICATION	 first column exists 
WITHOUT_CLASSIFICATION	 next user did specify perms 
WITHOUT_CLASSIFICATION	 transport mode 
WITHOUT_CLASSIFICATION	 returns list the distinct exprs without duplicates for given clause name 
WITHOUT_CLASSIFICATION	 remove the znodes weve already tried from this list 
WITHOUT_CLASSIFICATION	 utf continuation bytes have high bits equal 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 pause dont get banned 
WITHOUT_CLASSIFICATION	 lets use timeout more than the socket lifetime simulate reconnect 
WITHOUT_CLASSIFICATION	 delete any stale log files left around from previous failed tests 
WITHOUT_CLASSIFICATION	 process singlecolumn long inner bigonly join vectorized row batch 
WITHOUT_CLASSIFICATION	 get the query string from the conf file the compileinternal method might hide sensitive information during query redaction 
WITHOUT_CLASSIFICATION	 check any the txns the list committed 
WITHOUT_CLASSIFICATION	 whether characters such and are interpreted valid boolean literals 
WITHOUT_CLASSIFICATION	 test the environment variables can set this test fails all the other tests will also fail because environment not getting setup 
WITHOUT_CLASSIFICATION	 copy default aux classes jsonhbase 
WITHOUT_CLASSIFICATION	 call the tests 
WITHOUT_CLASSIFICATION	 format column for create statement 
WITHOUT_CLASSIFICATION	 with trimtrue parsing can handle spaces 
WITHOUT_CLASSIFICATION	 orcrecordupdater inconsistent about when creates empty files and when does not this creates empty bucket hive 
WITHOUT_CLASSIFICATION	 not run cbo build error message 
WITHOUT_CLASSIFICATION	 remove the branches 
WITHOUT_CLASSIFICATION	 normally trailing fractional digits are removed but emulate the oldhivedecimal setscale and internalstorage need trailing zeroes here note this can cause decimal that has too many decimal digits because trailing zeroes for represent that case punt and convert with biginteger alternate code 
WITHOUT_CLASSIFICATION	 count only the keys 
WITHOUT_CLASSIFICATION	 just examine the middle and lower words 
WITHOUT_CLASSIFICATION	 get available map memory 
WITHOUT_CLASSIFICATION	 can fail with 
WITHOUT_CLASSIFICATION	 not create counter does not exist 
WITHOUT_CLASSIFICATION	 add list bucketing location mappings 
WITHOUT_CLASSIFICATION	 streaming ingest dir cannot have updatedelete events 
WITHOUT_CLASSIFICATION	 test that two different databases dont collide their locks 
WITHOUT_CLASSIFICATION	 counter name starts with vertex then just return max value across all vertex since trigger validation only interested violation that are greater than limit any vertex violation use case shufflebytes for any single vertex limit perform action 
WITHOUT_CLASSIFICATION	 get token info check renew date 
WITHOUT_CLASSIFICATION	 cannot combine any the children bail out 
WITHOUT_CLASSIFICATION	 keep the hash set result for its spill information 
WITHOUT_CLASSIFICATION	 case test with originals and base single split strategy with two splits compacted 
WITHOUT_CLASSIFICATION	 the result should have project top otherwise 
WITHOUT_CLASSIFICATION	 set watch the znode 
WITHOUT_CLASSIFICATION	 droppartition event partitioned table 
WITHOUT_CLASSIFICATION	 batchindex taskname getoperatorid candidate classname batch 
WITHOUT_CLASSIFICATION	 this point may have parsed integer 
WITHOUT_CLASSIFICATION	 for partition key type this will primitive typeinfo 
WITHOUT_CLASSIFICATION	 codes 
WITHOUT_CLASSIFICATION	 data needs deletion check trash may skipped 
WITHOUT_CLASSIFICATION	 the rule has been applied bail out 
WITHOUT_CLASSIFICATION	 using old table object hence reset the owner current user for new table 
WITHOUT_CLASSIFICATION	 still broken for partitions 
WITHOUT_CLASSIFICATION	 the createtable above does not update the location the tbl object when the client thrift client and the code below relies the location being present the tbl object get the table from the metastore 
WITHOUT_CLASSIFICATION	 num total and completed tasks 
WITHOUT_CLASSIFICATION	 check that the added partitions are expected 
WITHOUT_CLASSIFICATION	 alter table partition column column newtype only takes one column time 
WITHOUT_CLASSIFICATION	 hadoop conf var names 
WITHOUT_CLASSIFICATION	 add countkeycol replace distinct 
WITHOUT_CLASSIFICATION	 gbyrsgby top bottom 
WITHOUT_CLASSIFICATION	 supports random access 
WITHOUT_CLASSIFICATION	 get aggregate stats for all partitions table and for all but default 
WITHOUT_CLASSIFICATION	 configuration and things set from 
WITHOUT_CLASSIFICATION	 look comments dummystoreoperator for additional explanation 
WITHOUT_CLASSIFICATION	 bypass the clause and select the second disjunct 
WITHOUT_CLASSIFICATION	 force local cache have deltas 
WITHOUT_CLASSIFICATION	 the child operators cleanup input file has changed 
WITHOUT_CLASSIFICATION	 attach the resources the session cleanup 
WITHOUT_CLASSIFICATION	 special case for because java doesnt strip zeros correctly that number 
WITHOUT_CLASSIFICATION	 generate groupbyoperator 
WITHOUT_CLASSIFICATION	 nonnull only for writing serverside 
WITHOUT_CLASSIFICATION	 set the umask conf such that filesdirs get created with tabledir permissions following three assumptions are made actual filesdirs creation done recordwriter underlying output format assumed that they use default permissions while creation default permissions umask honored underlying filesystem 
WITHOUT_CLASSIFICATION	 registry for system functions 
WITHOUT_CLASSIFICATION	 tracks new additions via add while the loop processing existing ones 
WITHOUT_CLASSIFICATION	 required required optional 
WITHOUT_CLASSIFICATION	 apply prejoin order optimizations 
WITHOUT_CLASSIFICATION	 try forceevict the fragments the requisite size 
WITHOUT_CLASSIFICATION	 register this comparator 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 total entries valid fake 
WITHOUT_CLASSIFICATION	 such dropping table partition 
WITHOUT_CLASSIFICATION	 tracks number exprs from correlated predicates added select list 
WITHOUT_CLASSIFICATION	 for left semi joins may apply the filters now 
WITHOUT_CLASSIFICATION	 compute stats 
WITHOUT_CLASSIFICATION	 group deptno aggregate 
WITHOUT_CLASSIFICATION	 checkmetastore call will fill result with partitions that are present filesystem and missing metastore accessed through and partitions that are not present filesystem and metadata exists metastore accessed through getpartitionnotonfs 
WITHOUT_CLASSIFICATION	 metadata ppd 
WITHOUT_CLASSIFICATION	 datanucleus throws npe when try serialize table object retrieved from metastore workaround that reset following objects 
WITHOUT_CLASSIFICATION	 struct column such root 
WITHOUT_CLASSIFICATION	 catalogs are actually not supported 
WITHOUT_CLASSIFICATION	 merge statement 
WITHOUT_CLASSIFICATION	 each listener called above might set different parameter the event this write permission allowed the listener side avoid breaking compatibility change the api method calls 
WITHOUT_CLASSIFICATION	 fill array with pattern that will never match sync 
WITHOUT_CLASSIFICATION	 now that uri and times are set correctly set the original tables uri and times 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 dont exceed the range have one 
WITHOUT_CLASSIFICATION	 get the count txns from the given list are open state the returned count same the input number txns then means all are open state 
WITHOUT_CLASSIFICATION	 see hive 
WITHOUT_CLASSIFICATION	 then castnull 
WITHOUT_CLASSIFICATION	 negate 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 some other process probably writing the file just sleep 
WITHOUT_CLASSIFICATION	 union all removal kicks and get subdirs ekoifman tree ext hiveunionsubdir orcacidversion delta hiveunionsubdir orcacidversion delta hiveunionsubdir orcacidversion delta 
WITHOUT_CLASSIFICATION	 double scalarscalar 
WITHOUT_CLASSIFICATION	 initialize mapredwork 
WITHOUT_CLASSIFICATION	 groupsetposition includes all the positions 
WITHOUT_CLASSIFICATION	 main path created new file cache someone created one parallel someone created one parallel and then went stale 
WITHOUT_CLASSIFICATION	 test minor compaction 
WITHOUT_CLASSIFICATION	 reached end file 
WITHOUT_CLASSIFICATION	 unique rows 
WITHOUT_CLASSIFICATION	 does this hashcode belong this reducer 
WITHOUT_CLASSIFICATION	 task failed fetch its error code available also keep track the total number failures for that 
WITHOUT_CLASSIFICATION	 merge should convert hll dense 
WITHOUT_CLASSIFICATION	 verify the data the same 
WITHOUT_CLASSIFICATION	 merge only useful for extended merging not have inputs 
WITHOUT_CLASSIFICATION	 obtained from the hiveexception thrown 
WITHOUT_CLASSIFICATION	 this number carefully chosen minimize overhead and typically allows one vectorizedrowbatch fit cache 
WITHOUT_CLASSIFICATION	 merge remove and reduce project possible 
WITHOUT_CLASSIFICATION	 avoid concurrent modify the hashmap 
WITHOUT_CLASSIFICATION	 this gives easy way get compaction can only wait for those this utility started 
WITHOUT_CLASSIFICATION	 check all record writers implement statistics atleast one doesnt implement stats interface will fallback conventional way 
WITHOUT_CLASSIFICATION	 print primary key containing parents 
WITHOUT_CLASSIFICATION	 taskdisplay doesnt have tostring using json 
WITHOUT_CLASSIFICATION	 needed virtual columns are those used the query 
WITHOUT_CLASSIFICATION	 other alter operations are already supported hive 
WITHOUT_CLASSIFICATION	 static partition 
WITHOUT_CLASSIFICATION	 token available replace the placeholder 
WITHOUT_CLASSIFICATION	 get semi join here this mapside groupbyoperator needs removed 
WITHOUT_CLASSIFICATION	 corresponding writable classes for the following hadoop 
WITHOUT_CLASSIFICATION	 partitions 
WITHOUT_CLASSIFICATION	 rowfilterexpression applied the whole table dbnameobjectname for example rowfilterexpression can key and key and 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 operand not 
WITHOUT_CLASSIFICATION	 have when matched and boolean expr then delete 
WITHOUT_CLASSIFICATION	 prec scale 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 update the tables cache 
WITHOUT_CLASSIFICATION	 rwxrwxrwx 
WITHOUT_CLASSIFICATION	 try again with left input also having nulls 
WITHOUT_CLASSIFICATION	 computes the skew for all the mapreduce irrespective success failure 
WITHOUT_CLASSIFICATION	 check theres least some degree stats available 
WITHOUT_CLASSIFICATION	 spill the current block tmp file 
WITHOUT_CLASSIFICATION	 sort order 
WITHOUT_CLASSIFICATION	 convert the ngram list format suitable for hive 
WITHOUT_CLASSIFICATION	 get our singlecolumn string hash multiset information for this specialized class 
WITHOUT_CLASSIFICATION	 the rule for headers header and buffer array element for some freelist can only modified the corresponding freelist lock held 
WITHOUT_CLASSIFICATION	 compare register values and store the max register value 
WITHOUT_CLASSIFICATION	 load the schema version stored metastore 
WITHOUT_CLASSIFICATION	 compile and execute can get called from different threads case 
WITHOUT_CLASSIFICATION	 role 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 queryid for the query current transaction 
WITHOUT_CLASSIFICATION	 generic http server error 
WITHOUT_CLASSIFICATION	 now check that stats were updated 
WITHOUT_CLASSIFICATION	 replace existing blanks with new blanks 
WITHOUT_CLASSIFICATION	 this value always seconds and includes suffix 
WITHOUT_CLASSIFICATION	 create the group operator 
WITHOUT_CLASSIFICATION	 create the corresponding client 
WITHOUT_CLASSIFICATION	 contains explicit field and partition 
WITHOUT_CLASSIFICATION	 validate the ddl valid operation the table 
WITHOUT_CLASSIFICATION	 update the childop selectop 
WITHOUT_CLASSIFICATION	 weve read bits roll them into the result 
WITHOUT_CLASSIFICATION	 stream data into streaming table with buckets then copy the data into another bucketed table 
WITHOUT_CLASSIFICATION	 keep connection open hang associated resources temp tables locks 
WITHOUT_CLASSIFICATION	 string including style literal characters cat kanji 
WITHOUT_CLASSIFICATION	 case jackson able deserializeit may use different implementation for the map which may not preserve order 
WITHOUT_CLASSIFICATION	 retrieve the additional httpheaders 
WITHOUT_CLASSIFICATION	 least one partition per expression not ifexists 
WITHOUT_CLASSIFICATION	 expr translation helper methods 
WITHOUT_CLASSIFICATION	 deserialize and test 
WITHOUT_CLASSIFICATION	 details pig added support for boolean fields which shipped this version pig depends antlr hcatalog depends heavily hive which this time uses antlr antlr and are incompatible pig and hive cannot depended the same project pig did not use antlr for its parser and can coexist with hive that pig version depended hcatalog this time 
WITHOUT_CLASSIFICATION	 rcfile 
WITHOUT_CLASSIFICATION	 repeat again 
WITHOUT_CLASSIFICATION	 descriptionvalue 
WITHOUT_CLASSIFICATION	 map this key back the confvars associated with 
WITHOUT_CLASSIFICATION	 minihs will become leader 
WITHOUT_CLASSIFICATION	 new array cannot contain the records wthe same key just advance dont check 
WITHOUT_CLASSIFICATION	 try out some metastore operations 
WITHOUT_CLASSIFICATION	 now oldordinal relative oldinput 
WITHOUT_CLASSIFICATION	 oldhivedecimal returns the hash code its internal bigdecimal our testhivedecimal verifies the matches new 
WITHOUT_CLASSIFICATION	 not fold namedstruct only struct 
WITHOUT_CLASSIFICATION	 columnscalar 
WITHOUT_CLASSIFICATION	 three different cases 
WITHOUT_CLASSIFICATION	 count query 
WITHOUT_CLASSIFICATION	 taskkill should also received during rejected submission will let that logic handle retries 
WITHOUT_CLASSIFICATION	 cannot 
WITHOUT_CLASSIFICATION	 create schema with serde then map 
WITHOUT_CLASSIFICATION	 construct 
WITHOUT_CLASSIFICATION	 would only apply after the callback for the current message 
WITHOUT_CLASSIFICATION	 set scratch directory 
WITHOUT_CLASSIFICATION	 partitioned table delete specific partitions 
WITHOUT_CLASSIFICATION	 call open read split mockmocktable 
WITHOUT_CLASSIFICATION	 create delta file bucket 
WITHOUT_CLASSIFICATION	 note the metadata cache may deallocate additional buffers but not this one 
WITHOUT_CLASSIFICATION	 get the path corresponding the dynamic partition columns 
WITHOUT_CLASSIFICATION	 validate the column names that are present are the same missing columns will implicitly defaulted null 
WITHOUT_CLASSIFICATION	 convert and then try instantiate replicationtask 
WITHOUT_CLASSIFICATION	 into same work 
WITHOUT_CLASSIFICATION	 any join participants from other has alias like posintname which size should caculated for each resolver 
WITHOUT_CLASSIFICATION	 the decendants contains limitoperatorreturn false 
WITHOUT_CLASSIFICATION	 create the folder and its parents not there 
WITHOUT_CLASSIFICATION	 tablefunction may able provide its output iterator case can then for mapside processing and for the last ptf reduceside chain can forward rows one one this will save the timespace populate and read output partition 
WITHOUT_CLASSIFICATION	 create the create table grants with new config 
WITHOUT_CLASSIFICATION	 parse the message field 
WITHOUT_CLASSIFICATION	 for ppd need column expression map that during the walk the processor knows how transform the internal col names following steps are dependant the fact that called 
WITHOUT_CLASSIFICATION	 create dir for mpart 
WITHOUT_CLASSIFICATION	 init objectinspectors 
WITHOUT_CLASSIFICATION	 executor create the ats events 
WITHOUT_CLASSIFICATION	 swallow exception 
WITHOUT_CLASSIFICATION	 simulate emitting records processnextrecord with small memory usage limit 
WITHOUT_CLASSIFICATION	 return partition metadata 
WITHOUT_CLASSIFICATION	 any repeated null key column match for whole batch 
WITHOUT_CLASSIFICATION	 updated the count will computed again 
WITHOUT_CLASSIFICATION	 remaining batches 
WITHOUT_CLASSIFICATION	 map from each basework its cloned jobconf 
WITHOUT_CLASSIFICATION	 here already saw current file and now found another file for the same bucket the current file not the last file the logical bucket 
WITHOUT_CLASSIFICATION	 tag want remain trash after deletion multiple files share the same content then 
WITHOUT_CLASSIFICATION	 get list events matching dbpattern tblpattern 
WITHOUT_CLASSIFICATION	 there should delta dir per partition location 
WITHOUT_CLASSIFICATION	 all chunks have been processed nothing more 
WITHOUT_CLASSIFICATION	 copied the entire buffer else theres more data process will handled next call 
WITHOUT_CLASSIFICATION	 delete gby gby sel from the pipeline 
WITHOUT_CLASSIFICATION	 release the lock 
WITHOUT_CLASSIFICATION	 tests with queries which can pushed down and executed with directsql 
WITHOUT_CLASSIFICATION	 this select for update query which takes lock the table entry already there nextwriteid 
WITHOUT_CLASSIFICATION	 inversion escaping happened are can reference directly 
WITHOUT_CLASSIFICATION	 singlecolumn string check for repeating 
WITHOUT_CLASSIFICATION	 use sampled partitioning 
WITHOUT_CLASSIFICATION	 record what type write this default nonacid old style 
WITHOUT_CLASSIFICATION	 setting file length longmaxvalue will let orc reader read file length from file system 
WITHOUT_CLASSIFICATION	 drop table already enforced hive only check for table level location even the table partitioned 
WITHOUT_CLASSIFICATION	 max txn does not change for transaction batch 
WITHOUT_CLASSIFICATION	 todo add tez session reconnect after tez 
WITHOUT_CLASSIFICATION	 alias 
WITHOUT_CLASSIFICATION	 set explicit session name control the download directory name 
WITHOUT_CLASSIFICATION	 check table exists 
WITHOUT_CLASSIFICATION	 set the values they are different the new stats 
WITHOUT_CLASSIFICATION	 set foreign key name null before sending listener 
WITHOUT_CLASSIFICATION	 for intermediate sum field 
WITHOUT_CLASSIFICATION	 create 
WITHOUT_CLASSIFICATION	 create the objectinspectors for the fields note currently columnarobject uses same objectinpector lazystruct 
WITHOUT_CLASSIFICATION	 were loading into instead into the warehouse then the olddbname and newdbname must the same 
WITHOUT_CLASSIFICATION	 collist will null for operators 
WITHOUT_CLASSIFICATION	 have filled digits and have more room our limit precision fast decimal 
WITHOUT_CLASSIFICATION	 set list work 
WITHOUT_CLASSIFICATION	 adjust groupingset position gbkeys for groupingset position needed note groupingid added map side only dont grpset 
WITHOUT_CLASSIFICATION	 calculate the variance sample result when count public vectorization code can use etc 
WITHOUT_CLASSIFICATION	 are using shared database then remove not known databases tables views 
WITHOUT_CLASSIFICATION	 empty list case 
WITHOUT_CLASSIFICATION	 test timestamp string 
WITHOUT_CLASSIFICATION	 move past parent field separator 
WITHOUT_CLASSIFICATION	 add the testnullappender the default route 
WITHOUT_CLASSIFICATION	 converted the expression search condition 
WITHOUT_CLASSIFICATION	 native not would decided annotation need evaluate that first 
WITHOUT_CLASSIFICATION	 not strictly necessary the whole queue check again 
WITHOUT_CLASSIFICATION	 dont create new object are already out memory 
WITHOUT_CLASSIFICATION	 now add all the default handlers 
WITHOUT_CLASSIFICATION	 skipping through comments 
WITHOUT_CLASSIFICATION	 extract order for each column from collation 
WITHOUT_CLASSIFICATION	 for each subquery also these different filesinks need linked each other 
WITHOUT_CLASSIFICATION	 skewed column names 
WITHOUT_CLASSIFICATION	 need look token info 
WITHOUT_CLASSIFICATION	 ifnotexists 
WITHOUT_CLASSIFICATION	 neither input has nulls verify that this propagates output 
WITHOUT_CLASSIFICATION	 check cases for arrif and mapkeyv for these should not generate paths like arrf mapv otherwise would have mismatch between type info and path 
WITHOUT_CLASSIFICATION	 make sure check format this right 
WITHOUT_CLASSIFICATION	 schema validation enforces that the key string 
WITHOUT_CLASSIFICATION	 ignored the attribute was not found which should never happen because the bean just told that has this attribute but this happens just dont output the attribute 
WITHOUT_CLASSIFICATION	 sort both the lists 
WITHOUT_CLASSIFICATION	 default column storage specification inherits from table level default 
WITHOUT_CLASSIFICATION	 allocate overflow batch columns hand 
WITHOUT_CLASSIFICATION	 arithmetic operations rely getting conf from sessionstate need initialize here 
WITHOUT_CLASSIFICATION	 spin until resolves extremely rare 
WITHOUT_CLASSIFICATION	 this may happen acid state absent from config 
WITHOUT_CLASSIFICATION	 must manually set with setmaxlength 
WITHOUT_CLASSIFICATION	 create nullappender 
WITHOUT_CLASSIFICATION	 only columns can sortedbucketed particular applying function column voids any assumptions 
WITHOUT_CLASSIFICATION	 user unsets queue name will fallback default session queue 
WITHOUT_CLASSIFICATION	 reset the pointer 
WITHOUT_CLASSIFICATION	 valid generated cookies found return null 
WITHOUT_CLASSIFICATION	 the set object containing the list 
WITHOUT_CLASSIFICATION	 process hints 
WITHOUT_CLASSIFICATION	 sintstring 
WITHOUT_CLASSIFICATION	 nonjavadoc this processor addresses the rsmj case that occurs tez the smallhash table side things the work that will part must connected the work via broadcast edge should not walk down the tree when encounter this pattern because the type work map work reduce work needs determined the basis the big table side because may mapwork need for shuffle reduce work 
WITHOUT_CLASSIFICATION	 bail here make the operation idempotent 
WITHOUT_CLASSIFICATION	 lets check that side files exist etc 
WITHOUT_CLASSIFICATION	 update ndv joined columns minvry vsy 
WITHOUT_CLASSIFICATION	 add alternative alias for the column this instance represents and its index the 
WITHOUT_CLASSIFICATION	 first batch always based batch size 
WITHOUT_CLASSIFICATION	 alphamm value for bits hash seems perform better for default hash bits 
WITHOUT_CLASSIFICATION	 wont updated 
WITHOUT_CLASSIFICATION	 compare ports 
WITHOUT_CLASSIFICATION	 lower this for big value testing 
WITHOUT_CLASSIFICATION	 estimated number reducers 
WITHOUT_CLASSIFICATION	 since did not remove reduce sink parents keep the original value expressions 
WITHOUT_CLASSIFICATION	 for join sel for lateral view sel for union does not count should copied both sides 
WITHOUT_CLASSIFICATION	 this should ideally happen separate thread 
WITHOUT_CLASSIFICATION	 test the need explicitly set the constant smaller value 
WITHOUT_CLASSIFICATION	 false return false 
WITHOUT_CLASSIFICATION	 currop now points the topmost tablescan operator 
WITHOUT_CLASSIFICATION	 automatic conversion double done here 
WITHOUT_CLASSIFICATION	 data with the separator bytes before creating put object 
WITHOUT_CLASSIFICATION	 since the file read pig need make sure the values are format that pig understands otherwise will turn the value null read 
WITHOUT_CLASSIFICATION	 get the bucket positions for the table 
WITHOUT_CLASSIFICATION	 add insert event twice with different event allow apply both events 
WITHOUT_CLASSIFICATION	 not match copy again from 
WITHOUT_CLASSIFICATION	 convert any nulls present map values empty strings this done the case backing dbs like oracle which persist empty strings nulls 
WITHOUT_CLASSIFICATION	 only one table lets check all partitions 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 setop 
WITHOUT_CLASSIFICATION	 that appear the small table portion the join output for outer joins 
WITHOUT_CLASSIFICATION	 there are deletes and reading original file must produce synthetic rowids order see any deletes apply 
WITHOUT_CLASSIFICATION	 noop default 
WITHOUT_CLASSIFICATION	 sign 
WITHOUT_CLASSIFICATION	 additional work for union operator see unionq 
WITHOUT_CLASSIFICATION	 this next section repeats the tests with maxlength parameter that less than the number current characters the string and thus affects the trim 
WITHOUT_CLASSIFICATION	 mostly this indicates that the initiator paying attention some table even though 
WITHOUT_CLASSIFICATION	 prior none singleton range 
WITHOUT_CLASSIFICATION	 batchsize when isrepeating 
WITHOUT_CLASSIFICATION	 combine equivalent work into single one sparkworks work graph 
WITHOUT_CLASSIFICATION	 not included equals 
WITHOUT_CLASSIFICATION	 create selectdesc 
WITHOUT_CLASSIFICATION	 try allocate using brute force approach from each arena 
WITHOUT_CLASSIFICATION	 user 
WITHOUT_CLASSIFICATION	 evaluate subsequent child expression over unselected ones only 
WITHOUT_CLASSIFICATION	 authorization setup using sessionstate should revisited eventually authorization and authentication are not session specific settings 
WITHOUT_CLASSIFICATION	 grouping reference 
WITHOUT_CLASSIFICATION	 verify the number buckets equals the number files this will not hold for dynamic partitions where not every reducer produced file for those partitions this case the table not bucketed hive requires files for 
WITHOUT_CLASSIFICATION	 will override and ignore isset fields 
WITHOUT_CLASSIFICATION	 all rows are filtered repeating null otherwise rows are filtered 
WITHOUT_CLASSIFICATION	 implementation verify oninit called when hmshandler initialized 
WITHOUT_CLASSIFICATION	 validate 
WITHOUT_CLASSIFICATION	 the isoriginal file the root the partition table thus from preacid conversion write and belongs primordial writeid 
WITHOUT_CLASSIFICATION	 this not cast process the function 
WITHOUT_CLASSIFICATION	 fallback picking the value from environment 
WITHOUT_CLASSIFICATION	 move the record from txncomponents into that the compactor 
WITHOUT_CLASSIFICATION	 errorcode 
WITHOUT_CLASSIFICATION	 data created export command 
WITHOUT_CLASSIFICATION	 this should also trigger meta listener notification via 
WITHOUT_CLASSIFICATION	 cache the client asks for else just return the value 
WITHOUT_CLASSIFICATION	 this assume round 
WITHOUT_CLASSIFICATION	 allocfraction 
WITHOUT_CLASSIFICATION	 todo this should making use confdir load configs setup for tez etc 
WITHOUT_CLASSIFICATION	 optional group containing repeated anonymous group bag containing 
WITHOUT_CLASSIFICATION	 use hiveinputformat that can control the number map tasks 
WITHOUT_CLASSIFICATION	 otherwise continue get the group type until 
WITHOUT_CLASSIFICATION	 gby 
WITHOUT_CLASSIFICATION	 mapping from constraint name list check constraints 
WITHOUT_CLASSIFICATION	 the metadata level there are restrictions column names 
WITHOUT_CLASSIFICATION	 there was authorization issue 
WITHOUT_CLASSIFICATION	 subscriber can get notification about drop table hcat listening topic named hcat and message selector string hcatevent hcatdroptable 
WITHOUT_CLASSIFICATION	 this read entity direct read entity and not indirect read that when 
WITHOUT_CLASSIFICATION	 show table information 
WITHOUT_CLASSIFICATION	 first try without qualifiers would resolve builtintemp functions 
WITHOUT_CLASSIFICATION	 hiveconf has not changed same object should returned 
WITHOUT_CLASSIFICATION	 this method called the rcfilereader constructor overwritten can access the opened file 
WITHOUT_CLASSIFICATION	 finds all contextual ngrams sequence words and passes the ngrams the 
WITHOUT_CLASSIFICATION	 slight hack communicate dynamicserde that the field ids are not being set but things are ordered 
WITHOUT_CLASSIFICATION	 and submit method when the job config created 
WITHOUT_CLASSIFICATION	 good 
WITHOUT_CLASSIFICATION	 create methods 
WITHOUT_CLASSIFICATION	 database 
WITHOUT_CLASSIFICATION	 lowvalue 
WITHOUT_CLASSIFICATION	 division inverse multiplication 
WITHOUT_CLASSIFICATION	 none the group expressions are constant nothing 
WITHOUT_CLASSIFICATION	 typespecific implementations 
WITHOUT_CLASSIFICATION	 output types they will the concatenation the input refs types and 
WITHOUT_CLASSIFICATION	 raise error user has specified partition column for stats 
WITHOUT_CLASSIFICATION	 create planner and copy context 
WITHOUT_CLASSIFICATION	 enable ssl support for hms 
WITHOUT_CLASSIFICATION	 connect all small dir map work the big dir map work 
WITHOUT_CLASSIFICATION	 add all the dependencies list 
WITHOUT_CLASSIFICATION	 find min ndv for joining columns 
WITHOUT_CLASSIFICATION	 add filter countc branches 
WITHOUT_CLASSIFICATION	 case exception assume unknown type bytes 
WITHOUT_CLASSIFICATION	 cant access metadata carry 
WITHOUT_CLASSIFICATION	 blindly add this integer list should sufficient for the test case 
WITHOUT_CLASSIFICATION	 batch statements 
WITHOUT_CLASSIFICATION	 also not loaded 
WITHOUT_CLASSIFICATION	 look everything front this lock see should block 
WITHOUT_CLASSIFICATION	 add original entries 
WITHOUT_CLASSIFICATION	 then merge the operators the works are going merge 
WITHOUT_CLASSIFICATION	 the can closed from under the task interrupted release cache buffers are assuming here that torelease will not present such cases 
WITHOUT_CLASSIFICATION	 null any the args are nulls 
WITHOUT_CLASSIFICATION	 least checkers always 
WITHOUT_CLASSIFICATION	 todo even out the batch sizes should replaced 
WITHOUT_CLASSIFICATION	 nothing 
WITHOUT_CLASSIFICATION	 the original partition files are deleted after the metadata change because the presence those files are used indicate whether the original partition directory contains archived unarchived files 
WITHOUT_CLASSIFICATION	 need set output name for reduce sink now that know the name the downstream work 
WITHOUT_CLASSIFICATION	 allocate the target related arrays 
WITHOUT_CLASSIFICATION	 make sure currently running txn considered aborted housekeeper 
WITHOUT_CLASSIFICATION	 still the same column 
WITHOUT_CLASSIFICATION	 each column has height 
WITHOUT_CLASSIFICATION	 add list bucketing predicate the table scan operator 
WITHOUT_CLASSIFICATION	 create fetch work 
WITHOUT_CLASSIFICATION	 the file name bucket number mapping maintained store the bucket number the execution context this needed for the following scenario insert overwrite table select from where and are sortedbucketed the same keys into the same number buckets although one mapper per file used possible that any mapper can pick any file depending the size the files the bucket number corresponding the input file stored name the output bucket file appropriately 
WITHOUT_CLASSIFICATION	 provide with new url access the datastore 
WITHOUT_CLASSIFICATION	 and terminate 
WITHOUT_CLASSIFICATION	 sampling filter then ignore the current filter 
WITHOUT_CLASSIFICATION	 create database with table 
WITHOUT_CLASSIFICATION	 set default location not specified and this physical table partition not view 
WITHOUT_CLASSIFICATION	 parse the struct using multichar delimiter 
WITHOUT_CLASSIFICATION	 drop destsequencefile 
WITHOUT_CLASSIFICATION	 timestamp format specified just use default lazy inspector 
WITHOUT_CLASSIFICATION	 files found for example empty tablepartition 
WITHOUT_CLASSIFICATION	 verify that the table was created successfully 
WITHOUT_CLASSIFICATION	 update the null counter 
WITHOUT_CLASSIFICATION	 all the partitions need updated single command can used 
WITHOUT_CLASSIFICATION	 connect using principal via beeline with inputstream 
WITHOUT_CLASSIFICATION	 comment column empty 
WITHOUT_CLASSIFICATION	 remove any parallel edge between semijoin and mapjoin 
WITHOUT_CLASSIFICATION	 unlike not append log when stopped 
WITHOUT_CLASSIFICATION	 will properly set string binary serialization via createlazyfield 
WITHOUT_CLASSIFICATION	 send some status periodically 
WITHOUT_CLASSIFICATION	 stick back into result variables 
WITHOUT_CLASSIFICATION	 partition spec was already validated caller when create tablespec object 
WITHOUT_CLASSIFICATION	 this source table not copy out 
WITHOUT_CLASSIFICATION	 set job name 
WITHOUT_CLASSIFICATION	 java class 
WITHOUT_CLASSIFICATION	 max rows can put into one block 
WITHOUT_CLASSIFICATION	 string types get converted double 
WITHOUT_CLASSIFICATION	 the skew keys match the join keys then add the list 
WITHOUT_CLASSIFICATION	 either tablehandle isnt partitioned null replexport after becomes null null this noopreplication export can skip looking ptns 
WITHOUT_CLASSIFICATION	 max length for the charvarchar then the return type reverts string 
WITHOUT_CLASSIFICATION	 the opparsecontext the parent selectoperator 
WITHOUT_CLASSIFICATION	 key 
WITHOUT_CLASSIFICATION	 indicator the chunked input does not know stop reading 
WITHOUT_CLASSIFICATION	 compute the size query when the nextvalue added the current query 
WITHOUT_CLASSIFICATION	 test bad args getxxx throws sqlexception 
WITHOUT_CLASSIFICATION	 check whether the materialized view invalidated 
WITHOUT_CLASSIFICATION	 compare the value each element array until match found 
WITHOUT_CLASSIFICATION	 set task 
WITHOUT_CLASSIFICATION	 expectation here not run into timeout 
WITHOUT_CLASSIFICATION	 any name does not matter 
WITHOUT_CLASSIFICATION	 information field made class allow readfield agnostic whether top level field within complex type being read 
WITHOUT_CLASSIFICATION	 unfortunately seem get instances varchar object inspectors without params when oldstyle udf has evaluate method with varchar arguments disallow varchar oldstyle udfs and only allow genericudfs defined with varchar arguments then might able enforce this properly 
WITHOUT_CLASSIFICATION	 note all maps and lists have absolutely sorted otherwise well produce different results for hashes based the jvm being used 
WITHOUT_CLASSIFICATION	 the interface for single byte array key hash multiset contains method 
WITHOUT_CLASSIFICATION	 guava versions have stats collection enabled default and not expose recordstats method check for newer versions the library and ensure that stats collection enabled default 
WITHOUT_CLASSIFICATION	 update previous comment there does seem one place that uses this and that authorize show databases hcat commandline which used webhcat and userlevel auth seems reasonable default this case the now deprecated hcatalog approached this another way and that was see the user had said above appropriate requested privileges for the hive root warehouse directory that seems the best mapping for user level privileges storage using that strategy here 
WITHOUT_CLASSIFICATION	 create map and fetch operators 
WITHOUT_CLASSIFICATION	 trim down the total number ngrams weve exceeded the maximum amount memory allowed note although kpf specifies the size the estimation buffer dont want keep performing nlogn trim operations each time the maximum hashmap size exceeded handle this actually maintain estimation buffer size kpf and trim down kpf whenever the hashmap size exceeds kpf this really has 
WITHOUT_CLASSIFICATION	 the bucket the valid range mark covered wish hive actually enforced bucketing all the time 
WITHOUT_CLASSIFICATION	 used half the mem for small joins now lets scale the rest 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 single string key hash map optimized for vector map join the key will deserialized and just the bytes will stored 
WITHOUT_CLASSIFICATION	 use the current directory not specified 
WITHOUT_CLASSIFICATION	 this consistently works locally but never ptest 
WITHOUT_CLASSIFICATION	 test that exclusive table locks coalesce one 
WITHOUT_CLASSIFICATION	 get tables make sure the locations are correct 
WITHOUT_CLASSIFICATION	 build via hive typeinfo and parquet schema 
WITHOUT_CLASSIFICATION	 get fieldschema stuff any 
WITHOUT_CLASSIFICATION	 set provider options 
WITHOUT_CLASSIFICATION	 test with and without specifying schema randomly 
WITHOUT_CLASSIFICATION	 convert the types needed for plugin api 
WITHOUT_CLASSIFICATION	 lowest middle high 
WITHOUT_CLASSIFICATION	 log summary every seconds 
WITHOUT_CLASSIFICATION	 picks topn pairs from input 
WITHOUT_CLASSIFICATION	 initialize one columns source deserializtion information 
WITHOUT_CLASSIFICATION	 wed need sleep once per round instead 
WITHOUT_CLASSIFICATION	 the directory this move task moving 
WITHOUT_CLASSIFICATION	 other operators functions 
WITHOUT_CLASSIFICATION	 join fil for the above complex operator tree selectivityjoin selectivityrs selectivityrs and 
WITHOUT_CLASSIFICATION	 read the keys before the delta flushed 
WITHOUT_CLASSIFICATION	 there are elements the map 
WITHOUT_CLASSIFICATION	 return the variable length from config 
WITHOUT_CLASSIFICATION	 pattern look for the hive query and whether matched 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 perform major compaction 
WITHOUT_CLASSIFICATION	 ensure hiveserver sitexml does not get override this 
WITHOUT_CLASSIFICATION	 need the base operatorjava implementation startendgroup the parent class has functionality those that map join cant use note the mapjoin can run the reducer only tez 
WITHOUT_CLASSIFICATION	 evaluate batch that temporary arrays the expression have residual values interfere later computation 
WITHOUT_CLASSIFICATION	 byte string maybe replace with localdirid and construct the fly longs reference overheads 
WITHOUT_CLASSIFICATION	 union 
WITHOUT_CLASSIFICATION	 authorize individually 
WITHOUT_CLASSIFICATION	 fail the update too get rid the duck for the next test 
WITHOUT_CLASSIFICATION	 hash aggregation not behaving properly disable 
WITHOUT_CLASSIFICATION	 stripe offset outside the split boundary then ignore the current stripe will handled some other mapper 
WITHOUT_CLASSIFICATION	 still files remains copied due failurechecksum mismatch after several attempts then throw error 
WITHOUT_CLASSIFICATION	 directory does not exist create 
WITHOUT_CLASSIFICATION	 find any constraints and drop them 
WITHOUT_CLASSIFICATION	 verify that have got correct set deletedeltas also 
WITHOUT_CLASSIFICATION	 first generate all the opinfos for the elements the from clause 
WITHOUT_CLASSIFICATION	 fakepart path partition added since the defined partition keys are valid 
WITHOUT_CLASSIFICATION	 operationhandle 
WITHOUT_CLASSIFICATION	 this replication spec then replacemode semantics might apply 
WITHOUT_CLASSIFICATION	 internal usage only length variable length data type cannot determined this length will used 
WITHOUT_CLASSIFICATION	 create the column descriptors 
WITHOUT_CLASSIFICATION	 return null only the file system schema not recognized 
WITHOUT_CLASSIFICATION	 createtable truncate insert 
WITHOUT_CLASSIFICATION	 have setup this again the underlying pmf keeps getting reinitialized with original reference closed 
WITHOUT_CLASSIFICATION	 now set other column nullable too 
WITHOUT_CLASSIFICATION	 tezjsonparser 
WITHOUT_CLASSIFICATION	 now the relevant tablescanoperators are known find there exists semijoin filter any them remove 
WITHOUT_CLASSIFICATION	 right now not handle the case that either them bucketed should relax this constraint with followup jira 
WITHOUT_CLASSIFICATION	 the hashcode and equals methods the key class 
WITHOUT_CLASSIFICATION	 location should allocate force capacity otherwise 
WITHOUT_CLASSIFICATION	 this the case when weve encountered decimal separator the fractional part will not change the number but will verify that the fractional part 
WITHOUT_CLASSIFICATION	 dryrun checks are meaningless for mutable table should always succeed unless there runtime ioexception 
WITHOUT_CLASSIFICATION	 whether theres spilled data processed used hold restored 
WITHOUT_CLASSIFICATION	 find dynamic partition columns relies consistent order via linkedhashmap 
WITHOUT_CLASSIFICATION	 sessionstate not available runtime and hivegetgetconf not safe call 
WITHOUT_CLASSIFICATION	 these are all values that put here just for testing 
WITHOUT_CLASSIFICATION	 types get initialized case they need setup any internal data structures 
WITHOUT_CLASSIFICATION	 keep track colnametoposmap for new select 
WITHOUT_CLASSIFICATION	 can get them from hdfs add group and permission 
WITHOUT_CLASSIFICATION	 split leaf join predicate expressions from left right 
WITHOUT_CLASSIFICATION	 project has any correlated reference make sure they are also provided the current correlate they will projected out the lhs 
WITHOUT_CLASSIFICATION	 special property starting with mapreduce that would also like effect changes 
WITHOUT_CLASSIFICATION	 that case will fail 
WITHOUT_CLASSIFICATION	 reprocess spilled data 
WITHOUT_CLASSIFICATION	 add spark job handle the hive history 
WITHOUT_CLASSIFICATION	 prefix operator 
WITHOUT_CLASSIFICATION	 ignore 
WITHOUT_CLASSIFICATION	 the user asked for stats collected some stats like number rows require scan the data however some other stats like number files not require complete scan 
WITHOUT_CLASSIFICATION	 input paths 
WITHOUT_CLASSIFICATION	 this ensures dont create skew with ducks and queries with simple rounding wed produce round whereas adding the last delta the next query wed round and thus give out intended note that fractions dont have all the same like this example 
WITHOUT_CLASSIFICATION	 expect fail since the time component not 
WITHOUT_CLASSIFICATION	 verify syntax error 
WITHOUT_CLASSIFICATION	 fastisint returns false 
WITHOUT_CLASSIFICATION	 old partition does not exist 
WITHOUT_CLASSIFICATION	 use kryo serialize hashmap 
WITHOUT_CLASSIFICATION	 caller responsible for setting children and input type information 
WITHOUT_CLASSIFICATION	 cause root cause and efirst useless exception its wrapped 
WITHOUT_CLASSIFICATION	 multikey specific lookup key 
WITHOUT_CLASSIFICATION	 storagehandler passed table params 
WITHOUT_CLASSIFICATION	 cast the input decimal casttype decimal try not lose precision for numeric types 
WITHOUT_CLASSIFICATION	 hive 
WITHOUT_CLASSIFICATION	 this only for the purpose authorization only the name matters 
WITHOUT_CLASSIFICATION	 specify orcsplit that starts beyond the offset the last stripe 
WITHOUT_CLASSIFICATION	 properties 
WITHOUT_CLASSIFICATION	 transformation left join for correlated predicates and inner join otherwise 
WITHOUT_CLASSIFICATION	 vertexs parent connections 
WITHOUT_CLASSIFICATION	 should generate 
WITHOUT_CLASSIFICATION	 out range probes 
WITHOUT_CLASSIFICATION	 set the key 
WITHOUT_CLASSIFICATION	 schema the mapreduce key object this homogeneous 
WITHOUT_CLASSIFICATION	 crucial here that dont reset the overflow batch will loose the small table 
WITHOUT_CLASSIFICATION	 convert tasks involving join into mapjoin hiveautoconvertjoin true the tasks involving join are converted consider the query select from join tkey tkey join tkey tkey there mapreduce task which performs way join the task would converted conditional task which would have children mapjoin considering the big table mapjoin considering the big table mapjoin considering the big table mapreduce join the original task note that the sizes all the inputs may not available compile time runtime determined which branch want pick from the above however set true and the sum any tables smaller than then mapjoin created instead the conditional task for the above the size less than the threshold then the task converted mapjoin task with the big table this case further optimization performed merging consecutive maponly jobs consider the query select from join tkey tkey join tkey tkey initially the plan would consist mapreduce jobs perform join for and followed another mapreduce job perform join the result with after the optimization both these tasks would converted maponly tasks these maponly jobs are then merged into single maponly job followup hive would possible merge maponly task with mapreduce task consider the query select tkey count from join tkey tkey group tkey initially the plan would consist mapreduce jobs perform join for and followed another mapreduce job perform groupby the result after the optimization the join task would converted maponly tasks after hive the maponly task would merged with the mapreduce task create single mapreduce task 
WITHOUT_CLASSIFICATION	 formatteron 
WITHOUT_CLASSIFICATION	 then add any cor var from the left input not need change 
WITHOUT_CLASSIFICATION	 missing one partition 
WITHOUT_CLASSIFICATION	 hash map which stores job credentials the key signature passed pig which 
WITHOUT_CLASSIFICATION	 create selection operator 
WITHOUT_CLASSIFICATION	 restore the reducer 
WITHOUT_CLASSIFICATION	 negative unix time 
WITHOUT_CLASSIFICATION	 this should removed eventually hive gives more detail explanation whats happening and hive why this done briefly for replication the graph huge and memory pressure going huge keep lot references around 
WITHOUT_CLASSIFICATION	 will cause underflow for result position must yield null 
WITHOUT_CLASSIFICATION	 can add verboselogging cause mockito log invocations 
WITHOUT_CLASSIFICATION	 fast 
WITHOUT_CLASSIFICATION	 scale updown 
WITHOUT_CLASSIFICATION	 note this may use additional inputs from the caller maximum query parallelism the cluster based physical constraints 
WITHOUT_CLASSIFICATION	 run compaction 
WITHOUT_CLASSIFICATION	 then need set the graph connection especially need connect this cloned parent work with all the grandparent works 
WITHOUT_CLASSIFICATION	 orc writer reuses streams need clean them here and extract data 
WITHOUT_CLASSIFICATION	 todo this makes many assumptions how generic args are done 
WITHOUT_CLASSIFICATION	 nuke trailing 
WITHOUT_CLASSIFICATION	 top level view should care about its access info 
WITHOUT_CLASSIFICATION	 fetch table ablias 
WITHOUT_CLASSIFICATION	 build tokselect tokselectexpr ast tree for count 
WITHOUT_CLASSIFICATION	 trigger and action expressions are not validated here since counters are not 
WITHOUT_CLASSIFICATION	 this might little bit too muchbut most cases this should true 
WITHOUT_CLASSIFICATION	 the first time 
WITHOUT_CLASSIFICATION	 use textfile default 
WITHOUT_CLASSIFICATION	 will rewrite include the filters transaction list can produce partial rewritings 
WITHOUT_CLASSIFICATION	 invalid character new database name 
WITHOUT_CLASSIFICATION	 confirm the batch sizes were the two calls create partitions 
WITHOUT_CLASSIFICATION	 single predicate condition 
WITHOUT_CLASSIFICATION	 the current table function has order info specified 
WITHOUT_CLASSIFICATION	 minihs will leader 
WITHOUT_CLASSIFICATION	 will try merge this clause into one the previously added ones 
WITHOUT_CLASSIFICATION	 have released the session trying reuse and going back into queue can start 
WITHOUT_CLASSIFICATION	 this bit should not for valid value references use for value marker 
WITHOUT_CLASSIFICATION	 logrefresherror always throws 
WITHOUT_CLASSIFICATION	 nothing but count the batch size 
WITHOUT_CLASSIFICATION	 just remember for later processing 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check that the mapping schema right check that the columnfamily mapped mapkey where key extends lazyprimitive and thus has type categoryprimitive 
WITHOUT_CLASSIFICATION	 replace original stddevsampx with sqrt sumx sumx sumx countx case countx when then null else countx end 
WITHOUT_CLASSIFICATION	 selsel rule 
WITHOUT_CLASSIFICATION	 signature string associate with hcattableinfo essentially 
WITHOUT_CLASSIFICATION	 extended desc table then show the complete details the table 
WITHOUT_CLASSIFICATION	 that produces multiple queries for that need least 
WITHOUT_CLASSIFICATION	 insert overwrite existing partition 
WITHOUT_CLASSIFICATION	 retvalue transient store this separately 
WITHOUT_CLASSIFICATION	 sort only references field positions collations field the collations field the newrel now need refer the new output positions its input its output does not change the input ordering theres need call propagateexpr 
WITHOUT_CLASSIFICATION	 create the appender 
WITHOUT_CLASSIFICATION	 lock correctly see the comment the lock field the locking needs reworked 
WITHOUT_CLASSIFICATION	 the small table result portion the output for outer join 
WITHOUT_CLASSIFICATION	 prompt 
WITHOUT_CLASSIFICATION	 not supported all 
WITHOUT_CLASSIFICATION	 deserialize the hcatpartitionspec using the target hcatclient instance 
WITHOUT_CLASSIFICATION	 otherwise queue the session and make sure update this pool 
WITHOUT_CLASSIFICATION	 scan through any remaining digits 
WITHOUT_CLASSIFICATION	 the original location exists here then must the extracted files because the previous step moved the previous original location 
WITHOUT_CLASSIFICATION	 mapping bucket size all splits bucket bytes 
WITHOUT_CLASSIFICATION	 tez requires use rpc for the query plan 
WITHOUT_CLASSIFICATION	 compactor types 
WITHOUT_CLASSIFICATION	 when inserting into new partition the add partition event takes care insert event 
WITHOUT_CLASSIFICATION	 any events were queued the responder give them the record reader now 
WITHOUT_CLASSIFICATION	 the end update free list head 
WITHOUT_CLASSIFICATION	 configure export work 
WITHOUT_CLASSIFICATION	 the server sends very frequently 
WITHOUT_CLASSIFICATION	 read one field one field 
WITHOUT_CLASSIFICATION	 this should removed when authenticator and the username mess cleaned 
WITHOUT_CLASSIFICATION	 release the unreleased buffers see class comment about refcounts 
WITHOUT_CLASSIFICATION	 expiration queue synchronized and notified upon when adding elements without jitter wouldnt need this and could simple look the first element and sleep for the wait time however when many things are added once may happen that will see the one that expires later first and will sleep past the earlier expiration times when wake may kill many sessions once avoid this will add queue under lock and recheck time before wait dont have worry about removals worst wed wake vain example expirations are added this order due jitter the expiration threads sees that first will sleep for then wake and kill all sessions once because they all have expired removing any effect from jitter instead expiration thread rechecks the first queue item and waits the queue nothing added the queue the item examined still the earliest expired someone adds the queue while waiting will notify the thread and would wake and recheck the queue 
WITHOUT_CLASSIFICATION	 replication destination will not external override set 
WITHOUT_CLASSIFICATION	 create rows file 
WITHOUT_CLASSIFICATION	 there should only single split line 
WITHOUT_CLASSIFICATION	 filter files starts with note hadoop consider files starts with hidden file however need replicate files starts with find least use cases for har files index and masterindex required files 
WITHOUT_CLASSIFICATION	 nanos 
WITHOUT_CLASSIFICATION	 retrieve the for the table initialized hiveinputformat 
WITHOUT_CLASSIFICATION	 dont request any locks here the table has already been locked 
WITHOUT_CLASSIFICATION	 insert table select should not return resultset 
WITHOUT_CLASSIFICATION	 generate plan param param srcrel return todo grouping sets roll throws semanticexception 
WITHOUT_CLASSIFICATION	 all keywrappers must 
WITHOUT_CLASSIFICATION	 nothing hanlde future rud where may want add new state types 
WITHOUT_CLASSIFICATION	 required required required optional optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 structs recursively compare the fields 
WITHOUT_CLASSIFICATION	 only altering the database property and owner currently supported 
WITHOUT_CLASSIFICATION	 prscgbymcrscgbyr map aggregation prscgbyrcomplete copies desc cgbym cgbyr and remove cgbym and crs 
WITHOUT_CLASSIFICATION	 not use byteswritable here avoid the bytecopy from 
WITHOUT_CLASSIFICATION	 firstname john greg firstname alan firstname and firstname owen 
WITHOUT_CLASSIFICATION	 list bucketed table cannot converted transactional 
WITHOUT_CLASSIFICATION	 create keyvalue structs and add the respective fields each one 
WITHOUT_CLASSIFICATION	 now apply resource plan any this expected pretty rare 
WITHOUT_CLASSIFICATION	 verify that udf not whitelist fails 
WITHOUT_CLASSIFICATION	 this isnt equal then bogus key values have been inserted error out 
WITHOUT_CLASSIFICATION	 for getpos 
WITHOUT_CLASSIFICATION	 schema size cannot matched then could because constant folding converting partition column expression constant expression the constant expression will then get pruned column pruner since will not reference any columns 
WITHOUT_CLASSIFICATION	 the situation here and other readers currently such setbuffers never called serde reader case and serde reader case the only one that uses vectors when the readers are created with vectors streams are actually not created all could have set vectors then set buffers wed trouble here may need implement that this scenario ever supported 
WITHOUT_CLASSIFICATION	 expected exception 
WITHOUT_CLASSIFICATION	 column type 
WITHOUT_CLASSIFICATION	 for any exception conversion integer produce null 
WITHOUT_CLASSIFICATION	 set the for future reference other listeners 
WITHOUT_CLASSIFICATION	 identd 
WITHOUT_CLASSIFICATION	 cant assume jdk implementing this explicitly return longcomparex longminvalue longminvalue 
WITHOUT_CLASSIFICATION	 that could especially valuable given that this almost always the same set 
WITHOUT_CLASSIFICATION	 tokquery above insert 
WITHOUT_CLASSIFICATION	 for each alias add object inspector for short the last element 
WITHOUT_CLASSIFICATION	 need send the state update again the state has changed since the last one 
WITHOUT_CLASSIFICATION	 might not exist 
WITHOUT_CLASSIFICATION	 ordinals for various reasons why error this type can thrown 
WITHOUT_CLASSIFICATION	 when columns the union operator empty 
WITHOUT_CLASSIFICATION	 check elements the innermost struct 
WITHOUT_CLASSIFICATION	 param did not parse url not url 
WITHOUT_CLASSIFICATION	 extension acidoutputformat that allows users add additional options todo since this only used for testing could not control the writer some other way simplify link 
WITHOUT_CLASSIFICATION	 have initialize the thread pool before start this one uses 
WITHOUT_CLASSIFICATION	 there are couple possibilities consider here see should recurse not path regex and may match multiple entries this likely load and should liststatus for all relevant matches and recursecheck each those simply passing the filestatus recursetrue makes sense for this path singular directoryfile and exists recursetrue check all its children applicable path singular entity that does not exist recursefalse check its parent this likely case needing create dir that does not exist yet 
WITHOUT_CLASSIFICATION	 output type boolean 
WITHOUT_CLASSIFICATION	 the record writer provides stats get from there instead the serde 
WITHOUT_CLASSIFICATION	 return join collations 
WITHOUT_CLASSIFICATION	 only one send can active the same time 
WITHOUT_CLASSIFICATION	 grouping sets expressions 
WITHOUT_CLASSIFICATION	 for all the other aggregations set the mode partial 
WITHOUT_CLASSIFICATION	 walk the other part ast 
WITHOUT_CLASSIFICATION	 wait for the first item arrive the queue and process 
WITHOUT_CLASSIFICATION	 abandon the reuse attempt 
WITHOUT_CLASSIFICATION	 assertassertequals 
WITHOUT_CLASSIFICATION	 the filesplit constructor hadoop and package private cant use this constructor used create the object and then call readfields just pass nulls this super constructor 
WITHOUT_CLASSIFICATION	 see for some examples the merge ast for example given merge into acidtbl using nonacidpart source acidtbla sourcea when matched then update set sourceb when not matched then insert valuessourcea sourceb get ast like this tokmerge toktabname acidtbl toktabref toktabname nonacidpart source toktableorcol acidtbl toktableorcol source tokmatched tokupdate toksetcolumnsclause toktableorcol toktableorcol source toknotmatched tokinsert tokvaluerow toktableorcol source toktableorcol source and need produce multiinsert like this execute from acidtbl right outer join nonacidpart acidtbla sourcea insert into table acidtbl select nonacidparta nonacidpartb where acidtbla null insert into table acidtbl select targetrowid nonacidparta nonacidpartb where sort acidtblrowid 
WITHOUT_CLASSIFICATION	 create table with unique name testdb 
WITHOUT_CLASSIFICATION	 the field corresponds column family hbase 
WITHOUT_CLASSIFICATION	 use case amt unbounded caught during translation 
WITHOUT_CLASSIFICATION	 year month 
WITHOUT_CLASSIFICATION	 newsortcollist had null value means that least one the input sort columns did not have representative found the output columns assume the data longer sorted 
WITHOUT_CLASSIFICATION	 return the new join replacement 
WITHOUT_CLASSIFICATION	 not bothering with removing the entry theres limited number hosts and good chance that the entry will make back when the used for long duration 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 push the node the stack 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 query parallelism might fubar 
WITHOUT_CLASSIFICATION	 create testnullappender drop events without queryid 
WITHOUT_CLASSIFICATION	 set dest name mapping new context chid tokfrom 
WITHOUT_CLASSIFICATION	 with aim consolidate the join algorithms either hash based joins mapjoinoperator sortmerge based joins this operator being introduced this operator executes sortmerge based algorithm replaces both the joinoperator and the smbmapjoinoperator for the tez side things works either the map phase reduce phase the basic algorithm follows the processop receives row from big table order process the operator does fetch for rows from the other tables once have set rows from the other tables till hit new key more rows are brought from the big table and join performed 
WITHOUT_CLASSIFICATION	 get the final state the spark job and parses its job info 
WITHOUT_CLASSIFICATION	 alias only can selected 
WITHOUT_CLASSIFICATION	 here need see remaining columns are dynamic partition columns 
WITHOUT_CLASSIFICATION	 convert text and write 
WITHOUT_CLASSIFICATION	 joinkey null process each row different group 
WITHOUT_CLASSIFICATION	 third value 
WITHOUT_CLASSIFICATION	 build column names 
WITHOUT_CLASSIFICATION	 finally write the hash code 
WITHOUT_CLASSIFICATION	 threads are not configured then they will executed current thread itself 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 through all bytes the byte 
WITHOUT_CLASSIFICATION	 create the lazyobject for storing the rows 
WITHOUT_CLASSIFICATION	 overflow 
WITHOUT_CLASSIFICATION	 for now check only table 
WITHOUT_CLASSIFICATION	 since for left join are only interested rows from left can get rid right side 
WITHOUT_CLASSIFICATION	 column stats will inaccurate 
WITHOUT_CLASSIFICATION	 move the hfiles files from the task output directory the location specified the user 
WITHOUT_CLASSIFICATION	 all nulls are now explicit 
WITHOUT_CLASSIFICATION	 mysql postgres sql server 
WITHOUT_CLASSIFICATION	 nonjavadoc see parsecontext 
WITHOUT_CLASSIFICATION	 might just the default which case can drop that one its empty 
WITHOUT_CLASSIFICATION	 set the arguments for 
WITHOUT_CLASSIFICATION	 set the base class 
WITHOUT_CLASSIFICATION	 process the bytes that can escaped the last one cant 
WITHOUT_CLASSIFICATION	 create dummy select select all columns 
WITHOUT_CLASSIFICATION	 intentionally using the deprecated method make sure returns correct results 
WITHOUT_CLASSIFICATION	 all fields have been parsed bytes have been parsed need set the startpositions fieldslength ensure can use the same formula calculate the length each field for missing fields their starting positions will all the same which will make their lengths and uncheckedgetfield will return these fields nulls 
WITHOUT_CLASSIFICATION	 run with recover and save the output file can checked 
WITHOUT_CLASSIFICATION	 this scales down because possibility rounding 
WITHOUT_CLASSIFICATION	 change the children the original join operator point the map 
WITHOUT_CLASSIFICATION	 wait for all threads ready release them the same time 
WITHOUT_CLASSIFICATION	 nonacidnonbucket copy copy base bucket deletedelta bucket deletedelta bucket delta bucket delta bucket delta bucket directories files 
WITHOUT_CLASSIFICATION	 this only needed new grouping set key being created 
WITHOUT_CLASSIFICATION	 check for the escaped colon remove before doing the expensive regex replace 
WITHOUT_CLASSIFICATION	 make one materialized view 
WITHOUT_CLASSIFICATION	 the scale the decimal 
WITHOUT_CLASSIFICATION	 the offset the destination array for the beginning this missing range 
WITHOUT_CLASSIFICATION	 test this supposed only allow events 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 the for loop below 
WITHOUT_CLASSIFICATION	 note that for temp tables there need rename directories 
WITHOUT_CLASSIFICATION	 they dont seem work the ipc timeout needs set instead 
WITHOUT_CLASSIFICATION	 source hiveeventsproto 
WITHOUT_CLASSIFICATION	 have cases where are running query like countkey count such cases the readcolids either emptyfor count has just the key column either case nothing gets added the scan readallcolumns true are going add all columns else are just going add key filter run 
WITHOUT_CLASSIFICATION	 objecttypeptr 
WITHOUT_CLASSIFICATION	 generate data file for test 
WITHOUT_CLASSIFICATION	 input output row resolvers 
WITHOUT_CLASSIFICATION	 generate the vertexsubmit information for all events 
WITHOUT_CLASSIFICATION	 partitioned table expect partition values convert user specified map have lower case key names 
WITHOUT_CLASSIFICATION	 fallback integer parsing 
WITHOUT_CLASSIFICATION	 save can verify end stream need mark support wrap with bufferedinputstream 
WITHOUT_CLASSIFICATION	 dont expect corr vars withing join union for now only expect cor vars top level filter 
WITHOUT_CLASSIFICATION	 get the maximum the number tasks the stages the job and cancel the job goes beyond the limit 
WITHOUT_CLASSIFICATION	 bail having clause uses select expression aliases for aggregation expressions could what hive does but this non standard behavior making sure this doesnt cause issues when translating through calcite not worth 
WITHOUT_CLASSIFICATION	 dont care about these 
WITHOUT_CLASSIFICATION	 the user hasnt been reading row use the fast path 
WITHOUT_CLASSIFICATION	 assert only expected assert with this call 
WITHOUT_CLASSIFICATION	 update the list byte size 
WITHOUT_CLASSIFICATION	 close the writer finalize the metadata 
WITHOUT_CLASSIFICATION	 since may need split the task lets walk the graph bottomup 
WITHOUT_CLASSIFICATION	 grouping happens execution phase the input payload should not enable grouping here 
WITHOUT_CLASSIFICATION	 note would cause too many hash collisions 
WITHOUT_CLASSIFICATION	 dont throw exception the target location only contains the stagingdirs 
WITHOUT_CLASSIFICATION	 suffix first 
WITHOUT_CLASSIFICATION	 with split update new version the row new insert 
WITHOUT_CLASSIFICATION	 junk after trailing blank padding 
WITHOUT_CLASSIFICATION	 production thisfieldid requiredness fieldtype thisname fieldvalue commaorsemicolon 
WITHOUT_CLASSIFICATION	 enable zero copy record reader 
WITHOUT_CLASSIFICATION	 consider query like select countdistinct from group with rollup assume that hivemapaggr set true and false which case the group would execute single mapreduce job for the groupby the group keys should abgroupingsetfor rollup 
WITHOUT_CLASSIFICATION	 the form dim loj fact roj dim dim semij fact then return null 
WITHOUT_CLASSIFICATION	 the job configuration passed the configuration will cloned from the pig job configuration this necessary for overriding metastore configuration arguments like the metastore jdbc connection string and password the case embedded metastore which you get when hivemetastoreuris 
WITHOUT_CLASSIFICATION	 turn off metastoreside authorization 
WITHOUT_CLASSIFICATION	 masking and filtering should created here 
WITHOUT_CLASSIFICATION	 copy info that may required the new copy the settableudf calls below could replaced using this mechanism well 
WITHOUT_CLASSIFICATION	 the task generated the first pass 
WITHOUT_CLASSIFICATION	 this only required support the deprecated methods 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 hiveintest turn notification listener meta store 
WITHOUT_CLASSIFICATION	 remove entries from well dont start looking there again but only the highest write include this compaction job highestwriteid will null upgrade scenarios 
WITHOUT_CLASSIFICATION	 todo add interval and complex types 
WITHOUT_CLASSIFICATION	 join eligible for sortmerge join only eligible for bucketized map join dont need check for bucketized map join here are guaranteed that the join keys contain all the 
WITHOUT_CLASSIFICATION	 still null 
WITHOUT_CLASSIFICATION	 type with padded spaces 
WITHOUT_CLASSIFICATION	 from expressions 
WITHOUT_CLASSIFICATION	 the columns correspond 
WITHOUT_CLASSIFICATION	 case the query served hiveserver dont pad with spaces 
WITHOUT_CLASSIFICATION	 set the security key provider that the minidfs cluster initialized with encryption 
WITHOUT_CLASSIFICATION	 this ends being set test mvn for instance 
WITHOUT_CLASSIFICATION	 lets not use them anywhere unless absolutely necessary 
WITHOUT_CLASSIFICATION	 parser has done some verification the order tokens doesnt need verified here 
WITHOUT_CLASSIFICATION	 find the new delta file and make sure has the right contents 
WITHOUT_CLASSIFICATION	 overflow use slower alternate 
WITHOUT_CLASSIFICATION	 consistent with other apis like makeexpressiontree null returned indicate that the filter could not pushed down due parsing issue etc 
WITHOUT_CLASSIFICATION	 expr 
WITHOUT_CLASSIFICATION	 test both repeating 
WITHOUT_CLASSIFICATION	 can have long messages and should trimmable when published the jira via the jiraservice 
WITHOUT_CLASSIFICATION	 append not supported the cluster try use create 
WITHOUT_CLASSIFICATION	 bgenjjtree constlistcontents 
WITHOUT_CLASSIFICATION	 use our specialized hash table loader 
WITHOUT_CLASSIFICATION	 update config hive thread local well and init the metastore client 
WITHOUT_CLASSIFICATION	 for last stripe need get the last from the last row 
WITHOUT_CLASSIFICATION	 through the set partition columns and find their representatives the values 
WITHOUT_CLASSIFICATION	 table partitioned single key 
WITHOUT_CLASSIFICATION	 double columnscalar 
WITHOUT_CLASSIFICATION	 expected row count the join query well run 
WITHOUT_CLASSIFICATION	 finish the current innot clause and start new clause replace the commar 
WITHOUT_CLASSIFICATION	 sha matches 
WITHOUT_CLASSIFICATION	 first 
WITHOUT_CLASSIFICATION	 try with extra delta 
WITHOUT_CLASSIFICATION	 histogram bins use the percentile approximation 
WITHOUT_CLASSIFICATION	 reorder the wait queue note assume that noone will take our capacity based the fact that are doing this under the epic lock the epic lock removed wed need the steps under the queue lock could pass update state 
WITHOUT_CLASSIFICATION	 try load the composite factory one was provided 
WITHOUT_CLASSIFICATION	 check for the operators who will process rows coming this map operator 
WITHOUT_CLASSIFICATION	 call the real methods for these 
WITHOUT_CLASSIFICATION	 this can happen for data stream when all the values are null 
WITHOUT_CLASSIFICATION	 complex type support for now 
WITHOUT_CLASSIFICATION	 will make sure the ephemeral node created server will present even under connection session interruption will automatically handle retries 
WITHOUT_CLASSIFICATION	 this must called after all the explicit register calls 
WITHOUT_CLASSIFICATION	 try again with value that wont fit digits make 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring javautilmap 
WITHOUT_CLASSIFICATION	 find out the operator invoked mapside reduceside get the deserialized querydef reconstruct the transient variables querydef create input partition store rows coming from previous operator 
WITHOUT_CLASSIFICATION	 verify both get node heartbeat 
WITHOUT_CLASSIFICATION	 test hcatcontext 
WITHOUT_CLASSIFICATION	 verify individual arguments 
WITHOUT_CLASSIFICATION	 otherwise throw which means the file owner dead 
WITHOUT_CLASSIFICATION	 read the items from the input stream and confirm they match 
WITHOUT_CLASSIFICATION	 bucket map join only split goes memory 
WITHOUT_CLASSIFICATION	 semijoin dpp work considered child because work needs 
WITHOUT_CLASSIFICATION	 the dimension columns 
WITHOUT_CLASSIFICATION	 push each aggregate function down each side that contains all its arguments note that count because has arguments can 
WITHOUT_CLASSIFICATION	 this for updatedelete incl merge because introduce this column into the query part rewrite 
WITHOUT_CLASSIFICATION	 few situations where need the default table path without object 
WITHOUT_CLASSIFICATION	 repeating non null 
WITHOUT_CLASSIFICATION	 end else could not allocate 
WITHOUT_CLASSIFICATION	 the clause contains any expression 
WITHOUT_CLASSIFICATION	 make sure actually the table name 
WITHOUT_CLASSIFICATION	 this method will only return full resource plan when activating one give the caller the result atomically with the activation 
WITHOUT_CLASSIFICATION	 wait for the server bootup 
WITHOUT_CLASSIFICATION	 create hadoop configuration without inheriting default settings 
WITHOUT_CLASSIFICATION	 need set the global null that this reuse may pointless 
WITHOUT_CLASSIFICATION	 noinspection constantconditions 
WITHOUT_CLASSIFICATION	 this because function returns null the mapping for key removed the table mutated 
WITHOUT_CLASSIFICATION	 note this implements rather than extending because that class authspecific class and refactoring would kludge too many things that are potentially public api the base though what very simple thing protect against csrf attacks and that simply add another header running with xsrf filter enabled then will reject all requests that not contain this thus add this here the clientside this simple check prevents random other websites from redirecting browser that has login credentials from making request their behalf 
WITHOUT_CLASSIFICATION	 scopes execution code blocks with own local variables parameters and exception handlers 
WITHOUT_CLASSIFICATION	 bitvectorsize can use bits bytes represent 
WITHOUT_CLASSIFICATION	 selnocomputesel never seen this condition and also removing parent not safe current graph walker 
WITHOUT_CLASSIFICATION	 cant handle distinct 
WITHOUT_CLASSIFICATION	 otherwise the case analyze table compute statistics for columns 
WITHOUT_CLASSIFICATION	 create tablescan operator 
WITHOUT_CLASSIFICATION	 msb bits 
WITHOUT_CLASSIFICATION	 serialize path offset length using filesplit 
WITHOUT_CLASSIFICATION	 connect using token via beeline using script 
WITHOUT_CLASSIFICATION	 determine the keys for the current clause 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 enablecstr 
WITHOUT_CLASSIFICATION	 getwritableobject will convert lazyprimitive actual primitive writable objects 
WITHOUT_CLASSIFICATION	 pack the output into the scratch longs 
WITHOUT_CLASSIFICATION	 run cleaner 
WITHOUT_CLASSIFICATION	 the dispatcher fires the processor corresponding the closest matching rule and passes 
WITHOUT_CLASSIFICATION	 need reset during reopen when needed 
WITHOUT_CLASSIFICATION	 might generate other types that are not recognized field reference nested field but since this just additional optimization bail out without introducing the select groupby below the right input the left semijoin 
WITHOUT_CLASSIFICATION	 make sure nanos are preserved 
WITHOUT_CLASSIFICATION	 off dont sit and hammer the metastore tight loop 
WITHOUT_CLASSIFICATION	 finish the scheduled compaction for ttp and manually compact ttp make them comparable again 
WITHOUT_CLASSIFICATION	 the current subexpression precalculated groupby etc 
WITHOUT_CLASSIFICATION	 parsing statement now done logic 
WITHOUT_CLASSIFICATION	 until change made use the admin option default false with authorization 
WITHOUT_CLASSIFICATION	 corresponds semanalyzer gengroupbyplanmr corresponds semanalyzer gengroupbyplanmr corresponds semanalyzer 
WITHOUT_CLASSIFICATION	 table doesnt exist allow creating new one only the database state older than the update 
WITHOUT_CLASSIFICATION	 new batch has been fetched its not empty have more elements process 
WITHOUT_CLASSIFICATION	 maximum number paritions aggregated per cache node 
WITHOUT_CLASSIFICATION	 bottom operator does not contain offsetfetch 
WITHOUT_CLASSIFICATION	 heartbeat only for active tasks errors etc will reported directly 
WITHOUT_CLASSIFICATION	 tests for droppartitionstring dbname string tblname liststring partvals options method 
WITHOUT_CLASSIFICATION	 and mutable read position for thread safety when sharing hash map 
WITHOUT_CLASSIFICATION	 result could cached this object were made immutable 
WITHOUT_CLASSIFICATION	 the has decimal second vint flag not set 
WITHOUT_CLASSIFICATION	 append the stripe buffer the new orc file 
WITHOUT_CLASSIFICATION	 for singular arg count should not include null countcase when and departmentid not null then else null end 
WITHOUT_CLASSIFICATION	 put the mapping from part prunerpred 
WITHOUT_CLASSIFICATION	 hashmap 
WITHOUT_CLASSIFICATION	 parent table the same database change the actual destination otherwise keep name 
WITHOUT_CLASSIFICATION	 used for buffering appends before flush them out 
WITHOUT_CLASSIFICATION	 isinunmanaged 
WITHOUT_CLASSIFICATION	 the index the child which the last row was forwarded key group 
WITHOUT_CLASSIFICATION	 this means the reader has already been closed 
WITHOUT_CLASSIFICATION	 added for rounding off 
WITHOUT_CLASSIFICATION	 ceil integer argument noop but less code handle this way 
WITHOUT_CLASSIFICATION	 boolean double atomiclong 
WITHOUT_CLASSIFICATION	 get rid tokselexpr 
WITHOUT_CLASSIFICATION	 use different batch for vectorized input file format readers they can their work overlapped with work the row collection that vectorrow deserialization does this allows the partitions mix modes for flush the previously batched rows file change 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this call accessed from server side 
WITHOUT_CLASSIFICATION	 set these streams only the stripe different 
WITHOUT_CLASSIFICATION	 there cte walk the whole ast 
WITHOUT_CLASSIFICATION	 this optimizer for replacing tempfetching from temp with single direct fetching which means not needed any more when conversion completed 
WITHOUT_CLASSIFICATION	 selectivity key cardinality semijoin domain cardinality 
WITHOUT_CLASSIFICATION	 locations origloc becomes important 
WITHOUT_CLASSIFICATION	 cancel the previous expansion any 
WITHOUT_CLASSIFICATION	 dont read enough data for the first message decoded 
WITHOUT_CLASSIFICATION	 test params 
WITHOUT_CLASSIFICATION	 for functions that dont support window this provides the rows remaining added output functions that return window can throw this method shouldnt called for ranking fns return leadlag fns return the leadlag amt 
WITHOUT_CLASSIFICATION	 retrieve creation metadata needed 
WITHOUT_CLASSIFICATION	 there can only one instance per path 
WITHOUT_CLASSIFICATION	 build rel for select clause 
WITHOUT_CLASSIFICATION	 doas user different than logged user need check that that logged user authorized run doas 
WITHOUT_CLASSIFICATION	 task requested unknown host got host since host full and only host left random pool 
WITHOUT_CLASSIFICATION	 replacing inactive plan 
WITHOUT_CLASSIFICATION	 update cacheusage reference the pending entry 
WITHOUT_CLASSIFICATION	 remember for additional processing later 
WITHOUT_CLASSIFICATION	 are not pulling constants are pulling constants but this not constant 
WITHOUT_CLASSIFICATION	 only change txnmgr the setting has changed 
WITHOUT_CLASSIFICATION	 verify the fetched log from the beginning log file 
WITHOUT_CLASSIFICATION	 since interval types not currently supported table columns need create them expressions 
WITHOUT_CLASSIFICATION	 set the move task dependent the current task 
WITHOUT_CLASSIFICATION	 try read the given named url from the connection configuration file 
WITHOUT_CLASSIFICATION	 fieldcount get types for key value 
WITHOUT_CLASSIFICATION	 setup the completer for the database 
WITHOUT_CLASSIFICATION	 lets try populate those stats that dont require full scan 
WITHOUT_CLASSIFICATION	 the future can add checking for username groupname etc based for example 
WITHOUT_CLASSIFICATION	 this tests the case where older data has ambiguous structure but the correct interpretation can determined from the repeated name array 
WITHOUT_CLASSIFICATION	 likely found table scan operator 
WITHOUT_CLASSIFICATION	 mybyte 
WITHOUT_CLASSIFICATION	 exit the jvm ctrlc received and current statement executing 
WITHOUT_CLASSIFICATION	 finally for all the pools that have changes promote queued queries and rebalance 
WITHOUT_CLASSIFICATION	 interpret ctrlc request cancel the currently executing query 
WITHOUT_CLASSIFICATION	 following sel will for columns from udtf not adding sel here 
WITHOUT_CLASSIFICATION	 sort operator get deterministic results 
WITHOUT_CLASSIFICATION	 replace any that appear the prefix with regular 
WITHOUT_CLASSIFICATION	 inverse please see comments for multiply divide digit commad scale down fraction digits negative exponent number zeros after dot down shift 
WITHOUT_CLASSIFICATION	 timestamp not between 
WITHOUT_CLASSIFICATION	 number columns vector for each column number rows that qualify havent been filtered out array positions selected values 
WITHOUT_CLASSIFICATION	 test one random hiprecision decimal add 
WITHOUT_CLASSIFICATION	 first aggregation calculation for group 
WITHOUT_CLASSIFICATION	 write base file partition 
WITHOUT_CLASSIFICATION	 offset some offset middle the slice but see todo for firststart 
WITHOUT_CLASSIFICATION	 relocate all assigned slots from the old hash table 
WITHOUT_CLASSIFICATION	 count the number digits the mantissa including the decimal point and also locate the decimal point 
WITHOUT_CLASSIFICATION	 deleteupdate generates delete event for the original row 
WITHOUT_CLASSIFICATION	 was using hadoopinternal api get tasklogs disable until mapreduce fixed 
WITHOUT_CLASSIFICATION	 rewrite logic the original left input will joined with the new right input that has generated correlated variables propagated for any generated cor vars that are not used the join key pass them along joined later with the correlatorrels that produce them 
WITHOUT_CLASSIFICATION	 keylength hashcode slot 
WITHOUT_CLASSIFICATION	 run with each split strategy make sure there are differences 
WITHOUT_CLASSIFICATION	 default many existing explain classesmethods are nonvectorized vectorized methodsclasses have detail levels summary operator expression detail you the right you get more detail and the information for the previous levels included the default summary the path enumerations are used mark methodsclasses that lead vectorization specific ones can avoid displaying headers for things that have vectorization information below for example the tezwork class marked summarypath because leads both summary and operator methodsclasses and marked operatorpath because only display operator information for operator expression and detail typically live inside summary operator classes 
WITHOUT_CLASSIFICATION	 hash bits ref dont match 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that inner join singlecolumn string using hash map 
WITHOUT_CLASSIFICATION	 gets lock 
WITHOUT_CLASSIFICATION	 drop part listener events fired for public listeners historically for drop table case limiting internal listeners for now avoid unexpected calls for public listeners 
WITHOUT_CLASSIFICATION	 shuffle read metrics 
WITHOUT_CLASSIFICATION	 setup resolve make connections 
WITHOUT_CLASSIFICATION	 timeout msec 
WITHOUT_CLASSIFICATION	 there are cases where increment counters insert overwrite all partitions are newly created insert into table which creates new partitions some new partitions 
WITHOUT_CLASSIFICATION	 ascii are http control characters that need escaped and are and respectively 
WITHOUT_CLASSIFICATION	 thrift cannot return null result 
WITHOUT_CLASSIFICATION	 set the bucketing version 
WITHOUT_CLASSIFICATION	 the multiple parts partition predicate are joined using and 
WITHOUT_CLASSIFICATION	 used and not readfield 
WITHOUT_CLASSIFICATION	 get script failed with code some number 
WITHOUT_CLASSIFICATION	 restart asynchronously dont block the caller 
WITHOUT_CLASSIFICATION	 easier case take quick path 
WITHOUT_CLASSIFICATION	 authorization checks passed 
WITHOUT_CLASSIFICATION	 default false 
WITHOUT_CLASSIFICATION	 lru could also implement lru doubly linked list cacheentry keeps its node 
WITHOUT_CLASSIFICATION	 replacing getaliastowork should use that information instead 
WITHOUT_CLASSIFICATION	 shortmaxvalue 
WITHOUT_CLASSIFICATION	 gets lock 
WITHOUT_CLASSIFICATION	 returns true the join conditions execute over the same keys 
WITHOUT_CLASSIFICATION	 get configuration parameters 
WITHOUT_CLASSIFICATION	 next arena being allocated 
WITHOUT_CLASSIFICATION	 put the mapping from table scan operator prunerpred 
WITHOUT_CLASSIFICATION	 add path components explicitly because simply concatenating two path string not safe for example foo yields foo which will parsed authority path 
WITHOUT_CLASSIFICATION	 the default ones are created case null tests override this 
WITHOUT_CLASSIFICATION	 not make remote call under any circumstances this supposed async 
WITHOUT_CLASSIFICATION	 should now only have the unexpected folders left 
WITHOUT_CLASSIFICATION	 flush the metastore cache this assures that dont pick objects from previous query running this same thread this has done after get our semantic analyzer this when the connection the metastore made but before analyze 
WITHOUT_CLASSIFICATION	 note this includes any outer join keys that need into the small table area 
WITHOUT_CLASSIFICATION	 firstname owen 
WITHOUT_CLASSIFICATION	 longstats 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 need set type name should always timestamplocaltz 
WITHOUT_CLASSIFICATION	 update the credential provider location the jobconf 
WITHOUT_CLASSIFICATION	 external table 
WITHOUT_CLASSIFICATION	 the simd optimized form 
WITHOUT_CLASSIFICATION	 keys 
WITHOUT_CLASSIFICATION	 map join work 
WITHOUT_CLASSIFICATION	 set back that column pruner the optimizer will not the 
WITHOUT_CLASSIFICATION	 support for statistics that seems popular answer 
WITHOUT_CLASSIFICATION	 give 
WITHOUT_CLASSIFICATION	 verify that have got correct set deltas 
WITHOUT_CLASSIFICATION	 test gtltltegte for numbers 
WITHOUT_CLASSIFICATION	 verify that flattening and unflattenting nonulls works 
WITHOUT_CLASSIFICATION	 current txn aborted this wont read any data from other txns safe unregister the minopentxnid from minhistorylevel for the aborted txns even the txns the list are partially aborted safe delete from minhistorylevel the remaining txns are either 
WITHOUT_CLASSIFICATION	 reverse the value 
WITHOUT_CLASSIFICATION	 amnodeinfo will only cleared when querycomplete received for this query when detect failure the side failure heartbeat single queuelookupcallable added here have make sure one instance stays the queue till the query completes 
WITHOUT_CLASSIFICATION	 offer accepted and gets evicted 
WITHOUT_CLASSIFICATION	 the root path not useful anymore 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the partitions that are not required 
WITHOUT_CLASSIFICATION	 count returns true since count produces empty result set 
WITHOUT_CLASSIFICATION	 base env impl simply defers systemgetenv 
WITHOUT_CLASSIFICATION	 all bit numbers qualify multiply get nanoseconds 
WITHOUT_CLASSIFICATION	 carefully handle nulls 
WITHOUT_CLASSIFICATION	 populate map task 
WITHOUT_CLASSIFICATION	 the rhs references table sources and this qbjointree has lefttree hand the lefttree and let recursively handle there are cases passing condition down the leftside rightside dont contains references the lefttrees rightalias pass the lists down the leftside contains refs the lefttrees rightalias the rightside doesnt switch the leftcondal and leftconal lists and pass down the rightside contains refs the lefttrees rightalias the leftside doesnt switch the rightcondal and rightconal lists and pass down case both contain references the lefttrees rightalias cannot push the condition down either contain references both left right cannot push forward 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 there are two cases arraytype and arraystruct either case the element type the array represented tuple field schema the bags field schema the second case struct more naturally translates the tuple the first case arraytype simulate the tuple putting the single field tuple 
WITHOUT_CLASSIFICATION	 returning null from this can serve err condition 
WITHOUT_CLASSIFICATION	 crossing reduce sink file sink means the pruning isnt for this parent 
WITHOUT_CLASSIFICATION	 aggregation for semijoin 
WITHOUT_CLASSIFICATION	 druid returns bad request when not found 
WITHOUT_CLASSIFICATION	 generate groupbyoperator for partial aggregation 
WITHOUT_CLASSIFICATION	 merge only the register length matches 
WITHOUT_CLASSIFICATION	 not currently handled 
WITHOUT_CLASSIFICATION	 fall through miss locality localityrequested 
WITHOUT_CLASSIFICATION	 assume might bad will not try kill the query here just scrap the 
WITHOUT_CLASSIFICATION	 bloom known have higher fpp make tests pass give room for another 
WITHOUT_CLASSIFICATION	 calculate tags individually since the schema can evolve and can have different tags worst case both schemas are same and would end doing calculations twice get the same tag determine index value from fileschema determine index value from recordschema 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 its essentially mapjoindesc 
WITHOUT_CLASSIFICATION	 return the url based the aggregated connection properties 
WITHOUT_CLASSIFICATION	 fill down null flags undone 
WITHOUT_CLASSIFICATION	 walk through leaf predicates building 
WITHOUT_CLASSIFICATION	 address 
WITHOUT_CLASSIFICATION	 return 
WITHOUT_CLASSIFICATION	 scaledown 
WITHOUT_CLASSIFICATION	 ensure associated master key available 
WITHOUT_CLASSIFICATION	 undone unknown option 
WITHOUT_CLASSIFICATION	 release memory simple deallocation 
WITHOUT_CLASSIFICATION	 that can read the input format 
WITHOUT_CLASSIFICATION	 check privileges input and output objects 
WITHOUT_CLASSIFICATION	 the external client handling umbilical responses and the connection read the incoming data are not coupled calling close here make sure error one will cause the other closed well 
WITHOUT_CLASSIFICATION	 sum all nonnull long column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 data expected series data chunks the form chunk sizechunk byteschunk sizechunk bytes the final data chunk should length chunk which will indicate end input 
WITHOUT_CLASSIFICATION	 from the jar file the parent lib folder 
WITHOUT_CLASSIFICATION	 verify the data are intact even after applying applied event once again missing objects 
WITHOUT_CLASSIFICATION	 select all locks for this ext and see which ones are missing 
WITHOUT_CLASSIFICATION	 dont write rowseparatorbyte because that should handled file format 
WITHOUT_CLASSIFICATION	 set cache directory 
WITHOUT_CLASSIFICATION	 there are inputs rel does not need changed 
WITHOUT_CLASSIFICATION	 fail transactional property specified 
WITHOUT_CLASSIFICATION	 there are different cases for group depending mapreduce side hash aggregation grouping sets and column stats dont have column stats just assume hash aggregation disabled following are the possible cases and rule for cardinality estimation 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 doas set true for hiveserver will create proxy object for the session impl 
WITHOUT_CLASSIFICATION	 stop kafka ingestion first 
WITHOUT_CLASSIFICATION	 real grant time added metastore 
WITHOUT_CLASSIFICATION	 mix mix bit values reversibly this reversible any information abc before mix still abc after mix four pairs abc inputs are run through mix through mix reverse there are least bits the output that are sometimes the same for one pair and different for another pair this was tested for pairs that differed one bit two bits any combination top bits abc any combination bottom bits abc differ defined for and transformed the output delta gray code string commonly produced subtraction look like single bit difference the base values were pseudorandom all zero but one bit set all zero plus counter that starts zero some values for arotck arrangement that satisfy this are well didnt quite get bits diffing for differ defined with onebit base and twobit delta used choose the operations constants and arrangements the variables this does not achieve avalanche there are input bits abc that fail affect some output bits abc especially the most thoroughly mixed value but doesnt really even achieve avalanche this allows some parallelism readafterwrites are good doubling the number bits affected the goal mixing pulls the opposite direction the goal parallelism did what could rotates seem cost much shifts every machine could lay hands and rotates are much kinder the top and bottom bits used rotates define mixabc rotc rota rotb rotc rota rotb mixabc 
WITHOUT_CLASSIFICATION	 have found reduce sink 
WITHOUT_CLASSIFICATION	 replace trailing 
WITHOUT_CLASSIFICATION	 parse integer portion 
WITHOUT_CLASSIFICATION	 actually only most one loop 
WITHOUT_CLASSIFICATION	 previous call updated 
WITHOUT_CLASSIFICATION	 multiply self 
WITHOUT_CLASSIFICATION	 all key input columns are repeating generate key once lookup once since the key repeated must use entry regardless selectedinuse 
WITHOUT_CLASSIFICATION	 plan make sure need locks its possible theres nothing lock 
WITHOUT_CLASSIFICATION	 test the group needs partition level sort use the style shuffle shufflesort shouldnt used for this purpose see hive 
WITHOUT_CLASSIFICATION	 nothing else updated after the first update 
WITHOUT_CLASSIFICATION	 filter expression since will taken care partition pruner 
WITHOUT_CLASSIFICATION	 currently are not handling dynamic sized windows implied range based windows 
WITHOUT_CLASSIFICATION	 create union above all the branches 
WITHOUT_CLASSIFICATION	 dates are also valid timestamps dates are within 
WITHOUT_CLASSIFICATION	 should called after session registry checked 
WITHOUT_CLASSIFICATION	 test null values 
WITHOUT_CLASSIFICATION	 try deserialize using deserializeread our writable row objects created serde 
WITHOUT_CLASSIFICATION	 for canceling the query should bound session 
WITHOUT_CLASSIFICATION	 could get either query hint select expr 
WITHOUT_CLASSIFICATION	 get old table 
WITHOUT_CLASSIFICATION	 every row qualified newsizen then can ignore the sel vector streamline future operations selectedinuse will remain false 
WITHOUT_CLASSIFICATION	 initialized which may cause drop events 
WITHOUT_CLASSIFICATION	 check whether monotonic preserving cast otherwise cannot push 
WITHOUT_CLASSIFICATION	 sub fields 
WITHOUT_CLASSIFICATION	 load hiveserversitexml this hiveserver and file exists metastore can embedded within hiveserver such cases the conf params hiveserversitexml will override whats defined 
WITHOUT_CLASSIFICATION	 call setugi only unsecure mode 
WITHOUT_CLASSIFICATION	 the aggregation type minmax extrapolate from the leftright borders 
WITHOUT_CLASSIFICATION	 output columns 
WITHOUT_CLASSIFICATION	 make list before opening the rpc attack surface 
WITHOUT_CLASSIFICATION	 first adjust count expression any 
WITHOUT_CLASSIFICATION	 most the stuff can handle are generic function descriptions handle the special cases 
WITHOUT_CLASSIFICATION	 context for reading vectorized input file format 
WITHOUT_CLASSIFICATION	 exponent part 
WITHOUT_CLASSIFICATION	 must deterministic order map for consistent qtest output across java versions see hive 
WITHOUT_CLASSIFICATION	 could not transform anything bail out 
WITHOUT_CLASSIFICATION	 names have the name and names have the view name 
WITHOUT_CLASSIFICATION	 this check case the ciphertext actually makes sense some way 
WITHOUT_CLASSIFICATION	 txn 
WITHOUT_CLASSIFICATION	 for tokfunction the function name stored the first child unless its our special dictionary 
WITHOUT_CLASSIFICATION	 arithmetic with type timestamp and type intervalyearmonth longcolumnvector storing 
WITHOUT_CLASSIFICATION	 compute keys and values standardobjects 
WITHOUT_CLASSIFICATION	 the table was already marked transactionaltrue then the new value must match the old value any attempt alter the previous value will throw error exception will still thrown the previous value was null and attempt made set this behaviour can changed the future 
WITHOUT_CLASSIFICATION	 the reduce sink has not been introduced due bucketingsorting ignore 
WITHOUT_CLASSIFICATION	 once nodeid includes fragmentid this becomes lot more reliable 
WITHOUT_CLASSIFICATION	 need fill information about the key and value the reducer 
WITHOUT_CLASSIFICATION	 accumulo token information 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 create vectorized expr 
WITHOUT_CLASSIFICATION	 compute the total size per bucket 
WITHOUT_CLASSIFICATION	 changing this file make sure make corresponding changes 
WITHOUT_CLASSIFICATION	 repeating non null 
WITHOUT_CLASSIFICATION	 this constant boolean expression return the value 
WITHOUT_CLASSIFICATION	 this offer will rejected 
WITHOUT_CLASSIFICATION	 create reducesink operator followed another limit 
WITHOUT_CLASSIFICATION	 make special case for and 
WITHOUT_CLASSIFICATION	 create new map join operator 
WITHOUT_CLASSIFICATION	 end astnodeoriginjava 
WITHOUT_CLASSIFICATION	 only check one file exit the loop when have least one 
WITHOUT_CLASSIFICATION	 running state 
WITHOUT_CLASSIFICATION	 lookup cache entries table used the query for cache invalidation 
WITHOUT_CLASSIFICATION	 create test table with autopurge true 
WITHOUT_CLASSIFICATION	 print message reached least rows for join operand wont print message for the last join operand since the size will never goes joinemitinterval 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 europeuk 
WITHOUT_CLASSIFICATION	 show partition information 
WITHOUT_CLASSIFICATION	 group value 
WITHOUT_CLASSIFICATION	 nothing further test 
WITHOUT_CLASSIFICATION	 initialize the results 
WITHOUT_CLASSIFICATION	 build the expression based the partition predicate 
WITHOUT_CLASSIFICATION	 also try other patterns 
WITHOUT_CLASSIFICATION	 create list bucketing subdirectory only storedasdirectories 
WITHOUT_CLASSIFICATION	 fill the column vector with nulls 
WITHOUT_CLASSIFICATION	 events insert last repl repldumpidx 
WITHOUT_CLASSIFICATION	 check that start with default 
WITHOUT_CLASSIFICATION	 failure the sasl handler will throw exception indicating that the sasl negotiation failed 
WITHOUT_CLASSIFICATION	 the fact that stdev doesnt increase with increasing misscount captured outside 
WITHOUT_CLASSIFICATION	 allow null values for map 
WITHOUT_CLASSIFICATION	 statoptimization not applied for any reason the fetchtask should still not have been set 
WITHOUT_CLASSIFICATION	 this the option which will make delete writer 
WITHOUT_CLASSIFICATION	 need new hashmap since stats object reused across calls 
WITHOUT_CLASSIFICATION	 iterate backwards from the destination table the top the tree based the output column names get the new columns 
WITHOUT_CLASSIFICATION	 maxdecimal with round 
WITHOUT_CLASSIFICATION	 add data files the partitioned table 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 iterate through the list 
WITHOUT_CLASSIFICATION	 newproject plus any aggregates that the oldagg produces 
WITHOUT_CLASSIFICATION	 assumes deserializer not buffering itself position over uncompressed stream not sure what effect this has stats about job 
WITHOUT_CLASSIFICATION	 check elements the innermost union 
WITHOUT_CLASSIFICATION	 when not yet seen number lines fetch batch from remote hive server 
WITHOUT_CLASSIFICATION	 child process add here explicitly 
WITHOUT_CLASSIFICATION	 create the project before because need new project with extra column 
WITHOUT_CLASSIFICATION	 its hadoop 
WITHOUT_CLASSIFICATION	 columns statistics for complex datatypes are not supported yet 
WITHOUT_CLASSIFICATION	 triggerexpression 
WITHOUT_CLASSIFICATION	 store ugi transport the rpc setugi 
WITHOUT_CLASSIFICATION	 firstname lastname 
WITHOUT_CLASSIFICATION	 this yields empty because starting idx out bounds 
WITHOUT_CLASSIFICATION	 for debug tracing the name the map reduce task 
WITHOUT_CLASSIFICATION	 need differentiate between unmatched pattern and nonexistent database 
WITHOUT_CLASSIFICATION	 downstream code expects set valid value 
WITHOUT_CLASSIFICATION	 materialized 
WITHOUT_CLASSIFICATION	 cpartcols ppartcols have constant node expressions avoid the merge 
WITHOUT_CLASSIFICATION	 once called first will never able write again 
WITHOUT_CLASSIFICATION	 base case are eager child stateful 
WITHOUT_CLASSIFICATION	 match the given bytes with the like pattern 
WITHOUT_CLASSIFICATION	 for some reason even with mbeanexception available them runtime exceptions can still find their way through treat them the same mbeanexception 
WITHOUT_CLASSIFICATION	 settable 
WITHOUT_CLASSIFICATION	 instead retrying with this task will try pick different suitable task 
WITHOUT_CLASSIFICATION	 sharedwrite 
WITHOUT_CLASSIFICATION	 hive behavior where double decimal decimal gone 
WITHOUT_CLASSIFICATION	 check that the union has come out unscathed scathing unions allowed 
WITHOUT_CLASSIFICATION	 todo should checked server side embedded metastore throws metaexception remote metastore throws tprotocolexception 
WITHOUT_CLASSIFICATION	 map that says which mapjoin belongs which work item 
WITHOUT_CLASSIFICATION	 can later run the same logic that run 
WITHOUT_CLASSIFICATION	 same union order reveresed 
WITHOUT_CLASSIFICATION	 swe lock are examining exclusive 
WITHOUT_CLASSIFICATION	 check the character numbers with the length 
WITHOUT_CLASSIFICATION	 user wants file store based configuration 
WITHOUT_CLASSIFICATION	 partition columns will always the last 
WITHOUT_CLASSIFICATION	 drop partition will clean the partition entry from the compaction queue and hence cleaner have effect 
WITHOUT_CLASSIFICATION	 sequence file write 
WITHOUT_CLASSIFICATION	 list input expressions particular aggregate needs more will add expression the end and will create extra 
WITHOUT_CLASSIFICATION	 precompute groupby keys and store reducekeys 
WITHOUT_CLASSIFICATION	 throw error the user asked for sort merge bucketed mapjoin enforced and sort merge bucketed mapjoin cannot performed 
WITHOUT_CLASSIFICATION	 type mismatch when string col filtered string that looks like date 
WITHOUT_CLASSIFICATION	 show locks filter 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 attempt extended acl operations only its enabled but dont fail the operation regardless 
WITHOUT_CLASSIFICATION	 additional argument needed which the outputcolumn 
WITHOUT_CLASSIFICATION	 this delta directory can considered base 
WITHOUT_CLASSIFICATION	 setbyvalue 
WITHOUT_CLASSIFICATION	 figure out which stripes need read 
WITHOUT_CLASSIFICATION	 skip padded values 
WITHOUT_CLASSIFICATION	 add projection column projection vectorization context 
WITHOUT_CLASSIFICATION	 argumentcompletor always adds space after matched token this undesirable for function names because space after the opening parenthesis unnecessary and uncommon hive stack custom completor top our argumentcompletor 
WITHOUT_CLASSIFICATION	 column type char and constant type string then convert the constant char 
WITHOUT_CLASSIFICATION	 get result vector 
WITHOUT_CLASSIFICATION	 operator top 
WITHOUT_CLASSIFICATION	 implicitconvertable 
WITHOUT_CLASSIFICATION	 verify all scopes were recorded 
WITHOUT_CLASSIFICATION	 already existing table 
WITHOUT_CLASSIFICATION	 fetch name and type 
WITHOUT_CLASSIFICATION	 todo could generate vector row batches that vectorized execution may get triggered 
WITHOUT_CLASSIFICATION	 iterate for the second time get all the dependency 
WITHOUT_CLASSIFICATION	 are guaranteed that can get data here since size not zero 
WITHOUT_CLASSIFICATION	 post pass 
WITHOUT_CLASSIFICATION	 date highlow value stored long stats but allow users set highlow value using either date format yyyymmdd numeric format days since epoch 
WITHOUT_CLASSIFICATION	 lockid 
WITHOUT_CLASSIFICATION	 the hcatalog 
WITHOUT_CLASSIFICATION	 transformation sqcountcheckcount true filter generated top subquery which then joined left inner with outer query this transformation done add run time check using sqcountcheck throw error subquery producing zero row since with aggregate this will produce wrong results because further rewrite such queries into join 
WITHOUT_CLASSIFICATION	 the first child should the table are deleting from 
WITHOUT_CLASSIFICATION	 the session active but not found the pool internal error 
WITHOUT_CLASSIFICATION	 calcite 
WITHOUT_CLASSIFICATION	 complete path futures and schedule split generation 
WITHOUT_CLASSIFICATION	 want handle counters 
WITHOUT_CLASSIFICATION	 could muxdemux operators currently not supported 
WITHOUT_CLASSIFICATION	 need insert null before processing first row for the case preceding and preceding 
WITHOUT_CLASSIFICATION	 for analyze repl load walk through the dir structure available the path looking each and then each table and then setting the appropriate import job its place 
WITHOUT_CLASSIFICATION	 dont insist nullstructserde produce correct column names 
WITHOUT_CLASSIFICATION	 this sets the map operator contexts correctly 
WITHOUT_CLASSIFICATION	 length compression buffer compressed uncompressed length 
WITHOUT_CLASSIFICATION	 verify that udf black list fails even though its included whitelist 
WITHOUT_CLASSIFICATION	 cpu cost hashtable construction cost 
WITHOUT_CLASSIFICATION	 not fill tokenizer until user requests since filling could read data not meant for this instantiation 
WITHOUT_CLASSIFICATION	 case order only reducer used need another shuffle 
WITHOUT_CLASSIFICATION	 number output columns array pathnames each which corresponds column mapping from pathnames enum partname array returned column values object pool nonnull text avoid creating objects all the time array null column values input objectinspectors 
WITHOUT_CLASSIFICATION	 iso timestamps 
WITHOUT_CLASSIFICATION	 populated column map type deprecated slated for removal with 
WITHOUT_CLASSIFICATION	 assume default that would find everything 
WITHOUT_CLASSIFICATION	 net transfer cost 
WITHOUT_CLASSIFICATION	 remove parent reducesink operators 
WITHOUT_CLASSIFICATION	 execute query 
WITHOUT_CLASSIFICATION	 want try these whether they succeed fail 
WITHOUT_CLASSIFICATION	 ignore exception simply dont add this attribute back the resultant set 
WITHOUT_CLASSIFICATION	 initialize workload management 
WITHOUT_CLASSIFICATION	 entire batch filtered out 
WITHOUT_CLASSIFICATION	 generate umbilical token applies all splits 
WITHOUT_CLASSIFICATION	 array string 
WITHOUT_CLASSIFICATION	 smile mapper used read query results that are serialized binary instead json 
WITHOUT_CLASSIFICATION	 parts 
WITHOUT_CLASSIFICATION	 this the central piece for bucket map join and smb join has the following responsibilities group incoming splits based bucketing generate new serialized events for the grouped splits create routing table for the bucket map join and send serialized version payload for the edgemanager for smb join generate grouping according bucketing for the small table side 
WITHOUT_CLASSIFICATION	 case test with originals and deltas two split strategies with two splits for each 
WITHOUT_CLASSIFICATION	 there are nulls the inputcolvector 
WITHOUT_CLASSIFICATION	 null object not serialize 
WITHOUT_CLASSIFICATION	 original scale less than use original scale value otherwise preserve least fractional digits 
WITHOUT_CLASSIFICATION	 project the subset fields 
WITHOUT_CLASSIFICATION	 anything else preserve original call 
WITHOUT_CLASSIFICATION	 hive servers session input stream not used 
WITHOUT_CLASSIFICATION	 first stripes will satisfy the predicate and merged single split last stripe will 
WITHOUT_CLASSIFICATION	 skip walking the children 
WITHOUT_CLASSIFICATION	 register comes before the unregister for the previous dag 
WITHOUT_CLASSIFICATION	 insert query move itself noop 
WITHOUT_CLASSIFICATION	 return the mocked storagedescriptor 
WITHOUT_CLASSIFICATION	 fractional part 
WITHOUT_CLASSIFICATION	 find the base created for iow 
WITHOUT_CLASSIFICATION	 otherwise build timeline existing segments metadata storage 
WITHOUT_CLASSIFICATION	 ptf functions 
WITHOUT_CLASSIFICATION	 need notify any queries waiting the change from pending status 
WITHOUT_CLASSIFICATION	 try these best effort 
WITHOUT_CLASSIFICATION	 append regexes that user wanted add 
WITHOUT_CLASSIFICATION	 hostname 
WITHOUT_CLASSIFICATION	 specify the columns deserialize into range starting column number 
WITHOUT_CLASSIFICATION	 only need check conformance alter table enabled acid insertonly tables dont have conform acid requirement like orc bucketing 
WITHOUT_CLASSIFICATION	 calculate collection 
WITHOUT_CLASSIFICATION	 totalsize and the numfiles are set 
WITHOUT_CLASSIFICATION	 bucket columns are empty then numbuckets must set 
WITHOUT_CLASSIFICATION	 tests with queries which cannot executed with directsql because the contain like after falling back orm the number partitions cannot fetched the method they are fetched the method 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlnclob 
WITHOUT_CLASSIFICATION	 retrieve hivetxntimeout milliseconds its defined seconds then divide give safety factor 
WITHOUT_CLASSIFICATION	 add maintenance thread that will attempt trigger cache clean continuously 
WITHOUT_CLASSIFICATION	 didnt try launch job either means there was work got here the result communication failure with the either way want wait 
WITHOUT_CLASSIFICATION	 merge with the downstream col list 
WITHOUT_CLASSIFICATION	 store varchar type stripped pads 
WITHOUT_CLASSIFICATION	 then actually the compaction 
WITHOUT_CLASSIFICATION	 sanity check should not receive keys with tags 
WITHOUT_CLASSIFICATION	 for unpartitioned table partition values are specified 
WITHOUT_CLASSIFICATION	 this parameter constant 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 need add nonmatch row with nulls for small table values 
WITHOUT_CLASSIFICATION	 try obtaining udaf evaluators determine the ret type 
WITHOUT_CLASSIFICATION	 signed comparisons 
WITHOUT_CLASSIFICATION	 not creating view need track view expansions 
WITHOUT_CLASSIFICATION	 deserialize just the columns buffered batch which has only the nonkey inputs and streamed column outputs 
WITHOUT_CLASSIFICATION	 operands converted timestamp result interval daytime 
WITHOUT_CLASSIFICATION	 bgenjjtree field 
WITHOUT_CLASSIFICATION	 clone the token wed need set the service the one are talking 
WITHOUT_CLASSIFICATION	 alternate connect string specification configuration 
WITHOUT_CLASSIFICATION	 concurrent increase and revocation before the message sent 
WITHOUT_CLASSIFICATION	 add field separator 
WITHOUT_CLASSIFICATION	 are using test specific database then just drop the database 
WITHOUT_CLASSIFICATION	 stats again 
WITHOUT_CLASSIFICATION	 for mapside invocation ptfs cannot utilize the currentkeys null check decide invoking startpartition streaming mode hence this extra flag 
WITHOUT_CLASSIFICATION	 reserve bytes for writevaluerecord fill there might junk there null them 
WITHOUT_CLASSIFICATION	 enforce bucketingsorting disabled numbuckets will not set 
WITHOUT_CLASSIFICATION	 copy over configs touched above method 
WITHOUT_CLASSIFICATION	 skewed info 
WITHOUT_CLASSIFICATION	 set the configuration such that proxyuser can act behalf all users belonging the real groups that the 
WITHOUT_CLASSIFICATION	 skip the for loop will skip its value 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 are closing file without writing any data 
WITHOUT_CLASSIFICATION	 replicate the drop events and check tables are getting dropped target well 
WITHOUT_CLASSIFICATION	 alan firstname 
WITHOUT_CLASSIFICATION	 big table alias 
WITHOUT_CLASSIFICATION	 test repeating nonnull noselection 
WITHOUT_CLASSIFICATION	 hive does not support resultsetmetadata preparedstatement and hive describe does not support queries have execute the query with limit 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 retrieve from side 
WITHOUT_CLASSIFICATION	 since serde reuses memory will need make copy 
WITHOUT_CLASSIFICATION	 done with this branch 
WITHOUT_CLASSIFICATION	 assert that has got added table schema 
WITHOUT_CLASSIFICATION	 select operator project these two columns 
WITHOUT_CLASSIFICATION	 repeated string lstring 
WITHOUT_CLASSIFICATION	 the fixed size for the aggregation class already known get the variable portion the size every numrowsestimatesize rows 
WITHOUT_CLASSIFICATION	 all the txns the list have preallocated write ids for the given table then just return this for idempotent case 
WITHOUT_CLASSIFICATION	 find all configurations where the key contains any string from hiddenset 
WITHOUT_CLASSIFICATION	 provide this public method help explain vectorization show the evaluator classes 
WITHOUT_CLASSIFICATION	 privileges required dont need check this object privileges 
WITHOUT_CLASSIFICATION	 make sure the key mismatch causes error 
WITHOUT_CLASSIFICATION	 definitely not long 
WITHOUT_CLASSIFICATION	 add all columns from lateral view 
WITHOUT_CLASSIFICATION	 constructing the row object etc which will reused for all rows 
WITHOUT_CLASSIFICATION	 general filter cannot pushed below windowing calculation applying the filter before the aggregation function changes the results the windowing invocation when the filter the partition expression the over clause can pushed down for now dont support this 
WITHOUT_CLASSIFICATION	 add list bucketing pruner 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 there are nulls the inputcolvector 
WITHOUT_CLASSIFICATION	 walking through all active nodes they dont have potential capacity 
WITHOUT_CLASSIFICATION	 dont invoke from within scheduler lock 
WITHOUT_CLASSIFICATION	 instantiate bloomfiltercheck based input column type 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 since may calculation and produce scratch column need map the right column 
WITHOUT_CLASSIFICATION	 adds null element 
WITHOUT_CLASSIFICATION	 the conversion going through bit integer 
WITHOUT_CLASSIFICATION	 assume the char maximum length was enforced when the object was created 
WITHOUT_CLASSIFICATION	 job callable task for job submit operation overrides behavior execute submit job also overrides the behavior cleanup kill the job case job submission request timed out interrupted 
WITHOUT_CLASSIFICATION	 before the drop 
WITHOUT_CLASSIFICATION	 cannot scaled decimals that cannot represented instead use biginteger instead 
WITHOUT_CLASSIFICATION	 for each source read get shared lock 
WITHOUT_CLASSIFICATION	 deletedata ignoreunknowntable ifpurge 
WITHOUT_CLASSIFICATION	 its not worth adding the extra state 
WITHOUT_CLASSIFICATION	 set the escaping related properties 
WITHOUT_CLASSIFICATION	 exception happens during docopyonce then need call getfilestoretry with copy error true retry 
WITHOUT_CLASSIFICATION	 validate the second parameter which should array strings 
WITHOUT_CLASSIFICATION	 finally add project project out the last columns 
WITHOUT_CLASSIFICATION	 handle implementation instance and invoke appropriate inputformat method 
WITHOUT_CLASSIFICATION	 addpartitionsempty list normal operation 
WITHOUT_CLASSIFICATION	 dont want the table dir 
WITHOUT_CLASSIFICATION	 used for lazyfetchpartitions cases 
WITHOUT_CLASSIFICATION	 definitely not short 
WITHOUT_CLASSIFICATION	 first incremental dump 
WITHOUT_CLASSIFICATION	 bgenjjtree typelist 
WITHOUT_CLASSIFICATION	 rolename 
WITHOUT_CLASSIFICATION	 child jvm wont need change debug parameters when creating its own children 
WITHOUT_CLASSIFICATION	 all partitions with blurbhasnewcolumn were added after the table schema changed 
WITHOUT_CLASSIFICATION	 bad input 
WITHOUT_CLASSIFICATION	 remove the primitive types 
WITHOUT_CLASSIFICATION	 cast decimal input returntype 
WITHOUT_CLASSIFICATION	 keep cause the original exception 
WITHOUT_CLASSIFICATION	 reached the end field 
WITHOUT_CLASSIFICATION	 now start with low message size limit this should prevent any connections 
WITHOUT_CLASSIFICATION	 start the job 
WITHOUT_CLASSIFICATION	 user 
WITHOUT_CLASSIFICATION	 use tblproperties 
WITHOUT_CLASSIFICATION	 remember original string representation constant 
WITHOUT_CLASSIFICATION	 bail out 
WITHOUT_CLASSIFICATION	 reduce might end creating expression with null type conditionnull null reduced condition null with null type since this condition which will always boolean type cast boolean type 
WITHOUT_CLASSIFICATION	 open session and set the test data 
WITHOUT_CLASSIFICATION	 add default size for columns for which stats were not available 
WITHOUT_CLASSIFICATION	 add constant object overhead for struct 
WITHOUT_CLASSIFICATION	 decimal binary conversion 
WITHOUT_CLASSIFICATION	 txn not empty txn get better msg 
WITHOUT_CLASSIFICATION	 transient members initialized transientinit method 
WITHOUT_CLASSIFICATION	 does not sort memory footprint zero 
WITHOUT_CLASSIFICATION	 they are not equal could zip till here 
WITHOUT_CLASSIFICATION	 remove the container mapping 
WITHOUT_CLASSIFICATION	 dont need track anything for this task new notifications etc 
WITHOUT_CLASSIFICATION	 overflow this not expected 
WITHOUT_CLASSIFICATION	 with copy copy 
WITHOUT_CLASSIFICATION	 check whether the specified baseworks operator tree contains operator 
WITHOUT_CLASSIFICATION	 disable json file writing 
WITHOUT_CLASSIFICATION	 create proper tablecolumn desc for spilled tables 
WITHOUT_CLASSIFICATION	 check the partitions partspec the same defined table schema 
WITHOUT_CLASSIFICATION	 todo hive add additional information such executors container size etc 
WITHOUT_CLASSIFICATION	 normal close when there are inserts 
WITHOUT_CLASSIFICATION	 overrides values from the hivetezsite 
WITHOUT_CLASSIFICATION	 build hive table scan rel 
WITHOUT_CLASSIFICATION	 get old stats object present 
WITHOUT_CLASSIFICATION	 check they are operating the same table not move 
WITHOUT_CLASSIFICATION	 bootstrap done now incremental first test dblevel repl loads both dblevel and tablelevel repllastid must updated 
WITHOUT_CLASSIFICATION	 the partition did not change 
WITHOUT_CLASSIFICATION	 recreate the remote client not active any more 
WITHOUT_CLASSIFICATION	 optional int attemptnumber 
WITHOUT_CLASSIFICATION	 verify that there are missing privileges 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 rowid 
WITHOUT_CLASSIFICATION	 always localize files from conf duplicates are handled level todo could the same thing below and only localize missing 
WITHOUT_CLASSIFICATION	 get ssl socket 
WITHOUT_CLASSIFICATION	 reset buffers store filter push down columns 
WITHOUT_CLASSIFICATION	 close after expiry time and cache access should have tore down the client 
WITHOUT_CLASSIFICATION	 should set false when using this plugin avoid getting serialized event runtime 
WITHOUT_CLASSIFICATION	 the reader doesnt support offsets adjust offsets match future splits cached split was starting row start that row would skipped byte 
WITHOUT_CLASSIFICATION	 this needed for serde pagingspec uses jacksoninject for injecting selectqueryconfig 
WITHOUT_CLASSIFICATION	 now create three types delete deltas first has rowids divisible but not second has rowids divisible but not and the third has rowids divisible both and this should produce delete deltas that will thoroughly test the sortmerge logic when the delete events the delete delta files interleave the sort order 
WITHOUT_CLASSIFICATION	 complex tree with multiple parents 
WITHOUT_CLASSIFICATION	 constant for now will make configurable later 
WITHOUT_CLASSIFICATION	 remaining fields are cells addressed column name within row 
WITHOUT_CLASSIFICATION	 could not abort all txns this batch this may happen because parallel with this operation there was activity one the txns this batch this not likely but may happen client experiences long pause between heartbeats unusually longextreme pauses between heartbeat calls and other logic checklock lock etc 
WITHOUT_CLASSIFICATION	 return the multiset count for the lookup key 
WITHOUT_CLASSIFICATION	 then find the leftmost logical sibling select because thats what hive uses for aliases 
WITHOUT_CLASSIFICATION	 hiveconf getconf and setconf are this class because alterhandler extends configurable always use the configuration from hms handler making alterhandler not extend configurable not the scope the fix for hive 
WITHOUT_CLASSIFICATION	 significant effect when kpf very high 
WITHOUT_CLASSIFICATION	 test min max generates each stripe 
WITHOUT_CLASSIFICATION	 not change the location tested that the location will changed even the location not set null just remain the same 
WITHOUT_CLASSIFICATION	 top operator not pure limit bail out 
WITHOUT_CLASSIFICATION	 use zero copy record reader 
WITHOUT_CLASSIFICATION	 queryinfo will only exist more work came after this was scheduled 
WITHOUT_CLASSIFICATION	 move over the separator for next search 
WITHOUT_CLASSIFICATION	 mergepartial 
WITHOUT_CLASSIFICATION	 deserialize the bloom filter 
WITHOUT_CLASSIFICATION	 may drive this via configuration well 
WITHOUT_CLASSIFICATION	 close the connection 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 set the signature for the view materialized view 
WITHOUT_CLASSIFICATION	 replsrctxnids 
WITHOUT_CLASSIFICATION	 set the thread name with the logging prefix 
WITHOUT_CLASSIFICATION	 derived 
WITHOUT_CLASSIFICATION	 for negative testing purpose 
WITHOUT_CLASSIFICATION	 helper object that efficiently copies the big table columns that are for the big table 
WITHOUT_CLASSIFICATION	 order which the results should 
WITHOUT_CLASSIFICATION	 update tags reduce sinks 
WITHOUT_CLASSIFICATION	 maximum number open transactions thats allowed 
WITHOUT_CLASSIFICATION	 optional int myint 
WITHOUT_CLASSIFICATION	 operationtype 
WITHOUT_CLASSIFICATION	 object hash 
WITHOUT_CLASSIFICATION	 make the new projrel provide null indicator 
WITHOUT_CLASSIFICATION	 marker track there starting double quote without ending double quote 
WITHOUT_CLASSIFICATION	 because that monotonically increasing give new unique row ids 
WITHOUT_CLASSIFICATION	 double 
WITHOUT_CLASSIFICATION	 not external table 
WITHOUT_CLASSIFICATION	 optional vectorized value expressions that need run each batch 
WITHOUT_CLASSIFICATION	 mapreduce job create temporary file 
WITHOUT_CLASSIFICATION	 make sure all the null entries this long column output vector have their data vector element set the correct value per the specification prevent later arithmetic errors zerodivide 
WITHOUT_CLASSIFICATION	 stageid 
WITHOUT_CLASSIFICATION	 aux values 
WITHOUT_CLASSIFICATION	 executionexception raised job execution gets exception return client with the exception 
WITHOUT_CLASSIFICATION	 died for any reason lets get new set hosts 
WITHOUT_CLASSIFICATION	 see the comment 
WITHOUT_CLASSIFICATION	 batch processop 
WITHOUT_CLASSIFICATION	 not repeating 
WITHOUT_CLASSIFICATION	 should noop for sparse 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqltimestamp javautilcalendar 
WITHOUT_CLASSIFICATION	 neginfinity start inclusive 
WITHOUT_CLASSIFICATION	 oracle requires special treatment usual 
WITHOUT_CLASSIFICATION	 scalarcolumn 
WITHOUT_CLASSIFICATION	 hide constructor 
WITHOUT_CLASSIFICATION	 when reading the hashtable mapjoinobjectvalue calculates alias filter and provide join 
WITHOUT_CLASSIFICATION	 need set this because with and client side split generation end not finding the map work this because thread local madness tez split generation multithreaded plan cache uses thread locals setting causes the split gen code use the conf instead the map work 
WITHOUT_CLASSIFICATION	 the list servers the can locate 
WITHOUT_CLASSIFICATION	 write the results into the file 
WITHOUT_CLASSIFICATION	 read from the given accumulo table 
WITHOUT_CLASSIFICATION	 server 
WITHOUT_CLASSIFICATION	 make multiple 
WITHOUT_CLASSIFICATION	 the keys used store info into the job configuration 
WITHOUT_CLASSIFICATION	 import tasks generated the event table updated for table level load then need update the repl state any object 
WITHOUT_CLASSIFICATION	 helper determine the size the container requested from yarn falls back mapreduces map size tez container size isnt set 
WITHOUT_CLASSIFICATION	 clear everything 
WITHOUT_CLASSIFICATION	 parse fraction portion 
WITHOUT_CLASSIFICATION	 this based vectorizer code minus the validation 
WITHOUT_CLASSIFICATION	 helpers 
WITHOUT_CLASSIFICATION	 some selected binary operators null etc one the expressions are 
WITHOUT_CLASSIFICATION	 return path may have aggrf aggrf gby and then select aggrf aggrf sel thus need use colexp find out which position corresponding which position 
WITHOUT_CLASSIFICATION	 look for reducesinkoperator 
WITHOUT_CLASSIFICATION	 see the comment inside 
WITHOUT_CLASSIFICATION	 the child single range 
WITHOUT_CLASSIFICATION	 rename needs change the data location and move the data the new location corresponding the new name the table not virtual view and the table not external table and the user didnt change the default location new location empty and 
WITHOUT_CLASSIFICATION	 basic adding and removing operations called only while holding lock 
WITHOUT_CLASSIFICATION	 first delete the materialized views 
WITHOUT_CLASSIFICATION	 null direction 
WITHOUT_CLASSIFICATION	 reuse existing perf logger 
WITHOUT_CLASSIFICATION	 check the new entry contains the existing 
WITHOUT_CLASSIFICATION	 merge the sidefile into the newly created hash table 
WITHOUT_CLASSIFICATION	 column value lengths for each the selected columns 
WITHOUT_CLASSIFICATION	 finally open the store 
WITHOUT_CLASSIFICATION	 inline map join operator 
WITHOUT_CLASSIFICATION	 smbjoin not supported 
WITHOUT_CLASSIFICATION	 the ast has children the second has partition spec 
WITHOUT_CLASSIFICATION	 ctas case the file output format and serde are defined the create table command rather than taking the default value 
WITHOUT_CLASSIFICATION	 ival 
WITHOUT_CLASSIFICATION	 this position parent constant reverse look colexprmap find the childcolname 
WITHOUT_CLASSIFICATION	 the utility this method not certain 
WITHOUT_CLASSIFICATION	 setting the hidden list 
WITHOUT_CLASSIFICATION	 assume that ams and run under the same user 
WITHOUT_CLASSIFICATION	 check for this pattern the pattern matching could simplified rules can applied during decorrelation correlaterelleft correlation condition true leftinputrel projecta rexnode aggregate groupby agg agg projectb references covar rightinputrel 
WITHOUT_CLASSIFICATION	 inserts are done 
WITHOUT_CLASSIFICATION	 transactions should committed 
WITHOUT_CLASSIFICATION	 code copied over from udfweekofyear implementation 
WITHOUT_CLASSIFICATION	 subscriber can get notification newly add partition particular table listening topic named dbnametablename and message selector string hcatevent hcataddpartition 
WITHOUT_CLASSIFICATION	 returns name hashfile made hashtablesink which read mapjoin 
WITHOUT_CLASSIFICATION	 wed increase the base limit and adjust dynamically based and processing perf delays 
WITHOUT_CLASSIFICATION	 skip the potential big table identified above 
WITHOUT_CLASSIFICATION	 load this metastore and file exists 
WITHOUT_CLASSIFICATION	 this tabledesc does not contain the partitioning columns 
WITHOUT_CLASSIFICATION	 the order which the two paths are added important the lateral view join operator depends having the select operator give the row first 
WITHOUT_CLASSIFICATION	 owid with the given value found searching now for rowid retrieve the actual compressedowid that matched check rowid outside the range all rowids present for this owid 
WITHOUT_CLASSIFICATION	 first use getparameters prune the stats 
WITHOUT_CLASSIFICATION	 the following actions are authorized through 
WITHOUT_CLASSIFICATION	 grouping should pruned which the last key columns see 
WITHOUT_CLASSIFICATION	 default just the hive catalog should cached 
WITHOUT_CLASSIFICATION	 partition spec string input file names big 
WITHOUT_CLASSIFICATION	 are using plain sasl connection with userpassword 
WITHOUT_CLASSIFICATION	 can get the table definition from tbl 
WITHOUT_CLASSIFICATION	 happening via statstask via user 
WITHOUT_CLASSIFICATION	 right trim slice byte array and return the new byte length 
WITHOUT_CLASSIFICATION	 overriden copy start index end index that needed through optimization for maskingfiltering 
WITHOUT_CLASSIFICATION	 nothing special just use the children methods 
WITHOUT_CLASSIFICATION	 error close the channel 
WITHOUT_CLASSIFICATION	 cant subtract null 
WITHOUT_CLASSIFICATION	 for subclasses 
WITHOUT_CLASSIFICATION	 the expression not null nonnullable column then can either remove the filter replace with empty 
WITHOUT_CLASSIFICATION	 submit job request maximum concurrent job submit requests are configured then submit request will executed thread from thread pool job submit request time out configured then request execution thread will interrupted thread times out also does best efforts identify job submitted and kill quietly 
WITHOUT_CLASSIFICATION	 input 
WITHOUT_CLASSIFICATION	 this query create pending cache entry but was never saved with real results cleanup this step required there may queries waiting this pending cache entry this entry will notify the waiters that this entry cannot used 
WITHOUT_CLASSIFICATION	 alright fine well use our defaults 
WITHOUT_CLASSIFICATION	 day hour minute second 
WITHOUT_CLASSIFICATION	 can this task merged with the child task this can happen big table being 
WITHOUT_CLASSIFICATION	 impossible throw any json exceptions 
WITHOUT_CLASSIFICATION	 copy nulls from the nonrepeating side 
WITHOUT_CLASSIFICATION	 doing singlevalue the entire input 
WITHOUT_CLASSIFICATION	 this code doesnt propagate 
WITHOUT_CLASSIFICATION	 check any the partitions already exists desttable 
WITHOUT_CLASSIFICATION	 test that existing sharedread table with new sharedwrite coalesces 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 comment 
WITHOUT_CLASSIFICATION	 combine credentials and credentials from job takes precedence for freshness 
WITHOUT_CLASSIFICATION	 fetch same schema 
WITHOUT_CLASSIFICATION	 set service and client 
WITHOUT_CLASSIFICATION	 semijoin dpp work considered descendant because work needs 
WITHOUT_CLASSIFICATION	 inner bigtable only join specific 
WITHOUT_CLASSIFICATION	 check the stats 
WITHOUT_CLASSIFICATION	 objectinspectoroi return 
WITHOUT_CLASSIFICATION	 finally add the partitioning columns 
WITHOUT_CLASSIFICATION	 needresult 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 physical dir gets created 
WITHOUT_CLASSIFICATION	 create two bucketed tables 
WITHOUT_CLASSIFICATION	 used the join input too large fit memory 
WITHOUT_CLASSIFICATION	 this get should fail because its variance way past maxvariance 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 bgenjjtree typedef 
WITHOUT_CLASSIFICATION	 assert bucket listing expected 
WITHOUT_CLASSIFICATION	 set the transactionlocking the derby metastore 
WITHOUT_CLASSIFICATION	 finally write out the pieces sign scale digits 
WITHOUT_CLASSIFICATION	 only increase the targets here 
WITHOUT_CLASSIFICATION	 calling from code that does not use filters asis 
WITHOUT_CLASSIFICATION	 make some noisy avoid disk caches data 
WITHOUT_CLASSIFICATION	 required required optional optional optional 
WITHOUT_CLASSIFICATION	 multiple clients may initialize the hook the same time 
WITHOUT_CLASSIFICATION	 and insert overwrite insert into table 
WITHOUT_CLASSIFICATION	 distinct 
WITHOUT_CLASSIFICATION	 send actual heartbeat only the task count 
WITHOUT_CLASSIFICATION	 tests whether there another list element another map keyvalue pair 
WITHOUT_CLASSIFICATION	 ensure that throw out any exceptions above highwatermark make link iswriteidvalidlong faster 
WITHOUT_CLASSIFICATION	 white flag with horizontal middle black stripe uff bytes 
WITHOUT_CLASSIFICATION	 with data 
WITHOUT_CLASSIFICATION	 mark txn aborted mark txn aborted 
WITHOUT_CLASSIFICATION	 todo mssplit certain dont need this already adds this resource static 
WITHOUT_CLASSIFICATION	 logger the string appender 
WITHOUT_CLASSIFICATION	 its not clear how filtering for stringcol should work which side coerced let the expression evaluation sort this one out not metastore 
WITHOUT_CLASSIFICATION	 try dropping table user should fail 
WITHOUT_CLASSIFICATION	 seems reasonable upper limit for this 
WITHOUT_CLASSIFICATION	 need make sure that the key type and the value types are settable 
WITHOUT_CLASSIFICATION	 create the ptfdesc from the qspec attached this 
WITHOUT_CLASSIFICATION	 arent building split start new one 
WITHOUT_CLASSIFICATION	 the following datetimeinterval arithmetic operations can done using the vectorized values 
WITHOUT_CLASSIFICATION	 unpartitioned table 
WITHOUT_CLASSIFICATION	 write committed txn should valid 
WITHOUT_CLASSIFICATION	 try invalid state transition the handle this ensures that the actual state change were interested actually happened since internally the handle serializes state changes 
WITHOUT_CLASSIFICATION	 are overwriting segments with new versions 
WITHOUT_CLASSIFICATION	 undone multiple types 
WITHOUT_CLASSIFICATION	 create some data 
WITHOUT_CLASSIFICATION	 insert having plan here 
WITHOUT_CLASSIFICATION	 when qbjointree merged into this one its leftpos filters can refer any the srces this qbjointree particular filterforpushing refers multiple srces this qbjointree collect them into postjoinfilters then add filter operator after the join operator for this qbjointree 
WITHOUT_CLASSIFICATION	 blank byte latin letter small capital bytes 
WITHOUT_CLASSIFICATION	 tests for int partitions method 
WITHOUT_CLASSIFICATION	 not special cased later subquery remove rule 
WITHOUT_CLASSIFICATION	 cluster info changes qam should called with the same fractions 
WITHOUT_CLASSIFICATION	 preempt specific host 
WITHOUT_CLASSIFICATION	 override this for concrete initialization 
WITHOUT_CLASSIFICATION	 not used 
WITHOUT_CLASSIFICATION	 this map which vectorized row batch columns are the big table key columns since may have key expressions that produce new scratch columns need mapping 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 final row computation will consider join type 
WITHOUT_CLASSIFICATION	 tablename 
WITHOUT_CLASSIFICATION	 here there only split since only have data for bucket 
WITHOUT_CLASSIFICATION	 the following data might changed 
WITHOUT_CLASSIFICATION	 index disabled 
WITHOUT_CLASSIFICATION	 drop partition will clean the partition entry from the compaction queue and hence worker have effect 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 generate lineage info for create view statements lineagelogger hook configured 
WITHOUT_CLASSIFICATION	 always have find least one location otw the test useless 
WITHOUT_CLASSIFICATION	 the future support arbitrary characters identifiers then well need escape any backticks identifier doubling them 
WITHOUT_CLASSIFICATION	 add materializations planner 
WITHOUT_CLASSIFICATION	 there could multiple keyvaluepairs separated comma 
WITHOUT_CLASSIFICATION	 singlecolumn string specific lookup key 
WITHOUT_CLASSIFICATION	 decrease refcount 
WITHOUT_CLASSIFICATION	 for now bail out decimal constants with larger scale than column scale 
WITHOUT_CLASSIFICATION	 the thread should stop after this 
WITHOUT_CLASSIFICATION	 create read intermediate data 
WITHOUT_CLASSIFICATION	 need the mapping and type information 
WITHOUT_CLASSIFICATION	 pick the maximum reducers across all parents the reduce tasks 
WITHOUT_CLASSIFICATION	 use this constructor when only ascending sort order used 
WITHOUT_CLASSIFICATION	 default prefix deltaprefix 
WITHOUT_CLASSIFICATION	 testonly counter 
WITHOUT_CLASSIFICATION	 dynamic partition keys should added field schemas 
WITHOUT_CLASSIFICATION	 invariant that avros tag ordering must match hives 
WITHOUT_CLASSIFICATION	 the buffer has lived the heap all along restore heap property 
WITHOUT_CLASSIFICATION	 configure preexechooks with disallow transform queries 
WITHOUT_CLASSIFICATION	 the main thread throws exception for some reason propagate the exception the client and initiate safe shutdown 
WITHOUT_CLASSIFICATION	 columns starts with tablefieldssize 
WITHOUT_CLASSIFICATION	 any child null set unknown true 
WITHOUT_CLASSIFICATION	 get the children expr strings 
WITHOUT_CLASSIFICATION	 precisionscale enforcement methods 
WITHOUT_CLASSIFICATION	 cannot handle null scalar parameter 
WITHOUT_CLASSIFICATION	 only support decimal columns when the have the same scale 
WITHOUT_CLASSIFICATION	 add the remaining fields 
WITHOUT_CLASSIFICATION	 unknown unknown unknown 
WITHOUT_CLASSIFICATION	 nonjavadoc serializes this value into the format used link javamathbiginteger this used for fast assignment decimal hivedecimalwritable internal storage see openjdk for reference implementation param scratch param signum return 
WITHOUT_CLASSIFICATION	 struct 
WITHOUT_CLASSIFICATION	 theres failure from here when the metadata updated there will data the partition error while trying read the partition the archive files have been moved the original partition directory but rerunning the archive command will allow recovery 
WITHOUT_CLASSIFICATION	 since this dynamic partitioned hash join the work for this join should reducework 
WITHOUT_CLASSIFICATION	 ignoreprotection 
WITHOUT_CLASSIFICATION	 test getter for configuration object 
WITHOUT_CLASSIFICATION	 note that not need lock for this entity this used operations like alter table partition where its actually the partition that needs locked even though the table 
WITHOUT_CLASSIFICATION	 replace reducesinkop with hashtablesinkop for the rsops which are parents mjop 
WITHOUT_CLASSIFICATION	 this the object hash class variation 
WITHOUT_CLASSIFICATION	 augment 
WITHOUT_CLASSIFICATION	 boolean values are stores and convert and compare 
WITHOUT_CLASSIFICATION	 this column not included 
WITHOUT_CLASSIFICATION	 add initial column vectorization context when 
WITHOUT_CLASSIFICATION	 getmacroname null always treat different from others 
WITHOUT_CLASSIFICATION	 update the seed 
WITHOUT_CLASSIFICATION	 transactional found but the value not expected range 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 have data until the end current block had until the beginning 
WITHOUT_CLASSIFICATION	 record disk more data write buffer 
WITHOUT_CLASSIFICATION	 path has writing permissions 
WITHOUT_CLASSIFICATION	 singleton 
WITHOUT_CLASSIFICATION	 format list columns for create statement 
WITHOUT_CLASSIFICATION	 set the skipped field null 
WITHOUT_CLASSIFICATION	 check whether hiveconf initialize logj correctly 
WITHOUT_CLASSIFICATION	 partition name value 
WITHOUT_CLASSIFICATION	 events 
WITHOUT_CLASSIFICATION	 task has been queued for execution the driver 
WITHOUT_CLASSIFICATION	 thrown for example there such user the system 
WITHOUT_CLASSIFICATION	 this method tries merge the join with its left child the left 
WITHOUT_CLASSIFICATION	 nonqualified types should simply return the typeinfo associated with that type 
WITHOUT_CLASSIFICATION	 output will also repeating 
WITHOUT_CLASSIFICATION	 confirm the batch sizes were the two calls create partitions 
WITHOUT_CLASSIFICATION	 avoid npe below for some reason argument has multiline command 
WITHOUT_CLASSIFICATION	 check partition exists exists skip the overwrite 
WITHOUT_CLASSIFICATION	 check the query results were cacheable and created pending cache entry successfully saved the results the usage would have changed queryusingcache 
WITHOUT_CLASSIFICATION	 can prevent updates from being sent out the new node 
WITHOUT_CLASSIFICATION	 finally write out the pieces sign power digits 
WITHOUT_CLASSIFICATION	 different paths 
WITHOUT_CLASSIFICATION	 make sure that the port unused 
WITHOUT_CLASSIFICATION	 now that have the big table index get real numreducers value based big table 
WITHOUT_CLASSIFICATION	 then uks 
WITHOUT_CLASSIFICATION	 with the releases probably running the other closing thread 
WITHOUT_CLASSIFICATION	 not check for acid does not create new parts and this expensive hell todo add api get table name list for archived parts with single call nobody uses this could skip the whole thing 
WITHOUT_CLASSIFICATION	 number dynamic partition columns involved partitions 
WITHOUT_CLASSIFICATION	 put the beginning relies the knowledge internal implementation pave 
WITHOUT_CLASSIFICATION	 this would usually block boundary this would usually block boundary 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream 
WITHOUT_CLASSIFICATION	 this must hadoop version earlier resorting the earlier method getting the properties which uses saslprops field 
WITHOUT_CLASSIFICATION	 source exists rename otherwise create empty directory 
WITHOUT_CLASSIFICATION	 more original files read 
WITHOUT_CLASSIFICATION	 reverify directory layout and query result using the same logic above 
WITHOUT_CLASSIFICATION	 matches filprojts 
WITHOUT_CLASSIFICATION	 there are columns projection only join just assume weight 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 read with partition filter 
WITHOUT_CLASSIFICATION	 fraction below implicitly 
WITHOUT_CLASSIFICATION	 filter the name list removing elements one one can slow arraylist 
WITHOUT_CLASSIFICATION	 set check before checking 
WITHOUT_CLASSIFICATION	 assert 
WITHOUT_CLASSIFICATION	 not cleaning the interrupt status 
WITHOUT_CLASSIFICATION	 rcfile specific parameter 
WITHOUT_CLASSIFICATION	 this list type 
WITHOUT_CLASSIFICATION	 handle overloaded methods first 
WITHOUT_CLASSIFICATION	 hide constructor for make benefit glorious singleton 
WITHOUT_CLASSIFICATION	 nothing trim ascii 
WITHOUT_CLASSIFICATION	 check configuration for any userprovided authorization definition 
WITHOUT_CLASSIFICATION	 loginfowriting value valueoffset length valuelength unlikely case length key and value for the very first entry want tell this apart from empty value well just advance one byte this byte will lost 
WITHOUT_CLASSIFICATION	 may need strip away the stop marker when thrift mode 
WITHOUT_CLASSIFICATION	 public static getinstanceint fields return 
WITHOUT_CLASSIFICATION	 are currently searching the data for place begin not return data yet 
WITHOUT_CLASSIFICATION	 map cvalue map rowvalues assertequals cvaluesize 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 start from clean slate metastore 
WITHOUT_CLASSIFICATION	 reducesink only available during compile 
WITHOUT_CLASSIFICATION	 get partition this partition should not have the newly added column since cascade option 
WITHOUT_CLASSIFICATION	 remove from current root task and add conditional task root tasks 
WITHOUT_CLASSIFICATION	 derby script format run file 
WITHOUT_CLASSIFICATION	 without data 
WITHOUT_CLASSIFICATION	 get the avglen 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 try deserialize 
WITHOUT_CLASSIFICATION	 have least nonblank skip trailing blank characters 
WITHOUT_CLASSIFICATION	 this check for controlling the correctness the current state 
WITHOUT_CLASSIFICATION	 could changed local execution optimization 
WITHOUT_CLASSIFICATION	 set other table properties 
WITHOUT_CLASSIFICATION	 small optimization delete delta cant raw format 
WITHOUT_CLASSIFICATION	 http mode 
WITHOUT_CLASSIFICATION	 retrieve the user name the final validation step 
WITHOUT_CLASSIFICATION	 not see the mesg follows type format which typically the case 
WITHOUT_CLASSIFICATION	 and have their type infos 
WITHOUT_CLASSIFICATION	 generate the map join operator 
WITHOUT_CLASSIFICATION	 verify escaped partition names dont return partitions 
WITHOUT_CLASSIFICATION	 udaf assumed deterministic 
WITHOUT_CLASSIFICATION	 verify added entry for each output 
WITHOUT_CLASSIFICATION	 convert the bucket mapjoin operator sortmerge map join operator 
WITHOUT_CLASSIFICATION	 not supported for tables sampler breaks separate dirs into splits resulting mismatch when the downstream task looks them again assuming they are table roots could somehow unset the flag for the main job when the sampler succeeds since the sampler will limit the input the the correct directories but dont care about 
WITHOUT_CLASSIFICATION	 the context table name can null repl load done full but need table name for alloc write and that received from source 
WITHOUT_CLASSIFICATION	 the result the last character which occupies bytes 
WITHOUT_CLASSIFICATION	 update based the final value the counters 
WITHOUT_CLASSIFICATION	 simply dispatch the call the right method for the actual sub type basework 
WITHOUT_CLASSIFICATION	 create mapping from the group columns the table columns 
WITHOUT_CLASSIFICATION	 generate the statement analyze table tablename compute statistics for columns nonpartitioned table case will generate tsselgbyrsgbyselfs operator staticpartitioned table case will generate operator dynamicpartitioned table case will generate operator however not need specify the partitionspec because the data going inserted that specific partition can compose the staticdynamic partition using select operator 
WITHOUT_CLASSIFICATION	 timestamp scalarcolumn 
WITHOUT_CLASSIFICATION	 just had leading zeroes and possibly dot and trailing blanks value 
WITHOUT_CLASSIFICATION	 get the next inlist value element needed 
WITHOUT_CLASSIFICATION	 formal conversion 
WITHOUT_CLASSIFICATION	 create new union operator 
WITHOUT_CLASSIFICATION	 undone also remember virtualcolumncount 
WITHOUT_CLASSIFICATION	 the input timestamps are stored long values 
WITHOUT_CLASSIFICATION	 partitioned insert 
WITHOUT_CLASSIFICATION	 write has changed not valid anymore need recompile 
WITHOUT_CLASSIFICATION	 not remove the parameter yet because have separate initialization routine that will use down below 
WITHOUT_CLASSIFICATION	 extract join key expressions from hivesortexchange 
WITHOUT_CLASSIFICATION	 requested 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 check that dont find unexpected columns 
WITHOUT_CLASSIFICATION	 jdbc url each list keyvalkeyval and sessvarlist sessconfmap hiveconflist hiveconfmap hivevarlist hivevarmap 
WITHOUT_CLASSIFICATION	 check the partition exists shouldnt 
WITHOUT_CLASSIFICATION	 catalog 
WITHOUT_CLASSIFICATION	 this simulates the completion delete from tab txn 
WITHOUT_CLASSIFICATION	 copied the entire buffer 
WITHOUT_CLASSIFICATION	 will using this for each while also sending rgs processing avoid buffers being unlocked run refcount one ahead each 
WITHOUT_CLASSIFICATION	 check that all the outputs have been processed not insert them into queue before the current vertex and try again its possible structure like this where may added the queue before 
WITHOUT_CLASSIFICATION	 otherwise can reuse the session either the kill has failed but the user managed return early fact can fail because the query has completed earlier the user 
WITHOUT_CLASSIFICATION	 files all 
WITHOUT_CLASSIFICATION	 catname 
WITHOUT_CLASSIFICATION	 build map column name col index original schema assumption hive table can not contain duplicate column names 
WITHOUT_CLASSIFICATION	 the map were building 
WITHOUT_CLASSIFICATION	 determine maximum all nonnull decimal column values maintain isgroupresultnull 
WITHOUT_CLASSIFICATION	 ignore errors ssl tests where the connection misconfigured 
WITHOUT_CLASSIFICATION	 add filters that apply more than one input 
WITHOUT_CLASSIFICATION	 new metadata always have two parameters 
WITHOUT_CLASSIFICATION	 the bucket set component this data structure proves too large there the option moving trove hppc effort reduce size 
WITHOUT_CLASSIFICATION	 copy the padding 
WITHOUT_CLASSIFICATION	 the caller could recheck the location but would probably find locked 
WITHOUT_CLASSIFICATION	 for selselcompute case move column exprsnames child parent 
WITHOUT_CLASSIFICATION	 found but there exists subpartition 
WITHOUT_CLASSIFICATION	 timer will null arent using the metrics 
WITHOUT_CLASSIFICATION	 found child mapjoin operator its size should already reflect any mapjoins connected stop processing 
WITHOUT_CLASSIFICATION	 todo implement remove all watches for the specified pathstring and its subtree 
WITHOUT_CLASSIFICATION	 nonjavadoc see long 
WITHOUT_CLASSIFICATION	 input data 
WITHOUT_CLASSIFICATION	 default serialization format 
WITHOUT_CLASSIFICATION	 which case user the latter match 
WITHOUT_CLASSIFICATION	 fill host with tasks leave host empty try running task host should preempt 
WITHOUT_CLASSIFICATION	 aggregates groupby 
WITHOUT_CLASSIFICATION	 unpartitioned table writing the scratch dir directly good enough 
WITHOUT_CLASSIFICATION	 now try pick another task update potentially the same task 
WITHOUT_CLASSIFICATION	 convert exprnode rexnode 
WITHOUT_CLASSIFICATION	 since conflicting txn rolled back commit succeeds 
WITHOUT_CLASSIFICATION	 the directory needs changed send the new directory 
WITHOUT_CLASSIFICATION	 sizes 
WITHOUT_CLASSIFICATION	 the table not partitioned return empty list 
WITHOUT_CLASSIFICATION	 turn mocked authorization 
WITHOUT_CLASSIFICATION	 loginfoargs har arg 
WITHOUT_CLASSIFICATION	 return for xxzz and xxyyzz 
WITHOUT_CLASSIFICATION	 get calcite return type for agg 
WITHOUT_CLASSIFICATION	 handle next round 
WITHOUT_CLASSIFICATION	 the sorting columns the child are more specific than those the parent assign sorting columns the child the parent 
WITHOUT_CLASSIFICATION	 this relates level event tracked via 
WITHOUT_CLASSIFICATION	 all done parsing lets run stuff 
WITHOUT_CLASSIFICATION	 build the versions list 
WITHOUT_CLASSIFICATION	 file successfully copied just skip this file from retry 
WITHOUT_CLASSIFICATION	 all must selected otherwise size would zero repeating property will not change 
WITHOUT_CLASSIFICATION	 will make sure the tasks place the wait queue held until gets scheduled 
WITHOUT_CLASSIFICATION	 just spot check because already checked the logic for long the code from the same template file 
WITHOUT_CLASSIFICATION	 copy remainder digits which start the top remainder 
WITHOUT_CLASSIFICATION	 want resolve the leftmost name the parent querys hence left walk down the ast until reach the bottom most dot 
WITHOUT_CLASSIFICATION	 begin write 
WITHOUT_CLASSIFICATION	 repeated int lint 
WITHOUT_CLASSIFICATION	 since there txn open are heartbeating the txn not individual locks 
WITHOUT_CLASSIFICATION	 for persistent function the function dropped all functions registered sessions are needed reloaded 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader long 
WITHOUT_CLASSIFICATION	 need shut down background thread gracefully driverclose will inform background thread cancel request sent 
WITHOUT_CLASSIFICATION	 note calcite considers tbls equal their names are the same hence need provide calcite the fully qualified table name dbnametblname and not the user provided aliases however hive name can not appear select list case join where table names differ only name hive would require user 
WITHOUT_CLASSIFICATION	 retest the locks 
WITHOUT_CLASSIFICATION	 configure http client for cookie based authentication 
WITHOUT_CLASSIFICATION	 build the resultset from response 
WITHOUT_CLASSIFICATION	 worker identity 
WITHOUT_CLASSIFICATION	 input rel 
WITHOUT_CLASSIFICATION	 this case need get the working directory and this requires filesystem handle revert original method 
WITHOUT_CLASSIFICATION	 verify table not fuond error 
WITHOUT_CLASSIFICATION	 the given task sparktask then search its work dag for 
WITHOUT_CLASSIFICATION	 try underlying client 
WITHOUT_CLASSIFICATION	 obtain col stats for non partition cols 
WITHOUT_CLASSIFICATION	 either got the tablename from the import statement first priority from the export dump 
WITHOUT_CLASSIFICATION	 add the regex match anything else afterwards the partial spec 
WITHOUT_CLASSIFICATION	 spark configurations are updated close the existing session case async queries confoverlay not empty sessionconf and conf are different objects 
WITHOUT_CLASSIFICATION	 maintain join keys child join schema update join key map with keys 
WITHOUT_CLASSIFICATION	 find our bearings the stream 
WITHOUT_CLASSIFICATION	 nonjavadoc this processor addresses the rsmj case that occurs spark the smallhash table side things the work that will part must connected the work via broadcast edge should not walk down the tree when encounter this pattern because the type work map work reduce work needs determined the basis the big table side because may mapwork need for shuffle reduce work 
WITHOUT_CLASSIFICATION	 todo handle replication changes tablestats 
WITHOUT_CLASSIFICATION	 check llapaware split orcsplit make sure its compatible 
WITHOUT_CLASSIFICATION	 nway all later small tables 
WITHOUT_CLASSIFICATION	 this doesnt create key index presumably because writeroptions are not set options 
WITHOUT_CLASSIFICATION	 get column 
WITHOUT_CLASSIFICATION	 are the last the concurrent operations finish commit 
WITHOUT_CLASSIFICATION	 consolidation since all leaves are required 
WITHOUT_CLASSIFICATION	 should set foo bar should set blah should ignored 
WITHOUT_CLASSIFICATION	 subclass must provide the link instance 
WITHOUT_CLASSIFICATION	 add partition keys table schema note this assumes that not ever have ptn keys columns inside the table schema well 
WITHOUT_CLASSIFICATION	 unixtimestamp polymorphic ignore class annotations 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 temporary until native vector map join with hybrid passes tests false 
WITHOUT_CLASSIFICATION	 get the databases 
WITHOUT_CLASSIFICATION	 write three files partition 
WITHOUT_CLASSIFICATION	 columns null then need create the leaf 
WITHOUT_CLASSIFICATION	 functions like nvl coalesce case can change null introduced nonpart column removal into nonnull and cause overaggressive prunning missing data incorrect result 
WITHOUT_CLASSIFICATION	 input pruning enough add the filter for the optimizer use later 
WITHOUT_CLASSIFICATION	 for smb join replaced with number part taskid making output file name big alias not partitioned table its bucket number 
WITHOUT_CLASSIFICATION	 hadoopproxyuser set env property 
WITHOUT_CLASSIFICATION	 worth waiting for the timeout 
WITHOUT_CLASSIFICATION	 versionversion 
WITHOUT_CLASSIFICATION	 submit accept dag session closed this will include reopening session time 
WITHOUT_CLASSIFICATION	 realativeoffsetword last value this was the first value written 
WITHOUT_CLASSIFICATION	 should final but writable 
WITHOUT_CLASSIFICATION	 cololdname colnewname columntype comment colcomment firstafter columnname cascaderestrict 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 count the tasks intermediate state waiting 
WITHOUT_CLASSIFICATION	 case column stats hash aggregation grouping sets 
WITHOUT_CLASSIFICATION	 rewriteenabled 
WITHOUT_CLASSIFICATION	 check that the data still exist 
WITHOUT_CLASSIFICATION	 meh 
WITHOUT_CLASSIFICATION	 lefttree null 
WITHOUT_CLASSIFICATION	 partition and bucket columns are sorted ascending order default 
WITHOUT_CLASSIFICATION	 generate reducesinkoperator 
WITHOUT_CLASSIFICATION	 submit spark job through local spark context while spark master local mode otherwise submit spark job through remote spark context 
WITHOUT_CLASSIFICATION	 connect this tablescanoperator child 
WITHOUT_CLASSIFICATION	 dataoutputstream byteswritable 
WITHOUT_CLASSIFICATION	 test with doasfalse 
WITHOUT_CLASSIFICATION	 firstname john sue 
WITHOUT_CLASSIFICATION	 weight 
WITHOUT_CLASSIFICATION	 this allows doas proxy user passed along across process boundary where delegation tokens are not supported for example ddl stmt via webhcat with doas parameter forks hcat which needs start session that proxies the end user 
WITHOUT_CLASSIFICATION	 otherwise just leave tez decide how much memory allocate 
WITHOUT_CLASSIFICATION	 clone readeroptions for deleteevents 
WITHOUT_CLASSIFICATION	 for submit operation tasks are not cancelled verify that new job request should fail with 
WITHOUT_CLASSIFICATION	 partitions match the specified partition filter 
WITHOUT_CLASSIFICATION	 common inner join result processing 
WITHOUT_CLASSIFICATION	 nothing here 
WITHOUT_CLASSIFICATION	 versioning addingdeleting fields 
WITHOUT_CLASSIFICATION	 remove nested dpps 
WITHOUT_CLASSIFICATION	 the dispatcher fires the processor corresponding the closest matching rule and passes the context along 
WITHOUT_CLASSIFICATION	 object inspectors corresponding the struct returned terminatepartial and the fields within the struct maxlength sumlength count countnulls ndv 
WITHOUT_CLASSIFICATION	 compute groupby columns from groupby keys 
WITHOUT_CLASSIFICATION	 helper function allow setcollection operations with exprnodedesc 
WITHOUT_CLASSIFICATION	 make sure that file does not exist 
WITHOUT_CLASSIFICATION	 the retainall method does set intersection 
WITHOUT_CLASSIFICATION	 test for user neo 
WITHOUT_CLASSIFICATION	 add constraints necessary 
WITHOUT_CLASSIFICATION	 initialize footer buffer 
WITHOUT_CLASSIFICATION	 output 
WITHOUT_CLASSIFICATION	 this method check the new column list includes all the old columns with same name and type the column comment does not count 
WITHOUT_CLASSIFICATION	 case are partitioning the segments based time and max row per segment maxpartitionsize 
WITHOUT_CLASSIFICATION	 pull out the first table from the show extended json 
WITHOUT_CLASSIFICATION	 for all the other column groups generate new values down 
WITHOUT_CLASSIFICATION	 setup tablescan desc 
WITHOUT_CLASSIFICATION	 but assume its extremely rare for individual partitions 
WITHOUT_CLASSIFICATION	 while and should done when start 
WITHOUT_CLASSIFICATION	 get the single tablescanoperator vectorization only supports one input tree 
WITHOUT_CLASSIFICATION	 passing creds prevents duplicate tokens from being added 
WITHOUT_CLASSIFICATION	 make sure dont compact dont need compact but 
WITHOUT_CLASSIFICATION	 fulltablename 
WITHOUT_CLASSIFICATION	 due hive define our own constant 
WITHOUT_CLASSIFICATION	 sessionopen will unset the queue name from conf but mockito intercepts the open call 
WITHOUT_CLASSIFICATION	 longdoubledecimal 
WITHOUT_CLASSIFICATION	 row null means there are more rows closeop another case can that the buffer full 
WITHOUT_CLASSIFICATION	 give the outthread chance finish before marking the operator done 
WITHOUT_CLASSIFICATION	 tests whether credential provider updated when set and when hiveconf sets jobconf should contain the mapred env variable equal and the property should equal value 
WITHOUT_CLASSIFICATION	 feed current full batch operator tree 
WITHOUT_CLASSIFICATION	 any tablepartition updated then update repl state table object 
WITHOUT_CLASSIFICATION	 fetch remaining logs 
WITHOUT_CLASSIFICATION	 after the join only selects and filters are allowed 
WITHOUT_CLASSIFICATION	 this might deadlock lets retry 
WITHOUT_CLASSIFICATION	 delete jars added using query 
WITHOUT_CLASSIFICATION	 setup client side split generation 
WITHOUT_CLASSIFICATION	 type for column and constant are different currently not support pushing them 
WITHOUT_CLASSIFICATION	 now add the key wrapper arrays 
WITHOUT_CLASSIFICATION	 have nonnull value hand 
WITHOUT_CLASSIFICATION	 lock should freed now 
WITHOUT_CLASSIFICATION	 run compaction worker compaction but not compact table but only transit the compaction request 
WITHOUT_CLASSIFICATION	 for list the value and key lengths record were overwritten with the relative offset new list record 
WITHOUT_CLASSIFICATION	 not selectedinuse 
WITHOUT_CLASSIFICATION	 fromeventid 
WITHOUT_CLASSIFICATION	 weve switched jodajava calendar which has more limited time range 
WITHOUT_CLASSIFICATION	 this assumes the distribution variable size keysaggregates the input the same the distribution variable sizes the hash entries 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 sets taskspec which has vertex its input and tasks belonging vertex 
WITHOUT_CLASSIFICATION	 another loop table bucketed 
WITHOUT_CLASSIFICATION	 succeeded adding all the values 
WITHOUT_CLASSIFICATION	 return type should have same length the input 
WITHOUT_CLASSIFICATION	 only for testing 
WITHOUT_CLASSIFICATION	 this our use case not having passwords stored the clear hive conf files 
WITHOUT_CLASSIFICATION	 priority 
WITHOUT_CLASSIFICATION	 for hive udtf operator 
WITHOUT_CLASSIFICATION	 were going kill some queries and reuse the sessions maybe restart and put the new ones back into the pool however the pool has shrunk will close them instead 
WITHOUT_CLASSIFICATION	 will test the reconfiguration the header size changing the password length 
WITHOUT_CLASSIFICATION	 the lower bits are the absolute value offset 
WITHOUT_CLASSIFICATION	 all are null none are selected 
WITHOUT_CLASSIFICATION	 need remove hive also not change default see smb operator 
WITHOUT_CLASSIFICATION	 the system properties 
WITHOUT_CLASSIFICATION	 loginfonuking dir 
WITHOUT_CLASSIFICATION	 compute the values 
WITHOUT_CLASSIFICATION	 original bucket files delta directory and deletedelta should have been cleaned 
WITHOUT_CLASSIFICATION	 modify partition column type and comment 
WITHOUT_CLASSIFICATION	 what want order ccend desc ccstart asc but derby has bug sort that currently running jobs are the end the list bottom screen and currently running ones are sorted start time 
WITHOUT_CLASSIFICATION	 hive doesnt have the concept notnull 
WITHOUT_CLASSIFICATION	 check start forward rows new child the current key group rows will not forwarded those children which have index less than the currentchildindex can call flush the buffer children from lastchildindex inclusive currentchildindex exclusive and propagate processgroup those children 
WITHOUT_CLASSIFICATION	 split into digit middle and lowest longwords remainder division 
WITHOUT_CLASSIFICATION	 scrutinize escape pair specifically replace 
WITHOUT_CLASSIFICATION	 would useful have enum for type insertdeleteload data 
WITHOUT_CLASSIFICATION	 densecolix index orc writer with includes skip the root column get the original text file index then add the root column again this makes many assumptions also this only works for primitive types vectordeserializer only supports these anyway the mapping for complex types with subcols orc would much more difficult build 
WITHOUT_CLASSIFICATION	 restrictionm allow only subquery expression per query 
WITHOUT_CLASSIFICATION	 outside need create new limit 
WITHOUT_CLASSIFICATION	 set generic options 
WITHOUT_CLASSIFICATION	 hive syntax allows define case expressions two ways case when then when then else end translated into the case function else clause optional case when then when then else end translated into the when function else clause optional however calcite only has the equivalent the when hive function thus need transform the case function into when further else clause not optional calcite example consider the following statement case when then fee when then fie end will transformed into case when then fee when then fie else null end 
WITHOUT_CLASSIFICATION	 multikey specific repeated lookup 
WITHOUT_CLASSIFICATION	 shamelessly copied from path hadoop 
WITHOUT_CLASSIFICATION	 ambiguous case which should assumed level according spec 
WITHOUT_CLASSIFICATION	 scalarscalar 
WITHOUT_CLASSIFICATION	 pretend that one field used 
WITHOUT_CLASSIFICATION	 buffer the leaf node 
WITHOUT_CLASSIFICATION	 just drop transactionalfalse for backward compatibility case someone has scripts with transactionalfalse 
WITHOUT_CLASSIFICATION	 create the join aux structures 
WITHOUT_CLASSIFICATION	 operator below 
WITHOUT_CLASSIFICATION	 some hiveexceptions semanticexception dont set canonical errormsg explicitly but there logic compile find appropriate canonical error and return its code error code this case want preserve for downstream code interpret 
WITHOUT_CLASSIFICATION	 the update statement remember splitupdate udi 
WITHOUT_CLASSIFICATION	 free the htable connections 
WITHOUT_CLASSIFICATION	 only single double was passed parameter single quantile being requested 
WITHOUT_CLASSIFICATION	 col 
WITHOUT_CLASSIFICATION	 build tokwhere null check for joining column 
WITHOUT_CLASSIFICATION	 type date longcolumnvector storing epoch days minus type date produces type intervaldaytime storing nanosecond interval longs 
WITHOUT_CLASSIFICATION	 initialize using objectinspector array and column projection array 
WITHOUT_CLASSIFICATION	 all columns data partition and virtual are added 
WITHOUT_CLASSIFICATION	 unlimited lifetime 
WITHOUT_CLASSIFICATION	 parse out the kerberos principal host realm 
WITHOUT_CLASSIFICATION	 copy credentials and any new config added back jobcontext 
WITHOUT_CLASSIFICATION	 this type can only created when splitupdate 
WITHOUT_CLASSIFICATION	 obtain delegation token for the give user from metastore 
WITHOUT_CLASSIFICATION	 list paths that dont need merge but need move the dest location 
WITHOUT_CLASSIFICATION	 exception expected 
WITHOUT_CLASSIFICATION	 negative tests 
WITHOUT_CLASSIFICATION	 sort itself should not reference cor vars 
WITHOUT_CLASSIFICATION	 additional bookkeeping info for the stored stats 
WITHOUT_CLASSIFICATION	 call open read split mockmocktable 
WITHOUT_CLASSIFICATION	 return result 
WITHOUT_CLASSIFICATION	 add the bits the bottom the current word 
WITHOUT_CLASSIFICATION	 then remove all the grants 
WITHOUT_CLASSIFICATION	 fractional part has starting with zeros 
WITHOUT_CLASSIFICATION	 create table select 
WITHOUT_CLASSIFICATION	 ensure the correct task was preempted 
WITHOUT_CLASSIFICATION	 note the reason this method exists outside the noarg getdeserializer method case there userimplemented messagefactory thats used and some the messages are older format and the rest another then what messagefactory default irrelevant should always use the one that was used create deserialize there exist only implementations this json and jms additional note rather than config parameter does make sense have this use jdbclike semantics that each messagefactory made available register itself for discoverability might worth pursuing 
WITHOUT_CLASSIFICATION	 set bit null byte when field not null 
WITHOUT_CLASSIFICATION	 tolerance for long range bias when bias enabled and when bias disabled and for short range bias 
WITHOUT_CLASSIFICATION	 extract the raw data size and update the stats for the current partition 
WITHOUT_CLASSIFICATION	 dont evaluate nondeterministic function since the value can only calculate during runtime 
WITHOUT_CLASSIFICATION	 since the output the udtf struct can just forward that 
WITHOUT_CLASSIFICATION	 get the standard objectinspector the row 
WITHOUT_CLASSIFICATION	 test single highprecision divide random inputs 
WITHOUT_CLASSIFICATION	 add result order are processing 
WITHOUT_CLASSIFICATION	 get col object out 
WITHOUT_CLASSIFICATION	 checkcorrect codec checkcorrect codec 
WITHOUT_CLASSIFICATION	 the child also decimal cast needed hope can target type narrower 
WITHOUT_CLASSIFICATION	 replaceoverwrite introduces new files 
WITHOUT_CLASSIFICATION	 have readentity 
WITHOUT_CLASSIFICATION	 check the sample columns are the same the table bucket columns 
WITHOUT_CLASSIFICATION	 throw new not sort order and unique 
WITHOUT_CLASSIFICATION	 inform the routing purgepolicy send out fake log message the error level with the mdc for this query setup with llap custom appender this message will not logged 
WITHOUT_CLASSIFICATION	 desired parallelism the reduce task 
WITHOUT_CLASSIFICATION	 nonjavadoc see windowtablefunction supports streaming all functions meet one these conditions the function implements returns non null object for the that implements invocation fixed window unbounded preceding following 
WITHOUT_CLASSIFICATION	 test that write blocks write but read can still acquire 
WITHOUT_CLASSIFICATION	 singlecolumn long outer get key 
WITHOUT_CLASSIFICATION	 since right has longer digit tail and doesnt move will shift the left digits our addition into the result 
WITHOUT_CLASSIFICATION	 old table not the cache but the new table can cached 
WITHOUT_CLASSIFICATION	 interval types can use long version 
WITHOUT_CLASSIFICATION	 convert nonacidorctbl acid table 
WITHOUT_CLASSIFICATION	 dont set dagclient null here execute will only clean operators its set 
WITHOUT_CLASSIFICATION	 should only called for testing 
WITHOUT_CLASSIFICATION	 make one entry produce false result 
WITHOUT_CLASSIFICATION	 add int values 
WITHOUT_CLASSIFICATION	 empty string 
WITHOUT_CLASSIFICATION	 timestamps are stored long convert and compare 
WITHOUT_CLASSIFICATION	 get past this then the column name did match the hive pattern for internal column name such col etc must match the schema for the appropriate column this means people cant use arbitrary column names such col and expect ignore 
WITHOUT_CLASSIFICATION	 return worst case unknown 
WITHOUT_CLASSIFICATION	 internal names 
WITHOUT_CLASSIFICATION	 remove unnecessary information from target 
WITHOUT_CLASSIFICATION	 autodetermine local mode allowed 
WITHOUT_CLASSIFICATION	 try the basic test with nonchunked stream 
WITHOUT_CLASSIFICATION	 repl dump 
WITHOUT_CLASSIFICATION	 done output does not need committed hive does not use outputcommitter 
WITHOUT_CLASSIFICATION	 transaction states 
WITHOUT_CLASSIFICATION	 list the current connections 
WITHOUT_CLASSIFICATION	 this map used for set the stats flag for the cloned filesinkoperators later process 
WITHOUT_CLASSIFICATION	 convert the partition filter expression into string expected hcat and pass setlocation 
WITHOUT_CLASSIFICATION	 type doesnt require any qualifiers 
WITHOUT_CLASSIFICATION	 start reading role names from next position 
WITHOUT_CLASSIFICATION	 not know what bail out for safety 
WITHOUT_CLASSIFICATION	 not column 
WITHOUT_CLASSIFICATION	 make sure can add back 
WITHOUT_CLASSIFICATION	 disable avoid verbose app state report yarncluster mode 
WITHOUT_CLASSIFICATION	 case test with originals and deltas but now with only one bucket covered will have originals insertdeltas for only one bucket but the deletedeltas will for two buckets two strategies with one split for each when splitupdate enabled not need account for buckets that arent covered the reason why are able because the valid user data has already been considered base for the covered buckets hence the uncovered buckets not have any relevant 
WITHOUT_CLASSIFICATION	 check first otherwise webhcatlog full stack traces from filesystem when clients check for status exitvalue completed etc 
WITHOUT_CLASSIFICATION	 remove the previously peeked element 
WITHOUT_CLASSIFICATION	 ignore the parent already exists 
WITHOUT_CLASSIFICATION	 construct valuetabledescs and 
WITHOUT_CLASSIFICATION	 offset before which this guaranteed end can only estimated 
WITHOUT_CLASSIFICATION	 return greater than because lefts digits below rights scale 
WITHOUT_CLASSIFICATION	 creation time will set server and not 
WITHOUT_CLASSIFICATION	 this flow usually taken for import command 
WITHOUT_CLASSIFICATION	 consider cleaning this code and eliminating the arrays vectorization only handles one operator tree 
WITHOUT_CLASSIFICATION	 ignore starting quote 
WITHOUT_CLASSIFICATION	 mysql returns the string not wellformed numeric value return intwritablevalueof but decided return null instead which more conservative 
WITHOUT_CLASSIFICATION	 convert the work containing sortmerge join into work had regular join note that the operator tree not changed still contains the smb join but the plan changed aliastowork etc contain all the paths was regular join this used convert the plan mapjoin and then the original smb join plan used 
WITHOUT_CLASSIFICATION	 singleton 
WITHOUT_CLASSIFICATION	 rerun ppd through project column pruning would have introduced above scans pushing filter just above hive can push into storage incase there are filters non partition cols this only 
WITHOUT_CLASSIFICATION	 entry null due zerodivide 
WITHOUT_CLASSIFICATION	 for sparktez rely the generated selectoperator the type casting consider sel int sel int sel double first merge sel and sel into union and then merge union with sel get union then selectoperator will inserted hence error will happen afterwards the solution here insert one after union which 
WITHOUT_CLASSIFICATION	 elements queue integer are index fetchoperator segments 
WITHOUT_CLASSIFICATION	 create partition specs 
WITHOUT_CLASSIFICATION	 check all the input txns are open state write should allocated only for open transactions 
WITHOUT_CLASSIFICATION	 not exceed the configured max reducers 
WITHOUT_CLASSIFICATION	 dependency check depth 
WITHOUT_CLASSIFICATION	 all ast nodes must implement this interface provides basic machinery for constructing the parent and child relationships between nodes 
WITHOUT_CLASSIFICATION	 check that the files are removed 
WITHOUT_CLASSIFICATION	 write lock for add evict and clean operation 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 strip and 
WITHOUT_CLASSIFICATION	 row resolver the subquery set the semanticanalyzer after the plan for the subquery genned this needed case the subquery select list contains tokallcolref 
WITHOUT_CLASSIFICATION	 worker stays initiated state and wait the above alter table retunrs almost immediately the test here check that seconds pass that the command driver actually blocks before cancel fired 
WITHOUT_CLASSIFICATION	 the column number and type information for this one column long reduce key 
WITHOUT_CLASSIFICATION	 lock required here 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioinputstream int 
WITHOUT_CLASSIFICATION	 unspecified default one will used for line break 
WITHOUT_CLASSIFICATION	 prefixed with avoid finding entry cmpath 
WITHOUT_CLASSIFICATION	 indicates that the recorded value null 
WITHOUT_CLASSIFICATION	 entire query can run locally save the current tracker value and restore when done 
WITHOUT_CLASSIFICATION	 shallow copy astnode 
WITHOUT_CLASSIFICATION	 recursive types 
WITHOUT_CLASSIFICATION	 this computes stats and should stats deduplicated too 
WITHOUT_CLASSIFICATION	 operationstate 
WITHOUT_CLASSIFICATION	 most general case where the left and right keys might have nulls and caller requires valued logic return select edeptno edeptno select deptno from emp becomes select edeptno case when ctc then false when dti not null then true when edeptno null then null when ctck ctc then null else false end from left join select count countdeptno from emp cross join select distinct deptno true from emp edeptno dtdeptno keys are not null can remove and simplify select edeptno case when dti not null then true else false end from left join select distinct deptno true from emp edeptno dtdeptno could further simplify select edeptno dti not null from left join select distinct deptno true from emp edeptno dtdeptno but have not yet the logic true can just kill the record the condition evaluates false unknown thus the query simplifies inner join select edeptno true from inner join select distinct deptno from emp edeptno dtdeptno 
WITHOUT_CLASSIFICATION	 encapsulates statistics about the duration all reduce tasks corresponding specific jobid the stats are computed the hadoopjobexechelper when the job completes and then populated inside the queryplan for each job from where can later accessed the reducer statistics consist the run times all the reduce tasks for job all the run times are milliseconds 
WITHOUT_CLASSIFICATION	 values 
WITHOUT_CLASSIFICATION	 get the last colname for the reduce key 
WITHOUT_CLASSIFICATION	 some parts session state like mrstats and vars need proper synchronization 
WITHOUT_CLASSIFICATION	 first drop any databases catalog 
WITHOUT_CLASSIFICATION	 input data col col blue red green green red blue null red col data empty string unset null property 
WITHOUT_CLASSIFICATION	 both inputs nonnegative 
WITHOUT_CLASSIFICATION	 the real workhorse spend time and energy this method there need keep hcatstorer lean and fast 
WITHOUT_CLASSIFICATION	 expected error 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlsqlxml 
WITHOUT_CLASSIFICATION	 get next batch 
WITHOUT_CLASSIFICATION	 note print systemerr instead sserr because cant parse our commandline havent even begun and therefore cannot expected have reasonably constructed started the sessionstate 
WITHOUT_CLASSIFICATION	 new true this 
WITHOUT_CLASSIFICATION	 item for new partition queued now 
WITHOUT_CLASSIFICATION	 conversionhelper can called without method parameter length checkings for terminatepartial and merge calls 
WITHOUT_CLASSIFICATION	 first known state was completed wait for the app launch start 
WITHOUT_CLASSIFICATION	 mapping from the regex lines the log file where find true 
WITHOUT_CLASSIFICATION	 copy insert branch and duplicate the first branch will the update for the merge statement while the new branch will the insert for the 
WITHOUT_CLASSIFICATION	 isnull copying necessary 
WITHOUT_CLASSIFICATION	 not use map join case cross product 
WITHOUT_CLASSIFICATION	 have reached running state now check running nodes threshold met 
WITHOUT_CLASSIFICATION	 will wait for seconds for the task cancelled its still not cancelled unlikely will just move 
WITHOUT_CLASSIFICATION	 sit the cache while not use 
WITHOUT_CLASSIFICATION	 bgenjjtree include 
WITHOUT_CLASSIFICATION	 set timeout undo everything 
WITHOUT_CLASSIFICATION	 create table 
WITHOUT_CLASSIFICATION	 dont register with deleteonexit 
WITHOUT_CLASSIFICATION	 nonempty java opts with xmx specified 
WITHOUT_CLASSIFICATION	 add partition column stats 
WITHOUT_CLASSIFICATION	 this can use significant resources and should not done the main query thread 
WITHOUT_CLASSIFICATION	 this corr scalar subquery with agg expect one aggregate 
WITHOUT_CLASSIFICATION	 sparkllap always wraps query under subquery until that removed from sparkllap 
WITHOUT_CLASSIFICATION	 make sure the default value expression type exactly same columns type 
WITHOUT_CLASSIFICATION	 dynamic partition columns 
WITHOUT_CLASSIFICATION	 gbinputrels schema like this 
WITHOUT_CLASSIFICATION	 the connection should fail since the dry run 
WITHOUT_CLASSIFICATION	 always set these explain can see 
WITHOUT_CLASSIFICATION	 table partitioned and immutable then the presence the partition alone enough throw error not need check for emptiness decide throw error 
WITHOUT_CLASSIFICATION	 test for boolean type 
WITHOUT_CLASSIFICATION	 marker annotations for functions that reflector should ignore pretend does not exist 
WITHOUT_CLASSIFICATION	 drop occured part replicating drop but the destination table was newer than the event being replicated ignore but drop any partitions inside that are older 
WITHOUT_CLASSIFICATION	 column type 
WITHOUT_CLASSIFICATION	 can pretty sure that entire line can processed single command since always add line separator the end while calling 
WITHOUT_CLASSIFICATION	 the input the gby has tab alias for the column then add entry based that tabalias for this query select count from group needs tabaliasb colaliasx the gby tabaliasb comes from looking the rowresolver that the ancestor before any gbyreducesinks added for the gby operation 
WITHOUT_CLASSIFICATION	 inputs 
WITHOUT_CLASSIFICATION	 good performance for common case where small table has complex objects 
WITHOUT_CLASSIFICATION	 specialized class for doing vectorized map join that inner join multikey using hash map 
WITHOUT_CLASSIFICATION	 create postfiltering evaluators needed 
WITHOUT_CLASSIFICATION	 want lock here the database lock will cover the tables and putting lock will actually cause deadlock ourselves 
WITHOUT_CLASSIFICATION	 before readbatch initial the size offsets lengths the default value 
WITHOUT_CLASSIFICATION	 clientprotocol 
WITHOUT_CLASSIFICATION	 write the value out 
WITHOUT_CLASSIFICATION	 handler multiline sql 
WITHOUT_CLASSIFICATION	 test setting fetch size below max 
WITHOUT_CLASSIFICATION	 candidate for preemption 
WITHOUT_CLASSIFICATION	 remove this task from its children tasks 
WITHOUT_CLASSIFICATION	 nulls possible left right 
WITHOUT_CLASSIFICATION	 integerflag 
WITHOUT_CLASSIFICATION	 used memory totalmemory freememory 
WITHOUT_CLASSIFICATION	 start would throw already existed here 
WITHOUT_CLASSIFICATION	 the tabletracker here should new instance and not existing one this can only happen when break between loading partitions 
WITHOUT_CLASSIFICATION	 then load the sparkclientimpl config 
WITHOUT_CLASSIFICATION	 peel off the levels get the underlying array 
WITHOUT_CLASSIFICATION	 this task the callback instead 
WITHOUT_CLASSIFICATION	 find functions which name contains tofind the default database 
WITHOUT_CLASSIFICATION	 min 
WITHOUT_CLASSIFICATION	 isset assignments 
WITHOUT_CLASSIFICATION	 for integers have optional minmax filtering 
WITHOUT_CLASSIFICATION	 compare timestamp against timestamp long seconds and double seconds with fractional nanoseconds timestampcol timestampcolumn timestampcol longdoublecolumn longdoublecol timestampcolumn timestampcol timestampscalar timestampcol longdoublescalar longdoublecol timestampscalar timestampscalar timestampcolumn timestampscalar longdoublecolumn longdoublescalar timestampcolumn 
WITHOUT_CLASSIFICATION	 specifies 
WITHOUT_CLASSIFICATION	 tracks the number elements with the same rank the current time 
WITHOUT_CLASSIFICATION	 table partitionedadd partdir and partitiondesc 
WITHOUT_CLASSIFICATION	 this point have number save fastresult round have exponent will power operation fastresult 
WITHOUT_CLASSIFICATION	 set true for columns that needed skip loading into memory 
WITHOUT_CLASSIFICATION	 simulate the unknown source 
WITHOUT_CLASSIFICATION	 blank byte blank byte blank byte blank byte blank byte white start bytes 
WITHOUT_CLASSIFICATION	 the only way get the return object inspector and its return type initialize 
WITHOUT_CLASSIFICATION	 from the moment that have two destination clauses know that this multiinsert query thus set property right value using would equivalent but use avoid setting the property multiple times 
WITHOUT_CLASSIFICATION	 expressions 
WITHOUT_CLASSIFICATION	 max value stored registered cached determine the bit width for 
WITHOUT_CLASSIFICATION	 common code 
WITHOUT_CLASSIFICATION	 val 
WITHOUT_CLASSIFICATION	 workerspath the directory path where all the worker znodes are located 
WITHOUT_CLASSIFICATION	 the variable was the right need swap things around 
WITHOUT_CLASSIFICATION	 test third argument with nulls 
WITHOUT_CLASSIFICATION	 entry either expired was invalidated due table updates 
WITHOUT_CLASSIFICATION	 because the length cant known now 
WITHOUT_CLASSIFICATION	 static partition spec ends with 
WITHOUT_CLASSIFICATION	 use the earlier match 
WITHOUT_CLASSIFICATION	 return true the table bucketedsorted the specified positions the number buckets the sort order should also match along with the 
WITHOUT_CLASSIFICATION	 skip empty lines 
WITHOUT_CLASSIFICATION	 base with delete deltas 
WITHOUT_CLASSIFICATION	 remember this rel dont fire rule again review jhyde oct rules should not save state rule should recognize patterns where does does not need 
WITHOUT_CLASSIFICATION	 possible that the background init thread has finished parallel queued the message for but also returned the session the user 
WITHOUT_CLASSIFICATION	 lets use the table from the cache 
WITHOUT_CLASSIFICATION	 create the list children 
WITHOUT_CLASSIFICATION	 warning note this point createdbexportdump lives only world where replicationspec replication scope later make this work for nonrepl cases analysis this logic might become necessary also this using replv semantics with listfiles laziness copy export time 
WITHOUT_CLASSIFICATION	 would nice there was way determine quotes are needed 
WITHOUT_CLASSIFICATION	 one input table 
WITHOUT_CLASSIFICATION	 get left table alias 
WITHOUT_CLASSIFICATION	 get udaf evaluator 
WITHOUT_CLASSIFICATION	 find the last matching xmx following word boundaries format xmxsizeggmmkk 
WITHOUT_CLASSIFICATION	 add custom cookies passed the jdbc driver 
WITHOUT_CLASSIFICATION	 are doing work here wed normally constructor later decide not specialize well just waste any scratch columns allocated 
WITHOUT_CLASSIFICATION	 set some info for the query 
WITHOUT_CLASSIFICATION	 remember min value and ignore from the denominator 
WITHOUT_CLASSIFICATION	 interrupt all thread and verify get and expected message 
WITHOUT_CLASSIFICATION	 returns false index already exists map 
WITHOUT_CLASSIFICATION	 update condition 
WITHOUT_CLASSIFICATION	 reads the the next field afterwards reading positioned the next field return return true when the field was not null and data put the appropriate current member otherwise false when the field null 
WITHOUT_CLASSIFICATION	 otherwise discard the escape char 
WITHOUT_CLASSIFICATION	 due data freshness 
WITHOUT_CLASSIFICATION	 create top project fixing nullability fields 
WITHOUT_CLASSIFICATION	 test failing due guava dependency druid should have less dependency guava 
WITHOUT_CLASSIFICATION	 allow operation txn 
WITHOUT_CLASSIFICATION	 have tez installed 
WITHOUT_CLASSIFICATION	 extract the bits num into value from right left 
WITHOUT_CLASSIFICATION	 need deep copy here since doing something like lastfrom from instead will make 
WITHOUT_CLASSIFICATION	 serialize the row into bytestream param obj the object for the current field param objinspector the objectinspector for the current object param level the current level separator param writebinary whether write primitive object utf variable length string fixed width byte array onto the byte stream throws ioexception error writing the serialization stream return true serializing nonnull object otherwise false 
WITHOUT_CLASSIFICATION	 expect json file updated 
WITHOUT_CLASSIFICATION	 replace unparsable synonyms 
WITHOUT_CLASSIFICATION	 examine the buddy block and its subblocks detail 
WITHOUT_CLASSIFICATION	 test droppartitionbyname 
WITHOUT_CLASSIFICATION	 hive describe outputs emptystring null row before partition information 
WITHOUT_CLASSIFICATION	 this keep track subquery correlated and contains aggregate 
WITHOUT_CLASSIFICATION	 flip the bits because calcite considers that means that the column participates the groupby and does not opposed groupingid 
WITHOUT_CLASSIFICATION	 must obtain vectorized equivalents for filter and value expressions 
WITHOUT_CLASSIFICATION	 create currently replicated noop 
WITHOUT_CLASSIFICATION	 will ask for preliminary mapping this allows escape the unmanaged path quickly the common case its still possible that resource plan will updated and 
WITHOUT_CLASSIFICATION	 there are actual accumulo index columns defined then build the comma separated list accumulo columns 
WITHOUT_CLASSIFICATION	 note all dist cols have single output col name 
WITHOUT_CLASSIFICATION	 should prepare the valid write ids list based validtxnlist current txn txn exists the caller then they would pass null for validtxnlist and required get the current state txns make validtxnlist 
WITHOUT_CLASSIFICATION	 extra fields but dont 
WITHOUT_CLASSIFICATION	 create base outputformat 
WITHOUT_CLASSIFICATION	 partitionvals 
WITHOUT_CLASSIFICATION	 gives progress over uncompressed stream assumes deserializer not buffering itself 
WITHOUT_CLASSIFICATION	 create and drop some additional metadata test drop counts 
WITHOUT_CLASSIFICATION	 stringvalue 
WITHOUT_CLASSIFICATION	 accumulo rangeinputsplit doesnt preserve usesasl the 
WITHOUT_CLASSIFICATION	 send the client 
WITHOUT_CLASSIFICATION	 see for why recheck the queue 
WITHOUT_CLASSIFICATION	 rounding setscale methods 
WITHOUT_CLASSIFICATION	 case compaction this the file the current bucket 
WITHOUT_CLASSIFICATION	 column expression the table being filtered the semijoin optimization 
WITHOUT_CLASSIFICATION	 this method overridden each task todo execute should return taskhandle return status executing the task 
WITHOUT_CLASSIFICATION	 double columncolumn 
WITHOUT_CLASSIFICATION	 dummy value for use tests 
WITHOUT_CLASSIFICATION	 called after the tasks have been generated run another round optimization 
WITHOUT_CLASSIFICATION	 maponly job the task needs processed 
WITHOUT_CLASSIFICATION	 get metastorethrift privilege object for this principal and object not looking 
WITHOUT_CLASSIFICATION	 null should smaller than any other value put null the front end 
WITHOUT_CLASSIFICATION	 need shift everything bits left and then shift back populate the sign field 
WITHOUT_CLASSIFICATION	 expression either the leftright side equality predicate the subquery where clause the entire conjunct for the where clause for subquery where and then the expressions analyzed are the left and right sides the equality predicate and the exprtype tracks whether the expr has reference subquery table source has reference outerparent query table source 
WITHOUT_CLASSIFICATION	 role names are caseinsensitive 
WITHOUT_CLASSIFICATION	 partitionkeys 
WITHOUT_CLASSIFICATION	 initialize the users process only when you receive the first row 
WITHOUT_CLASSIFICATION	 mapjoin should not affected join reordering 
WITHOUT_CLASSIFICATION	 unexpected 
WITHOUT_CLASSIFICATION	 file handle 
WITHOUT_CLASSIFICATION	 specify the external warehouse root 
WITHOUT_CLASSIFICATION	 rsgbrs 
WITHOUT_CLASSIFICATION	 indicates whether node disabled for whatever reason commfailure busy etc 
WITHOUT_CLASSIFICATION	 note the following code removing folded constants exprs deeply coupled with columnpruner optimizer assuming columnprunner will remove constant columns dont deal with output columns except one case that the join operator followed redistribution operator 
WITHOUT_CLASSIFICATION	 make sure skip backwardcompat checking for those tests that dont generate events 
WITHOUT_CLASSIFICATION	 convert hcatschema and pass hcatinputformat 
WITHOUT_CLASSIFICATION	 add those that are not part the final set residual 
WITHOUT_CLASSIFICATION	 parse value 
WITHOUT_CLASSIFICATION	 options for the python script that are here because our option parser cannot ignore the unknown ones 
WITHOUT_CLASSIFICATION	 handle structs composed partition columns 
WITHOUT_CLASSIFICATION	 read and decode dictionary ids 
WITHOUT_CLASSIFICATION	 srsr lock are examining shared read 
WITHOUT_CLASSIFICATION	 thats what this this alter and swallow 
WITHOUT_CLASSIFICATION	 the output the key expression the input column 
WITHOUT_CLASSIFICATION	 unregister may come after the new dag has started running the methods are expected synchronized hence the following check sufficient 
WITHOUT_CLASSIFICATION	 used capture view conversions this used check for recursive cte invocations 
WITHOUT_CLASSIFICATION	 calculate the variance family variance variancesample standarddeviation result when count public vectorization code can use etc 
WITHOUT_CLASSIFICATION	 optimize for most common case primitive 
WITHOUT_CLASSIFICATION	 check the pipeout files are removed 
WITHOUT_CLASSIFICATION	 volatile ensures that static access returns metrics instance fullyinitialized state alternative synchronize static access which has performance penalties 
WITHOUT_CLASSIFICATION	 fields but for uniformity 
WITHOUT_CLASSIFICATION	 eventtime 
WITHOUT_CLASSIFICATION	 the sortmerge join creates the output sorted and bucketized the same columns 
WITHOUT_CLASSIFICATION	 none 
WITHOUT_CLASSIFICATION	 have able peek ahead one byte 
WITHOUT_CLASSIFICATION	 note not currently part the hiverelnode interface 
WITHOUT_CLASSIFICATION	 add one more record and close 
WITHOUT_CLASSIFICATION	 add any redirects 
WITHOUT_CLASSIFICATION	 get from conf pick changes make sure not set too low and kill the metastore maxsleep the max time each backoff will wait for thus the total time wait for successful lock acquisition approximately see backoff maxnumwaits maxsleep 
WITHOUT_CLASSIFICATION	 case schema not file system 
WITHOUT_CLASSIFICATION	 wait before launching the next round connection retries 
WITHOUT_CLASSIFICATION	 create new view 
WITHOUT_CLASSIFICATION	 whether need transformation for each parent 
WITHOUT_CLASSIFICATION	 found udf metastore now add the function registry 
WITHOUT_CLASSIFICATION	 remove operator 
WITHOUT_CLASSIFICATION	 all currently open txns any have txnid than commithighwatermark 
WITHOUT_CLASSIFICATION	 move works following the current reduce work into new spark work 
WITHOUT_CLASSIFICATION	 add subdirectory the work queue maxdepth not yet reached 
WITHOUT_CLASSIFICATION	 protection against construction 
WITHOUT_CLASSIFICATION	 update the existing row and insert another row newlyconverted acid table 
WITHOUT_CLASSIFICATION	 these parameters controls the maximum time job submitstatuslist operation executed templeton service time out the execution interrupted and timeoutexception returned client time out for list and status operation there action needed they are read requests for submit operation best effort kill the job its generated enabling this parameter may have following side effects there possibility for having active job for some time when the client gets response for submit operation and list operation from client could potential show the newly created job which may eventually killed with guarantees submit operation retried client then there possibility duplicate jobs triggered time out configs should configured seconds 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 xmx specified 
WITHOUT_CLASSIFICATION	 test various combinations 
WITHOUT_CLASSIFICATION	 for nonviews need some extra fixes 
WITHOUT_CLASSIFICATION	 test owner 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 add owner privilege user owner the object 
WITHOUT_CLASSIFICATION	 remove writeset info for current txn since its about abort 
WITHOUT_CLASSIFICATION	 there backup servers 
WITHOUT_CLASSIFICATION	 spot check correctness decimal column multiply decimal scalar the case for addition checks all the cases for the template dont that redundantly here 
WITHOUT_CLASSIFICATION	 invariant rightlength leftlength rightoffset within the buffers 
WITHOUT_CLASSIFICATION	 get names these roles and its ancestors 
WITHOUT_CLASSIFICATION	 dynamic partition pruning enabled some all cases 
WITHOUT_CLASSIFICATION	 get the skewed values all the tables 
WITHOUT_CLASSIFICATION	 skip the test java cryptography extension jce unlimited strength jurisdiction policy files not installed 
WITHOUT_CLASSIFICATION	 should return tbl and tbl 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 grouping sets are not allowed 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this hook used for verifying the column access information that generated and maintained the queryplan object the all the hook does print out the columns accessed from each table recorded the columnaccessinfo the queryplan 
WITHOUT_CLASSIFICATION	 statsdata 
WITHOUT_CLASSIFICATION	 get the exprnodedesc corresponding the first start node 
WITHOUT_CLASSIFICATION	 this the first time have realized are stack trace this case the previous line was the error message add that the stack trace well 
WITHOUT_CLASSIFICATION	 map built during translation 
WITHOUT_CLASSIFICATION	 process the input columns find nonnull value for each row track the unassigned batchindex the rows that have not received nonnull value yet similar selected array 
WITHOUT_CLASSIFICATION	 genericudtf stateful too copy 
WITHOUT_CLASSIFICATION	 could not get stats cannot convert 
WITHOUT_CLASSIFICATION	 put into the map that this task was before decided update 
WITHOUT_CLASSIFICATION	 dont currently support the between ends being columns they must scalars 
WITHOUT_CLASSIFICATION	 verify that the new version added schema 
WITHOUT_CLASSIFICATION	 the same object 
WITHOUT_CLASSIFICATION	 for testing 
WITHOUT_CLASSIFICATION	 turn escape off 
WITHOUT_CLASSIFICATION	 drop table overk 
WITHOUT_CLASSIFICATION	 this rebuild theres nothing here 
WITHOUT_CLASSIFICATION	 default value true however optimization deems this edge important should set this false this does not guarantee that the edge will stay however increases the chances 
WITHOUT_CLASSIFICATION	 and test dropping this specific table 
WITHOUT_CLASSIFICATION	 joinkeysjoinkeysoi are initialized after making merge queue setup lazily runtime 
WITHOUT_CLASSIFICATION	 tries get lock and gets waiting state 
WITHOUT_CLASSIFICATION	 write intermediate file the specified path the format the path 
WITHOUT_CLASSIFICATION	 that its easy find reason for local mode execution failures 
WITHOUT_CLASSIFICATION	 delete from tab 
WITHOUT_CLASSIFICATION	 sets taskspec with inputs and tasks belonging vertex 
WITHOUT_CLASSIFICATION	 verify was preempted also verify that finished single executor otherwise could have run anyway 
WITHOUT_CLASSIFICATION	 refer groupingid column 
WITHOUT_CLASSIFICATION	 hive the following code change only needed for hbase due hbase and will not required once hive bumps its hbase version that time will only need here 
WITHOUT_CLASSIFICATION	 this constructor used momentarily create the object match can called 
WITHOUT_CLASSIFICATION	 caller will return the batch 
WITHOUT_CLASSIFICATION	 check for required fields check for substruct validity 
WITHOUT_CLASSIFICATION	 test 
WITHOUT_CLASSIFICATION	 use the same logic 
WITHOUT_CLASSIFICATION	 serde properties 
WITHOUT_CLASSIFICATION	 parameter added the restricted list add test 
WITHOUT_CLASSIFICATION	 insert data 
WITHOUT_CLASSIFICATION	 thread pool for callbacks completion execution work unit 
WITHOUT_CLASSIFICATION	 only the big table input source should vectorized applicable 
WITHOUT_CLASSIFICATION	 key used save the partition dropped partspecs 
WITHOUT_CLASSIFICATION	 plain acid table acid table with customized tblproperties 
WITHOUT_CLASSIFICATION	 optional vectorized key expressions that need run each batch 
WITHOUT_CLASSIFICATION	 initialize with estimated element size record initial buffer size 
WITHOUT_CLASSIFICATION	 jar spec there manifest must the first entry the zip 
WITHOUT_CLASSIFICATION	 exclude the newlygenerated select columns from etc resolution 
WITHOUT_CLASSIFICATION	 the foo bar and blah params order not guaranteed 
WITHOUT_CLASSIFICATION	 this should cost based decision but till enable the extended cost model will use the given value for the variable 
WITHOUT_CLASSIFICATION	 todo authorization 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 returns the root node the ast only makes sense call this after successful parse 
WITHOUT_CLASSIFICATION	 input and output serdes 
WITHOUT_CLASSIFICATION	 noncbo path retries execute create view and believe will throw the same error message 
WITHOUT_CLASSIFICATION	 apply druid transformation rules 
WITHOUT_CLASSIFICATION	 here know represents group expression 
WITHOUT_CLASSIFICATION	 seconds 
WITHOUT_CLASSIFICATION	 add identity 
WITHOUT_CLASSIFICATION	 there one repeated field for mapcol the field name map and its original type mapkeyvalue 
WITHOUT_CLASSIFICATION	 pause for just bit for retrying avoid immediately jumping back into the deadlock 
WITHOUT_CLASSIFICATION	 tries get the job result job request completed otherwise sets job status failed such that execute thread can necessary clean based failed state 
WITHOUT_CLASSIFICATION	 use synchronized map since even read actions cause the lru get updated 
WITHOUT_CLASSIFICATION	 the columns the old column descriptor the columns the new one then change the old storage descriptors column descriptor convert the mfieldschemas their thrift object counterparts because maintain datastore identity identity the model objects are managed jdo not the application 
WITHOUT_CLASSIFICATION	 logger can resource stream real file cannot use copy 
WITHOUT_CLASSIFICATION	 replace filter 
WITHOUT_CLASSIFICATION	 the base writer 
WITHOUT_CLASSIFICATION	 can directly add positions into cordefoutputs since join 
WITHOUT_CLASSIFICATION	 create column info with new tablealias 
WITHOUT_CLASSIFICATION	 lrr case just store boundaries which could split boundaries reader positions wouldnt able account for torn rows correctly because the semantics our exact reader positions and inexact split boundaries are different cannot even tell lrr use exact boundaries there can mismatch original midfile split wrt first row when caching may produce incorrect result adjust the split boundary and also dont adjust depending where falls best wed end with spurious disk reads cache row boundaries but splits include torn rows this structure implies that when reading split skip the first torn row but fully read the last torn row linerecordreader does want support different scheme wed need store more offsets and make logic account for that 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader 
WITHOUT_CLASSIFICATION	 has been replaced hivesmbjoincacherow 
WITHOUT_CLASSIFICATION	 small value bytes use small length from valuewordref 
WITHOUT_CLASSIFICATION	 hint true shouldremove redundant anyway 
WITHOUT_CLASSIFICATION	 table 
WITHOUT_CLASSIFICATION	 rowsetlog should contain execution well performance logs 
WITHOUT_CLASSIFICATION	 there are paths under which the instances get registered standard path used zkregistrybase where all instances register themselves also stores metadata secure unsecure leader latch path used for activepassive configuration where all instances register under leader path but only one among them the leader secure unsecure 
WITHOUT_CLASSIFICATION	 the tokens have distinction between identifiers and quotedidentifiers ugly solution just surround all identifiers with quotes 
WITHOUT_CLASSIFICATION	 there any aggregate function this group not unnecessary 
WITHOUT_CLASSIFICATION	 the condition fetched here can reference udf that not deterministic but defined part the select list when view play but the condition after the pushdown will resolve using the udf from select list the check here for deterministic filters should based the resolved expression refer test case 
WITHOUT_CLASSIFICATION	 and must have columns also the partition locations must lie within the table directory 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 unable get the database set the dbname empty 
WITHOUT_CLASSIFICATION	 udftostring need the following mappings 
WITHOUT_CLASSIFICATION	 bytes key hash multiset optimized for vector map join this the abstract base for the multikey and string bytes key hash multiset implementations 
WITHOUT_CLASSIFICATION	 need update roottoworkmap case the key since even 
WITHOUT_CLASSIFICATION	 also compute the correct cfcq pairs can assert the right argument was passed 
WITHOUT_CLASSIFICATION	 value needs converted match the type params length etc 
WITHOUT_CLASSIFICATION	 serialize 
WITHOUT_CLASSIFICATION	 then need truncate the exceptions list accordingly 
WITHOUT_CLASSIFICATION	 what can about 
WITHOUT_CLASSIFICATION	 build new 
WITHOUT_CLASSIFICATION	 verify the data are intact even after applying applied event once again existing objects 
WITHOUT_CLASSIFICATION	 alter partitioned table set table property 
WITHOUT_CLASSIFICATION	 character then reverse the whole string 
WITHOUT_CLASSIFICATION	 dbtable return table 
WITHOUT_CLASSIFICATION	 convert integer string 
WITHOUT_CLASSIFICATION	 test that underflow produces null 
WITHOUT_CLASSIFICATION	 index set child 
WITHOUT_CLASSIFICATION	 map from integer tag nondistinct aggrs with key parameters 
WITHOUT_CLASSIFICATION	 copy columnvectors overflowbatch remember buffered columns compactly the buffered vrbs without other columns scratch columns 
WITHOUT_CLASSIFICATION	 instantiate default values not specified 
WITHOUT_CLASSIFICATION	 function calls from the query plan 
WITHOUT_CLASSIFICATION	 add the partition again that drop table with partition can 
WITHOUT_CLASSIFICATION	 getters 
WITHOUT_CLASSIFICATION	 attribute methods 
WITHOUT_CLASSIFICATION	 url host port 
WITHOUT_CLASSIFICATION	 any input has not been rewritten not rewrite this rel 
WITHOUT_CLASSIFICATION	 mock beeline 
WITHOUT_CLASSIFICATION	 set conf use llap user rather than current user for llap registry 
WITHOUT_CLASSIFICATION	 currently only print the first port consistent with old behavior 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 project everything from the lhs and then those from the original 
WITHOUT_CLASSIFICATION	 not need update stats alter tablepartition operations 
WITHOUT_CLASSIFICATION	 number rows that match the regex but have missing groups 
WITHOUT_CLASSIFICATION	 create partial select query 
WITHOUT_CLASSIFICATION	 and preemption should attempted host despite host having available capacity 
WITHOUT_CLASSIFICATION	 session has expired and will returned later 
WITHOUT_CLASSIFICATION	 generate the cmd line run the child jvm 
WITHOUT_CLASSIFICATION	 apply sarg needed and otherwise determine what rgs read 
WITHOUT_CLASSIFICATION	 found best match during this processing use 
WITHOUT_CLASSIFICATION	 compare with old cacheend 
WITHOUT_CLASSIFICATION	 can mux operator 
WITHOUT_CLASSIFICATION	 null for tables virtualview for views materializedview for mvs 
WITHOUT_CLASSIFICATION	 now that the properties are can instantiate sessionstate 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 repeated string tableswritten 
WITHOUT_CLASSIFICATION	 generic udtfs 
WITHOUT_CLASSIFICATION	 output will also repeating and null 
WITHOUT_CLASSIFICATION	 add the table spec for the destination table 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 same session object expected 
WITHOUT_CLASSIFICATION	 partitioning columns the parent are not assigned assign partitioning columns the child the parent 
WITHOUT_CLASSIFICATION	 create dbs 
WITHOUT_CLASSIFICATION	 matches 
WITHOUT_CLASSIFICATION	 this dependency removed for hbase 
WITHOUT_CLASSIFICATION	 ensures that object cached that everyone uses the same instance 
WITHOUT_CLASSIFICATION	 try create rcfilereader 
WITHOUT_CLASSIFICATION	 and their types 
WITHOUT_CLASSIFICATION	 the tests here are heavily based some timing there some chance fail 
WITHOUT_CLASSIFICATION	 perform incremental normalization 
WITHOUT_CLASSIFICATION	 set the inferred sort columns for the file this filesink produces 
WITHOUT_CLASSIFICATION	 round 
WITHOUT_CLASSIFICATION	 diskdata 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 mock beelineopts 
WITHOUT_CLASSIFICATION	 intentionally set this high that will not trigger major compaction for ttp 
WITHOUT_CLASSIFICATION	 make sure check for side file case streaming ingest died 
WITHOUT_CLASSIFICATION	 rewrite the delete update into insert crazy but works deletes and update actually are inserts into the delta file hive delete delete from tablename where will rewritten insert into table tablename partition partcols select rowid partcols from tablename sort rowid update update tablename set expr where will rewritten insert into table tablename partition partcols select all partcolsfrom tablename sort rowid where all all the nonpartition columns the expressions from the set clause will reattached later the where clause will also reattached later the sort clause put there that records come out the right order enable merge read 
WITHOUT_CLASSIFICATION	 consider this enable issue not not vectorized issue 
WITHOUT_CLASSIFICATION	 our outputs are the transitive outputs our inputs 
WITHOUT_CLASSIFICATION	 need know aggregate count since corr subq with count aggregate 
WITHOUT_CLASSIFICATION	 this only when not initialized but may need find way tell the caller how initialize the valid size 
WITHOUT_CLASSIFICATION	 pass unparsed name here 
WITHOUT_CLASSIFICATION	 add expr the list predicates rejected from further pushing that know add createfilter 
WITHOUT_CLASSIFICATION	 this partition 
WITHOUT_CLASSIFICATION	 exception expected only filter enabled and injection disabled 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 catch with the big table 
WITHOUT_CLASSIFICATION	 get service ticket from the authorization header 
WITHOUT_CLASSIFICATION	 call this method may called after all the all fields have been read check for unread fields note that when optimizing reading stop reading unneeded include columns worrying about whether all data consumed not appropriate often arent reading all design since parses the line through the last desired column does support this function 
WITHOUT_CLASSIFICATION	 bad format 
WITHOUT_CLASSIFICATION	 process join filters 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 compacts 
WITHOUT_CLASSIFICATION	 test with batch size and decaying factor 
WITHOUT_CLASSIFICATION	 load set 
WITHOUT_CLASSIFICATION	 position beginning 
WITHOUT_CLASSIFICATION	 input the script 
WITHOUT_CLASSIFICATION	 set global member indicating which virtual columns are possible used 
WITHOUT_CLASSIFICATION	 removes the threadlocal variables closes underlying hms connection 
WITHOUT_CLASSIFICATION	 tests the case when tblpathpapbpcfile for table with partition does not throw hiveexception 
WITHOUT_CLASSIFICATION	 exceeds this value 
WITHOUT_CLASSIFICATION	 set the table write all the acid file sinks 
WITHOUT_CLASSIFICATION	 the getsplits call should have resulted lock acidtbl 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this for all complex types and binary 
WITHOUT_CLASSIFICATION	 case keepalive ensure that timeout handler does not close connection until entire 
WITHOUT_CLASSIFICATION	 some tests expected pass invalid schema 
WITHOUT_CLASSIFICATION	 max 
WITHOUT_CLASSIFICATION	 replace the reducer with our fully vectorized reduce operator tree 
WITHOUT_CLASSIFICATION	 longer relevant for 
WITHOUT_CLASSIFICATION	 set functions create the null check query for notin subquery predicates for subquery predicate like not select from where the not null check query select count from where and null this subquery joined with the outer query plan the join condition the join condition ensures that case there are null values the joining column the query returns rows the ast tree for this tokquery tok from toksubquery the input subquery with correlation removed subqueryalias tokinsert tokdestination tokselect tokselectexpr ast tree for count tokwhere null check for joining column 
WITHOUT_CLASSIFICATION	 supports keeping object without having import that definition 
WITHOUT_CLASSIFICATION	 preven subsequent runs until new trigger set 
WITHOUT_CLASSIFICATION	 since hive here then its not acid table there should never any deltas 
WITHOUT_CLASSIFICATION	 then try the brute force search for something throw away 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 get the job handle associated with the spark job 
WITHOUT_CLASSIFICATION	 just safe check ensure that are not reading empty delete files 
WITHOUT_CLASSIFICATION	 since are creating with scale fraction digits zero trim 
WITHOUT_CLASSIFICATION	 loginfogetting parent ptnrootgetname 
WITHOUT_CLASSIFICATION	 synthetic predicate with dynamic values 
WITHOUT_CLASSIFICATION	 get around hbase failure single node see bug 
WITHOUT_CLASSIFICATION	 terminate the old task and make current task dependent 
WITHOUT_CLASSIFICATION	 after concatenating them with and operator 
WITHOUT_CLASSIFICATION	 start the shuffle service before the listener until its service well 
WITHOUT_CLASSIFICATION	 map zcolumninfo for 
WITHOUT_CLASSIFICATION	 replicate table definition 
WITHOUT_CLASSIFICATION	 make sure that the number column aliases the clause matches 
WITHOUT_CLASSIFICATION	 contains reducer the optimization always since there exists reducer the sortingbucketing properties due the sortmerge join operator are lost anyway the plan cannot wrong 
WITHOUT_CLASSIFICATION	 runs the templeton controller job with args utilizes toolrunner run the actual job 
WITHOUT_CLASSIFICATION	 intentional fall through 
WITHOUT_CLASSIFICATION	 hiveconf hivestatsndverror default produces vectors 
WITHOUT_CLASSIFICATION	 raise custom exception like ioexception and verify expected message this should not invoke cancel operation 
WITHOUT_CLASSIFICATION	 were performing binary search need restart 
WITHOUT_CLASSIFICATION	 close the client connection with zookeeper 
WITHOUT_CLASSIFICATION	 loop once again with the new cause current 
WITHOUT_CLASSIFICATION	 why this checking for deltasisempty hive 
WITHOUT_CLASSIFICATION	 retrieve attempt log into logdir 
WITHOUT_CLASSIFICATION	 verify data 
WITHOUT_CLASSIFICATION	 type promotion everything goes decimal 
WITHOUT_CLASSIFICATION	 the field bits which fields include for each grouping set 
WITHOUT_CLASSIFICATION	 the size the biggest small table 
WITHOUT_CLASSIFICATION	 test forward scan 
WITHOUT_CLASSIFICATION	 small case just write the value bytes only 
WITHOUT_CLASSIFICATION	 generate the kerberos ticket under the following scenarios cookie authentication disabled the first time when the request sent the server returns which sometimes means the cookie has expired 
WITHOUT_CLASSIFICATION	 tried 
WITHOUT_CLASSIFICATION	 cte referenced queryblock add subquery for now sqalias the alias used alias specified used the cte name works just like table references adding done copying ast cte setting astorigin cloned ast trigger phase new qbexpr update data structs remove this table reference move invocation 
WITHOUT_CLASSIFICATION	 number rows means that statistics from metastore not reliable 
WITHOUT_CLASSIFICATION	 repeated nonnull fill down column 
WITHOUT_CLASSIFICATION	 handle the case for unpartitioned table 
WITHOUT_CLASSIFICATION	 try zerodivide show repeating null produced 
WITHOUT_CLASSIFICATION	 end pattern 
WITHOUT_CLASSIFICATION	 setup hashcode 
WITHOUT_CLASSIFICATION	 test setter for map object 
WITHOUT_CLASSIFICATION	 choose array size have two hash tables hold entries the sum the two should have bit more than twice much space the 
WITHOUT_CLASSIFICATION	 added conf member set the repl command specific config entries without affecting the configs 
WITHOUT_CLASSIFICATION	 the number reducers the child more specific than that the parent assign the number reducers the child the parent 
WITHOUT_CLASSIFICATION	 wont happen 
WITHOUT_CLASSIFICATION	 wrap the transport exception rte since subjectdoas then goes and unwraps this for out the doas block then unwrap one more time our catch clause get back the tte ugh 
WITHOUT_CLASSIFICATION	 propagate 
WITHOUT_CLASSIFICATION	 nothing case proc null 
WITHOUT_CLASSIFICATION	 the output precision greater than the input which should cover least rows the scale the same the input 
WITHOUT_CLASSIFICATION	 convert inputs 
WITHOUT_CLASSIFICATION	 tables that were serialized with columnsetserde doesnt have metadata this hack applies all such tables 
WITHOUT_CLASSIFICATION	 cancel currently executing tasks 
WITHOUT_CLASSIFICATION	 the list element object inspector 
WITHOUT_CLASSIFICATION	 skip rest checks user admin 
WITHOUT_CLASSIFICATION	 standard error allowed for ndv estimates for fmsketch lower value indicates higher accuracy and 
WITHOUT_CLASSIFICATION	 replace existing view 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlarray 
WITHOUT_CLASSIFICATION	 generate reducesinkoperator there special case when want the rows randomly distributed reducers for load balancing problem that happens when there distinct operator set the numpartitioncolumns for this purpose this 
WITHOUT_CLASSIFICATION	 drop any left over catalogs 
WITHOUT_CLASSIFICATION	 treeset anyway uses treemap use plain treemap able get value collisions 
WITHOUT_CLASSIFICATION	 this idempotent 
WITHOUT_CLASSIFICATION	 adding column names used later 
WITHOUT_CLASSIFICATION	 expect cache requests from the middle here 
WITHOUT_CLASSIFICATION	 count any input except null which for count and output long just modes partial complete 
WITHOUT_CLASSIFICATION	 some filters may have been specified the show locks statement add them the query 
WITHOUT_CLASSIFICATION	 gen gbrsgbrsgb pipeline 
WITHOUT_CLASSIFICATION	 will considered enable and novalidate and relyfalse 
WITHOUT_CLASSIFICATION	 the driver not already available the url add the one provided 
WITHOUT_CLASSIFICATION	 check that src and dest are the same file system 
WITHOUT_CLASSIFICATION	 dynamic partition list the statstask 
WITHOUT_CLASSIFICATION	 this method ends with anything except retry signal the caller should fail the operation and propagate the error the its caller metastore client thus must reset retry counters 
WITHOUT_CLASSIFICATION	 job request type 
WITHOUT_CLASSIFICATION	 start the scheduled poll task 
WITHOUT_CLASSIFICATION	 load data 
WITHOUT_CLASSIFICATION	 all the vertices belong the same dag just use numbers 
WITHOUT_CLASSIFICATION	 make sure the table the target database didnt get clobbered 
WITHOUT_CLASSIFICATION	 fall through 
WITHOUT_CLASSIFICATION	 this input rel not rewritten 
WITHOUT_CLASSIFICATION	 add single child and restart the loop 
WITHOUT_CLASSIFICATION	 write credential with token file 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 read the schema version stored metastore 
WITHOUT_CLASSIFICATION	 possible that read and write entities contain old version the object before was modified statstask get the latest versions the object 
WITHOUT_CLASSIFICATION	 the buffer not the full heap demote the top item the heap into the list 
WITHOUT_CLASSIFICATION	 make sure all leaves are visited least once 
WITHOUT_CLASSIFICATION	 successfully convert bucket map join 
WITHOUT_CLASSIFICATION	 evaluate the hashcode 
WITHOUT_CLASSIFICATION	 represents select expression the context windowing these can refer the output windowing functions and can navigate the partition using leadlag functions 
WITHOUT_CLASSIFICATION	 filter condition true ignore 
WITHOUT_CLASSIFICATION	 actually run which different under doasfalse this seems intended 
WITHOUT_CLASSIFICATION	 reducers dont produce enough files well the same for tables for now 
WITHOUT_CLASSIFICATION	 jar then this would needed 
WITHOUT_CLASSIFICATION	 dont retry immediately use delay with exponential backoff 
WITHOUT_CLASSIFICATION	 lrfu cache policy doesnt store locked blocks when cache the block locked simply nothing here the fact that was never updated will allow add properly the first notifyunlock well set priority account for the inbound one lock not heap 
WITHOUT_CLASSIFICATION	 extract information 
WITHOUT_CLASSIFICATION	 fold after replacing possible 
WITHOUT_CLASSIFICATION	 should have also been thrown out 
WITHOUT_CLASSIFICATION	 add the newly generated clause subexpr 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 test that dont drop the unnecessary tuple the table has the corresponding struct 
WITHOUT_CLASSIFICATION	 get children key node 
WITHOUT_CLASSIFICATION	 this wasnt empty txn wed get better msg 
WITHOUT_CLASSIFICATION	 will set the larger the parents 
WITHOUT_CLASSIFICATION	 mark that branch the big table branch 
WITHOUT_CLASSIFICATION	 this will create project which will project out the column positions 
WITHOUT_CLASSIFICATION	 return the next back servers port 
WITHOUT_CLASSIFICATION	 merge should convert hll dense 
WITHOUT_CLASSIFICATION	 case the expression tablecol col can regex this can only happen without clause dont allow this for exprresolver the group case 
WITHOUT_CLASSIFICATION	 schema only 
WITHOUT_CLASSIFICATION	 calculate filter propagation directions for each alias for innersemi join for left outer join for right outer 
WITHOUT_CLASSIFICATION	 nonnative and nonmanaged tables are not supported movetask requires filenames specific format 
WITHOUT_CLASSIFICATION	 extract each entry from the pathenv 
WITHOUT_CLASSIFICATION	 return true 
WITHOUT_CLASSIFICATION	 possible 
WITHOUT_CLASSIFICATION	 value needs converted match type params 
WITHOUT_CLASSIFICATION	 dump metrics string json 
WITHOUT_CLASSIFICATION	 dump all the events except drop 
WITHOUT_CLASSIFICATION	 gained again 
WITHOUT_CLASSIFICATION	 not using here because forces connection the 
WITHOUT_CLASSIFICATION	 default aggregate counters across the entire dag example shufflebytes would mean shufflebytes each vertex aggregated together create dag level shufflebytes use case shufflebytes across the entire dag limit perform action 
WITHOUT_CLASSIFICATION	 skewed info 
WITHOUT_CLASSIFICATION	 make sure the updates are not sent out order compared how apply them 
WITHOUT_CLASSIFICATION	 number mantissa digits before decimal point 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 let cleaner delete obsolete filesdirs 
WITHOUT_CLASSIFICATION	 release all the previous buffers that may not have been able release due reuse 
WITHOUT_CLASSIFICATION	 get the actual length first 
WITHOUT_CLASSIFICATION	 ensure this set the config that the can read 
WITHOUT_CLASSIFICATION	 drop test and its tables and views 
WITHOUT_CLASSIFICATION	 change the engine tez 
WITHOUT_CLASSIFICATION	 test for only colnames being empty 
WITHOUT_CLASSIFICATION	 this guard for special druid types hyperunique currently not support doing anything special with them hive however those columns are there and they can actually read normal dimensions with select query thus print the warning and just read them string 
WITHOUT_CLASSIFICATION	 operations that have objects type commandparams function are authorized solely the type 
WITHOUT_CLASSIFICATION	 note this doesnt maintain proper newstream semantics any could either clone this instead enforce that this only called once 
WITHOUT_CLASSIFICATION	 str 
WITHOUT_CLASSIFICATION	 move logic that can reused 
WITHOUT_CLASSIFICATION	 separator for open write ids separator for aborted write ids 
WITHOUT_CLASSIFICATION	 process the position alias groupby and orderby 
WITHOUT_CLASSIFICATION	 there are none theyre not readable 
WITHOUT_CLASSIFICATION	 get the characterbyte the offset the string equal the fieldid 
WITHOUT_CLASSIFICATION	 default column name 
WITHOUT_CLASSIFICATION	 definitely int most ints fall here 
WITHOUT_CLASSIFICATION	 right trim slice byte array and place the result into element vector 
WITHOUT_CLASSIFICATION	 though given short hcat the map will emit 
WITHOUT_CLASSIFICATION	 null 
WITHOUT_CLASSIFICATION	 columns have been added 
WITHOUT_CLASSIFICATION	 needed initargs for certain execution paths 
WITHOUT_CLASSIFICATION	 singlecolumn string specific variables 
WITHOUT_CLASSIFICATION	 multiple concurrent local mode job submissions can cause collisions working dirs and system dirs workaround rename map red working dir temp dir such cases 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 get forwarded hosts address 
WITHOUT_CLASSIFICATION	 validate and setup patternstr 
WITHOUT_CLASSIFICATION	 duplicate column names currently simple algorithm this can optimized later need but should not major bottleneck the number columns are anyway not big 
WITHOUT_CLASSIFICATION	 should only need insert the token the first time 
WITHOUT_CLASSIFICATION	 deserialize key into vector row columns 
WITHOUT_CLASSIFICATION	 that uniont null converted just 
WITHOUT_CLASSIFICATION	 bitset for flagging aborted transactions bit true aborted false open default value means there are open txn the snapshot 
WITHOUT_CLASSIFICATION	 test databasemetadata queries which not have parent statement 
WITHOUT_CLASSIFICATION	 looks like some pools were removed kill running queries requeue the queued ones 
WITHOUT_CLASSIFICATION	 top 
WITHOUT_CLASSIFICATION	 hive probably wont support really care 
WITHOUT_CLASSIFICATION	 copy log file 
WITHOUT_CLASSIFICATION	 for fully specified ptn follow strict checks for existence partitions metadata for unpartitioned tables follow filechecks for partially specified tables this would then need filechecks the start ptn write doing metadata checks can get potentially very expensive fat conf there are large number partitions that match the partial specifications 
WITHOUT_CLASSIFICATION	 get partitions name ascending descending 
WITHOUT_CLASSIFICATION	 start the process add the cache 
WITHOUT_CLASSIFICATION	 make sure all partitioning columns referenced actually exist and are the correct order the end the list columns produced the view also move the field schema descriptors from derivedschema the partitioning key 
WITHOUT_CLASSIFICATION	 assume each has unique serde 
WITHOUT_CLASSIFICATION	 set the key check this new group same group 
WITHOUT_CLASSIFICATION	 short running updated nothing expect rows writeset 
WITHOUT_CLASSIFICATION	 partvalues 
WITHOUT_CLASSIFICATION	 the interface for single long key hash map lookup method 
WITHOUT_CLASSIFICATION	 the thread still active and needs cancelled then cancel this may happen case task got interrupted timed out 
WITHOUT_CLASSIFICATION	 replace the crs sel operator 
WITHOUT_CLASSIFICATION	 lets remember the join operators have processed 
WITHOUT_CLASSIFICATION	 transaction and locking methods 
WITHOUT_CLASSIFICATION	 remove the semijoin optimization branch along with all the mappings the parent has all the branches collect them and remove them 
WITHOUT_CLASSIFICATION	 race with querycomplete 
WITHOUT_CLASSIFICATION	 updated when add this the queue 
WITHOUT_CLASSIFICATION	 wait for the current future 
WITHOUT_CLASSIFICATION	 stages elapsed time 
WITHOUT_CLASSIFICATION	 time after which metastore cache updated from metastore the background update thread 
WITHOUT_CLASSIFICATION	 dont push sampling predicate since createfilter always creates filter with issamplepred false also the filterop with sampling pred always 
WITHOUT_CLASSIFICATION	 bunch things get setup the context based conf but need only the tmp directory 
WITHOUT_CLASSIFICATION	 web port 
WITHOUT_CLASSIFICATION	 did not add any factor there are common factors can 
WITHOUT_CLASSIFICATION	 will succeed and transition for cleaning 
WITHOUT_CLASSIFICATION	 overlay the values any system properties whose names appear the list confvars 
WITHOUT_CLASSIFICATION	 this conditions need pushed into semijoin since this condition corresponds 
WITHOUT_CLASSIFICATION	 create dbtpart part part test recycle single file part recycle table 
WITHOUT_CLASSIFICATION	 create the temp directories 
WITHOUT_CLASSIFICATION	 remove the paths which are not part 
WITHOUT_CLASSIFICATION	 both neededcolumnids and neededcolumns should never null when neededcolumnids empty list means needed column not need any column evaluate 
WITHOUT_CLASSIFICATION	 store types and tables separately because one cannot use table servicemethod struct 
WITHOUT_CLASSIFICATION	 used struct and union complex type readers indicate the final field has been fully read and the current complex type finished 
WITHOUT_CLASSIFICATION	 optimization the conditionaltask avoids linking movetask that are expensive blobstorage systems instead linking creates one movetask where the source the first movetask source and target the second movetask target 
WITHOUT_CLASSIFICATION	 needs major compaction 
WITHOUT_CLASSIFICATION	 only can have single partition spec 
WITHOUT_CLASSIFICATION	 all others from the remote service cause the task fail 
WITHOUT_CLASSIFICATION	 this file something dont hold locks for 
WITHOUT_CLASSIFICATION	 since left integer always some products here are not included 
WITHOUT_CLASSIFICATION	 have field and are positioned read 
WITHOUT_CLASSIFICATION	 default for all other objects this false 
WITHOUT_CLASSIFICATION	 string char varchar and binary for char and varchar when the caller takes responsibility for truncationpadding issues when true conversion needed into external buffer least bytes use get the result otherwise currentbytes currentbytesstart and currentbyteslength are the result 
WITHOUT_CLASSIFICATION	 group path alias according work 
WITHOUT_CLASSIFICATION	 there cannot exist any sampling predicate 
WITHOUT_CLASSIFICATION	 all rows from right side will present resultset 
WITHOUT_CLASSIFICATION	 this will work with the new support rewriting load into ias 
WITHOUT_CLASSIFICATION	 assumption acid columns are currently always the beginning the arrays 
WITHOUT_CLASSIFICATION	 authorization errors 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 combinesort temp and normal table results 
WITHOUT_CLASSIFICATION	 have estimation lowerbound and higherbound use estimation between lowerbound and higherbound 
WITHOUT_CLASSIFICATION	 only split pruning hive has been fixed the writer 
WITHOUT_CLASSIFICATION	 set temp file containing error output sent client 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring javalangstring javalangstring javalangstring 
WITHOUT_CLASSIFICATION	 create tmp dir for mergefilework 
WITHOUT_CLASSIFICATION	 the small key length the key big length allbitson then the key length stored the writebuffers 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring javalangstring 
WITHOUT_CLASSIFICATION	 datanucleus objects get detached all over the place for real reason 
WITHOUT_CLASSIFICATION	 now lets take look input sizes 
WITHOUT_CLASSIFICATION	 extract possible candidates pushed down 
WITHOUT_CLASSIFICATION	 remove restrictions the variables that can set using set command 
WITHOUT_CLASSIFICATION	 can generate ranges from rowid 
WITHOUT_CLASSIFICATION	 arraylistmultimap important here retain the ordering for the splits 
WITHOUT_CLASSIFICATION	 lookup ndvs side 
WITHOUT_CLASSIFICATION	 the has other siblings will add considered next iteration 
WITHOUT_CLASSIFICATION	 cannot wrap reader for nonvectorized pipeline 
WITHOUT_CLASSIFICATION	 example 
WITHOUT_CLASSIFICATION	 this the small table side case smb join need send each split the corresponding bucketbased task the other side case split needs multiple downstream tasks need clone the event and send the right destination 
WITHOUT_CLASSIFICATION	 keep order name consistent with jdo 
WITHOUT_CLASSIFICATION	 delta may present from previous failed task attempt 
WITHOUT_CLASSIFICATION	 minhistorylevel will have entry for the open txn 
WITHOUT_CLASSIFICATION	 nested complex types cannot folded cleanly 
WITHOUT_CLASSIFICATION	 the same process this will still only get the functions from the first metastore 
WITHOUT_CLASSIFICATION	 add only the corresponding family has not already been added 
WITHOUT_CLASSIFICATION	 write the byte stream every keyvalue pairs 
WITHOUT_CLASSIFICATION	 base javaobject primitives javafieldref entry javaobject javafieldref 
WITHOUT_CLASSIFICATION	 start writing array contents 
WITHOUT_CLASSIFICATION	 skip past blank characters 
WITHOUT_CLASSIFICATION	 vectorized row batch not for example original inspector for orc table etc 
WITHOUT_CLASSIFICATION	 stats part 
WITHOUT_CLASSIFICATION	 all them were false return false 
WITHOUT_CLASSIFICATION	 hours 
WITHOUT_CLASSIFICATION	 get the sort aliases these are aliased the entries the select list 
WITHOUT_CLASSIFICATION	 grab the oldest inmemory buffered batch and dump disk 
WITHOUT_CLASSIFICATION	 statics for when the mock created via filesystemget 
WITHOUT_CLASSIFICATION	 this must hadoop where was protected 
WITHOUT_CLASSIFICATION	 map table name the correct columnstatstask 
WITHOUT_CLASSIFICATION	 use system zone when converting from timestamp timestamptz 
WITHOUT_CLASSIFICATION	 this tests checks that appropriate delta and deletedeltas are included when minor compactions specifies valid open txn range 
WITHOUT_CLASSIFICATION	 only case full outer join with smb enabled which not possible convert regular join 
WITHOUT_CLASSIFICATION	 print next vertex 
WITHOUT_CLASSIFICATION	 dbnamematching alone 
WITHOUT_CLASSIFICATION	 attributes 
WITHOUT_CLASSIFICATION	 todo even listener for default new true this 
WITHOUT_CLASSIFICATION	 join current union task old task 
WITHOUT_CLASSIFICATION	 verify the eventid was passed the nontransactional listener 
WITHOUT_CLASSIFICATION	 check whether there column needed the windowing operation that missing the project expressions for instance the windowing operation over aggregation column hive expects that column the select clause the query the idea that there column missing will replace the old project operator two new project operators project operator containing the original columns the project operator plus all the columns that were missing project top the previous one that will take out the columns that were missing and were added the previous project 
WITHOUT_CLASSIFICATION	 insert the current constant value into exprnodestructs list there struct corresponding the current element create new one insert 
WITHOUT_CLASSIFICATION	 notify listeners 
WITHOUT_CLASSIFICATION	 comparing paths multiple times creates lots objects creates pressure for tables having large number partitions such cases use precomputed paths for comparison 
WITHOUT_CLASSIFICATION	 skip duplicated grouping keys happens when define column alias 
WITHOUT_CLASSIFICATION	 link sel 
WITHOUT_CLASSIFICATION	 try repeating both sides 
WITHOUT_CLASSIFICATION	 minihs will become leader 
WITHOUT_CLASSIFICATION	 update the last access time for this node 
WITHOUT_CLASSIFICATION	 check input objects length doesnt match then output new writable with correct params 
WITHOUT_CLASSIFICATION	 have ioexception other than 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 vectorizer does not vectorize row deserialize mode the input format has input formats will clear the isvectorized flag they are doing vrb work 
WITHOUT_CLASSIFICATION	 byte 
WITHOUT_CLASSIFICATION	 values outside the column type bounds will fail runtime 
WITHOUT_CLASSIFICATION	 process join keys 
WITHOUT_CLASSIFICATION	 must have column name followed with type 
WITHOUT_CLASSIFICATION	 subtraction with type date longcolumnvector storing days and type timestamp produces 
WITHOUT_CLASSIFICATION	 process the combine splits 
WITHOUT_CLASSIFICATION	 spilled small tables 
WITHOUT_CLASSIFICATION	 the first directory becomes the base for combining 
WITHOUT_CLASSIFICATION	 use the tez hash table loader 
WITHOUT_CLASSIFICATION	 field expression should resolved 
WITHOUT_CLASSIFICATION	 this sql standard maxn zero items should null 
WITHOUT_CLASSIFICATION	 look for functions without pattern 
WITHOUT_CLASSIFICATION	 default list bucketing directory name internal use only not for client 
WITHOUT_CLASSIFICATION	 test acid with vectorization combine 
WITHOUT_CLASSIFICATION	 node already exists 
WITHOUT_CLASSIFICATION	 get all items into array and sort them 
WITHOUT_CLASSIFICATION	 wait for the child process finish 
WITHOUT_CLASSIFICATION	 create new union and sort 
WITHOUT_CLASSIFICATION	 finally can create the grouped edge 
WITHOUT_CLASSIFICATION	 not need anything 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 decimal addition subtraction 
WITHOUT_CLASSIFICATION	 setup expr col map 
WITHOUT_CLASSIFICATION	 since raw data was possibly escaped make split work now need remove escape chars they dont interfere with downstream processing 
WITHOUT_CLASSIFICATION	 get the map for postovertex 
WITHOUT_CLASSIFICATION	 compactor states should really enum 
WITHOUT_CLASSIFICATION	 check privileges 
WITHOUT_CLASSIFICATION	 copy files with retry logic failure source file dropped changed 
WITHOUT_CLASSIFICATION	 only consider tables for which hold either exclusive shared write lock 
WITHOUT_CLASSIFICATION	 create new sparkwork for all the small tables this work 
WITHOUT_CLASSIFICATION	 leadership status change happens inside synchronized methods also use single threaded executor service for handling notifications which guarantees ordering for notification handling leadership status change happens when tez sessions are getting created the notleader notification will get queued executor service 
WITHOUT_CLASSIFICATION	 the next row will require another call increasebufferspace since this new buffer should used 
WITHOUT_CLASSIFICATION	 only one them 
WITHOUT_CLASSIFICATION	 iterate over the selects search for aggregation trees use string keys eliminate duplicate trees 
WITHOUT_CLASSIFICATION	 the denominator the tablesample clause 
WITHOUT_CLASSIFICATION	 unpartitioned table 
WITHOUT_CLASSIFICATION	 now that have exited read lock safe remove any invalid entries 
WITHOUT_CLASSIFICATION	 authentication only authentication and integrity checking using signatures authentication integrity and confidentiality checking 
WITHOUT_CLASSIFICATION	 following special cases for different type subqueries which have aggregate and implicit group and are correlatd existsnot exists not allowed throw error for now plan allow this later scalar this should return true since later subquery remove rule need know about this case always allowed but returns true for cases with aggregate other than count since later subquery remove rule need know about this case 
WITHOUT_CLASSIFICATION	 initialize common server configs needed both binary http modes 
WITHOUT_CLASSIFICATION	 partitioned table delete all 
WITHOUT_CLASSIFICATION	 required required required required optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 test that existing sharedwrite table with new sharedread coalesces 
WITHOUT_CLASSIFICATION	 the serialized all null key and its hash code 
WITHOUT_CLASSIFICATION	 the evaluate yields true then pass all rows else pass rows 
WITHOUT_CLASSIFICATION	 configuration for the application master 
WITHOUT_CLASSIFICATION	 hdfs scratch dir 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 move the specified work from the sparkwork the targetwork note that order not break the graph since need for the edges 
WITHOUT_CLASSIFICATION	 print current state before exiting 
WITHOUT_CLASSIFICATION	 retrieve results 
WITHOUT_CLASSIFICATION	 other integer types not supported yet 
WITHOUT_CLASSIFICATION	 file locations searched the correct order 
WITHOUT_CLASSIFICATION	 add tracking information check source state already known and send out update 
WITHOUT_CLASSIFICATION	 finishable state checked the task via explicit query the taskrunnercallable 
WITHOUT_CLASSIFICATION	 with the parent based its position the list parents 
WITHOUT_CLASSIFICATION	 the objects that have been printed 
WITHOUT_CLASSIFICATION	 have add this one manually for tests the initialized via the metastorediretsql and dont run the schema creation sql that includes the insert for which can locked the entry happens via notificationevent insertion 
WITHOUT_CLASSIFICATION	 error storage specification 
WITHOUT_CLASSIFICATION	 add input path 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 construct the edgemanager descriptor used all edges which need the routing table 
WITHOUT_CLASSIFICATION	 parser only allows fooab not foofooa foob 
WITHOUT_CLASSIFICATION	 assign row from list standard objects count 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqldate javautilcalendar 
WITHOUT_CLASSIFICATION	 when there are live nodes the cluster and this timeout elapses the query failed 
WITHOUT_CLASSIFICATION	 required optional optional optional optional optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 gce firewall set through instance tags 
WITHOUT_CLASSIFICATION	 first handle special cases one the special case methods cannot handle returns null 
WITHOUT_CLASSIFICATION	 this case happens only when prs key empty which case can use number distribution keys and key serialization info from crs 
WITHOUT_CLASSIFICATION	 nothing this case 
WITHOUT_CLASSIFICATION	 path the filesinkoperator table blobstore path 
WITHOUT_CLASSIFICATION	 aggregate size from aggregation buffers 
WITHOUT_CLASSIFICATION	 the union operators from the operator tree later 
WITHOUT_CLASSIFICATION	 since sql case insenstive just make sure that the comparison column names and check expressions column reference work convert the key lower case 
WITHOUT_CLASSIFICATION	 can get away with the use varname here because varname hivename for pwd 
WITHOUT_CLASSIFICATION	 the assumption here the path file only case this different acid deltas the isfile check avoided here for performance reasons 
WITHOUT_CLASSIFICATION	 integermaxvalue 
WITHOUT_CLASSIFICATION	 these delims passed serde params comment passed table params 
WITHOUT_CLASSIFICATION	 the values from 
WITHOUT_CLASSIFICATION	 max rows rows from left side 
WITHOUT_CLASSIFICATION	 there should only column sourceoperator 
WITHOUT_CLASSIFICATION	 avoid denominator getting larger and aggressively reducing number rows will ease out denominator 
WITHOUT_CLASSIFICATION	 process singlecolumn string leftsemi join vectorized row batch 
WITHOUT_CLASSIFICATION	 get the table from the client again verify the name has been updated 
WITHOUT_CLASSIFICATION	 txnmanagerfactory singleton the default true has already been created and wont throw 
WITHOUT_CLASSIFICATION	 hint provided use that size 
WITHOUT_CLASSIFICATION	 case the dynamic value resolves null value 
WITHOUT_CLASSIFICATION	 clean 
WITHOUT_CLASSIFICATION	 will transform using clause and make look like onclause lets generate valid onclause ast from using 
WITHOUT_CLASSIFICATION	 serialize the union tagvalue 
WITHOUT_CLASSIFICATION	 for shell commands use unstripped command 
WITHOUT_CLASSIFICATION	 used alias 
WITHOUT_CLASSIFICATION	 try nulls both sides 
WITHOUT_CLASSIFICATION	 know the job has finished check the futures here ourselves 
WITHOUT_CLASSIFICATION	 alterpartition only for changing the partition location the table rename 
WITHOUT_CLASSIFICATION	 create the final group operator 
WITHOUT_CLASSIFICATION	 check whether the username the token what expect 
WITHOUT_CLASSIFICATION	 the task longer required and asks for deallocation 
WITHOUT_CLASSIFICATION	 check the last node 
WITHOUT_CLASSIFICATION	 the conf string for columnsbuffersize 
WITHOUT_CLASSIFICATION	 keep this within chars width more columns needs added then update min terminal width requirement and separator width accordingly 
WITHOUT_CLASSIFICATION	 should only managed tables passed here check table the default table location based the old warehouse root then change the table location the default based the current warehouse root the existing table directory will also moved the new default database directory 
WITHOUT_CLASSIFICATION	 track which small tables havent been processed yet 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 hive hiveserver the jars for this udf may have been loaded different thread and the current thread may not able resolve the udf test for this condition 
WITHOUT_CLASSIFICATION	 addpartitions normal operation 
WITHOUT_CLASSIFICATION	 create type with nestinglevel levels nesting 
WITHOUT_CLASSIFICATION	 write final length chunk 
WITHOUT_CLASSIFICATION	 might have multiple ranges coming from children 
WITHOUT_CLASSIFICATION	 serialize the row component using the rowidfactory the normal case this will just 
WITHOUT_CLASSIFICATION	 build map which tracks the name column inputs signature corresponding table column name this will used replace column references check expression ast with corresponding column name input 
WITHOUT_CLASSIFICATION	 specifying the right type info length tells which the last column 
WITHOUT_CLASSIFICATION	 pull out deterministic exprs from nondeterministic and push down deterministic expressions separate filter 
WITHOUT_CLASSIFICATION	 convert the metastore thrift objects result objects 
WITHOUT_CLASSIFICATION	 running the movetask and task parallel may cause the mvtask write and task write for the same partition 
WITHOUT_CLASSIFICATION	 currently not used hive codebase but intended authorize actions that are directly userlevel theres storage based aspect this can follow one two routes can allow default that way this call stays out the way can deny default that way privileges are authorized that not understood and explicitly allowed both approaches have merit but given that things like grants and revokes that are userlevel not make sense from the context storagepermission based auth denying seems more canonical here 
WITHOUT_CLASSIFICATION	 now the correct way through objectinspectors 
WITHOUT_CLASSIFICATION	 yarn service has started llap application now for some reason state changes complete then fail fast 
WITHOUT_CLASSIFICATION	 the parent ops for hashtablesinkop 
WITHOUT_CLASSIFICATION	 simply need remember that weve seen union 
WITHOUT_CLASSIFICATION	 there are skewed values nothing needs done 
WITHOUT_CLASSIFICATION	 different locks from same txn should not conflict with each other 
WITHOUT_CLASSIFICATION	 temporary selected vector 
WITHOUT_CLASSIFICATION	 transform case when with just thenelse into statement 
WITHOUT_CLASSIFICATION	 currently metastore does not store column stats for partition column calculate the ndv from partition list 
WITHOUT_CLASSIFICATION	 sleep until all threads with clean tasks are completed 
WITHOUT_CLASSIFICATION	 gettable checks whether database exists should moved here 
WITHOUT_CLASSIFICATION	 return the serialized bytes 
WITHOUT_CLASSIFICATION	 read the keys and values 
WITHOUT_CLASSIFICATION	 check list elements are primitive objects 
WITHOUT_CLASSIFICATION	 create three catalogs 
WITHOUT_CLASSIFICATION	 accurate short value cannot obtained 
WITHOUT_CLASSIFICATION	 export command uses metadata 
WITHOUT_CLASSIFICATION	 dont call tez doesnt sign fragments 
WITHOUT_CLASSIFICATION	 because the implementation the jsonparserfactory are sure that can get tezjsonparser 
WITHOUT_CLASSIFICATION	 algorithm convert decimal three bit words three enough for the decimal since represent the decimal with trailing zeroes trimmed skip leading zeroes the words once find real data nonzero byte add sign byte buffer necessary add bytes from the rest bit words return byte count 
WITHOUT_CLASSIFICATION	 create temp table directory 
WITHOUT_CLASSIFICATION	 timestamp 
WITHOUT_CLASSIFICATION	 try one sorted 
WITHOUT_CLASSIFICATION	 input row resolver 
WITHOUT_CLASSIFICATION	 and pass settaskplan the last parameter 
WITHOUT_CLASSIFICATION	 child need for pruning 
WITHOUT_CLASSIFICATION	 validate location string 
WITHOUT_CLASSIFICATION	 ignore this exception there problem itll fail when trying read write 
WITHOUT_CLASSIFICATION	 the drop has fail nonexistent partitions cannot batch expressions that because actually have check each separate expression for existence could small optimization for the case where expr has all columns and all operators are equality assume those would always match one partition which may not true with legacy nonnormalized column values this probably popular case but thats kinda hacky lets not for now 
WITHOUT_CLASSIFICATION	 tokensig could null 
WITHOUT_CLASSIFICATION	 return true this data type handled the output vector integer 
WITHOUT_CLASSIFICATION	 otherwise dont know what make maybe 
WITHOUT_CLASSIFICATION	 serdes here 
WITHOUT_CLASSIFICATION	 this method used validate check expression since check expression isnt allowed have subquery 
WITHOUT_CLASSIFICATION	 try running priority task 
WITHOUT_CLASSIFICATION	 length files cannot orc files not valid for 
WITHOUT_CLASSIFICATION	 default can always use the multikey class 
WITHOUT_CLASSIFICATION	 there was parallel cache eviction the evictor accounting for the memory 
WITHOUT_CLASSIFICATION	 partition mixed case 
WITHOUT_CLASSIFICATION	 but for now will just retry will evict more each time 
WITHOUT_CLASSIFICATION	 test default table types returned 
WITHOUT_CLASSIFICATION	 set metastoreoverlay parameters 
WITHOUT_CLASSIFICATION	 retrieve the stats obj that was just written 
WITHOUT_CLASSIFICATION	 start the split falls somewhere within before this slice note the linerecordreader will skip the first row even start directly its start because cannot know its the start not unless its note that give special treatment here unlike the eof below 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 for the virtual columns the internalname upper case and the alias lower case since put them the fieldnames became lower cased look the inputrr for the fieldname alias 
WITHOUT_CLASSIFICATION	 multiple move happens only first move will chosen 
WITHOUT_CLASSIFICATION	 making this public note that its ordering undefined 
WITHOUT_CLASSIFICATION	 add any partition key values provided part job info 
WITHOUT_CLASSIFICATION	 for remote jdbc client try set the conf var using set foobar 
WITHOUT_CLASSIFICATION	 wait for all the events written off the order service important 
WITHOUT_CLASSIFICATION	 test for duplicate publish 
WITHOUT_CLASSIFICATION	 also initialize paritioned table test against 
WITHOUT_CLASSIFICATION	 data should get sorted your etlmerge process here group the data partitionvalues rowidbucketid order the groups rowidwriteid rowidrowid 
WITHOUT_CLASSIFICATION	 throw the wasnt rolled back 
WITHOUT_CLASSIFICATION	 hour granularity 
WITHOUT_CLASSIFICATION	 map cvalue map rowvalues assertequals cvaluesize 
WITHOUT_CLASSIFICATION	 else expression 
WITHOUT_CLASSIFICATION	 tstage just simple way generate test data 
WITHOUT_CLASSIFICATION	 use list for easy cumtomize 
WITHOUT_CLASSIFICATION	 pairwise columnhasnulls columnisrepeating 
WITHOUT_CLASSIFICATION	 then try get from all 
WITHOUT_CLASSIFICATION	 conditions for being partition column 
WITHOUT_CLASSIFICATION	 this method could beyond the integer ranges until scale back need twice more variables 
WITHOUT_CLASSIFICATION	 now release single session from 
WITHOUT_CLASSIFICATION	 could generate different error messages 
WITHOUT_CLASSIFICATION	 check the record record already encoded once does reuse the encoder 
WITHOUT_CLASSIFICATION	 this method takes object accepts whatever types that are passed 
WITHOUT_CLASSIFICATION	 call this first then send interrupt the thread 
WITHOUT_CLASSIFICATION	 deleterule 
WITHOUT_CLASSIFICATION	 rows 
WITHOUT_CLASSIFICATION	 finally remove the rest the expression from the tree 
WITHOUT_CLASSIFICATION	 note necessary merge task the parent the move task and not the other way around for the proper execution the execute method 
WITHOUT_CLASSIFICATION	 verify the union has been hidden and just the main type has been returned 
WITHOUT_CLASSIFICATION	 build new environmentcontext with ifpurge 
WITHOUT_CLASSIFICATION	 not antisymmetric 
WITHOUT_CLASSIFICATION	 ensure that are consistent when comparing the base class 
WITHOUT_CLASSIFICATION	 has its reducesink parent removed 
WITHOUT_CLASSIFICATION	 deleted then good the last parameter ifexists set true 
WITHOUT_CLASSIFICATION	 test repeating case 
WITHOUT_CLASSIFICATION	 retrieve delegation token for the given user 
WITHOUT_CLASSIFICATION	 check see that the vertices are correct 
WITHOUT_CLASSIFICATION	 escape the escape and escape the asterisk 
WITHOUT_CLASSIFICATION	 add the tables well outputs 
WITHOUT_CLASSIFICATION	 case grouping sets groupby will output values for every setgroup this the index the column that information will sent 
WITHOUT_CLASSIFICATION	 set parameter false connection int smallint allowed 
WITHOUT_CLASSIFICATION	 all iface apis throw texception 
WITHOUT_CLASSIFICATION	 seemed not close files properly error situation 
WITHOUT_CLASSIFICATION	 legacy handling 
WITHOUT_CLASSIFICATION	 release all locks including persistent locks 
WITHOUT_CLASSIFICATION	 should make 
WITHOUT_CLASSIFICATION	 within range specified 
WITHOUT_CLASSIFICATION	 check highest digit for rounding 
WITHOUT_CLASSIFICATION	 start removing lru nodes 
WITHOUT_CLASSIFICATION	 suppress here real issue will get caught where clause handling 
WITHOUT_CLASSIFICATION	 the included columns the reader file schema that include acid columns present 
WITHOUT_CLASSIFICATION	 logdebugclassname logical logical batchindex batchindex key continues savekey savejoinresultname 
WITHOUT_CLASSIFICATION	 should lockid 
WITHOUT_CLASSIFICATION	 query will get cancelled before creating partitions 
WITHOUT_CLASSIFICATION	 copy order 
WITHOUT_CLASSIFICATION	 some nonzero offsets 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 add shutdown hook flush the history history file 
WITHOUT_CLASSIFICATION	 store partition key expr maptargetwork 
WITHOUT_CLASSIFICATION	 cancel hcat and jobtracker tokens 
WITHOUT_CLASSIFICATION	 have estimation lowerbound and higherbound use estimation between lowerbound and higherbound 
WITHOUT_CLASSIFICATION	 aggregate mode should followed union that need analyze 
WITHOUT_CLASSIFICATION	 need preserve currentdate 
WITHOUT_CLASSIFICATION	 newer overrides the older 
WITHOUT_CLASSIFICATION	 continue analyzing from the child astnode 
WITHOUT_CLASSIFICATION	 cannot create nullwritable instances 
WITHOUT_CLASSIFICATION	 warm couple times 
WITHOUT_CLASSIFICATION	 todo simple wrap rethrow for now clean with error 
WITHOUT_CLASSIFICATION	 explain analyze composed two steps step analyzestaterunning run the query and collect the runtime rows 
WITHOUT_CLASSIFICATION	 the the validate input method 
WITHOUT_CLASSIFICATION	 shouldnt happen getall 
WITHOUT_CLASSIFICATION	 end string 
WITHOUT_CLASSIFICATION	 read all the fields and create partitions sds and serdes 
WITHOUT_CLASSIFICATION	 merge histograms 
WITHOUT_CLASSIFICATION	 not allow users override zerocopy setting the rest can taken from user config 
WITHOUT_CLASSIFICATION	 local temp dir specific this driver 
WITHOUT_CLASSIFICATION	 print inline vertex 
WITHOUT_CLASSIFICATION	 overflow not error here just means this smaller 
WITHOUT_CLASSIFICATION	 allow undecorated char and varchar support scratch column type names 
WITHOUT_CLASSIFICATION	 with all input columns repeating 
WITHOUT_CLASSIFICATION	 make sure this has enough integer room accomodate others integer digits 
WITHOUT_CLASSIFICATION	 datewritablev mutable datestatsagg needs its own copy 
WITHOUT_CLASSIFICATION	 print information about calls that took longer time info level 
WITHOUT_CLASSIFICATION	 validate input both new and old uri should contain valid host names and valid schemes port optional both the uris since hdfs uri doesnt have port 
WITHOUT_CLASSIFICATION	 enable metric collection for hiveserver 
WITHOUT_CLASSIFICATION	 must check that not blank because otherwise you could get false positive the blank value was value you were legitimately testing see was the set 
WITHOUT_CLASSIFICATION	 dag specific counters 
WITHOUT_CLASSIFICATION	 could exponential notations 
WITHOUT_CLASSIFICATION	 first search the classpath 
WITHOUT_CLASSIFICATION	 many filesinkdescs are linked each other good idea keep track tasks for first filesinkdesc others can use 
WITHOUT_CLASSIFICATION	 positive number flip the first bit 
WITHOUT_CLASSIFICATION	 nothing default 
WITHOUT_CLASSIFICATION	 virtual 
WITHOUT_CLASSIFICATION	 pathchildrencache tried mkdir when the znode wasnt there and failed 
WITHOUT_CLASSIFICATION	 production double 
WITHOUT_CLASSIFICATION	 allow partial partition specification for nonscan since noscan fast 
WITHOUT_CLASSIFICATION	 create semijoin optimizations only for hinted columns 
WITHOUT_CLASSIFICATION	 ignore temporary tables 
WITHOUT_CLASSIFICATION	 use the row buffer size force lots rebuffering 
WITHOUT_CLASSIFICATION	 not from subquery 
WITHOUT_CLASSIFICATION	 both parts are scaling easy just check overflow 
WITHOUT_CLASSIFICATION	 exception from runtime that will show the full stack client 
WITHOUT_CLASSIFICATION	 revert false 
WITHOUT_CLASSIFICATION	 typecheckprocfactor expects typecheckctx have unparse translator 
WITHOUT_CLASSIFICATION	 finally try 
WITHOUT_CLASSIFICATION	 the number joins number input tables this not star join 
WITHOUT_CLASSIFICATION	 run minor compaction 
WITHOUT_CLASSIFICATION	 this how many bytes need store those additonal bits vint 
WITHOUT_CLASSIFICATION	 the index where the current char starts 
WITHOUT_CLASSIFICATION	 setup symbolfunction chain 
WITHOUT_CLASSIFICATION	 only dag failed killed the vertex status fetched from 
WITHOUT_CLASSIFICATION	 get the and for this branch 
WITHOUT_CLASSIFICATION	 clean trash 
WITHOUT_CLASSIFICATION	 map keep track which smb join operators and their information annotate their mapwork with 
WITHOUT_CLASSIFICATION	 needed for type parity 
WITHOUT_CLASSIFICATION	 create the parquet filterpredicate without including columns that not exist the schema such partition columns 
WITHOUT_CLASSIFICATION	 from txnid from txnid from txnid 
WITHOUT_CLASSIFICATION	 srsracquired lock are examining acquired can acquire because two shared reads can acquire together and there must 
WITHOUT_CLASSIFICATION	 insert filter operator between targetchild and inputparent 
WITHOUT_CLASSIFICATION	 here its nonacid schema file check from before table was marked transactional basexdeltaxx from load data 
WITHOUT_CLASSIFICATION	 found subdirectory depth less than number partition keys validate the partition directory name matches with the corresponding partition colname currentdepth 
WITHOUT_CLASSIFICATION	 full outer join 
WITHOUT_CLASSIFICATION	 the resulting privileges need filtered privilege type and username 
WITHOUT_CLASSIFICATION	 authorized perform action 
WITHOUT_CLASSIFICATION	 check optimizedonly hash table restrictions 
WITHOUT_CLASSIFICATION	 note the definitions what odbc and jdbc keywords exclude are different different places for now just return the odbc version here that excludes hive keywords that are also odbc reserved keywords could also exclude sql 
WITHOUT_CLASSIFICATION	 determine row schema for tsop 
WITHOUT_CLASSIFICATION	 configure the output key and value classes 
WITHOUT_CLASSIFICATION	 validationlevel 
WITHOUT_CLASSIFICATION	 test for setting the maximum partition count 
WITHOUT_CLASSIFICATION	 expr alias parses but only allowed for udtfs this check not needed and invalid when there transform the 
WITHOUT_CLASSIFICATION	 test random scan 
WITHOUT_CLASSIFICATION	 for partitioned table partitionvals are specified 
WITHOUT_CLASSIFICATION	 oid for spnego gssapi mechanism 
WITHOUT_CLASSIFICATION	 the aggregation batch vector needs know when start new batch 
WITHOUT_CLASSIFICATION	 sort 
WITHOUT_CLASSIFICATION	 integer digit fraction digits trailing zeroes are suppressed 
WITHOUT_CLASSIFICATION	 init lock manager 
WITHOUT_CLASSIFICATION	 retrieve from side 
WITHOUT_CLASSIFICATION	 dummy registry does not cache information and forwards all requests metastore 
WITHOUT_CLASSIFICATION	 are only trying convert bucketmapjoin sortbucketmapjoin 
WITHOUT_CLASSIFICATION	 use int outputtypeinfo 
WITHOUT_CLASSIFICATION	 calculate all the arguments 
WITHOUT_CLASSIFICATION	 note explicit format use throwable instead varargs 
WITHOUT_CLASSIFICATION	 handle synthetic row ids for the original files 
WITHOUT_CLASSIFICATION	 use task attempt number from conf provided 
WITHOUT_CLASSIFICATION	 grpset col needs constructed 
WITHOUT_CLASSIFICATION	 just insert the record the usual way default the simple behavior 
WITHOUT_CLASSIFICATION	 find the argument the operator which constant 
WITHOUT_CLASSIFICATION	 inverse wordshifted for accuracy shift back here 
WITHOUT_CLASSIFICATION	 required required required optional optional 
WITHOUT_CLASSIFICATION	 keeps track completed dags queryidentifiers need unique across applications 
WITHOUT_CLASSIFICATION	 well treat this the aggregate col stats for partpart tab col 
WITHOUT_CLASSIFICATION	 setters 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 that would block 
WITHOUT_CLASSIFICATION	 validate connection 
WITHOUT_CLASSIFICATION	 pick the correct parent only one the parents not reducesink that what are looking for 
WITHOUT_CLASSIFICATION	 check different encryption zones 
WITHOUT_CLASSIFICATION	 get table logical schema row type note table logical schema non partition cols partition cols virtual cols 
WITHOUT_CLASSIFICATION	 weve already obtained lock the table dont lock the partition too 
WITHOUT_CLASSIFICATION	 create list one 
WITHOUT_CLASSIFICATION	 unregister task from the known and running structures 
WITHOUT_CLASSIFICATION	 clean out previous contents 
WITHOUT_CLASSIFICATION	 note that update uses dynamic partitioning thus lock the table not partition 
WITHOUT_CLASSIFICATION	 default timestamp format still works 
WITHOUT_CLASSIFICATION	 read data from the znode for this server node this data could either config string new releases server end 
WITHOUT_CLASSIFICATION	 get mapping tables columns used 
WITHOUT_CLASSIFICATION	 need store this record not done yet case should produce result 
WITHOUT_CLASSIFICATION	 todo dont want some random jars unknown provenance sitting around care ideally should try reuse jars and verify using some checksum 
WITHOUT_CLASSIFICATION	 handle password 
WITHOUT_CLASSIFICATION	 will not try partial rewriting for nonrebuild incremental rewriting disabled 
WITHOUT_CLASSIFICATION	 the client should have been cached already for the common case otherwise this may actually introduce delay compilation for the first query 
WITHOUT_CLASSIFICATION	 job vars 
WITHOUT_CLASSIFICATION	 lets take look the operator memory requirements 
WITHOUT_CLASSIFICATION	 since the operator tree dag nodes with mutliple parents will visited more than once this can made configurable 
WITHOUT_CLASSIFICATION	 always inc the batch buffer index 
WITHOUT_CLASSIFICATION	 coltype 
WITHOUT_CLASSIFICATION	 failed something that was rendered irrelevant while were failing 
WITHOUT_CLASSIFICATION	 confirm the file really fixed and replace the old file 
WITHOUT_CLASSIFICATION	 stringexpr uses boyer moore horspool algorithm find faster threadsafe because holds final member instances only see 
WITHOUT_CLASSIFICATION	 add any input columns referenced windowfn args expressions 
WITHOUT_CLASSIFICATION	 the values from timestampgettime 
WITHOUT_CLASSIFICATION	 txn write txn write txn write txn write 
WITHOUT_CLASSIFICATION	 form result from lower and middle words 
WITHOUT_CLASSIFICATION	 input not rewritten produces correlated variables terminate rewrite 
WITHOUT_CLASSIFICATION	 between and 
WITHOUT_CLASSIFICATION	 filter timestamp against long seconds double seconds with fractional nanoseconds 
WITHOUT_CLASSIFICATION	 test second argument with nulls and repeating 
WITHOUT_CLASSIFICATION	 add nothing more 
WITHOUT_CLASSIFICATION	 out allocated columns 
WITHOUT_CLASSIFICATION	 checking var exists and its value right 
WITHOUT_CLASSIFICATION	 hive 
WITHOUT_CLASSIFICATION	 table name will lower case unless specified hbasetablename property 
WITHOUT_CLASSIFICATION	 open the next file 
WITHOUT_CLASSIFICATION	 maximum tolerable variance number partitions between cached node and our request 
WITHOUT_CLASSIFICATION	 drops partitions batches partnotinfs split into batches based batchsize and dropped the dropping will through retryutilities which will retry when there failure after reducing the batchsize decayingfactor retrying will cease when maxretries 
WITHOUT_CLASSIFICATION	 corresponding the columns its size should the same columns for example table has two columns key and value may mask value reversevalue then 
WITHOUT_CLASSIFICATION	 objectregistry available via the this setup part the tez processor construction that available whenever instance the objectcache created the assumption that tez will initialize the processor before anything else 
WITHOUT_CLASSIFICATION	 create selectop with granularity column 
WITHOUT_CLASSIFICATION	 directly invoke execdriver 
WITHOUT_CLASSIFICATION	 adjust negative result again doing what does 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 need check there are overflow digits the high word 
WITHOUT_CLASSIFICATION	 list registered applications 
WITHOUT_CLASSIFICATION	 write vint using our temporary byte buffer instead paying the thread local performance cost 
WITHOUT_CLASSIFICATION	 for varchar char type return the max length the type 
WITHOUT_CLASSIFICATION	 for the grouping set corresponding the rollup 
WITHOUT_CLASSIFICATION	 bgenjjtree senumdef 
WITHOUT_CLASSIFICATION	 set the related attributes according the keys and values 
WITHOUT_CLASSIFICATION	 handles cases where the query has predicate constantcolumnname 
WITHOUT_CLASSIFICATION	 note are overwriting the constant vector value 
WITHOUT_CLASSIFICATION	 not impersonation cli mode 
WITHOUT_CLASSIFICATION	 compression 
WITHOUT_CLASSIFICATION	 record column names that needed stats for but couldnt 
WITHOUT_CLASSIFICATION	 complete split futures 
WITHOUT_CLASSIFICATION	 check whether one the operators part work that input for the work the other operator work merge work work work work work 
WITHOUT_CLASSIFICATION	 currently the unions are not merged each union has only parents nway union will lead union operators 
WITHOUT_CLASSIFICATION	 original bucket files delta directories and previous base directory should have been cleaned 
WITHOUT_CLASSIFICATION	 return the fullyqualified path path resolving the path through any symlinks mount point 
WITHOUT_CLASSIFICATION	 create database and table 
WITHOUT_CLASSIFICATION	 simulate different filesystems returning different uri 
WITHOUT_CLASSIFICATION	 arraylist 
WITHOUT_CLASSIFICATION	 note ddl way alter partition use the msc api directly 
WITHOUT_CLASSIFICATION	 todo jdk 
WITHOUT_CLASSIFICATION	 the following code used collect column stats when 
WITHOUT_CLASSIFICATION	 sourcet this not strictly speaking invlaid but does ensure that all columns from target table are all null for every row this would make any when matched clause invalid since dont have rowid the when not matched could meaningful but its just data from source satisfying sourcet not worth the effort support this 
WITHOUT_CLASSIFICATION	 nodes stale after this 
WITHOUT_CLASSIFICATION	 here only register the whole table for postexec hook present the case will register writeentity movetask when the list dynamically created partitions are known 
WITHOUT_CLASSIFICATION	 move the result getcolumns forward match the columns the query 
WITHOUT_CLASSIFICATION	 this verify that does not revert default scheme information 
WITHOUT_CLASSIFICATION	 the background operation thread was cancelled 
WITHOUT_CLASSIFICATION	 note distinct expr can part key 
WITHOUT_CLASSIFICATION	 note although hiveproxy has method that allows check were being called from the metastore from the client dont have initialized hiveproxy till explicitly initialize being from the client side have chickenandegg problem now track whether not were running from clientside the sbap itself 
WITHOUT_CLASSIFICATION	 split identify partition parts 
WITHOUT_CLASSIFICATION	 first throw away digits below round digit 
WITHOUT_CLASSIFICATION	 add the path the list input paths 
WITHOUT_CLASSIFICATION	 stop the appenders for the operation log 
WITHOUT_CLASSIFICATION	 redact sensitive information before logging 
WITHOUT_CLASSIFICATION	 and 
WITHOUT_CLASSIFICATION	 sharing this state assumes splits will succeed fail get together same also start with null and only set true the first call would only the globaldisable thing the first failure wthe api error not any random failure 
WITHOUT_CLASSIFICATION	 load properties from hive configurations including both spark properties 
WITHOUT_CLASSIFICATION	 mark the original abandoned dont need anymore 
WITHOUT_CLASSIFICATION	 write record byte buffer 
WITHOUT_CLASSIFICATION	 denom product all ndvs except the least all 
WITHOUT_CLASSIFICATION	 order expedite things general case are not actually going reopen anything instead will try give out existing session from the pool and restart the problematic one background 
WITHOUT_CLASSIFICATION	 iterate through each day the year make sure datedatewritablev match 
WITHOUT_CLASSIFICATION	 serialize the output info into the configuration 
WITHOUT_CLASSIFICATION	 map table alias rowcontainer 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 not async wasnt submitted for some reason failure etc 
WITHOUT_CLASSIFICATION	 int can happen cases where grouping used without grouping sets all other cases should long 
WITHOUT_CLASSIFICATION	 files created windows machines have different line endings than files created unixlinux windows uses carriage return and line feed line ending whereas unix uses just line feed 
WITHOUT_CLASSIFICATION	 loginfomodifying config values for acid write true these props are now enabled elsewhere see commit diffs would better instead throw they are not set for exmaple user has set for some reason well run query contrary what they wanted but throwing now would backwards incompatible 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 new transactions should allowed open 
WITHOUT_CLASSIFICATION	 txns overlap could replace wstxnid with txnid though any decent should infer this make sure rhs join only has rows just inserted part this committxn and lhs only has committed txns and conflict but not and dont currently track writeset all 
WITHOUT_CLASSIFICATION	 event alter stats update event 
WITHOUT_CLASSIFICATION	 whether cache current rdd 
WITHOUT_CLASSIFICATION	 null first 
WITHOUT_CLASSIFICATION	 the table was dropped before got around cleaning 
WITHOUT_CLASSIFICATION	 indicate the read buffer has data for example when reading data disk could pull 
WITHOUT_CLASSIFICATION	 tests multimap structure for parquet 
WITHOUT_CLASSIFICATION	 some other key collision keep probing 
WITHOUT_CLASSIFICATION	 safe cancel delegation tokens now 
WITHOUT_CLASSIFICATION	 update column expression map 
WITHOUT_CLASSIFICATION	 make almost sure get definite order touch blocks order large number times 
WITHOUT_CLASSIFICATION	 class builder 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 any bigtablecandidates from multisourced bigtablecandidates should 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 scan the output directory for existing files and add watches 
WITHOUT_CLASSIFICATION	 note its not clear that need track this unlike poolmanager dont have nonpool sessions the pool itself could internally track the ses sions gave out since 
WITHOUT_CLASSIFICATION	 mix functions 
WITHOUT_CLASSIFICATION	 create tables and load data 
WITHOUT_CLASSIFICATION	 are calling this here because expect the method completely async also dont want this call itself thread because want the percenttophysics conversion logic consistent between all the separate calls one master thread processing round note allocation manager does not have cluster state wont update anything when the 
WITHOUT_CLASSIFICATION	 bgenjjtree fieldlist 
WITHOUT_CLASSIFICATION	 default value set milliseconds for test purpose 
WITHOUT_CLASSIFICATION	 the autocommit mode always enabled for this connection per jdbc spec 
WITHOUT_CLASSIFICATION	 constant propagation optimizer 
WITHOUT_CLASSIFICATION	 augment source with col which has will cause update target otherwise 
WITHOUT_CLASSIFICATION	 queryparallelism 
WITHOUT_CLASSIFICATION	 only one digit add leading 
WITHOUT_CLASSIFICATION	 todo can support date int any types which would have fixed length value 
WITHOUT_CLASSIFICATION	 obtain relevant object inspector for this typeinfo 
WITHOUT_CLASSIFICATION	 the final ptf ptfchain can stream its output then set the its outputshape the returned the 
WITHOUT_CLASSIFICATION	 remember rhs table for semijoin 
WITHOUT_CLASSIFICATION	 note that illegal and common default when the value missing 
WITHOUT_CLASSIFICATION	 time zone found file metadata property name writertimezone convert the timestamp that writer time zone order emulate time zone agnostic behavior not then the file was written older version hive convert the timestamp the servers reader time zone for backwards compatibility reasons unless the session level configuration set true which case assume was written time zone agnostic writer dont convert 
WITHOUT_CLASSIFICATION	 memory manager uses cache policy trigger evictions create the policy first 
WITHOUT_CLASSIFICATION	 find whether exists local driver accept the url 
WITHOUT_CLASSIFICATION	 shrinked size for this split counter part this normal mode whats different that this evaluated unit row using recordreadergetpos and that evaluated unit split using inputsplitgetlength 
WITHOUT_CLASSIFICATION	 methods need not called many times 
WITHOUT_CLASSIFICATION	 string including invalid style literal characters 
WITHOUT_CLASSIFICATION	 run spark dynamic partition pruning 
WITHOUT_CLASSIFICATION	 and mark group keys repeating 
WITHOUT_CLASSIFICATION	 return true case one the children column expr 
WITHOUT_CLASSIFICATION	 invalid partition exception 
WITHOUT_CLASSIFICATION	 note closing stmt object are also reverting any session specific config changes 
WITHOUT_CLASSIFICATION	 query that should return nothing 
WITHOUT_CLASSIFICATION	 spark installation provided use the sparksubmit script otherwise call the sparksubmit class directly which has some caveats like having provide proper 
WITHOUT_CLASSIFICATION	 first try our cast method that will handle few special cases 
WITHOUT_CLASSIFICATION	 getbytes function says pos the ordinal position the first byte the blob value extracted the first byte position 
WITHOUT_CLASSIFICATION	 save original projection 
WITHOUT_CLASSIFICATION	 statstask require table already exist 
WITHOUT_CLASSIFICATION	 todo error handling distinguish ioconnection failures attribute appropriate spill output 
WITHOUT_CLASSIFICATION	 convert just the decimal digits dot sign etc into bytes this much faster than converting the biginteger value from unscaledvalue which 
WITHOUT_CLASSIFICATION	 the table names match check the partitions 
WITHOUT_CLASSIFICATION	 verify 
WITHOUT_CLASSIFICATION	 postpone the join processing for this pair also spilling this big table row 
WITHOUT_CLASSIFICATION	 and one without 
WITHOUT_CLASSIFICATION	 look for normal match 
WITHOUT_CLASSIFICATION	 check were operating the same database not move 
WITHOUT_CLASSIFICATION	 get the tables the default database 
WITHOUT_CLASSIFICATION	 compareto 
WITHOUT_CLASSIFICATION	 since can rely approx lastaccesstime but dont want performance hit 
WITHOUT_CLASSIFICATION	 only bits but long behave unsigned 
WITHOUT_CLASSIFICATION	 cache size 
WITHOUT_CLASSIFICATION	 done only for nonviews 
WITHOUT_CLASSIFICATION	 create planner with hook update the mapping tables when node copied when registered 
WITHOUT_CLASSIFICATION	 remove the location container tokens 
WITHOUT_CLASSIFICATION	 this point transport must contain client ugi doesnt then its old client 
WITHOUT_CLASSIFICATION	 get the error details from the underlying exception 
WITHOUT_CLASSIFICATION	 compare sorted strings rather than comparing exact strings 
WITHOUT_CLASSIFICATION	 filtering old authorizer 
WITHOUT_CLASSIFICATION	 change the selected vector 
WITHOUT_CLASSIFICATION	 this string constant used alterhandler figure out that should not attempt update stats set any clientside task which wishes signal that stats 
WITHOUT_CLASSIFICATION	 sum size all files the partition smaller than size required 
WITHOUT_CLASSIFICATION	 copy required allow incremental replication work correctly 
WITHOUT_CLASSIFICATION	 bgenjjtree typedouble 
WITHOUT_CLASSIFICATION	 getcolumn instead using directly 
WITHOUT_CLASSIFICATION	 parent can understand this expression 
WITHOUT_CLASSIFICATION	 child keys are null empty bail out 
WITHOUT_CLASSIFICATION	 noop when authtype nosasl 
WITHOUT_CLASSIFICATION	 vectorized implementation broundcol function 
WITHOUT_CLASSIFICATION	 stats are same need update 
WITHOUT_CLASSIFICATION	 now convert acid 
WITHOUT_CLASSIFICATION	 ignore and hope for the best 
WITHOUT_CLASSIFICATION	 prepare stringbuilders for lists use onetomany queries 
WITHOUT_CLASSIFICATION	 get the base values cache 
WITHOUT_CLASSIFICATION	 databases created 
WITHOUT_CLASSIFICATION	 have just obtained all needed splitting some block now need put the space remaining from that block into lower free lists well put most one block into each list since blocks can always combined make largerlevel block each bit the remaining targetsized blocks count one block list offset from targetsized list bit index the merges here too since the block just allocated could immediately moved out then the resulting free space abandoned 
WITHOUT_CLASSIFICATION	 the stack has been explored already till that level obtained cached string 
WITHOUT_CLASSIFICATION	 get current bucket file name 
WITHOUT_CLASSIFICATION	 combine not operator with the child operator otherwise the following optimization from bottom could lead incorrect result such notx and not null should not optimized notx but null 
WITHOUT_CLASSIFICATION	 reuse existing text member char writable 
WITHOUT_CLASSIFICATION	 considered using urlencoder but seemed too much 
WITHOUT_CLASSIFICATION	 max this table either the the big table cannot convert 
WITHOUT_CLASSIFICATION	 set port 
WITHOUT_CLASSIFICATION	 setvalue should able handle null input 
WITHOUT_CLASSIFICATION	 newparts 
WITHOUT_CLASSIFICATION	 hltxnid means its associated with transaction 
WITHOUT_CLASSIFICATION	 createdrop functions are marked admin functions usage available functions query are not restricted sql standard authorization 
WITHOUT_CLASSIFICATION	 hash bits dont match 
WITHOUT_CLASSIFICATION	 need make sure that the this hiveservers sessions sessionstate stored the thread local for the handler thread 
WITHOUT_CLASSIFICATION	 try appending non extendable shard spec 
WITHOUT_CLASSIFICATION	 the import specification asked for only particular partition loaded load only that and ignore all the others 
WITHOUT_CLASSIFICATION	 stripe will satisfy the predicate 
WITHOUT_CLASSIFICATION	 for use ddl statements that require exclusive lock 
WITHOUT_CLASSIFICATION	 ideally would like this check based the number splits the absence easy way get the number splits this based the total number files pessimistically assumming that 
WITHOUT_CLASSIFICATION	 tez below transactional get the following ekoifman tree ext hiveunionsubdir orcacidversion delta bucket hiveunionsubdir orcacidversion delta bucket hiveunionsubdir orcacidversion delta bucket directories files 
WITHOUT_CLASSIFICATION	 check results need emitted results only need emitted there nonnull entry table that preserved there are nonnull entries 
WITHOUT_CLASSIFICATION	 rename for nonlocal file will transfering the original file permissions from source the destination else case mvfile where copy from source destination will inherit the destinations parent group ownership 
WITHOUT_CLASSIFICATION	 parse out sentences using javas texthandling api 
WITHOUT_CLASSIFICATION	 authorize all droppedpartitions one shot 
WITHOUT_CLASSIFICATION	 class 
WITHOUT_CLASSIFICATION	 possible 
WITHOUT_CLASSIFICATION	 javaobject primitives javaarray 
WITHOUT_CLASSIFICATION	 read parameters 
WITHOUT_CLASSIFICATION	 have issues with stats just scale linearily 
WITHOUT_CLASSIFICATION	 mgr true 
WITHOUT_CLASSIFICATION	 user asked for mapside join 
WITHOUT_CLASSIFICATION	 add counter default rounding 
WITHOUT_CLASSIFICATION	 high word gets integer rounding middle and lower longwords are cleared 
WITHOUT_CLASSIFICATION	 get the destination and check table 
WITHOUT_CLASSIFICATION	 dont check version its dry run 
WITHOUT_CLASSIFICATION	 finally add sort clause this needed for the update toksortby toknullsfirst toktableorcol cmvbasetable 
WITHOUT_CLASSIFICATION	 allowing fallback default timestamp parsing custom patterns fail 
WITHOUT_CLASSIFICATION	 metastore returns object type such global global when object specified such privileges are not applicable this authorization mode ignore them 
WITHOUT_CLASSIFICATION	 use simpleentry save the offset and rowcount limit clause key simpleentry offset 
WITHOUT_CLASSIFICATION	 mergefilework subclass mapwork dont need distinguish here 
WITHOUT_CLASSIFICATION	 walk through window expressions construct rexnodes for those 
WITHOUT_CLASSIFICATION	 nothing unregister 
WITHOUT_CLASSIFICATION	 bootstrap skip current table update 
WITHOUT_CLASSIFICATION	 first two stripes will satisfy condition and hence single split 
WITHOUT_CLASSIFICATION	 nonjavadoc see javalangstring javalangstring javalangstring javautillist 
WITHOUT_CLASSIFICATION	 use version existing max segment generate new shard spec 
WITHOUT_CLASSIFICATION	 assume dag dag and that its connected add direct dependencies 
WITHOUT_CLASSIFICATION	 create the aggregate 
WITHOUT_CLASSIFICATION	 move pending the allocator can release 
WITHOUT_CLASSIFICATION	 joda datetime only has precision millis cut off any fractional portion 
WITHOUT_CLASSIFICATION	 add files compare the arguments list 
WITHOUT_CLASSIFICATION	 calculate window size 
WITHOUT_CLASSIFICATION	 initialize one columns array entries 
WITHOUT_CLASSIFICATION	 estimate the size each entry datatype with unknown size stringstruct etc assumed bytes for now 
WITHOUT_CLASSIFICATION	 see can arrive smaller number using distinct stats from key columns 
WITHOUT_CLASSIFICATION	 logger console 
WITHOUT_CLASSIFICATION	 add column info for non partion cols object inspector fields 
WITHOUT_CLASSIFICATION	 get the tag value 
WITHOUT_CLASSIFICATION	 when minor compaction runs collapse per statement delta files inside single transaction longer need statementid the file name 
WITHOUT_CLASSIFICATION	 note fetchone doesnt throw new not supported 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 spot check decimal scalarcolumn modulo 
WITHOUT_CLASSIFICATION	 test whether precision will fit within decimal bit signed long with decimal digits 
WITHOUT_CLASSIFICATION	 instantiate empty list that dont error out iterator fetching were here then the next check pos will show our caller that that weve exhausted our event supply 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 libjars should only set sqoop autoshipped 
WITHOUT_CLASSIFICATION	 verify data and layout 
WITHOUT_CLASSIFICATION	 get the sign the big decimal 
WITHOUT_CLASSIFICATION	 rather unexpected usage exception 
WITHOUT_CLASSIFICATION	 matter whether has acquired not cannot pass exclusive 
WITHOUT_CLASSIFICATION	 push down limit through outer join note run this after ppd support old style join syntax select from left outer join where rxrx and 
WITHOUT_CLASSIFICATION	 save work initialized later with smb information 
WITHOUT_CLASSIFICATION	 skewedcolvalues 
WITHOUT_CLASSIFICATION	 check should use delegation tokens authenticate the call below gets hold the tokens they are set hadoop this should happen the mapreduce tasks the client added the tokens into hadoops credential store the front end during job submission 
WITHOUT_CLASSIFICATION	 this default implementation locking only works for incremental maintenance which only works for transactional manager thus cannot acquire lock 
WITHOUT_CLASSIFICATION	 this ast has only one child then partition spec specified 
WITHOUT_CLASSIFICATION	 add into conditional task 
WITHOUT_CLASSIFICATION	 hashmap javaobject 
WITHOUT_CLASSIFICATION	 tolerance for long range bias and for short range bias 
WITHOUT_CLASSIFICATION	 hive 
WITHOUT_CLASSIFICATION	 dump information call took more than sec 
WITHOUT_CLASSIFICATION	 number rows means that statistics from metastore not reliable and means statistics gathering disabled estimate only num rows since could actual number rows 
WITHOUT_CLASSIFICATION	 ptf variables 
WITHOUT_CLASSIFICATION	 start iterating through the foreign keys this list might contain more than single foreign key and each foreign key might contain multiple columns the outer loop retrieves the information that common for single key table information while the inner loop checks adds information about each column 
WITHOUT_CLASSIFICATION	 just from the back and throw away everything think wrong skip last item the file 
WITHOUT_CLASSIFICATION	 compare the results 
WITHOUT_CLASSIFICATION	 create the recordreader 
WITHOUT_CLASSIFICATION	 remaining size needed after putting files the return path list 
WITHOUT_CLASSIFICATION	 the currently read token beginning array object move stream forward skipping any child tokens till were the corresponding endarray endobject token 
WITHOUT_CLASSIFICATION	 fetch the row inserted after schema altered and verify 
WITHOUT_CLASSIFICATION	 set the current ugi fake user 
WITHOUT_CLASSIFICATION	 first start the queries from the queue 
WITHOUT_CLASSIFICATION	 ignore 
WITHOUT_CLASSIFICATION	 now try invalid alter table 
WITHOUT_CLASSIFICATION	 copy value 
WITHOUT_CLASSIFICATION	 properties passed the client used execution hooks 
WITHOUT_CLASSIFICATION	 the datastructure doing the actual storage during mapjoins has some per row overhead 
WITHOUT_CLASSIFICATION	 split around the tab character 
WITHOUT_CLASSIFICATION	 for possible series equal keys 
WITHOUT_CLASSIFICATION	 mapreduce jobs will run locally based data size 
WITHOUT_CLASSIFICATION	 check isrepeating handling 
WITHOUT_CLASSIFICATION	 partitionlist not evaluated until the run phase 
WITHOUT_CLASSIFICATION	 the original exception lost not changing the interface maintain backward compatibility 
WITHOUT_CLASSIFICATION	 for simplicity always have parents while storing pools flat structure well first distribute them levels then add level level 
WITHOUT_CLASSIFICATION	 acquire txn batch 
WITHOUT_CLASSIFICATION	 all fields have been parsed bytes have been parsed need set the startposition fieldslength ensure can use the same formula calculate the length each field for missing fields their starting positions will all the same which will make their lengths and uncheckedgetfield will return these fields nulls 
WITHOUT_CLASSIFICATION	 all reducesinkoperators this subtree this set used when start remove unnecessary 
WITHOUT_CLASSIFICATION	 the operator stack the dispatcher generates the plan from the operator tree 
WITHOUT_CLASSIFICATION	 the rewritten where clause 
WITHOUT_CLASSIFICATION	 construct the astnode for the column that will join with the outerquery expression for select from where select from this will build outerqueryexpr ast returned call buildsqjoinexpr 
WITHOUT_CLASSIFICATION	 contains aliases from subquery 
WITHOUT_CLASSIFICATION	 nonjavadoc this provides lazydouble like class which can initialized from data stored binary format see int int 
WITHOUT_CLASSIFICATION	 apply join order optimizations reordering mst algorithm join optimizations failed because missing stats continue with 
WITHOUT_CLASSIFICATION	 sds probably view 
WITHOUT_CLASSIFICATION	 relycstr 
WITHOUT_CLASSIFICATION	 the same day the month 
WITHOUT_CLASSIFICATION	 lock are trying acquire exclusive 
WITHOUT_CLASSIFICATION	 assuming this means are not doing auth 
WITHOUT_CLASSIFICATION	 unsupported inmemory structure 
WITHOUT_CLASSIFICATION	 bgenjjtree typei 
WITHOUT_CLASSIFICATION	 stringval 
WITHOUT_CLASSIFICATION	 ddlsemanticanalyzer has already checked partial partition specs are allowed thus should not need check here 
WITHOUT_CLASSIFICATION	 create the dest directory not exist 
WITHOUT_CLASSIFICATION	 since addition commutative can add any order 
WITHOUT_CLASSIFICATION	 its marked too many aborted already know need compact 
WITHOUT_CLASSIFICATION	 add the parameter here cannot change runtime 
WITHOUT_CLASSIFICATION	 now that reordered qbjointrees update leftaliases all 
WITHOUT_CLASSIFICATION	 the target table exists and newer same current update based repllastid then just noop 
WITHOUT_CLASSIFICATION	 multiple distincts not supported with skew data 
WITHOUT_CLASSIFICATION	 part also virtual column but part col should not this list 
WITHOUT_CLASSIFICATION	 create parent does not exist recreation not error 
WITHOUT_CLASSIFICATION	 now need descratchify this location get rid any scratchdd from the location 
WITHOUT_CLASSIFICATION	 read the entire data back and see did everything right 
WITHOUT_CLASSIFICATION	 get table 
WITHOUT_CLASSIFICATION	 iterate through all the elements pig schema and validations dictated semantics consult hcatschema table when need helps with debug messages 
WITHOUT_CLASSIFICATION	 add this partition postexecution hook 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader long 
WITHOUT_CLASSIFICATION	 skip method finding the method name didnt change and class name didnt change 
WITHOUT_CLASSIFICATION	 create expression tree with type cast from string timestamp 
WITHOUT_CLASSIFICATION	 just copy the information since there nothing far 
WITHOUT_CLASSIFICATION	 iterate over all the fields picking the nested structs within them 
WITHOUT_CLASSIFICATION	 additional combinations for each the second and third arguments are omitted have coverage all the source templates already 
WITHOUT_CLASSIFICATION	 date and time parts 
WITHOUT_CLASSIFICATION	 store the position the argument for the function the input 
WITHOUT_CLASSIFICATION	 when uri instance initialized creates bunch private string fields never bothering about their possible duplication would best could tell uri constructor intern these strings right away without this option can only use reflection fix strings these fields after uri has been created 
WITHOUT_CLASSIFICATION	 the grouping set key present after the grouping keys before the distinct keys 
WITHOUT_CLASSIFICATION	 see javadoc need clean the cache data anymore 
WITHOUT_CLASSIFICATION	 have insert into fooab parser will enforce that columns are listed toktabcolname present 
WITHOUT_CLASSIFICATION	 move with our counts 
WITHOUT_CLASSIFICATION	 need initialize the lock manager 
WITHOUT_CLASSIFICATION	 verify the fetched logs from the beginning the log file 
WITHOUT_CLASSIFICATION	 final string query 
WITHOUT_CLASSIFICATION	 enabledisable bitpacking 
WITHOUT_CLASSIFICATION	 all must selected otherwise size would zero repeating property will not change 
WITHOUT_CLASSIFICATION	 verify the scratch directories has been cleaned 
WITHOUT_CLASSIFICATION	 bitsetwordsinuse transient force dumping into lower form 
WITHOUT_CLASSIFICATION	 subset counters that should interest for when one wants limit their publishing nondisplay names should used 
WITHOUT_CLASSIFICATION	 unscaled 
WITHOUT_CLASSIFICATION	 clear state from previous txn 
WITHOUT_CLASSIFICATION	 alternate 
WITHOUT_CLASSIFICATION	 dir 
WITHOUT_CLASSIFICATION	 turn metastoreside authorization 
WITHOUT_CLASSIFICATION	 marked where the projected expr coming from that the types will become nullable for the original projections which are now coming out 
WITHOUT_CLASSIFICATION	 set the system properties needed pig 
WITHOUT_CLASSIFICATION	 interrupt the current thread after second 
WITHOUT_CLASSIFICATION	 null gets stored into column which binary field 
WITHOUT_CLASSIFICATION	 since sortmerge join only follow the big table 
WITHOUT_CLASSIFICATION	 fully contained topoffset toplimit bottomlimit 
WITHOUT_CLASSIFICATION	 backtrack bucket columns crs prs any 
WITHOUT_CLASSIFICATION	 note never instantiate task without taskfactoryget youre not okay with equals breaking doing via taskfactoryget makes sure that generated and two tasks the same type dont show equal which important for things like iterating over array without this dta dtb and dtc would show one item the list children thus were instantiating via helper method that instantiates via taskfactoryget 
WITHOUT_CLASSIFICATION	 authorization header must have payload 
WITHOUT_CLASSIFICATION	 retain only valid intersections discard disjoint ranges 
WITHOUT_CLASSIFICATION	 param had scheme not url 
WITHOUT_CLASSIFICATION	 whether the native vectorized map join operator has performed its 
WITHOUT_CLASSIFICATION	 whether current batch has any forwarded keys mapping index lined wkeys index the batch mapping index the batch linear hash result size the current batch 
WITHOUT_CLASSIFICATION	 interface they aggregate the size the aggregation buffer 
WITHOUT_CLASSIFICATION	 the unassigned batchindex for the rows that have not received nonnull value yet temporary work array 
WITHOUT_CLASSIFICATION	 cross with outer join currently not merge 
WITHOUT_CLASSIFICATION	 build calcite rel 
WITHOUT_CLASSIFICATION	 nonacid transactional conversion property itself must mutexed prevent concurrent writes see hive for use cases 
WITHOUT_CLASSIFICATION	 writing this there case where this could false this just protection from possible future changes 
WITHOUT_CLASSIFICATION	 this flow usually taken for repl load our input the result files listing should expand out files 
WITHOUT_CLASSIFICATION	 for subqueries the and alias should appended since same aliases can reused within different subqueries for query like select select from where subq join select from where subq 
WITHOUT_CLASSIFICATION	 for kerberos setup 
WITHOUT_CLASSIFICATION	 interface for vector map join hash table which could hash map hash multiset hash set for single byte array key 
WITHOUT_CLASSIFICATION	 after 
WITHOUT_CLASSIFICATION	 create this doas that gets security context based passed ugi 
WITHOUT_CLASSIFICATION	 nonjavadoc see javautilcalendar 
WITHOUT_CLASSIFICATION	 why this even metastore 
WITHOUT_CLASSIFICATION	 remove dpp predicates 
WITHOUT_CLASSIFICATION	 such abc 
WITHOUT_CLASSIFICATION	 found expression that can try reduce 
WITHOUT_CLASSIFICATION	 set insert and update dont set off but delete does 
WITHOUT_CLASSIFICATION	 the existing table newer than our update dont allow the update 
WITHOUT_CLASSIFICATION	 precondition check verify whether the table created and data fetched correctly 
WITHOUT_CLASSIFICATION	 metaconf cleanup should trigger event listener 
WITHOUT_CLASSIFICATION	 bitwise the bitvector with the bitvector the agg buffer 
WITHOUT_CLASSIFICATION	 when new buffer fetched resultsetnext should called more times 
WITHOUT_CLASSIFICATION	 key doesnt contain its encoded value from previous iterator 
WITHOUT_CLASSIFICATION	 one 
WITHOUT_CLASSIFICATION	 this method gets the basic stats from metastore for tablepartitions this will make use the statistics from optimizer when available execution engine tez spark optimization applied only during physical compilation because dpp changing the stats such case will get the basic stats from metastore when statistics absent metastore will use the fallback 
WITHOUT_CLASSIFICATION	 preallocated member for storing index into the hashmapresults for each spilled row 
WITHOUT_CLASSIFICATION	 remove this alias from the alias list 
WITHOUT_CLASSIFICATION	 print out nth partition key for debugging 
WITHOUT_CLASSIFICATION	 the schema the same then bail out 
WITHOUT_CLASSIFICATION	 test that the server code exists and responds basic requests 
WITHOUT_CLASSIFICATION	 this statement will attempt move kvtxt out stickybitdir user foo expected return 
WITHOUT_CLASSIFICATION	 dpp now look ndvs both sides see the selectivity parent opsselgbrsgbrs 
WITHOUT_CLASSIFICATION	 additional bits beyond bits the secondssinceepoch part timestamp 
WITHOUT_CLASSIFICATION	 equals 
WITHOUT_CLASSIFICATION	 hive doesnt have autoincrement concept 
WITHOUT_CLASSIFICATION	 get list joins which cannot converted sort merge join only selects and filters operators are allowed between the table scan and join currently more operators can added the method 
WITHOUT_CLASSIFICATION	 have filled digits and have more room our limit precision fast decimal however since are processing fractional digits rounding away 
WITHOUT_CLASSIFICATION	 the summary query returns only one row 
WITHOUT_CLASSIFICATION	 couldnt think good way reuse the keys and value objects without even more allocations take the easy and safe approach 
WITHOUT_CLASSIFICATION	 compute distribution 
WITHOUT_CLASSIFICATION	 arena cannot change after have marked released 
WITHOUT_CLASSIFICATION	 set partition cols tsdesc 
WITHOUT_CLASSIFICATION	 the token file location should first argument pig 
WITHOUT_CLASSIFICATION	 extraction can subset columns this the projection the batch column numbers 
WITHOUT_CLASSIFICATION	 ignore shutting down anyway 
WITHOUT_CLASSIFICATION	 simulate the join driving the test big table data our test small table hashmap and create the expected output multiset testrow testrow and occurrence count 
WITHOUT_CLASSIFICATION	 type name difference adornment 
WITHOUT_CLASSIFICATION	 have stream for included column but future might have data streams its more like has least one column included that has index stream 
WITHOUT_CLASSIFICATION	 dynamic partitioning with custom path resolve the custom path using partition column values 
WITHOUT_CLASSIFICATION	 the first call markfailed should have removed the record from compactionqueue repeated call should fail 
WITHOUT_CLASSIFICATION	 this doesnt get used but its still necessary see 
WITHOUT_CLASSIFICATION	 first segment granularity has here 
WITHOUT_CLASSIFICATION	 this filter that should perform comparison for sorted searches 
WITHOUT_CLASSIFICATION	 list partitions 
WITHOUT_CLASSIFICATION	 base bucket bucket deletedelta bucket bucket delta bucket bucket delta bucket 
WITHOUT_CLASSIFICATION	 these data structures are needed create the new project 
WITHOUT_CLASSIFICATION	 add the value the vector 
WITHOUT_CLASSIFICATION	 decimal validation 
WITHOUT_CLASSIFICATION	 verify create tablefunction calls only add foreign key constraints table 
WITHOUT_CLASSIFICATION	 case with nulls 
WITHOUT_CLASSIFICATION	 change sessions default queue tezq and rerun test sequence 
WITHOUT_CLASSIFICATION	 after constant folding 
WITHOUT_CLASSIFICATION	 separator for open txns separator for aborted txns 
WITHOUT_CLASSIFICATION	 for top constraining sel 
WITHOUT_CLASSIFICATION	 pick the first host always weak attempt cache affinity 
WITHOUT_CLASSIFICATION	 the key given user ignored case parquet need supply null 
WITHOUT_CLASSIFICATION	 check table params 
WITHOUT_CLASSIFICATION	 test that adding file the remote context makes available executors 
WITHOUT_CLASSIFICATION	 set determines that task complete 
WITHOUT_CLASSIFICATION	 assume this the table are now 
WITHOUT_CLASSIFICATION	 generate new task 
WITHOUT_CLASSIFICATION	 read big value length wrote with the value 
WITHOUT_CLASSIFICATION	 compute the number values want read this page 
WITHOUT_CLASSIFICATION	 pattern identify errors related the client closing the socket early idea borrowed from netty sslhandler 
WITHOUT_CLASSIFICATION	 sql standard return null for zero elements 
WITHOUT_CLASSIFICATION	 all columns need least subset the parentofparents bucket cols 
WITHOUT_CLASSIFICATION	 replace table scan operator 
WITHOUT_CLASSIFICATION	 added the multi group optimization 
WITHOUT_CLASSIFICATION	 the test table has rows total query time should 
WITHOUT_CLASSIFICATION	 assume the latter pretty high dont check for now 
WITHOUT_CLASSIFICATION	 generate the beeline args per hive conf and execute the given script 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 tsimplejsonprotocol does not support deserialization isbinariesaddfalse 
WITHOUT_CLASSIFICATION	 was not present the cache maybe because was added another 
WITHOUT_CLASSIFICATION	 prefer left cause right might missing 
WITHOUT_CLASSIFICATION	 prettier error messages frontend 
WITHOUT_CLASSIFICATION	 ddddddddd hhmmssnnnnnnnnn 
WITHOUT_CLASSIFICATION	 there sortmerge join followed regular join the smbjoinoperator may not get initialized all consider the following query smb join for the mapper processing the smj not initialized need close either 
WITHOUT_CLASSIFICATION	 simple one long key map join benchmarks build with mvn clean install dskiptests pdistitests main hive directory from itestshivejmh directory run java jar targetbenchmarksjar inner innerbigonly leftsemi outer rowmodehashmap rowmodeoptimized vectorpassthrough nativevectorfast 
WITHOUT_CLASSIFICATION	 recursively remove nonparent task from its children 
WITHOUT_CLASSIFICATION	 write out serialized plan with counters log file 
WITHOUT_CLASSIFICATION	 object inspector corresponding the input parameter 
WITHOUT_CLASSIFICATION	 already existing database 
WITHOUT_CLASSIFICATION	 now all txns are removed from minhistorylevel all entries from txntowriteid would cleaned 
WITHOUT_CLASSIFICATION	 delta writer 
WITHOUT_CLASSIFICATION	 default origin given time zone when aligning multiperiod granularities 
WITHOUT_CLASSIFICATION	 destroy before returning the pool 
WITHOUT_CLASSIFICATION	 set the inferred bucket columns for the file this filesink produces 
WITHOUT_CLASSIFICATION	 the project trivial raw join 
WITHOUT_CLASSIFICATION	 all them were true return true 
WITHOUT_CLASSIFICATION	 todo also account for tezinternal session restarts 
WITHOUT_CLASSIFICATION	 jdk 
WITHOUT_CLASSIFICATION	 test partition listing with partial spec specified but not 
WITHOUT_CLASSIFICATION	 rename fails the file with same name already exist 
WITHOUT_CLASSIFICATION	 stats from metastore only once 
WITHOUT_CLASSIFICATION	 greater than equal 
WITHOUT_CLASSIFICATION	 dictionary 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 this should fail 
WITHOUT_CLASSIFICATION	 check rightinputrel contains correlation 
WITHOUT_CLASSIFICATION	 extra capacity case overrun avoid resizing 
WITHOUT_CLASSIFICATION	 try another table 
WITHOUT_CLASSIFICATION	 set the lock object with dummy data and then set needed 
WITHOUT_CLASSIFICATION	 evaluate the predicate expression 
WITHOUT_CLASSIFICATION	 sumvcolc 
WITHOUT_CLASSIFICATION	 int not yet supported 
WITHOUT_CLASSIFICATION	 read data 
WITHOUT_CLASSIFICATION	 timeout after reset 
WITHOUT_CLASSIFICATION	 future 
WITHOUT_CLASSIFICATION	 call the process function 
WITHOUT_CLASSIFICATION	 all the other fks 
WITHOUT_CLASSIFICATION	 use them 
WITHOUT_CLASSIFICATION	 complete transaction simulate writing partitions 
WITHOUT_CLASSIFICATION	 never resize the pool assume this initialization that changes might have make the factory interface more complicated 
WITHOUT_CLASSIFICATION	 get the usergroups for checking permissions based the current ugi 
WITHOUT_CLASSIFICATION	 middle word gets integer rounding 
WITHOUT_CLASSIFICATION	 check udfs against the when none allow all udfs for 
WITHOUT_CLASSIFICATION	 true only for operators which produce atmost output row per input row this will allow the output column names directly translated input column names 
WITHOUT_CLASSIFICATION	 avoid that will remove the and inserted enforce bucketingsorting 
WITHOUT_CLASSIFICATION	 need factor this prevent overwhelming spark executormemory 
WITHOUT_CLASSIFICATION	 add all distinct params note distinct expr can not part key assume plan 
WITHOUT_CLASSIFICATION	 the canaccept part this log message does not account for this allocation 
WITHOUT_CLASSIFICATION	 another task higher priority may have come during the wait lookup the queue again pick the task the highest priority 
WITHOUT_CLASSIFICATION	 for outer joins since the small table key can null when there match must have physical scratch column for those keys cannot use the projection optimization used inner joins above 
WITHOUT_CLASSIFICATION	 bfs 
WITHOUT_CLASSIFICATION	 todo handle compound partition keys partition when multiple window clauses are present same select even predicate can not pushed past all them might still able push below some them select from select key value avgcint over partition key sumcfloat overpartition value from where value select from select key value avgcint over partition key from select key value sumcfloat overpartition value from where value 
WITHOUT_CLASSIFICATION	 note only run for columns command and assume basic stats means col stats 
WITHOUT_CLASSIFICATION	 tolerance for estimated count 
WITHOUT_CLASSIFICATION	 they all modify primordial rows 
WITHOUT_CLASSIFICATION	 there are fields the struct 
WITHOUT_CLASSIFICATION	 test path sql enabled and broken 
WITHOUT_CLASSIFICATION	 timeout 
WITHOUT_CLASSIFICATION	 once the eventid reaches then just increment sequentially this avoid longer values 
WITHOUT_CLASSIFICATION	 the output column type string initialize the buffer receive data 
WITHOUT_CLASSIFICATION	 jump table figure out whether wait acquire keep looking since java doesnt have function pointers grumble grumble store character that well use determine which function call the table maps the lock type the lock are looking acquire the lock type the lock are checking the lock state the lock 
WITHOUT_CLASSIFICATION	 ptf declarations 
WITHOUT_CLASSIFICATION	 load upgrade order for the given dbtype 
WITHOUT_CLASSIFICATION	 the optimizer will automatically convert maponly job 
WITHOUT_CLASSIFICATION	 construct path pattern find all dynamically generated paths 
WITHOUT_CLASSIFICATION	 the case proxy users the getcurrentuser will return the real user for oozie due the doas that happened just before the server started executing the method getdelegationtoken the metastore 
WITHOUT_CLASSIFICATION	 undone used tests 
WITHOUT_CLASSIFICATION	 authority use default one applies 
WITHOUT_CLASSIFICATION	 hardcoded for reproducibility 
WITHOUT_CLASSIFICATION	 ensure txn timesout 
WITHOUT_CLASSIFICATION	 unique the store func and out file name table our case 
WITHOUT_CLASSIFICATION	 path format segmentoutputpath 
WITHOUT_CLASSIFICATION	 create alias work which contains the merge operator 
WITHOUT_CLASSIFICATION	 template classname valuetype 
WITHOUT_CLASSIFICATION	 output keys and aggregates into the output batch 
WITHOUT_CLASSIFICATION	 check permisssion partition dirs and files created 
WITHOUT_CLASSIFICATION	 druid table not external table store the schema metadata store 
WITHOUT_CLASSIFICATION	 inserting hive variables 
WITHOUT_CLASSIFICATION	 set ours 
WITHOUT_CLASSIFICATION	 now read all the ranges from cache disk 
WITHOUT_CLASSIFICATION	 hiveserver metadata api types ends here 
WITHOUT_CLASSIFICATION	 sign and 
WITHOUT_CLASSIFICATION	 that can only process records 
WITHOUT_CLASSIFICATION	 read from the stream using the protocol for each column final schema 
WITHOUT_CLASSIFICATION	 get key columns from inputs those are the columns which will distribute also the columns will sort 
WITHOUT_CLASSIFICATION	 owner testowner 
WITHOUT_CLASSIFICATION	 dont enforce during test driver setup shutdown 
WITHOUT_CLASSIFICATION	 statementcancel after resultsetclose should noop 
WITHOUT_CLASSIFICATION	 for every pattern 
WITHOUT_CLASSIFICATION	 mrv job tag used identify templeton launcher child jobs each child job will tagged with the parent jobid that launcher task restart all previously running child jobs can killed before the child job launched again 
WITHOUT_CLASSIFICATION	 throw error default value isnt what hive allows 
WITHOUT_CLASSIFICATION	 start with empty priv set 
WITHOUT_CLASSIFICATION	 the ctas query does specify location use the table location else use the location 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 verify that the schematool ran preupgrade scripts and ignored errors 
WITHOUT_CLASSIFICATION	 use any nonnull values found remember the remaining unassigned 
WITHOUT_CLASSIFICATION	 restrict with any ranges found from where predicates 
WITHOUT_CLASSIFICATION	 first insert round 
WITHOUT_CLASSIFICATION	 over the tables and populate the related structures have materialize the table alias list since might 
WITHOUT_CLASSIFICATION	 arrays never change will just shallow assignment here instead copy 
WITHOUT_CLASSIFICATION	 fastisbyte returns false 
WITHOUT_CLASSIFICATION	 come ride the api rollercoaster the best part that ctx has teztaskattemptid inside 
WITHOUT_CLASSIFICATION	 having seen the root operator before means there was branch the operator graph theres typically two reasons for that muxdemux multi insert muxdemux will hit the same leaf again multi insert will result into vertex with multiple operators 
WITHOUT_CLASSIFICATION	 incremental repl with alters dbtablepartition 
WITHOUT_CLASSIFICATION	 generate the meta data for key index for key 
WITHOUT_CLASSIFICATION	 nothing for the zkcreate models 
WITHOUT_CLASSIFICATION	 timestamp columncolumn 
WITHOUT_CLASSIFICATION	 generate output column names 
WITHOUT_CLASSIFICATION	 cleanup 
WITHOUT_CLASSIFICATION	 assignment the last thing the try happen assume success 
WITHOUT_CLASSIFICATION	 order determine the sum field type precisionscale for modepartial and modefinal 
WITHOUT_CLASSIFICATION	 the base table for the group matches the skewed keys 
WITHOUT_CLASSIFICATION	 case substring from index with length 
WITHOUT_CLASSIFICATION	 swallow the exception and let the call determine what 
WITHOUT_CLASSIFICATION	 abstract class for hash multiset result 
WITHOUT_CLASSIFICATION	 hcatdriverrundrop table tblname hcatdriverruncreate table junitsemanalysis int partitioned string stored rcfile hcatdriverrunalter table tblname add partition liststring partvals new arrayliststring partvalsadd mapstringstring map tblname 
WITHOUT_CLASSIFICATION	 create standard settable struct object inspector 
WITHOUT_CLASSIFICATION	 acquire all locks for given query atomically blocks all into remain waiting state wait will undo any acquire which may have happened part this metastore transaction and then record which lock blocked the lock were testing info 
WITHOUT_CLASSIFICATION	 validate query materialization materialized views query results caching this check needs occur before constant folding which may remove some 
WITHOUT_CLASSIFICATION	 create new inputformat instance this the first time see this class 
WITHOUT_CLASSIFICATION	 hdfs scratch dir 
WITHOUT_CLASSIFICATION	 dynamic partitioning usecase 
WITHOUT_CLASSIFICATION	 val sign significand exponent this sign unscaledvalue scale make valthis need scale updown such that unscaledvalue significand exponent scale notice that must the scaling carefully check overflow and 
WITHOUT_CLASSIFICATION	 drop every table the default database 
WITHOUT_CLASSIFICATION	 parallel edge was found for the given mapjoin need down further skip this operator pipeline 
WITHOUT_CLASSIFICATION	 move the clock forward and check the delayed queue 
WITHOUT_CLASSIFICATION	 whether number open transactions reaches the threshold 
WITHOUT_CLASSIFICATION	 subscribe feeds from the movetask that movetask can forward the list 
WITHOUT_CLASSIFICATION	 merge those that can merged 
WITHOUT_CLASSIFICATION	 reducers not benefit from llap point printing 
WITHOUT_CLASSIFICATION	 test initial metadata count metrics 
WITHOUT_CLASSIFICATION	 test null both sides 
WITHOUT_CLASSIFICATION	 drop 
WITHOUT_CLASSIFICATION	 override public partitionid throw new runtimeexceptionnot applicable 
WITHOUT_CLASSIFICATION	 calculate hat estimation the next digit 
WITHOUT_CLASSIFICATION	 currently only give the initial event the task the first heartbeat given that the split ready seems pointless wait but thats how tez works 
WITHOUT_CLASSIFICATION	 got here means its acquire info lock 
WITHOUT_CLASSIFICATION	 want preserve columnname was original input query that rewrite 
WITHOUT_CLASSIFICATION	 the persistent function discarded try reload 
WITHOUT_CLASSIFICATION	 record repeating and nulls state restored later 
WITHOUT_CLASSIFICATION	 the session will restarted and return 
WITHOUT_CLASSIFICATION	 this doesnt create partition 
WITHOUT_CLASSIFICATION	 test long 
WITHOUT_CLASSIFICATION	 column numbers batch corresponding expression result arguments 
WITHOUT_CLASSIFICATION	 serialization fails will throw incompatible metastore error the client 
WITHOUT_CLASSIFICATION	 the regex changed make sure compile the regex again 
WITHOUT_CLASSIFICATION	 reprocess the spilled data 
WITHOUT_CLASSIFICATION	 key and value objects are created once initialize and then reused for every getcurrentkey and getcurrentvalue call this important since rcfile makes assumption this fact 
WITHOUT_CLASSIFICATION	 nonnull confvar only defined confvars 
WITHOUT_CLASSIFICATION	 expected 
WITHOUT_CLASSIFICATION	 fail the retry 
WITHOUT_CLASSIFICATION	 decimal precision trailing zeroes 
WITHOUT_CLASSIFICATION	 note metrics have not been initialized this will return null which means arent 
WITHOUT_CLASSIFICATION	 writeids allocated txns under txnhwm then find writehwm from nextwriteid 
WITHOUT_CLASSIFICATION	 renaming test make test framework skip 
WITHOUT_CLASSIFICATION	 optional string key 
WITHOUT_CLASSIFICATION	 will try reuse this but session queued before 
WITHOUT_CLASSIFICATION	 hmgettable result will not have privileges set does not retrieve that part from metastore unset privileges null before comparing 
WITHOUT_CLASSIFICATION	 handle the different map definition parquet definition has group repeated group map mapkeyvalue required binary key utf optional binary value utf definition has groups optional group map repeated group map mapkeyvalue required binary key utf optional binary value utf 
WITHOUT_CLASSIFICATION	 loginfosearching for dynpathspec 
WITHOUT_CLASSIFICATION	 ignore 
WITHOUT_CLASSIFICATION	 inserts are not tracked writeset 
WITHOUT_CLASSIFICATION	 filesink cannot simply cloned requires some special processing subqueries for the union will processed independent mapreduce jobs possibly running parallel those subqueries cannot write the same directory clone the filesink but create subdirectory the final path 
WITHOUT_CLASSIFICATION	 allow 
WITHOUT_CLASSIFICATION	 this dfs file 
WITHOUT_CLASSIFICATION	 this config contains all the configuration that master node wants provide the hcatalog 
WITHOUT_CLASSIFICATION	 the command has associated schema make sure gets printed use 
WITHOUT_CLASSIFICATION	 the group args are passed add the violating value the error msg 
WITHOUT_CLASSIFICATION	 test equals operator for strings and integers 
WITHOUT_CLASSIFICATION	 code borrowed from 
WITHOUT_CLASSIFICATION	 add the expression into the bloomfilter 
WITHOUT_CLASSIFICATION	 get the selectoperator ancestor 
WITHOUT_CLASSIFICATION	 rewrite the above plan correlaterelleft correlation condition true leftinputrel projecta rexnode aggregate groupby aggagg projectb may reference covar filter references corvar rightinputrel correlated reference 
WITHOUT_CLASSIFICATION	 compressed 
WITHOUT_CLASSIFICATION	 the securitycontext set authfilter 
WITHOUT_CLASSIFICATION	 check the sizes neededcolumns and partnames here either size aggrstats null after several retries thus can 
WITHOUT_CLASSIFICATION	 default monday beginning the week 
WITHOUT_CLASSIFICATION	 nlines read all lines log file 
WITHOUT_CLASSIFICATION	 same for char 
WITHOUT_CLASSIFICATION	 using shared instance the umbilical server 
WITHOUT_CLASSIFICATION	 try again with some different data values 
WITHOUT_CLASSIFICATION	 default should set true the spark config 
WITHOUT_CLASSIFICATION	 insert into should skip and increment partition number 
WITHOUT_CLASSIFICATION	 add nonnull parameters the schema 
WITHOUT_CLASSIFICATION	 static partitions 
WITHOUT_CLASSIFICATION	 skim off the exponent 
WITHOUT_CLASSIFICATION	 nonjavadoc see int int 
WITHOUT_CLASSIFICATION	 using old config value tests backwards compatibility 
WITHOUT_CLASSIFICATION	 operationcompleted 
WITHOUT_CLASSIFICATION	 whether the cache has been initialized not 
WITHOUT_CLASSIFICATION	 one these runs the output each reducer 
WITHOUT_CLASSIFICATION	 column was not found table schema its new column 
WITHOUT_CLASSIFICATION	 user does not specify queue use session default 
WITHOUT_CLASSIFICATION	 set global member indicating where store not vectorized information necessary 
WITHOUT_CLASSIFICATION	 run our value expressions over whole batch 
WITHOUT_CLASSIFICATION	 this called once per dont get the starting duck count here 
WITHOUT_CLASSIFICATION	 authorization checks are performed the used 
WITHOUT_CLASSIFICATION	 tokreplication replid ismetadataonly 
WITHOUT_CLASSIFICATION	 only used spillbigtablerow 
WITHOUT_CLASSIFICATION	 amreporter after the server that gets the correct address knows how deal with 
WITHOUT_CLASSIFICATION	 just logged exception with case jdo humongous callstack make new one 
WITHOUT_CLASSIFICATION	 merge the two closest bins into their average location weighted their heights the height the new bin the sum the heights the old bins double 
WITHOUT_CLASSIFICATION	 nothing new added the queue while analyze runs 
WITHOUT_CLASSIFICATION	 are running local mode then the amount memory used the child jvm can longer default the memory used the parent jvm 
WITHOUT_CLASSIFICATION	 empty out the file 
WITHOUT_CLASSIFICATION	 methods setreset gettable modifier 
WITHOUT_CLASSIFICATION	 orientation 
WITHOUT_CLASSIFICATION	 get session update session allocation kill query destroy session restart session return session back pool move session different pool 
WITHOUT_CLASSIFICATION	 prefix partition with something avoid being hidden file 
WITHOUT_CLASSIFICATION	 note all the fields are only modified master thread 
WITHOUT_CLASSIFICATION	 create the walker the rules dispatcher and the context create walker which walks the tree dfs manner while maintaining the operator stack the dispatcher 
WITHOUT_CLASSIFICATION	 were inside replication scope then the table not existing not error 
WITHOUT_CLASSIFICATION	 must deterministic order map see hive use instead mapsnewhashmap 
WITHOUT_CLASSIFICATION	 find the minmax based the offset and length and more for original 
WITHOUT_CLASSIFICATION	 parameter value not changed false connection int smallint throws exception 
WITHOUT_CLASSIFICATION	 columninfos for table alias 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 get all partitions that matches with the partition spec 
WITHOUT_CLASSIFICATION	 null does equal null here 
WITHOUT_CLASSIFICATION	 reducer already 
WITHOUT_CLASSIFICATION	 try with erroneously generated void 
WITHOUT_CLASSIFICATION	 here create project for the following reasons gby only accepts arg position the input however need sum vcolc this can better reuse the function 
WITHOUT_CLASSIFICATION	 have extracted the existence from the hash set result dont keep 
WITHOUT_CLASSIFICATION	 this method should called movetask when there are dynamic partitions generated 
WITHOUT_CLASSIFICATION	 try not leave any files open 
WITHOUT_CLASSIFICATION	 string value stringengetvalue does not apply variable expansion does variable expansion 
WITHOUT_CLASSIFICATION	 add negative self 
WITHOUT_CLASSIFICATION	 table location should not printed with hbase backed tables 
WITHOUT_CLASSIFICATION	 seems that not used anything 
WITHOUT_CLASSIFICATION	 repl metadata export has repllastid and replscopemetadata import repl metadata dump table metadata changed allows override has repllastid 
WITHOUT_CLASSIFICATION	 may need setup localdir for relocalization which usually setup environmentpwd used for relocalization add the user specified configuration confpbbinarystream 
WITHOUT_CLASSIFICATION	 set the thread local username thrifthttpservlet 
WITHOUT_CLASSIFICATION	 called from multiple parallel threads aclentries modifiable the method will not thread safe and could cause random concurrency issues this test case checks the aclentries returned from hadoopfilestatus threadsafe not 
WITHOUT_CLASSIFICATION	 directly deserialize with the caller reading fieldbyfield serialization format the caller responsible for calling the read method for the right type each field after calling readnextfield reading some fields require results object receive value information separate results object created the caller initialization per different field even for the same type some type values are reference either bytes the deserialization buffer other type specific buffers those references are only valid until the next time set called 
WITHOUT_CLASSIFICATION	 commenting part hive and are not supported for clobs tablenames filter short assertequals tablenamessize 
WITHOUT_CLASSIFICATION	 get nanos since epoch fromzone 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 change the plan this structure projecta replace corvar input ref from join join left condition true leftinputrel aggregategroupby singlevalue projectb everything from input plus literal true projinputrel 
WITHOUT_CLASSIFICATION	 lock are trying acquire shared read 
WITHOUT_CLASSIFICATION	 find all the agg expressions use list for all countdistinct 
WITHOUT_CLASSIFICATION	 check for partition 
WITHOUT_CLASSIFICATION	 index where the buffer minallocation units headers array 
WITHOUT_CLASSIFICATION	 sort the list get sorted deterministic output for ease testing 
WITHOUT_CLASSIFICATION	 capture stderr 
WITHOUT_CLASSIFICATION	 constant just return 
WITHOUT_CLASSIFICATION	 because zero zero need mention javadoc 
WITHOUT_CLASSIFICATION	 first just allocate just the output columns will using 
WITHOUT_CLASSIFICATION	 this case weve determined that theres too much data prune dynamically 
WITHOUT_CLASSIFICATION	 the return type will the concatenation input type and original values type 
WITHOUT_CLASSIFICATION	 insert overwrite 
WITHOUT_CLASSIFICATION	 call isnt equal type and has been determined that value generate might required should rather generate value generator 
WITHOUT_CLASSIFICATION	 convert the column the correct type when needed and set row obj 
WITHOUT_CLASSIFICATION	 this valid error message 
WITHOUT_CLASSIFICATION	 for nested subqueries the alias mapping not maintained currently 
WITHOUT_CLASSIFICATION	 since demuxoperator may appear multiple times muxoperators parents list use newchildindextag instead childoperatorstag example join mux gby demux this case the parent list mux demux gby demux need have two childoperatorstags the index this demuxoperator 
WITHOUT_CLASSIFICATION	 testing nulls and repeating 
WITHOUT_CLASSIFICATION	 non aggregate mode analyze union operator 
WITHOUT_CLASSIFICATION	 inside should not care about its access info 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 its important read the correct nulls truth the path needed for splitgrouper 
WITHOUT_CLASSIFICATION	 prefix extend start 
WITHOUT_CLASSIFICATION	 debug only 
WITHOUT_CLASSIFICATION	 run constant propagation twice because after predicate pushdown filter expressions are combined and may become eligible for reduction like not null filter 
WITHOUT_CLASSIFICATION	 uniqueconstraints 
WITHOUT_CLASSIFICATION	 assume this always comes from user operation that took the lock 
WITHOUT_CLASSIFICATION	 for password based authentication 
WITHOUT_CLASSIFICATION	 build druid query 
WITHOUT_CLASSIFICATION	 lock ids are unique across the system 
WITHOUT_CLASSIFICATION	 allow this form 
WITHOUT_CLASSIFICATION	 start index since the variable from table column 
WITHOUT_CLASSIFICATION	 vertices elapsed time 
WITHOUT_CLASSIFICATION	 restored the renamed tables 
WITHOUT_CLASSIFICATION	 can only flush after the updateaggregations done the potentially new entry aggs can flushed out the hash table 
WITHOUT_CLASSIFICATION	 whatever 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 assertassertequals statlength 
WITHOUT_CLASSIFICATION	 validate the create view statement this point the createvwdesc gets all the information for semanticcheck 
WITHOUT_CLASSIFICATION	 first field the row key 
WITHOUT_CLASSIFICATION	 move marker according delta change delta 
WITHOUT_CLASSIFICATION	 update stats for transactional tables full acid with overwrite even though are marking stats not being accurate 
WITHOUT_CLASSIFICATION	 export trivially retriable after clearing out the staging dir provided 
WITHOUT_CLASSIFICATION	 prevent instantiation 
WITHOUT_CLASSIFICATION	 now having clause can contain subquery predicate invoke genfilterplan handle subquery algebraic transformation just done for subquery predicates appearing the where clause 
WITHOUT_CLASSIFICATION	 udaf filter condition groupby caluse param funtion etc 
WITHOUT_CLASSIFICATION	 clauses need combined keeping only common elements 
WITHOUT_CLASSIFICATION	 format partitionpval add only when table partition colname matches 
WITHOUT_CLASSIFICATION	 create lock but send the heartbeat with long delay the lock will get expired 
WITHOUT_CLASSIFICATION	 all the integer types float and double string char varchar 
WITHOUT_CLASSIFICATION	 currently commit after selecting the txns abort whether serializable readcommitted the effect the same could use for update select from txns and the whole performtimeouts single huge transaction but the only benefit would make sure someone cannot heartbeat one these txns the same time the attempt heartbeat would block and fail immediately after its unblocked with current multiple txns implementation possible for someone send heartbeat the very end the expire interval and just after the select from txns made which case heartbeat will succeed but txn will still aborted solving this corner case not worth the perf penalty the client should heartbeat timely way 
WITHOUT_CLASSIFICATION	 check for column encoding specification 
WITHOUT_CLASSIFICATION	 set temp location 
WITHOUT_CLASSIFICATION	 outer key copying only used when are using the input bigtable batch the output 
WITHOUT_CLASSIFICATION	 generate the temporary file name 
WITHOUT_CLASSIFICATION	 there may multi distinct clauses for one column 
WITHOUT_CLASSIFICATION	 for debugging 
WITHOUT_CLASSIFICATION	 check this table sampled and needs more than input pruning 
WITHOUT_CLASSIFICATION	 set the session configuration 
WITHOUT_CLASSIFICATION	 optional bytes token 
WITHOUT_CLASSIFICATION	 serializes object text for transport byte headerb encoded like below bytes magic string identify serialized stream bytes numbitvectors because bitvectorsize bytes are enough hold positions 
WITHOUT_CLASSIFICATION	 null filters are supported simplify client code 
WITHOUT_CLASSIFICATION	 step function increase the polling timeout every sec starting from 
WITHOUT_CLASSIFICATION	 bytes the higherorder bits come from the second vint that follows the nanos field 
WITHOUT_CLASSIFICATION	 metadata 
WITHOUT_CLASSIFICATION	 the case that return type the genericudf not boolean and not all partition agree result make the node unknown they all agree replace the node 
WITHOUT_CLASSIFICATION	 the entry relevant aborted txns shouldnt removed from txntowriteid aborted txn would removed from txns only after the compaction also committed txn open txn retained 
WITHOUT_CLASSIFICATION	 merge the other estimation into the current one 
WITHOUT_CLASSIFICATION	 provide instance the code doesnt try make real instance just want test that fail before trying make connector with null password 
WITHOUT_CLASSIFICATION	 for example select deptno count sumbonus mindistinct sal from emp group deptno becomes select deptno sumcnt sumbonus minsal from select deptno count cnt sumbonus sal from emp group deptno sal aggregate 
WITHOUT_CLASSIFICATION	 positive number 
WITHOUT_CLASSIFICATION	 extract the delegation token from the ugi and add the job 
WITHOUT_CLASSIFICATION	 unpartitioned table row for delete inserts are not tracked writeset 
WITHOUT_CLASSIFICATION	 need total poolthreadcount threads start same there are poolthreadcount threads thread pool and another one which has started them the thread which sees atomic counter poolthreadcount the last thread join and wake all threads start all once 
WITHOUT_CLASSIFICATION	 sessionid 
WITHOUT_CLASSIFICATION	 shutdown hiveserver has been deregistered from zookeeper and has active sessions 
WITHOUT_CLASSIFICATION	 theres some bogus code that can modify the queue name forceset for pool sessions 
WITHOUT_CLASSIFICATION	 for avro type the serialization class parameter optional 
WITHOUT_CLASSIFICATION	 prepare 
WITHOUT_CLASSIFICATION	 our dbname null were interested all events 
WITHOUT_CLASSIFICATION	 one and only one 
WITHOUT_CLASSIFICATION	 init exec and set parameters included 
WITHOUT_CLASSIFICATION	 txn batch 
WITHOUT_CLASSIFICATION	 add rows corresponding the grouping sets for each row create rows one for each grouping set key since mapside aggregation has already been performed the number rows would have been reduced moreover the rows corresponding the grouping keys come together there higher chance finding the rows the hash table 
WITHOUT_CLASSIFICATION	 outerrr belongs outer query and required resolve correlated references 
WITHOUT_CLASSIFICATION	 set the operator plan 
WITHOUT_CLASSIFICATION	 top distinct can always merge whether bottom distinct not top all can only merge bottom also all that say should bail out top all and bottom distinct 
WITHOUT_CLASSIFICATION	 hdfs temp table space 
WITHOUT_CLASSIFICATION	 the big table bucket file names small tables 
WITHOUT_CLASSIFICATION	 this serious black magic the following lines nothing afaict but without them the subsequent call listpartitionvalues fails 
WITHOUT_CLASSIFICATION	 since getbuckethashcode uses this hivedecimal return the old much slower but compatible hash code 
WITHOUT_CLASSIFICATION	 operator that handles the output these joinoperator 
WITHOUT_CLASSIFICATION	 remove from the running list 
WITHOUT_CLASSIFICATION	 the non explain code path dont need track query rewrites all add fns during plan generation are noops the get rewrite methods are called thrown 
WITHOUT_CLASSIFICATION	 this mapping which big table columns input and keyvalue expressions will 
WITHOUT_CLASSIFICATION	 large cross product generate the vector optimization using repeating vectorized row batch optimization the overflow batch 
WITHOUT_CLASSIFICATION	 not public since must have column information 
WITHOUT_CLASSIFICATION	 create the parameter declaration string 
WITHOUT_CLASSIFICATION	 map may not contain all sources since input list may have been optimized out nonexistent tho such sources may still referenced the tablescanoperator its null then the partition probably doesnt exist lets use table permission 
WITHOUT_CLASSIFICATION	 the file still cached 
WITHOUT_CLASSIFICATION	 set recursive traversal case the cached query was union generated tez 
WITHOUT_CLASSIFICATION	 zero check 
WITHOUT_CLASSIFICATION	 make look like streaming api use case 
WITHOUT_CLASSIFICATION	 for vectorized reduceside operators getting inputs from reduce sink the row object inspector will get flattened version the object inspector where the nested keyvalue structs are replaced with single struct example key reducesinkkeyint value colint colint would get converted the following for vectorized input keyreducesinkkeyint valuecolint valuecolint the exprnodeevaluator initialzation below gets broken with the flattened object inpsectors convert back the form that contains the nested keyvalue structs 
WITHOUT_CLASSIFICATION	 keepalive information the client should informed and will have take care resubmitting the work some parts fault tolerance here 
WITHOUT_CLASSIFICATION	 object that can take set columns row vectorized row batch and serialized 
WITHOUT_CLASSIFICATION	 dont change the table object returned the metastore well mess with its caches 
WITHOUT_CLASSIFICATION	 skewedinfo 
WITHOUT_CLASSIFICATION	 create more deltaxx that compactor has dir file compact 
WITHOUT_CLASSIFICATION	 write key element 
WITHOUT_CLASSIFICATION	 decided treat this map regular object 
WITHOUT_CLASSIFICATION	 needmerge 
WITHOUT_CLASSIFICATION	 return the key itself since mapping was availablereturned 
WITHOUT_CLASSIFICATION	 nothing can here 
WITHOUT_CLASSIFICATION	 propagate null values for twoinput operator and set isrepeating and nonulls appropriately 
WITHOUT_CLASSIFICATION	 since local jobs are run sequentially all relevant information already available therefore need fetch job debug info asynchronously 
WITHOUT_CLASSIFICATION	 there are nulls null array entries are already initialized 
WITHOUT_CLASSIFICATION	 this combination the jar stuff from conf and not from conf 
WITHOUT_CLASSIFICATION	 dont allow for public 
WITHOUT_CLASSIFICATION	 boundary case require least one nonpartitioned column 
WITHOUT_CLASSIFICATION	 generate the new queryid needed 
WITHOUT_CLASSIFICATION	 fail trying set transactional true but doesnt satisfy bucketing and inputoutputformat requirement 
WITHOUT_CLASSIFICATION	 consider query like insert overwrite table select from join tkey tkey where and are sorted and bucketed key into the same number buckets dont need reducer enforce bucketing and sorting for the field below captures the fact that the reducer introduced enforce sorting bucketing has been removed this case sortmerge join needed and the sortmerge join between and 
WITHOUT_CLASSIFICATION	 opoutputvertexname may null 
WITHOUT_CLASSIFICATION	 should not happen take care all existing types 
WITHOUT_CLASSIFICATION	 sign whether interval positive negative 
WITHOUT_CLASSIFICATION	 depending filesystem implementation flush may may not anything 
WITHOUT_CLASSIFICATION	 sleeptime milliseconds between batches delegation tokens dropped 
WITHOUT_CLASSIFICATION	 later can extend this the union all case well 
WITHOUT_CLASSIFICATION	 implement this logic using replacechildren instead replacing the root node itself because windowing logic stores multiple pointers the ast and replacing root might lead some pointers leading nonrewritten version 
WITHOUT_CLASSIFICATION	 cannot send the ecb consumer discard whatever already there 
WITHOUT_CLASSIFICATION	 set create time 
WITHOUT_CLASSIFICATION	 compare start position 
WITHOUT_CLASSIFICATION	 keys reserved for updating listenerevent parameters this key will have the event identifier that processed during event this event identifier might shared across other implementations 
WITHOUT_CLASSIFICATION	 make sure the broken signature doesnt work 
WITHOUT_CLASSIFICATION	 invalid cases 
WITHOUT_CLASSIFICATION	 this time completes adding remaining partitions 
WITHOUT_CLASSIFICATION	 queryid the command time which lock was acquired mode the lock explicitlock 
WITHOUT_CLASSIFICATION	 shift one 
WITHOUT_CLASSIFICATION	 rebuild that more efficient than the full rebuild 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 methods setreset listpartitionnames modifier 
WITHOUT_CLASSIFICATION	 must send vectorptfoperator 
WITHOUT_CLASSIFICATION	 its false need close recordreader 
WITHOUT_CLASSIFICATION	 scratch batch that will used play back big table rows that were spilled 
WITHOUT_CLASSIFICATION	 int 
WITHOUT_CLASSIFICATION	 insert event partitioned table with dynamic addpartition 
WITHOUT_CLASSIFICATION	 arbitrary dont expect caller hang out for mins 
WITHOUT_CLASSIFICATION	 replace with the milliseconds conversion 
WITHOUT_CLASSIFICATION	 child the type the column 
WITHOUT_CLASSIFICATION	 distribute the available memory between the tasks 
WITHOUT_CLASSIFICATION	 are using test specific database then just drop the database and recreate 
WITHOUT_CLASSIFICATION	 write back the final null byte before the last fields 
WITHOUT_CLASSIFICATION	 need close the dummyops well the operator pipeline not considered closeddone unless all operators are done for broadcast joins that includes the dummy parents 
WITHOUT_CLASSIFICATION	 this simulates that cleaning thread will error out while cleaning the notifications 
WITHOUT_CLASSIFICATION	 todopc validate that there are types that refer this 
WITHOUT_CLASSIFICATION	 test after recovery 
WITHOUT_CLASSIFICATION	 sign zero dot digits support toformatstring which can add lot trailing zeroes 
WITHOUT_CLASSIFICATION	 sort keys are specified use edge that does not sort 
WITHOUT_CLASSIFICATION	 conversion functions take single parameter 
WITHOUT_CLASSIFICATION	 will break the uncompressed data the cache the chunks that are the size the prevalent orc compression buffer the default maximum allocation since cannot allocate bigger chunks whichever less 
WITHOUT_CLASSIFICATION	 resets the aggregation calculation variables 
WITHOUT_CLASSIFICATION	 chararray unbounded hivepig lossless 
WITHOUT_CLASSIFICATION	 could scheduling guaranteed task when higher priority task cannot scheduled try take duck away from lower priority task here 
WITHOUT_CLASSIFICATION	 close called udtfoperator 
WITHOUT_CLASSIFICATION	 get serializable details the destination tables 
WITHOUT_CLASSIFICATION	 function try fold 
WITHOUT_CLASSIFICATION	 checkh for exists and not exists the sub query must have more correlated predicates 
WITHOUT_CLASSIFICATION	 string should have been truncated 
WITHOUT_CLASSIFICATION	 param currentkey the current key param currentvalue the current value 
WITHOUT_CLASSIFICATION	 tries get lock and gets waiting state 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 same jobs running sidebyside 
WITHOUT_CLASSIFICATION	 finally evaluate the aggregators 
WITHOUT_CLASSIFICATION	 sort bucket names for the big table 
WITHOUT_CLASSIFICATION	 special date formatting functions 
WITHOUT_CLASSIFICATION	 see the indexes for colstats 
WITHOUT_CLASSIFICATION	 now read the bigendian compacted twos complement int parts compacted means they are stripped leading and xffs this why the intlengthpos tricks bellow length all the bytes have read after skip skip pos where start reading the current int intlength how many bytes read for the current int 
WITHOUT_CLASSIFICATION	 remove this entry from the table usage mappings 
WITHOUT_CLASSIFICATION	 the group spans vectorizedrowbatch swap the relevant columns into our batch buffers write the batch temporary storage 
WITHOUT_CLASSIFICATION	 location should created for views 
WITHOUT_CLASSIFICATION	 this can only happen once decompress time 
WITHOUT_CLASSIFICATION	 should update currently refers the source database name 
WITHOUT_CLASSIFICATION	 part virtual 
WITHOUT_CLASSIFICATION	 verify that getnextnotification returns all events 
WITHOUT_CLASSIFICATION	 update startindex 
WITHOUT_CLASSIFICATION	 the from and strings havent changed dont need preprocess again regenerate the mappings code points that need replaced deleted 
WITHOUT_CLASSIFICATION	 get the internal array structure 
WITHOUT_CLASSIFICATION	 schema provided user and the schema computed pig the time calling store must match 
WITHOUT_CLASSIFICATION	 exception for type checking for simplicity constructing the row objectinspector 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 populate target 
WITHOUT_CLASSIFICATION	 not from cache still need hook the plans 
WITHOUT_CLASSIFICATION	 done with this part 
WITHOUT_CLASSIFICATION	 give value 
WITHOUT_CLASSIFICATION	 explicitly create immutable lists here forces the guava lib run the transformations and not them lazily the reason being the function class used for transformations additionally also creates the corresponding replcopytasks which cannot evaluated lazily since the query 
WITHOUT_CLASSIFICATION	 example from hivedecimaladd header comments 
WITHOUT_CLASSIFICATION	 rebuild inplace the selected array with rows destine forwarded 
WITHOUT_CLASSIFICATION	 the number spark tasks executed the hiveserver since the last restart 
WITHOUT_CLASSIFICATION	 test more than one lock can handled lock request 
WITHOUT_CLASSIFICATION	 repeated lintstring 
WITHOUT_CLASSIFICATION	 throw new not sort order and unique 
WITHOUT_CLASSIFICATION	 batch allocation should always happen atomically either write ids for all txns allocated none 
WITHOUT_CLASSIFICATION	 the probe failed must allocate set aggregation buffers and push the keywrapperbuffers pair into the hash very important clone the keywrapper the one have from our keywrappersbatch going resetreused next batch 
WITHOUT_CLASSIFICATION	 dpp work considered child because work needs finish for execute 
WITHOUT_CLASSIFICATION	 evict key that not from this batch initial intended 
WITHOUT_CLASSIFICATION	 newtable has exist this point compile 
WITHOUT_CLASSIFICATION	 using endpoint identification algorithm https enables 
WITHOUT_CLASSIFICATION	 reset the buffer 
WITHOUT_CLASSIFICATION	 need update the keys 
WITHOUT_CLASSIFICATION	 since compilation always blocking rpc call and schema ready after compilation can return when are the running state 
WITHOUT_CLASSIFICATION	 theoretically the key prefix could any unique string shared between tablescanoperator when publishing and statstask when aggregating here use dbnametablename partitionsec the prefix for easy read during explain and debugging 
WITHOUT_CLASSIFICATION	 this break tie insert delete given row done within the same txn that currentwriteid the same for both events and want the delete event sort since needs sent that inputsplit options options can skip 
WITHOUT_CLASSIFICATION	 add the entry the cache structures while under write lock 
WITHOUT_CLASSIFICATION	 remove any cached results from the previous test 
WITHOUT_CLASSIFICATION	 case select use fetch task instead move task the select from analyze table column rewrite dont create fetch task instead create column stats task later 
WITHOUT_CLASSIFICATION	 the task has started all operators within the task have started 
WITHOUT_CLASSIFICATION	 nothing can done 
WITHOUT_CLASSIFICATION	 read the altered partition via cachedstore 
WITHOUT_CLASSIFICATION	 read the tag 
WITHOUT_CLASSIFICATION	 partitions need update 
WITHOUT_CLASSIFICATION	 should key and reversevalue 
WITHOUT_CLASSIFICATION	 abstract class for hash set result 
WITHOUT_CLASSIFICATION	 columns down the dag the lvj will transform internal column names from something like key col because this need undo this transformation using the column expression map the column names propagate the dag 
WITHOUT_CLASSIFICATION	 common hash code routines 
WITHOUT_CLASSIFICATION	 todo parse converttoacidsql make sure has alter table defaulttflat set tblproperties transactionaltrue converttommsql make sure has alter table defaulttflattext set tblproperties transactionaltrue 
WITHOUT_CLASSIFICATION	 zero easy 
WITHOUT_CLASSIFICATION	 reuse the same type for all only ivy can return more than one probably all jars 
WITHOUT_CLASSIFICATION	 assert mapjoinpos 
WITHOUT_CLASSIFICATION	 fail some inserts that have records txncomponents 
WITHOUT_CLASSIFICATION	 rcfile read 
WITHOUT_CLASSIFICATION	 test string column char literal comparison 
WITHOUT_CLASSIFICATION	 return the multiset count for the lookup key 
WITHOUT_CLASSIFICATION	 credentials can change across dags ideally construct only once per dag 
WITHOUT_CLASSIFICATION	 initialize load path 
WITHOUT_CLASSIFICATION	 newpart 
WITHOUT_CLASSIFICATION	 some hive features depends several configuration legacy build and add these configuration jobconf here 
WITHOUT_CLASSIFICATION	 note for collapse false this just sets keyssame 
WITHOUT_CLASSIFICATION	 int length outputgetlength offset 
WITHOUT_CLASSIFICATION	 this column not included 
WITHOUT_CLASSIFICATION	 fail compaction that have failed records 
WITHOUT_CLASSIFICATION	 note for now dont have seterror here caller will seterror throw 
WITHOUT_CLASSIFICATION	 resultdec assertassertequals 
WITHOUT_CLASSIFICATION	 set the hook that will disallow creating nonwhitelisted udfs anywhere the plan are not using specific hook for genericudfbridge that doesnt work minillap because the daemon embedded the client also gets this hook and kryo brittle 
WITHOUT_CLASSIFICATION	 careful maintenance the flag 
WITHOUT_CLASSIFICATION	 checks the value contains any the passwordstrings and yes return true 
WITHOUT_CLASSIFICATION	 java calendar index starting 
WITHOUT_CLASSIFICATION	 ignore error 
WITHOUT_CLASSIFICATION	 reset conf vars 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 can not have correct table stats then both the table stats and column stats are not useful 
WITHOUT_CLASSIFICATION	 returns absolute offset the match 
WITHOUT_CLASSIFICATION	 take topk closest neighbors and compute the bias corrected cardinality 
WITHOUT_CLASSIFICATION	 whether optimize union followed select followed filesink creates subdirectories the final output should not turned systems 
WITHOUT_CLASSIFICATION	 find all the valid cookies associated with the request 
WITHOUT_CLASSIFICATION	 unable parse the connect command 
WITHOUT_CLASSIFICATION	 extract the conditions that can useful 
WITHOUT_CLASSIFICATION	 now back bed until its time this again 
WITHOUT_CLASSIFICATION	 not using stats and this the first sink the path meaning that should use stats infer parallelism 
WITHOUT_CLASSIFICATION	 repeating 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 cartesian product our ranges the child ranges 
WITHOUT_CLASSIFICATION	 temp macros are not allowed have qualified names 
WITHOUT_CLASSIFICATION	 compare among potentially multiple matches 
WITHOUT_CLASSIFICATION	 insert one more row this should trigger reached for ttp 
WITHOUT_CLASSIFICATION	 this function will merge csold into csnew 
WITHOUT_CLASSIFICATION	 bloomfilter objectbitset objectdata long numbits int numhashfunctions int 
WITHOUT_CLASSIFICATION	 functioncat functionschem functionname remarks 
WITHOUT_CLASSIFICATION	 throw exception the tablepartition bucketed one the columns 
WITHOUT_CLASSIFICATION	 this happens either when the input file the big table changed closeop needs fetch all the left data from the small tables and try join them 
WITHOUT_CLASSIFICATION	 run cbo 
WITHOUT_CLASSIFICATION	 case there are multiple columns referenced the same column name wont 
WITHOUT_CLASSIFICATION	 parsing necessary the end the parents end move past parent field separator 
WITHOUT_CLASSIFICATION	 returns number lines the printed throwable stack trace 
WITHOUT_CLASSIFICATION	 single source can process multiple columns and will send event for each them 
WITHOUT_CLASSIFICATION	 match was found add the clause the corresponding list 
WITHOUT_CLASSIFICATION	 projections are handled using generate not the load 
WITHOUT_CLASSIFICATION	 operators that belong each work 
WITHOUT_CLASSIFICATION	 restore the previous properties for framework name address etc 
WITHOUT_CLASSIFICATION	 filters 
WITHOUT_CLASSIFICATION	 tracks containerids and taskattemptids can kept independent the running dag 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader long 
WITHOUT_CLASSIFICATION	 cleanup basefsdir since can shared across tests 
WITHOUT_CLASSIFICATION	 show locks partition dstoday 
WITHOUT_CLASSIFICATION	 cant happen 
WITHOUT_CLASSIFICATION	 get the maxlen 
WITHOUT_CLASSIFICATION	 used hook unions 
WITHOUT_CLASSIFICATION	 utc default 
WITHOUT_CLASSIFICATION	 explictly ignoring getter visibility any for autojson serialization trigger based getters 
WITHOUT_CLASSIFICATION	 try deserialize using serde class our writable row objects created serializewrite 
WITHOUT_CLASSIFICATION	 joda pattern matching expects fractional seconds length match the number the pattern you want match you need different patterns with sss 
WITHOUT_CLASSIFICATION	 the first bounds check requires least one more byte beyond for int hence parse the first byte vintvlong determine the number bytes 
WITHOUT_CLASSIFICATION	 confget takes care parameter replacement iteratorvalue does not 
WITHOUT_CLASSIFICATION	 create table and load kvtxt 
WITHOUT_CLASSIFICATION	 used determine cleaner thread already running 
WITHOUT_CLASSIFICATION	 table aliased select for example 
WITHOUT_CLASSIFICATION	 try again 
WITHOUT_CLASSIFICATION	 read the record with different record reader and evolved schema 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 verify that unpartitioned table rename succeeded 
WITHOUT_CLASSIFICATION	 execute set command 
WITHOUT_CLASSIFICATION	 add credential provider password the child processs environment 
WITHOUT_CLASSIFICATION	 scalar queries should expect valuecount less than 
WITHOUT_CLASSIFICATION	 expect query return error state 
WITHOUT_CLASSIFICATION	 processbatch size currentbatchsize numcols batchnumcols selectedinuse batchselectedinuse 
WITHOUT_CLASSIFICATION	 ignore this multiple clients may trying create the same partition addpartitiondesc has ifexists flag but its not propagated and throws 
WITHOUT_CLASSIFICATION	 have some sort expression tree try sql filter pushdown 
WITHOUT_CLASSIFICATION	 tell reducerecordsource flush last record this reduce side smb 
WITHOUT_CLASSIFICATION	 time based counters dag done already dont update these counters 
WITHOUT_CLASSIFICATION	 again split for base 
WITHOUT_CLASSIFICATION	 transactions just the header row 
WITHOUT_CLASSIFICATION	 found delete events this location needs compacting 
WITHOUT_CLASSIFICATION	 determine this previous the last slice need read for this split 
WITHOUT_CLASSIFICATION	 get int view the buffer 
WITHOUT_CLASSIFICATION	 from precision maxs maxps scale maxs 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 level create col col countc for each branch 
WITHOUT_CLASSIFICATION	 helper class generate mocked response 
WITHOUT_CLASSIFICATION	 the primarys nextrecord the next value return 
WITHOUT_CLASSIFICATION	 when processing dynamic partitioned hash joins some the small tables may not get processed before the mapjoins parents are removed during gentezworkprocess this keep 
WITHOUT_CLASSIFICATION	 bgenjjtree functiontype 
WITHOUT_CLASSIFICATION	 topic alan 
WITHOUT_CLASSIFICATION	 figure out the newlocalordinal relative the newinput 
WITHOUT_CLASSIFICATION	 this should never happen just added the lock 
WITHOUT_CLASSIFICATION	 check hive returns results correctly 
WITHOUT_CLASSIFICATION	 create struct clause 
WITHOUT_CLASSIFICATION	 plan maybe null driverclose called another thread for the same driver object 
WITHOUT_CLASSIFICATION	 column attributes provided create list null attributes 
WITHOUT_CLASSIFICATION	 this means that update set something something can whatever supported select something 
WITHOUT_CLASSIFICATION	 check that src exists and also checks permissions necessary rename src dest 
WITHOUT_CLASSIFICATION	 this should fail once finds out the threshold has been reached 
WITHOUT_CLASSIFICATION	 throw some canfinish variations just for fun 
WITHOUT_CLASSIFICATION	 now copy over the data where isnullindex false 
WITHOUT_CLASSIFICATION	 byteminvalue 
WITHOUT_CLASSIFICATION	 first start with high message size limit this should allow connections 
WITHOUT_CLASSIFICATION	 this the data copy 
WITHOUT_CLASSIFICATION	 settabletreereader that can avoid this check 
WITHOUT_CLASSIFICATION	 should throw 
WITHOUT_CLASSIFICATION	 tasks finished but some failed 
WITHOUT_CLASSIFICATION	 try infer common primitive category 
WITHOUT_CLASSIFICATION	 types are different need check whether can convert them 
WITHOUT_CLASSIFICATION	 with repeating 
WITHOUT_CLASSIFICATION	 subexpr the list containing generated clauses result this optimization 
WITHOUT_CLASSIFICATION	 decimal column vectors gets the same weight long column vectors 
WITHOUT_CLASSIFICATION	 not failing the job due failure constructing the log url 
WITHOUT_CLASSIFICATION	 different from sql compat mode 
WITHOUT_CLASSIFICATION	 hold the aggregation results for each row the partition number rows processed the partition 
WITHOUT_CLASSIFICATION	 the buffer has nsync bytes 
WITHOUT_CLASSIFICATION	 infotype 
WITHOUT_CLASSIFICATION	 resultset serialization settings 
WITHOUT_CLASSIFICATION	 this method necessary synchronize lazycreation the timers 
WITHOUT_CLASSIFICATION	 ignore priority 
WITHOUT_CLASSIFICATION	 here build aux structure that used verify that the foreign key that declared actually referencing valid primary key unique key also check that the types 
WITHOUT_CLASSIFICATION	 should only one object 
WITHOUT_CLASSIFICATION	 make array with qbjointree outer most inner mostn 
WITHOUT_CLASSIFICATION	 cannot estimated sample runtime 
WITHOUT_CLASSIFICATION	 the write ids returned for the transaction batch also sequential 
WITHOUT_CLASSIFICATION	 total rows generate rows cache most percentile extra rows generate different thread 
WITHOUT_CLASSIFICATION	 finally decompress data map per and return caller 
WITHOUT_CLASSIFICATION	 from here out choose whether want run llap 
WITHOUT_CLASSIFICATION	 since only have single distinct call 
WITHOUT_CLASSIFICATION	 equals 
WITHOUT_CLASSIFICATION	 returned the fractional part not set 
WITHOUT_CLASSIFICATION	 see there are additional knownfragments there are more fragments came after this cleanup was scheduled and theres nothing done 
WITHOUT_CLASSIFICATION	 the query here insertinto and the target immutable table verify that our destination empty before proceeding 
WITHOUT_CLASSIFICATION	 set setting read column ids with empty list 
WITHOUT_CLASSIFICATION	 read from dumpfile and instantiate self 
WITHOUT_CLASSIFICATION	 check that all correlated refs the filter condition are 
WITHOUT_CLASSIFICATION	 calculate number different entries and evaluate 
WITHOUT_CLASSIFICATION	 alter table can change the type partition key now check the column name only 
WITHOUT_CLASSIFICATION	 rely the fact that poll checks interrupt even when theres something the queue the structure replaced with smth that doesnt must check interrupt here because hive operators rely recordreader handle task interruption and unlike most rrs 
WITHOUT_CLASSIFICATION	 for multiinsert queries thus nodeofinterest the from clause 
WITHOUT_CLASSIFICATION	 read the null terminator 
WITHOUT_CLASSIFICATION	 test when third argument has nulls 
WITHOUT_CLASSIFICATION	 skip writing tags when feeding into mapjoin hashtable whether this can forward records directly instead shufflingsorting 
WITHOUT_CLASSIFICATION	 array hash multiset results can lookups the whole batch before output result 
WITHOUT_CLASSIFICATION	 find the root all custom paths from custom pattern the root the largest prefix input pattern string that doesnt match custompathpattern 
WITHOUT_CLASSIFICATION	 shorter than the required pattern 
WITHOUT_CLASSIFICATION	 the session taken out the pool but waiting for registration 
WITHOUT_CLASSIFICATION	 last work weve processed order hook the current 
WITHOUT_CLASSIFICATION	 test various set methods and copy constructors 
WITHOUT_CLASSIFICATION	 yarn property spark yarn mode 
WITHOUT_CLASSIFICATION	 task kill while the request still pending state means the request should retried 
WITHOUT_CLASSIFICATION	 build the exprnodefuncdesc with recursively built children 
WITHOUT_CLASSIFICATION	 sum all nonnull long column values for avg maintain isgroupresultnull after last row last group batch compute the group avg when sum nonnull 
WITHOUT_CLASSIFICATION	 allocate little extra space limit need reallocate 
WITHOUT_CLASSIFICATION	 serializerclass 
WITHOUT_CLASSIFICATION	 find the absolute minimum transaction 
WITHOUT_CLASSIFICATION	 null null null 
WITHOUT_CLASSIFICATION	 increment cursor for elements per innot clause 
WITHOUT_CLASSIFICATION	 the passed argument matches somewhat closely with accepted argument 
WITHOUT_CLASSIFICATION	 the following members have context information for the current partition file being read 
WITHOUT_CLASSIFICATION	 row format terminated clause 
WITHOUT_CLASSIFICATION	 propertyfile file 
WITHOUT_CLASSIFICATION	 incorrect precision expected but was 
WITHOUT_CLASSIFICATION	 get the root operator 
WITHOUT_CLASSIFICATION	 filtering has been applied yet selectedinuse false meaning that all rows qualify true then the selected array records the offsets qualifying rows 
WITHOUT_CLASSIFICATION	 created hint skip 
WITHOUT_CLASSIFICATION	 are composing query that returns single row update happened after 
WITHOUT_CLASSIFICATION	 dimension 
WITHOUT_CLASSIFICATION	 includes the columns that have data 
WITHOUT_CLASSIFICATION	 always using the rowid column 
WITHOUT_CLASSIFICATION	 keeps track vertices from which events are expected 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 create connection user 
WITHOUT_CLASSIFICATION	 the binary data 
WITHOUT_CLASSIFICATION	 return nulls for conversion operators 
WITHOUT_CLASSIFICATION	 cannot backtrack the expression bail out 
WITHOUT_CLASSIFICATION	 cant infer type 
WITHOUT_CLASSIFICATION	 measured nanoseconds from the epoch 
WITHOUT_CLASSIFICATION	 tokskewedlocations 
WITHOUT_CLASSIFICATION	 create batch with one string bytes column 
WITHOUT_CLASSIFICATION	 did find the filedir itself 
WITHOUT_CLASSIFICATION	 either there was nothing which could pushed down size there were complex predicates which dont support yet currently supported are one the form key size key size key and key size add residual 
WITHOUT_CLASSIFICATION	 aggexpr part distinct key 
WITHOUT_CLASSIFICATION	 support for dynamic partitions can added later 
WITHOUT_CLASSIFICATION	 now the operation with java bigdecimal 
WITHOUT_CLASSIFICATION	 optimize set add both 
WITHOUT_CLASSIFICATION	 semi join specific 
WITHOUT_CLASSIFICATION	 reset grace hashjoin context that there state maintained when operatorwork 
WITHOUT_CLASSIFICATION	 the jobtracker signalled that the threshold not exceeded 
WITHOUT_CLASSIFICATION	 check they are operating the same partition not move 
WITHOUT_CLASSIFICATION	 flush final partial batch 
WITHOUT_CLASSIFICATION	 check for substruct validity 
WITHOUT_CLASSIFICATION	 drop the tables when were done this makes the test work inside ide 
WITHOUT_CLASSIFICATION	 full stop byte 
WITHOUT_CLASSIFICATION	 create partitions 
WITHOUT_CLASSIFICATION	 this can only possible there merge work followed the union 
WITHOUT_CLASSIFICATION	 problem use new name 
WITHOUT_CLASSIFICATION	 last character ends token there are quotes all the text between the quotes considered single token this can happen for timestamp with local timezone 
WITHOUT_CLASSIFICATION	 input javawritable conversion needed 
WITHOUT_CLASSIFICATION	 right input positions are shifted newleftfieldcount 
WITHOUT_CLASSIFICATION	 value for the flag 
WITHOUT_CLASSIFICATION	 applydistinct 
WITHOUT_CLASSIFICATION	 case the expression regex col this can only happen without clause dont allow this for exprresolver the group case 
WITHOUT_CLASSIFICATION	 fileread writes the writer which writes orcwriter which writes cachewriter 
WITHOUT_CLASSIFICATION	 uses sampling which means its not bucketed 
WITHOUT_CLASSIFICATION	 this point everything that can consumed from appstatusbuilder has been consumed 
WITHOUT_CLASSIFICATION	 the absolute offset the beginning the key within the writebuffers 
WITHOUT_CLASSIFICATION	 table could the big table there need convert 
WITHOUT_CLASSIFICATION	 filter longdouble 
WITHOUT_CLASSIFICATION	 file ownershippermission checks should done the new table path 
WITHOUT_CLASSIFICATION	 read the record with different record reader 
WITHOUT_CLASSIFICATION	 execute process 
WITHOUT_CLASSIFICATION	 the last char escape char read the actual char 
WITHOUT_CLASSIFICATION	 referenced later 
WITHOUT_CLASSIFICATION	 getallwork returns topologically sorted list which use make sure that vertices are created before they are used edges 
WITHOUT_CLASSIFICATION	 debug instrumenter useful finding which fns get called and how often 
WITHOUT_CLASSIFICATION	 test readfully 
WITHOUT_CLASSIFICATION	 first check temp tables 
WITHOUT_CLASSIFICATION	 make sure qualify the name from the outset theres ambiguity 
WITHOUT_CLASSIFICATION	 column family primitive type for mapkey value can stored binary format pass the qualifier prefix cherry pick the qualifiers that match the prefix instead picking everything 
WITHOUT_CLASSIFICATION	 partitioned table query has only pruning filters 
WITHOUT_CLASSIFICATION	 index into list 
WITHOUT_CLASSIFICATION	 container affinity can implemented host affinity for llap not required until edges are used hive 
WITHOUT_CLASSIFICATION	 check parameter set validity public method 
WITHOUT_CLASSIFICATION	 didnt have predicate pushdown read everything 
WITHOUT_CLASSIFICATION	 load property file 
WITHOUT_CLASSIFICATION	 string comparison good enough since its form dateyyyymmdd 
WITHOUT_CLASSIFICATION	 main path found increfed 
WITHOUT_CLASSIFICATION	 such abcde 
WITHOUT_CLASSIFICATION	 reached this condition had replication state record for the object but its replacement has state disallow replacement 
WITHOUT_CLASSIFICATION	 sets the env variable value defined sets property simulate default credential 
WITHOUT_CLASSIFICATION	 setup our inner join specific members 
WITHOUT_CLASSIFICATION	 are going fail expensive stuff ranges are broken play safe 
WITHOUT_CLASSIFICATION	 total merge cost 
WITHOUT_CLASSIFICATION	 weve seen this already 
WITHOUT_CLASSIFICATION	 the fallback from failed sql jdo not possible 
WITHOUT_CLASSIFICATION	 some joins might null see processnode for leafnode clean them 
WITHOUT_CLASSIFICATION	 not modify the header here the caller will use this space 
WITHOUT_CLASSIFICATION	 skip leading zeroes word 
WITHOUT_CLASSIFICATION	 success 
WITHOUT_CLASSIFICATION	 and the new join rel 
WITHOUT_CLASSIFICATION	 note this path should specific concatenate never executed select query modify the existing move task already the candidate running tasks 
WITHOUT_CLASSIFICATION	 utils 
WITHOUT_CLASSIFICATION	 spill previously loaded tables make more room 
WITHOUT_CLASSIFICATION	 could not heartbeat the lock the operation has finished hence interrupt this work 
WITHOUT_CLASSIFICATION	 these will handled the output the table instead 
WITHOUT_CLASSIFICATION	 copy the tezsessionstate from the old clisessionstate 
WITHOUT_CLASSIFICATION	 closing the underlying bytestream should have effect the data should still accessible 
WITHOUT_CLASSIFICATION	 fetch the first group for all small table aliases 
WITHOUT_CLASSIFICATION	 stores mappings from local hdfs location for all resource types 
WITHOUT_CLASSIFICATION	 deserializerclass 
WITHOUT_CLASSIFICATION	 store the type for later retrieval 
WITHOUT_CLASSIFICATION	 the operator and need determine any the children are final candidates 
WITHOUT_CLASSIFICATION	 remove unnecessary expressions 
WITHOUT_CLASSIFICATION	 some debug information 
WITHOUT_CLASSIFICATION	 get parent schema 
WITHOUT_CLASSIFICATION	 string 
WITHOUT_CLASSIFICATION	 and must have locations within the table directory 
WITHOUT_CLASSIFICATION	 not much can about 
WITHOUT_CLASSIFICATION	 the max value number zeroes for bit hash can encoded using only bits will disable bit packing for any values 
WITHOUT_CLASSIFICATION	 will true there nonnull entry 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the structure inside cte like this tokcte toksubquery may refer toksubquery 
WITHOUT_CLASSIFICATION	 obtain udaf name 
WITHOUT_CLASSIFICATION	 preempt only specific hosts preemptions already exist those 
WITHOUT_CLASSIFICATION	 partitionlist 
WITHOUT_CLASSIFICATION	 there grouping key and row came this operator 
WITHOUT_CLASSIFICATION	 parentcatalogname 
WITHOUT_CLASSIFICATION	 throw away middle and lowest words 
WITHOUT_CLASSIFICATION	 takes place during optimization 
WITHOUT_CLASSIFICATION	 bootstrap load which should also replicate the aborted write ids both tables 
WITHOUT_CLASSIFICATION	 default separators are indexed instead indexed thus the separator offset byte the separator for the hive row for the row struct and the maps and 
WITHOUT_CLASSIFICATION	 optional queryidentifier 
WITHOUT_CLASSIFICATION	 friday august 
WITHOUT_CLASSIFICATION	 read through all values 
WITHOUT_CLASSIFICATION	 reset everything 
WITHOUT_CLASSIFICATION	 populate the load file work that columnstatstask can work 
WITHOUT_CLASSIFICATION	 update join statistics 
WITHOUT_CLASSIFICATION	 dplb 
WITHOUT_CLASSIFICATION	 operator wants some work the end group 
WITHOUT_CLASSIFICATION	 there should only dummy object the rowcontainer 
WITHOUT_CLASSIFICATION	 combined lastaccess and ownertester paramparam 
WITHOUT_CLASSIFICATION	 skip the partitions progress and the ones for which stats update disabled could filter the skipped partititons out part the initial names query 
WITHOUT_CLASSIFICATION	 input file has header footer cannot splitted 
WITHOUT_CLASSIFICATION	 check that the output done 
WITHOUT_CLASSIFICATION	 create filter sqcountcheckcount instead project because relfieldtrimmer ends getting rid project 
WITHOUT_CLASSIFICATION	 read operation mode 
WITHOUT_CLASSIFICATION	 change could not retrieve for all partitions 
WITHOUT_CLASSIFICATION	 fileids 
WITHOUT_CLASSIFICATION	 parse the response message authzid utfnul authcid utfnul passwd 
WITHOUT_CLASSIFICATION	 disconnect the connection union work and connect merge work 
WITHOUT_CLASSIFICATION	 convert the search condition into restriction the hbase scan 
WITHOUT_CLASSIFICATION	 should merge join 
WITHOUT_CLASSIFICATION	 cmapintstring 
WITHOUT_CLASSIFICATION	 for the second one explicitly set location make sure ends the specified place 
WITHOUT_CLASSIFICATION	 array byte 
WITHOUT_CLASSIFICATION	 now make the select produce regular columnsdynamic partition columns with 
WITHOUT_CLASSIFICATION	 nop 
WITHOUT_CLASSIFICATION	 test that opening jdbc connection nonexistent database throws hivesqlexception 
WITHOUT_CLASSIFICATION	 first allocation write should add the table the nextwriteid meta table the initial value for write should and hence add with number write ids allocated here 
WITHOUT_CLASSIFICATION	 events insert last repl repldumpidxy 
WITHOUT_CLASSIFICATION	 since new statistics derived from all relations involved 
WITHOUT_CLASSIFICATION	 force locality 
WITHOUT_CLASSIFICATION	 add more complex types 
WITHOUT_CLASSIFICATION	 set the stats key prefix the same directory name the directory name can changed the optimizer but the key should not changed 
WITHOUT_CLASSIFICATION	 underflows large 
WITHOUT_CLASSIFICATION	 change choose the appropriate file system 
WITHOUT_CLASSIFICATION	 null result from all ranges 
WITHOUT_CLASSIFICATION	 there joinvalue joinkey has all null elements 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 since have create space for find expired node will remove 
WITHOUT_CLASSIFICATION	 useful when the type column vector has not determined yet 
WITHOUT_CLASSIFICATION	 this parameter left for compatibility when reading existing configs removed druid 
WITHOUT_CLASSIFICATION	 note query priorities add them might here 
WITHOUT_CLASSIFICATION	 seems these two operators can merged check that plan meets some preconditions before doing particular the presence map joins the upstream plan cannot exceed the noconditional task size and already merged the big table cannot merge the broadcast 
WITHOUT_CLASSIFICATION	 when inittrue combine with will generate resolved parse tree from syntax tree readentity created under these conditions should all relevant the syntax tree even the ones without parents set mergeisdirect true here 
WITHOUT_CLASSIFICATION	 decimalformat longformatter new decimalformat 
WITHOUT_CLASSIFICATION	 read once gain access key and value objects 
WITHOUT_CLASSIFICATION	 check the contents second row 
WITHOUT_CLASSIFICATION	 trigger bootstrap dump which just creates table and other tables and constraints not loaded 
WITHOUT_CLASSIFICATION	 alter table commands require table ownership there should not output object but just case the table incorrectly added 
WITHOUT_CLASSIFICATION	 add key for reduce sink 
WITHOUT_CLASSIFICATION	 evaluate the aggregation over one the groups batches 
WITHOUT_CLASSIFICATION	 intentionally nothing 
WITHOUT_CLASSIFICATION	 idempotent case and just return 
WITHOUT_CLASSIFICATION	 clear out any rows the batch from previous partition since are going change the repeating partition column values 
WITHOUT_CLASSIFICATION	 todo see comments under blacklistnode 
WITHOUT_CLASSIFICATION	 ensures maps can deserialized when see for why that might used 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 sorting makes tests easier write since file names and rowids depend statementid this makes file name data mapping stable 
WITHOUT_CLASSIFICATION	 batching not enabled try add all the partitions one call 
WITHOUT_CLASSIFICATION	 for nonnative table property storagehandler should retained 
WITHOUT_CLASSIFICATION	 the length the scratch buffer that needs passed tobytes toformatbytes 
WITHOUT_CLASSIFICATION	 will inherit the name and status from the plan are replacing 
WITHOUT_CLASSIFICATION	 never call this function without first calling heartbeatlong long 
WITHOUT_CLASSIFICATION	 look for tables with empty pattern 
WITHOUT_CLASSIFICATION	 see committxn for more info this inequality 
WITHOUT_CLASSIFICATION	 table dropped after repl dump 
WITHOUT_CLASSIFICATION	 cache not yet prewarmed add this set which the prewarm thread can check that the prewarm thread does not add back 
WITHOUT_CLASSIFICATION	 future can reuse this conversion 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 current split from the same file preceding split and the preceding split has footerbuffer 
WITHOUT_CLASSIFICATION	 subquery was only condition where clause 
WITHOUT_CLASSIFICATION	 filter projection need stage 
WITHOUT_CLASSIFICATION	 can make this configurable 
WITHOUT_CLASSIFICATION	 the start the split was the middle the previous slice 
WITHOUT_CLASSIFICATION	 create new union operator 
WITHOUT_CLASSIFICATION	 set different uri one the criteria deciding whether return the same client not uris are checked for string equivalence even spaces make them different 
WITHOUT_CLASSIFICATION	 get the hashtable file and path 
WITHOUT_CLASSIFICATION	 get conf from user payload 
WITHOUT_CLASSIFICATION	 used please 
WITHOUT_CLASSIFICATION	 replace the default input output file format with those found 
WITHOUT_CLASSIFICATION	 the number single value rows that were generated the big table batch 
WITHOUT_CLASSIFICATION	 hit the end after getting optional integer and optional dot and optional blank padding 
WITHOUT_CLASSIFICATION	 positive number 
WITHOUT_CLASSIFICATION	 for given row put into proper partition based its hash value when memory threshold reached the biggest hash table memory will spilled disk the hash table specific partition already disk all later rows will put into row container for later use 
WITHOUT_CLASSIFICATION	 just creating orc reader going sanity checks make sure its valid orc file 
WITHOUT_CLASSIFICATION	 retrieve primary key constraints cannot null 
WITHOUT_CLASSIFICATION	 table metadata 
WITHOUT_CLASSIFICATION	 numfalses 
WITHOUT_CLASSIFICATION	 finally not reduce the size input enough bail out 
WITHOUT_CLASSIFICATION	 drop tables 
WITHOUT_CLASSIFICATION	 since integer always some products here are not included 
WITHOUT_CLASSIFICATION	 same primitive category but different qualifiers rely sort out the type params 
WITHOUT_CLASSIFICATION	 may need peel off the genericudfbridge that added cbo user 
WITHOUT_CLASSIFICATION	 for other usually not used types just quote the value 
WITHOUT_CLASSIFICATION	 names 
WITHOUT_CLASSIFICATION	 table write for this operation 
WITHOUT_CLASSIFICATION	 return new project 
WITHOUT_CLASSIFICATION	 note schema evolution currently does not support column index changes 
WITHOUT_CLASSIFICATION	 this next section repeats the tests with maxlength parameter that exactly the number current characters the string this shouldnt affect the trim 
WITHOUT_CLASSIFICATION	 verify the output 
WITHOUT_CLASSIFICATION	 evaluate children only scalar false 
WITHOUT_CLASSIFICATION	 input fulltablename format dbnametablename 
WITHOUT_CLASSIFICATION	 when registered the system registry 
WITHOUT_CLASSIFICATION	 deserialize and append new row using the current batch size the index 
WITHOUT_CLASSIFICATION	 construct aggregation function info 
WITHOUT_CLASSIFICATION	 the the tracking node must sequential node 
WITHOUT_CLASSIFICATION	 one dead session with dryrun 
WITHOUT_CLASSIFICATION	 trailing spaces are significant 
WITHOUT_CLASSIFICATION	 right child 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 copy the data 
WITHOUT_CLASSIFICATION	 apply best effort fetch the correct table alias not found fallback old logic 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 emitted from join operator will depend this factor 
WITHOUT_CLASSIFICATION	 note longer call addtasklocalfiles because all the resources are correctly updated the session resource lists now and thus added vertices something breaks might need called here 
WITHOUT_CLASSIFICATION	 prepare output descriptors for the input opt 
WITHOUT_CLASSIFICATION	 when kerberos enabled have add the accumulo delegation token the job that gets passed down the yarntez task 
WITHOUT_CLASSIFICATION	 all the sessions use that were not destroyed returned with failed update now die 
WITHOUT_CLASSIFICATION	 create table sets empty non null structures 
WITHOUT_CLASSIFICATION	 test repeating case nulls 
WITHOUT_CLASSIFICATION	 have cycle 
WITHOUT_CLASSIFICATION	 the sizes match prefer the table with fewer partitions 
WITHOUT_CLASSIFICATION	 partspec ordered hash map number dynamic partition columns number static partition columns path name corresponding columns the root path columns paths start from number buckets each partition 
WITHOUT_CLASSIFICATION	 asynchronously shutdown this instance hiveserver there are active client sessions 
WITHOUT_CLASSIFICATION	 repl status should return null 
WITHOUT_CLASSIFICATION	 need figure out the current transaction number and the list open transactions avoid needing transaction the underlying database well look the current transaction number first subsequently shows the open list thats 
WITHOUT_CLASSIFICATION	 note that this doesnt include appid assume that all the subsequent instances the same usercluster are logically the same all the paths will reused all the security tokensetc should transition between them etc 
WITHOUT_CLASSIFICATION	 insert reduceside 
WITHOUT_CLASSIFICATION	 checkpermissions returns false query not found throws failure 
WITHOUT_CLASSIFICATION	 set confoverlay parameters 
WITHOUT_CLASSIFICATION	 validation will later exclude vectorization virtual columns usage necessary 
WITHOUT_CLASSIFICATION	 output columns 
WITHOUT_CLASSIFICATION	 reference vectorization description needed for explain vectorization hash table loading etc 
WITHOUT_CLASSIFICATION	 files with write ids may not valid may affect snapshot isolation for ongoing txns well 
WITHOUT_CLASSIFICATION	 copy workdir 
WITHOUT_CLASSIFICATION	 the acl apis also expect the tradition usergroupother permission the form acl 
WITHOUT_CLASSIFICATION	 cleanup pathtoaliases 
WITHOUT_CLASSIFICATION	 the node may have been blacklisted this point which means may not the activenodelist 
WITHOUT_CLASSIFICATION	 iterate through any records because our read offset was past the stripe offset the rows from the last stripe will 
WITHOUT_CLASSIFICATION	 insert entries txntowriteid for newly allocated write ids 
WITHOUT_CLASSIFICATION	 adding keys pita theres way plug into timed rolling just create new fsm 
WITHOUT_CLASSIFICATION	 partcolsisnull 
WITHOUT_CLASSIFICATION	 owner 
WITHOUT_CLASSIFICATION	 prevent construction 
WITHOUT_CLASSIFICATION	 check for conditions that will lead local copy checks are are testing hive either source destination local filesystemfile aggregate filesize all source pathscan directory file less than configured size number files all source pathscan directory file less than configured size 
WITHOUT_CLASSIFICATION	 singleline 
WITHOUT_CLASSIFICATION	 setting statementid makes compacted delta files use 
WITHOUT_CLASSIFICATION	 depend linux openssl exit codes 
WITHOUT_CLASSIFICATION	 todo dont anything for now just log this for debugging may able make use this later for workload management 
WITHOUT_CLASSIFICATION	 update the lru node from what weve seen far 
WITHOUT_CLASSIFICATION	 for each input file 
WITHOUT_CLASSIFICATION	 retest waiting locks both have same ext 
WITHOUT_CLASSIFICATION	 operator needs invoke specific cleanup that operator can override 
WITHOUT_CLASSIFICATION	 the ptned table will not dumped gettable will return null 
WITHOUT_CLASSIFICATION	 widening cast not change ndv min max 
WITHOUT_CLASSIFICATION	 test validate that all tables exist the hms metastore 
WITHOUT_CLASSIFICATION	 how many rows this split 
WITHOUT_CLASSIFICATION	 match 
WITHOUT_CLASSIFICATION	 check for existence table 
WITHOUT_CLASSIFICATION	 update nondistinct groupby key value aggregations keycolx valuecolx 
WITHOUT_CLASSIFICATION	 hive though there are not restrictions hive table property key and could any combination the letters digits and even punctuations support conventional property name webhcat prepery name starting with letter digit probably with period underscore and hyphen only the middle like autopurge lastmodifiedby etc 
WITHOUT_CLASSIFICATION	 consider looked the possibility faster decimal double conversion using some their lower level logic that extracts the various parts out double the difficulty javas rounding rules are byzantine 
WITHOUT_CLASSIFICATION	 the reduce plan inputs have tags add all inputs that have tags 
WITHOUT_CLASSIFICATION	 number bits address registers 
WITHOUT_CLASSIFICATION	 are allowed interface has been added for the same 
WITHOUT_CLASSIFICATION	 already accounted for 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 current partitioned table with last partition replicated denoted 
WITHOUT_CLASSIFICATION	 key token start index 
WITHOUT_CLASSIFICATION	 try appending segment with conflicting interval 
WITHOUT_CLASSIFICATION	 first operator reduce task 
WITHOUT_CLASSIFICATION	 remove old join from child set all the rss 
WITHOUT_CLASSIFICATION	 methods that return scalar expressions 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the job not empty but runs fast have wait until all the taskendjobend 
WITHOUT_CLASSIFICATION	 are currently performing binary search 
WITHOUT_CLASSIFICATION	 log final state consolelogger 
WITHOUT_CLASSIFICATION	 decompose join condition number leaf predicates 
WITHOUT_CLASSIFICATION	 the union operator has been processed 
WITHOUT_CLASSIFICATION	 constant list projection known length 
WITHOUT_CLASSIFICATION	 could have this protected method wno class but half hive static there 
WITHOUT_CLASSIFICATION	 less frequently set parameter not passing param 
WITHOUT_CLASSIFICATION	 bail out 
WITHOUT_CLASSIFICATION	 determine the size small table inputs 
WITHOUT_CLASSIFICATION	 this will make the object completely unusable semantics clear are not defined 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 must 
WITHOUT_CLASSIFICATION	 this method should called subclasses beforeclass initializer 
WITHOUT_CLASSIFICATION	 directly serialize fieldbyfield the binarysortable format this alternative way serialize than what provided binarysortableserde 
WITHOUT_CLASSIFICATION	 the following evaluate methods since are supporting scratch column reuse must assume the column may have nonulls false and some isnull entries true proper assignments 
WITHOUT_CLASSIFICATION	 hashcode should positive flip all the bits its negative 
WITHOUT_CLASSIFICATION	 immutable 
WITHOUT_CLASSIFICATION	 outer join specific 
WITHOUT_CLASSIFICATION	 schemapattern null means that the schemapattern value should not used narrow the search 
WITHOUT_CLASSIFICATION	 for now just adding true condition where clause can remove the where clause from the ast requires moving all subsequent children left 
WITHOUT_CLASSIFICATION	 smallintvalue 
WITHOUT_CLASSIFICATION	 this basically the same except that dont use base encode binary data because were using printable string delimiter consider such row straq str string the delimiter and 
WITHOUT_CLASSIFICATION	 set not implemented ignore 
WITHOUT_CLASSIFICATION	 they will file urls 
WITHOUT_CLASSIFICATION	 look because driverrun committed 
WITHOUT_CLASSIFICATION	 need check with instanceof instead just checking vectorized because the row can vectorizedrowbatch when fetchoptimizer kicks even the operator pipeline not vectorized 
WITHOUT_CLASSIFICATION	 change the children the original join operator point the map join operator 
WITHOUT_CLASSIFICATION	 the number keys with sequential duplicates collapsed both null and nonnull the batch 
WITHOUT_CLASSIFICATION	 more timedout txns 
WITHOUT_CLASSIFICATION	 write record parquet format 
WITHOUT_CLASSIFICATION	 done with all the things 
WITHOUT_CLASSIFICATION	 whitespace characters 
WITHOUT_CLASSIFICATION	 could renewed return that information 
WITHOUT_CLASSIFICATION	 returns true trailing slash needed appended the url 
WITHOUT_CLASSIFICATION	 task provided location preference got host since host full and only host left random pool 
WITHOUT_CLASSIFICATION	 delayed due temporary resource availability 
WITHOUT_CLASSIFICATION	 spot check decimal colscalar modulo 
WITHOUT_CLASSIFICATION	 log classpaths 
WITHOUT_CLASSIFICATION	 schedule task invalidate cache entry and remove from lookup 
WITHOUT_CLASSIFICATION	 need and check any the tasks running llap mode 
WITHOUT_CLASSIFICATION	 contains results from last processed input record 
WITHOUT_CLASSIFICATION	 remove the operators till certain depth 
WITHOUT_CLASSIFICATION	 only worry about getting schema are dealing with avro 
WITHOUT_CLASSIFICATION	 full precision 
WITHOUT_CLASSIFICATION	 this assumes llap cluster owner always the user 
WITHOUT_CLASSIFICATION	 the number files for the table should same number buckets 
WITHOUT_CLASSIFICATION	 note this noop for custom udfs 
WITHOUT_CLASSIFICATION	 set marker that this conf has been processed 
WITHOUT_CLASSIFICATION	 dont instantiate 
WITHOUT_CLASSIFICATION	 get the kind expression 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 get unique skewed value list 
WITHOUT_CLASSIFICATION	 currently expressions are not allowed cluster distribute order sort clause for each the above clause types check 
WITHOUT_CLASSIFICATION	 generate unique for temp table path this path will fixed for the life the temp table 
WITHOUT_CLASSIFICATION	 with nulls and selected 
WITHOUT_CLASSIFICATION	 have tell apart partitions resulting from spec with different prefix lengths already have smth for the same prefix length can the two 
WITHOUT_CLASSIFICATION	 see hive for details 
WITHOUT_CLASSIFICATION	 the update has failed but the state has changed since then retry needed 
WITHOUT_CLASSIFICATION	 now the dumped path can one three things can dump which case expect set dirs each with name and with metadata file each and table dirs inside that can table dump dir which case expect metadata dump table question the dir and individual ptn dir hierarchy dump can incremental dump which means have several subdirs each which have the evid the dir name and each which correspond eventlevel dump currently only createtable and addpartition are handled all these dumps will tableptn level 
WITHOUT_CLASSIFICATION	 all inserts should basereader for normal read this should always delete delta not compacting 
WITHOUT_CLASSIFICATION	 patch the projection list for updates putting back the original set expressions 
WITHOUT_CLASSIFICATION	 pick any system properties that start with hive and set them our config this way can properly pull any hive values from the environment without needing know all 
WITHOUT_CLASSIFICATION	 for nonnative tables need exact match avoid hive the table location contains files and the string representation its path does not have trailing slash 
WITHOUT_CLASSIFICATION	 beeline 
WITHOUT_CLASSIFICATION	 comparison operations 
WITHOUT_CLASSIFICATION	 only our parent class can call this 
WITHOUT_CLASSIFICATION	 acid off there cant any acid tables nothing compact 
WITHOUT_CLASSIFICATION	 check and update partition cols necessary ideally this should done createvalue the partition constant per split but since hive uses and this does not call createvalue for each new recordreader creates this check required next 
WITHOUT_CLASSIFICATION	 function was properly called but threw its own exception unwrap and pass 
WITHOUT_CLASSIFICATION	 this child demuxoperator does not use tag just set the oldtag 
WITHOUT_CLASSIFICATION	 parameter was array primitives make sure the primitives are strings 
WITHOUT_CLASSIFICATION	 skip map processing for the first path element root array 
WITHOUT_CLASSIFICATION	 ignore 
WITHOUT_CLASSIFICATION	 only uses transactional and acid tables 
WITHOUT_CLASSIFICATION	 the same operator present times 
WITHOUT_CLASSIFICATION	 set some base data then stream some number partitions 
WITHOUT_CLASSIFICATION	 test that data inserted through hcatoutputformat readable from hive 
WITHOUT_CLASSIFICATION	 effect since 
WITHOUT_CLASSIFICATION	 have base the original files are obsolete 
WITHOUT_CLASSIFICATION	 formatteroff 
WITHOUT_CLASSIFICATION	 close the writer 
WITHOUT_CLASSIFICATION	 after bootstrap dump all the opened txns should aborted verify 
WITHOUT_CLASSIFICATION	 which field are start with consistent style with 
WITHOUT_CLASSIFICATION	 optional int aint 
WITHOUT_CLASSIFICATION	 base scan only 
WITHOUT_CLASSIFICATION	 valid merge register set size gets bigger also items 
WITHOUT_CLASSIFICATION	 dealing with string type 
WITHOUT_CLASSIFICATION	 move past union separator 
WITHOUT_CLASSIFICATION	 implementation notes since only local file systems are supported there need use hadoop version path class javanio package provides modern implementation file and directory operations which better then the traditional javaio are using here particular supports atomic creation temporary files with specified permissions the specified directory this also avoids various attacks possible when temp file name generated first followed file creation see for the description nio api and for the description interoperability between legacy api nio api avoid race conditions with readers the metrics file the implementation dumps metrics temporary file the same directory the actual metrics file and then renames the destination since both are located the same filesystem this rename likely atomic long the underlying support atomic renames note this reporter very similar would good unify the two 
WITHOUT_CLASSIFICATION	 scale fractional digits dot integer digits 
WITHOUT_CLASSIFICATION	 long count and double sum 
WITHOUT_CLASSIFICATION	 cant assume jdk implementing this explicitly return integercomparex integerminvalue integerminvalue 
WITHOUT_CLASSIFICATION	 not group across files case side work because there only reader per grouped split this would affect smb joins where want find the smallest key all the bucket files 
WITHOUT_CLASSIFICATION	 populate the groupby keys with the remapped arguments for aggregate the top groupset basically identity first fields aggregate 
WITHOUT_CLASSIFICATION	 containing the archived version the files 
WITHOUT_CLASSIFICATION	 objective here ensure that when exceptions are thrown hivemetastore api methods they bubble and are stored the objects 
WITHOUT_CLASSIFICATION	 replace 
WITHOUT_CLASSIFICATION	 evaluation the bytes constant vector expression after the vector 
WITHOUT_CLASSIFICATION	 consider validate type information 
WITHOUT_CLASSIFICATION	 use cbo and may apply maskingfiltering policies create copy the ast the reason that the generation the operator tree may modify the initial ast but need parse for second time would like parse the unmodified ast 
WITHOUT_CLASSIFICATION	 fields 
WITHOUT_CLASSIFICATION	 number entries store before being merged sparse map 
WITHOUT_CLASSIFICATION	 same file offset different lengths 
WITHOUT_CLASSIFICATION	 pig datetime can map date timestamp see which controlled hive target table information 
WITHOUT_CLASSIFICATION	 must call makeliteral not have the rexbuilderroundtime logic kick 
WITHOUT_CLASSIFICATION	 nonjavadoc see setting true correct only for special internal functions 
WITHOUT_CLASSIFICATION	 bucketing 
WITHOUT_CLASSIFICATION	 also remove the after the prefix 
WITHOUT_CLASSIFICATION	 destination table any true for full acid table and table should the destination table written using acid 
WITHOUT_CLASSIFICATION	 followed select star completely removed 
WITHOUT_CLASSIFICATION	 this point have arrived the level where need all the data and the 
WITHOUT_CLASSIFICATION	 otherwise the planner will throw exception different planners 
WITHOUT_CLASSIFICATION	 transaction manager 
WITHOUT_CLASSIFICATION	 numpartitionfields means random partitioning 
WITHOUT_CLASSIFICATION	 null input 
WITHOUT_CLASSIFICATION	 map values can primitive complex 
WITHOUT_CLASSIFICATION	 when splitupdate not enabled then all the deltas the current directories should considered usual 
WITHOUT_CLASSIFICATION	 retrievecd false not need deep retrieval the table column descriptor for instance this the case when are creating the table 
WITHOUT_CLASSIFICATION	 this not rebuild retrieve all the materializations turn not need force the materialization contents uptodate this not rebuild and apply the user parameters instead 
WITHOUT_CLASSIFICATION	 partvals 
WITHOUT_CLASSIFICATION	 get all the variable names being converted regex hiveconf using reflection 
WITHOUT_CLASSIFICATION	 theory the include path should come from the configuration 
WITHOUT_CLASSIFICATION	 this should connection 
WITHOUT_CLASSIFICATION	 required required required required optional optional 
WITHOUT_CLASSIFICATION	 trim the clob value max length int can hold 
WITHOUT_CLASSIFICATION	 test string 
WITHOUT_CLASSIFICATION	 number partitions for the chosen big table 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 hence the uncovered buckets not have any relevant data and can just ignore them 
WITHOUT_CLASSIFICATION	 the update options for outside the lock see below the synchronized block 
WITHOUT_CLASSIFICATION	 nonhive catalogs should not transactional 
WITHOUT_CLASSIFICATION	 start this server 
WITHOUT_CLASSIFICATION	 only scale adjustment needed 
WITHOUT_CLASSIFICATION	 iterate the global map and emit row for each key 
WITHOUT_CLASSIFICATION	 also allow lowercase versions all the keywords 
WITHOUT_CLASSIFICATION	 run query against nonacid table and shouldnt have txn logged conf 
WITHOUT_CLASSIFICATION	 used for 
WITHOUT_CLASSIFICATION	 bucketed just that get files 
WITHOUT_CLASSIFICATION	 update changed properties stats 
WITHOUT_CLASSIFICATION	 add ops existing collection 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 start log cleaner the start each test 
WITHOUT_CLASSIFICATION	 boolean that says whether the data distribution uniform hash not java hashcode 
WITHOUT_CLASSIFICATION	 have kerberos credentials 
WITHOUT_CLASSIFICATION	 table like tableacidtbl 
WITHOUT_CLASSIFICATION	 input provides the definition correlated variable 
WITHOUT_CLASSIFICATION	 copy uncompressed data cache put call moves position forward the size the data 
WITHOUT_CLASSIFICATION	 configuration 
WITHOUT_CLASSIFICATION	 not rexcall bail out 
WITHOUT_CLASSIFICATION	 date conversions supported genericudfdecimal 
WITHOUT_CLASSIFICATION	 call listlocatedstatus mockmocktable call check existence side file for mockmocktable call open mockmocktable call check existence side file for mockmocktable call open mockmocktable 
WITHOUT_CLASSIFICATION	 issupported 
WITHOUT_CLASSIFICATION	 add the previous nextkvreader back queue 
WITHOUT_CLASSIFICATION	 set host name conf 
WITHOUT_CLASSIFICATION	 verify that the provided object inspector can pull out these same values 
WITHOUT_CLASSIFICATION	 reset the previously supplied buffer that will receive the serialized data 
WITHOUT_CLASSIFICATION	 test the length first most cases avoid doing byte comparison 
WITHOUT_CLASSIFICATION	 overwrite 
WITHOUT_CLASSIFICATION	 checked for overflow based the outputtypeinfo 
WITHOUT_CLASSIFICATION	 init conf 
WITHOUT_CLASSIFICATION	 indexes during process call 
WITHOUT_CLASSIFICATION	 each iterator creates level encoding 
WITHOUT_CLASSIFICATION	 check the input projrel aggregate the entire input 
WITHOUT_CLASSIFICATION	 the table was not initially created with specified location 
WITHOUT_CLASSIFICATION	 add table via objectstore 
WITHOUT_CLASSIFICATION	 unexpected error placeholder tag not found throw 
WITHOUT_CLASSIFICATION	 add permanent udfs being used 
WITHOUT_CLASSIFICATION	 return hivesite jobconf 
WITHOUT_CLASSIFICATION	 createtable event with multiple partitions 
WITHOUT_CLASSIFICATION	 cancel the kills any avoid killing the returned sessions also sets the count for the async initialization 
WITHOUT_CLASSIFICATION	 load hash table for bucket mapjoin 
WITHOUT_CLASSIFICATION	 check cor var references are valid 
WITHOUT_CLASSIFICATION	 and the logs 
WITHOUT_CLASSIFICATION	 output instead input adding owner requirement output will catch that well 
WITHOUT_CLASSIFICATION	 initialize the rowbatchcontext 
WITHOUT_CLASSIFICATION	 check there data the resultset 
WITHOUT_CLASSIFICATION	 initialization 
WITHOUT_CLASSIFICATION	 still working 
WITHOUT_CLASSIFICATION	 input operator not the same position 
WITHOUT_CLASSIFICATION	 call the metastore get the currently queued and running compactions 
WITHOUT_CLASSIFICATION	 projection pruning this introduces select above hence needs run last due 
WITHOUT_CLASSIFICATION	 nonjavadoc see int javalangstring 
WITHOUT_CLASSIFICATION	 this total free memory available per executor case llap 
WITHOUT_CLASSIFICATION	 add interceptor that adds the xforwardedfor header with given ips 
WITHOUT_CLASSIFICATION	 not needed without semijoin reduction mapjoins when semijoins are enabled for parallel mapjoins 
WITHOUT_CLASSIFICATION	 cleanup the dag lock here since may have been created after the query completed 
WITHOUT_CLASSIFICATION	 the reader schema always comes without acid columns 
WITHOUT_CLASSIFICATION	 hightesttxnid 
WITHOUT_CLASSIFICATION	 this splitupdate initialize delete delta file path anticipation that they would write updatedelete events that separate file this writes file directory which starts with deletedelta the actual initialization writer only happens any delete events are written avoid empty files 
WITHOUT_CLASSIFICATION	 check explicit pool specifications valid cases where priority changed 
WITHOUT_CLASSIFICATION	 extract value from commaseparated keyvalue pairs 
WITHOUT_CLASSIFICATION	 attach for storage handlers 
WITHOUT_CLASSIFICATION	 for alter partition events 
WITHOUT_CLASSIFICATION	 both inputs repeat 
WITHOUT_CLASSIFICATION	 fieldtype means that this faked thrift field which does not serialize the field the stream result the only way get the field fall back the position the intention this hack fieldtype make real thrift prototype but there are lot additional work fulfill that and that protocol inherently does not support 
WITHOUT_CLASSIFICATION	 may resized later 
WITHOUT_CLASSIFICATION	 singleaggrel produces nullable type create the new 
WITHOUT_CLASSIFICATION	 let create more partitions total without any triggers 
WITHOUT_CLASSIFICATION	 extract the information necessary create the predicate for the new filter 
WITHOUT_CLASSIFICATION	 error stream always uses the default serde with single column 
WITHOUT_CLASSIFICATION	 hive requires this taskattemptid unique mrs taskattemptid composed the counterpart for spark should when therere multiple attempts for task hive will rely the partitionid figure out the data are duplicate not when collecting the final outputs see 
WITHOUT_CLASSIFICATION	 multiple newtags can point the same child when the child joinoperator first check contains the key childindex 
WITHOUT_CLASSIFICATION	 noarg ctor required for kyro serialization 
WITHOUT_CLASSIFICATION	 drop named primary key 
WITHOUT_CLASSIFICATION	 prefix the form dbnametblname 
WITHOUT_CLASSIFICATION	 check source partition exists 
WITHOUT_CLASSIFICATION	 singleton behaviour create the cache instance required 
WITHOUT_CLASSIFICATION	 this yields empty because starting index out bounds 
WITHOUT_CLASSIFICATION	 inspect the output type each key expression and remember the output columns 
WITHOUT_CLASSIFICATION	 try isrepeating path left input only nulls 
WITHOUT_CLASSIFICATION	 add direction token 
WITHOUT_CLASSIFICATION	 does any operator the tree stop the task from being converted conditional task 
WITHOUT_CLASSIFICATION	 single long key hash map optimized for vector map join 
WITHOUT_CLASSIFICATION	 grantinfo 
WITHOUT_CLASSIFICATION	 element for key byte hash table hashmap 
WITHOUT_CLASSIFICATION	 results 
WITHOUT_CLASSIFICATION	 map max nesting level one less because uses additional separator 
WITHOUT_CLASSIFICATION	 project can also generate constants need include them 
WITHOUT_CLASSIFICATION	 first argument charcount which consumed here 
WITHOUT_CLASSIFICATION	 the correct plugin 
WITHOUT_CLASSIFICATION	 hive doesnt have enum type were going treat them strings during the stage well check for enumness and 
WITHOUT_CLASSIFICATION	 lock used for synchronizing the state transition and its associated resource releases 
WITHOUT_CLASSIFICATION	 get all parents reduce sink 
WITHOUT_CLASSIFICATION	 give some time then dont delay shutdown too much 
WITHOUT_CLASSIFICATION	 column cannot push the predicate 
WITHOUT_CLASSIFICATION	 deallocated now will deallocated later 
WITHOUT_CLASSIFICATION	 group key 
WITHOUT_CLASSIFICATION	 field present partition but not table 
WITHOUT_CLASSIFICATION	 count cares not about nulls nor selection 
WITHOUT_CLASSIFICATION	 for clause 
WITHOUT_CLASSIFICATION	 end cleanup 
WITHOUT_CLASSIFICATION	 nothing currently 
WITHOUT_CLASSIFICATION	 update col stats map with col stats for columns from left side 
WITHOUT_CLASSIFICATION	 functiontype 
WITHOUT_CLASSIFICATION	 small table hts but since its idempotent should 
WITHOUT_CLASSIFICATION	 unknown type 
WITHOUT_CLASSIFICATION	 the table different dfs than the partition 
WITHOUT_CLASSIFICATION	 remove the last 
WITHOUT_CLASSIFICATION	 for comparison purposes can scale away those digits and can not scale since that could overflow 
WITHOUT_CLASSIFICATION	 simplifies things just add default ones for partitions 
WITHOUT_CLASSIFICATION	 order 
WITHOUT_CLASSIFICATION	 nothing matched see comment top 
WITHOUT_CLASSIFICATION	 scaled value might not equal but after scaling should 
WITHOUT_CLASSIFICATION	 ignore nsoe because that means theres nothing drop 
WITHOUT_CLASSIFICATION	 pending task which not finishable 
WITHOUT_CLASSIFICATION	 recurse 
WITHOUT_CLASSIFICATION	 deprecated hive values that are keeping for backwards compatibility 
WITHOUT_CLASSIFICATION	 assuming this only being done for join keys result shouldnt have recursively check any nested child expressions because the result the expression should exist 
WITHOUT_CLASSIFICATION	 only fetch the table have listener that needs 
WITHOUT_CLASSIFICATION	 removed this becomes performance issue 
WITHOUT_CLASSIFICATION	 must reuse super infogetpassword not accessible 
WITHOUT_CLASSIFICATION	 the value before the offset make byte segment reference absolute 
WITHOUT_CLASSIFICATION	 dont have file cache will add this one 
WITHOUT_CLASSIFICATION	 support for null constant object 
WITHOUT_CLASSIFICATION	 not date 
WITHOUT_CLASSIFICATION	 reenable the node preempted 
WITHOUT_CLASSIFICATION	 deadlock possible extreme cases not handled this will detected heartbeat 
WITHOUT_CLASSIFICATION	 regardless acquired waiting one shared write cannot pass another 
WITHOUT_CLASSIFICATION	 not empty means are creating basework whose operator tree contains union operators this case need save these baseworks and remove 
WITHOUT_CLASSIFICATION	 need spill from write buffer disk 
WITHOUT_CLASSIFICATION	 replicating then the partition already existing means need replace maybe the destination ptns repllastid older than the replacements 
WITHOUT_CLASSIFICATION	 str empty string 
WITHOUT_CLASSIFICATION	 remove the limit operator 
WITHOUT_CLASSIFICATION	 only used bucket map join 
WITHOUT_CLASSIFICATION	 there should delta dirs plus base dir the location 
WITHOUT_CLASSIFICATION	 this call deleting partitions that are already missing from filesystem parameter deletedata set false msck doing clean hms for some reason the partition already 
WITHOUT_CLASSIFICATION	 either delayedresources delayedlocality with unknown requested host request for preemption theres none pending single preemption pending and this the next task assigned will assigned once that slot becomes available 
WITHOUT_CLASSIFICATION	 the mapfield test multiple level map definition 
WITHOUT_CLASSIFICATION	 use the colfam and colqual get the value 
WITHOUT_CLASSIFICATION	 get evaluator for simple field expression 
WITHOUT_CLASSIFICATION	 the output has some extra fields set them null 
WITHOUT_CLASSIFICATION	 get the total number columns selected and for each output column store the base table points for insert overwrite table select tkey tkey udftvalue tvalue from join tkey tkey and tkey tkey the following arrays are created table mapping 
WITHOUT_CLASSIFICATION	 there are separate configuration parameters control whether merge for maponly job 
WITHOUT_CLASSIFICATION	 put existing column new list make sure the right position 
WITHOUT_CLASSIFICATION	 can never have more than this elements 
WITHOUT_CLASSIFICATION	 loj join preserves lhs types 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 perform some sanity checks the arguments 
WITHOUT_CLASSIFICATION	 file operations failed 
WITHOUT_CLASSIFICATION	 leave this ctor around for backward compat 
WITHOUT_CLASSIFICATION	 the value will have already been set before were called dont overwrite 
WITHOUT_CLASSIFICATION	 reader using blocking socket interrupt 
WITHOUT_CLASSIFICATION	 agentinfo 
WITHOUT_CLASSIFICATION	 heartbeatcount 
WITHOUT_CLASSIFICATION	 later the properties have come from the partition opposed from the table order support versioning 
WITHOUT_CLASSIFICATION	 remove the tag from key coming out reducer and store separate variable 
WITHOUT_CLASSIFICATION	 this first section repeats the tests with large maxlength parameter 
WITHOUT_CLASSIFICATION	 creates objects recursive manner 
WITHOUT_CLASSIFICATION	 since left integer always some products here are not included 
WITHOUT_CLASSIFICATION	 change the value for the next instance 
WITHOUT_CLASSIFICATION	 though intnum handed byte hcat the map will emit 
WITHOUT_CLASSIFICATION	 temporary 
WITHOUT_CLASSIFICATION	 ensure that both the partitions are the complete list 
WITHOUT_CLASSIFICATION	 guaranteed that there only list within this list because reduce sink always brings down the bucketing cols single list 
WITHOUT_CLASSIFICATION	 base only deltas 
WITHOUT_CLASSIFICATION	 marker comment look stats read ops 
WITHOUT_CLASSIFICATION	 safety check are merging join operators and there are postfiltering conditions they cannot outer joins 
WITHOUT_CLASSIFICATION	 since were provided qualifier prefix only accept qualifiers that start with this prefix 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 not external table 
WITHOUT_CLASSIFICATION	 used only for insert events this the number rows held memory before flush invoked 
WITHOUT_CLASSIFICATION	 remove env var that would default child jvm use parents memory default child jvm would use default memory for hadoop client 
WITHOUT_CLASSIFICATION	 elements 
WITHOUT_CLASSIFICATION	 set state current thread 
WITHOUT_CLASSIFICATION	 could verify precisely write time but just approximate allocation time 
WITHOUT_CLASSIFICATION	 write element element from the list 
WITHOUT_CLASSIFICATION	 because already confirm that the stats accurate impossible that the column types have been changed while the column stats still accurate 
WITHOUT_CLASSIFICATION	 timeout 
WITHOUT_CLASSIFICATION	 get all the values from getxxx methods 
WITHOUT_CLASSIFICATION	 map string string 
WITHOUT_CLASSIFICATION	 add the token the clientugi for securely talking the metastore 
WITHOUT_CLASSIFICATION	 colaccessinfo set only case semanticanalyzer 
WITHOUT_CLASSIFICATION	 exceptional use case for avro 
WITHOUT_CLASSIFICATION	 lsquareindex expression 
WITHOUT_CLASSIFICATION	 there are grouping keys grouping sets cannot present 
WITHOUT_CLASSIFICATION	 tokenidentifier 
WITHOUT_CLASSIFICATION	 ifexists currently verified ddlsemanticanalyzer 
WITHOUT_CLASSIFICATION	 strip off the file type any dont make gzcopy 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the dests can have different nondistinct aggregations have iterate over all 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 there should valid txns newer list that are not also older all values oldinvalidids should also newinvalidids oldhwm newhwm then all ids between oldhwm newhwm should exist newinvalidtxns gap the sequence means committed txn newer list lists are not equivalent 
WITHOUT_CLASSIFICATION	 derby oracle 
WITHOUT_CLASSIFICATION	 noop 
WITHOUT_CLASSIFICATION	 test success exception caught 
WITHOUT_CLASSIFICATION	 the method will update vectorgroupbydesc 
WITHOUT_CLASSIFICATION	 add these values mixedup green null 
WITHOUT_CLASSIFICATION	 right pad longer strings with multibyte characters 
WITHOUT_CLASSIFICATION	 objectname 
WITHOUT_CLASSIFICATION	 rewrite logic change the collations field reference the new input 
WITHOUT_CLASSIFICATION	 set output column vector entry since have one output column the logical index 
WITHOUT_CLASSIFICATION	 more than digits 
WITHOUT_CLASSIFICATION	 third time 
WITHOUT_CLASSIFICATION	 get all parent tasks 
WITHOUT_CLASSIFICATION	 check the columns well value types the partition clause are valid 
WITHOUT_CLASSIFICATION	 setup timeouts for various services 
WITHOUT_CLASSIFICATION	 declared cursor 
WITHOUT_CLASSIFICATION	 fields populated from builder 
WITHOUT_CLASSIFICATION	 simple implementation for now currently parquet uses heap buffers 
WITHOUT_CLASSIFICATION	 start the cachedstore update service 
WITHOUT_CLASSIFICATION	 note returns decimal strings with more than digits 
WITHOUT_CLASSIFICATION	 this method should not synchronized can lead deadlocks since calls sync method meanwhile the scheduler could try updating states via synchronized method 
WITHOUT_CLASSIFICATION	 gets lock 
WITHOUT_CLASSIFICATION	 table properties not match currently not merge 
WITHOUT_CLASSIFICATION	 build map map the original filesinkoperator and the cloned filesinkoperators 
WITHOUT_CLASSIFICATION	 optional bytes historytext 
WITHOUT_CLASSIFICATION	 load data into table note filepath has local the hive server 
WITHOUT_CLASSIFICATION	 challenge how the math get this raw binary back our decimal form briefly for the middle and upper binary words convert the middleupper word into decimal long words and then multiply those the binary words power and add the multiply results into the result decimal longwords 
WITHOUT_CLASSIFICATION	 since only have one table with data dont compact tables 
WITHOUT_CLASSIFICATION	 peel off otherwise return itself 
WITHOUT_CLASSIFICATION	 not valid range add residual 
WITHOUT_CLASSIFICATION	 gets lock 
WITHOUT_CLASSIFICATION	 total characters byte length 
WITHOUT_CLASSIFICATION	 test with schema evolution and include 
WITHOUT_CLASSIFICATION	 also creates the root directory 
WITHOUT_CLASSIFICATION	 admin has already customized this list honor that 
WITHOUT_CLASSIFICATION	 try with supplementary characters 
WITHOUT_CLASSIFICATION	 the max column width too large reset max allowed column width 
WITHOUT_CLASSIFICATION	 gather references from original query this map from aliases references keep all references will need modify them after creating 
WITHOUT_CLASSIFICATION	 already verified that should have the rowid mapping 
WITHOUT_CLASSIFICATION	 add this task into task tree set all parent tasks 
WITHOUT_CLASSIFICATION	 this filter generated one predicates need not extracted 
WITHOUT_CLASSIFICATION	 two int one double two random 
WITHOUT_CLASSIFICATION	 since partval constant safe cast exprnodedesc its value should normalized format leading zero integer date format yyyymmdd etc 
WITHOUT_CLASSIFICATION	 select count 
WITHOUT_CLASSIFICATION	 essential properties that shouldnt overridden users 
WITHOUT_CLASSIFICATION	 catalogname 
WITHOUT_CLASSIFICATION	 dont generate for nullsafes 
WITHOUT_CLASSIFICATION	 temporary map only create one partition context entry 
WITHOUT_CLASSIFICATION	 for minor compaction there progress report and dont filter deltas 
WITHOUT_CLASSIFICATION	 this the functionality matches 
WITHOUT_CLASSIFICATION	 gets lock 
WITHOUT_CLASSIFICATION	 add writertimezone property file metadata 
WITHOUT_CLASSIFICATION	 add all function arguments map 
WITHOUT_CLASSIFICATION	 set for single threading 
WITHOUT_CLASSIFICATION	 just write out the value asis 
WITHOUT_CLASSIFICATION	 now read the relative offset next record next record always before the 
WITHOUT_CLASSIFICATION	 this may happen for queries like select source table 
WITHOUT_CLASSIFICATION	 can now retry adding keyvalue into hash which flushed but for simplicity just forward them 
WITHOUT_CLASSIFICATION	 create lazybinary initialed with inputba 
WITHOUT_CLASSIFICATION	 weird but need placeholder otherwise rename cannot move file the right place 
WITHOUT_CLASSIFICATION	 check for recreated 
WITHOUT_CLASSIFICATION	 catch make sure locks can released when the query cancelled 
WITHOUT_CLASSIFICATION	 read tbl via cachedstore 
WITHOUT_CLASSIFICATION	 should never reach here unless there were failed tasks 
WITHOUT_CLASSIFICATION	 todo some thoughts here have current todo move some these methods over messagefactory instead being here can override them but before move them over should keep the following mind should return iterables not lists that makes sure that can memorysafe when implementing rather than forcing ourselves down path wherein returning list part our interface and then people use size somesuch which makes need materialize the entire list and not change also returning iterables allows things like iterablestransform for some these should not have magic names like tableobjjson because that breaks expectation couple things firstly that serialization format although that fine for this jsonmessagefactory and secondly that makes just have number mappings one for each obj type and sometimes the case with alter have multiples also any eventspecific item belongs that event message event itself opposed the factory its okay have utility accessor methods here that are used each the messages provide accessors adding couple those here 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 ids used ensure two taskinfos are different without using the underlying task instance 
WITHOUT_CLASSIFICATION	 lazystring has socalled null sequence the value empty string not 
WITHOUT_CLASSIFICATION	 default return earliest possible state 
WITHOUT_CLASSIFICATION	 they are different throw error 
WITHOUT_CLASSIFICATION	 check for timed out remote workers 
WITHOUT_CLASSIFICATION	 returns the result file object which will contain the query results 
WITHOUT_CLASSIFICATION	 try merge multiple ranges together 
WITHOUT_CLASSIFICATION	 its void change the type byte because once the types are run through getcommonclass byte and any other type will resolve type 
WITHOUT_CLASSIFICATION	 methods setreset caller checker 
WITHOUT_CLASSIFICATION	 semishared for updatedelete 
WITHOUT_CLASSIFICATION	 extrapolation not needed 
WITHOUT_CLASSIFICATION	 colstats 
WITHOUT_CLASSIFICATION	 the vertex that this operator output 
WITHOUT_CLASSIFICATION	 assume here nobody will try get session before open returns 
WITHOUT_CLASSIFICATION	 create list topop nodes 
WITHOUT_CLASSIFICATION	 implicitly handles users providing invalid authorizations 
WITHOUT_CLASSIFICATION	 storage instantiated the constructor 
WITHOUT_CLASSIFICATION	 the list expressions after select select transform 
WITHOUT_CLASSIFICATION	 quotes use for quoting tables where necessary 
WITHOUT_CLASSIFICATION	 handle all the getreuse requests wont actually give out anything here but merely map all the requests and place them appropriate order pool queues the only exception the reuse without queue contention can granted immediately cant reuse the session immediately will convert the reuse normal get because 
WITHOUT_CLASSIFICATION	 for orc parquet all the following statements are the same analyze table partition compute statistics analyze table partition compute statistics noscan 
WITHOUT_CLASSIFICATION	 group stats colname for each partition 
WITHOUT_CLASSIFICATION	 fingerprint 
WITHOUT_CLASSIFICATION	 run this optimization early since expanding the operator pipeline 
WITHOUT_CLASSIFICATION	 the inputops this vertex 
WITHOUT_CLASSIFICATION	 myenumstringlistmap 
WITHOUT_CLASSIFICATION	 read database auth calls for each authorization provider 
WITHOUT_CLASSIFICATION	 returning either vectorized nonvectorized reader from the same call requires breaking 
WITHOUT_CLASSIFICATION	 need fill mapwork with local work and bucket information for smb join 
WITHOUT_CLASSIFICATION	 count how many fields are there 
WITHOUT_CLASSIFICATION	 prevents under subscription 
WITHOUT_CLASSIFICATION	 this may happen are not projecting any column from current operator think count where are projecting rows without any columns such case estimate empty row size empty java object 
WITHOUT_CLASSIFICATION	 use large prime number seed the random number generator javas random number generator uses the linear congruential generator generate random numbers using the following recurrence relation mod where the seed java implementation uses this problematic because not prime number and hence the set numbers from dont form finite field these numbers dont come from finite field any give and may not pair wise independent however empirically passing prime numbers seeds seems work better than when passing composite numbers seeds ideally javas random should pick such that prime 
WITHOUT_CLASSIFICATION	 initialize indexbuilder for deleteevents hive 
WITHOUT_CLASSIFICATION	 sampling predicates can merged with predicates from children because ppdppr already applied but clarify the intention sampling just skips merging 
WITHOUT_CLASSIFICATION	 dont make any calls but catalog calls until the catalog has been created just told 
WITHOUT_CLASSIFICATION	 relative end position the windowing can negative 
WITHOUT_CLASSIFICATION	 isolate query conf 
WITHOUT_CLASSIFICATION	 show partition level privileges 
WITHOUT_CLASSIFICATION	 the way this works such originalcolumnnames the equivalent getneededcolumns from tsop they are assumed the same order the columns orc file and they are assumed equivalent the columns includedcolumns because was generated from the same column list some point the past minus the subtype columns therefore when thru all the top level orc file columns that are included order they match originalcolumnnames this way not depend names stored inside orc for sarg leaf column name resolution see mapsargcolumns method 
WITHOUT_CLASSIFICATION	 create after dropping needed events 
WITHOUT_CLASSIFICATION	 this submit blocks background threads are available run this operation 
WITHOUT_CLASSIFICATION	 easier read logs and for assumption done replication flow 
WITHOUT_CLASSIFICATION	 load data 
WITHOUT_CLASSIFICATION	 expect all the levels have items 
WITHOUT_CLASSIFICATION	 returnpath and hivetestmode bail 
WITHOUT_CLASSIFICATION	 use the flajoletmartin estimator estimate the number distinct valuesfm uses the location the least significant zero estimate logphindvs 
WITHOUT_CLASSIFICATION	 use maxmin range ndv gets scaled selectivity 
WITHOUT_CLASSIFICATION	 add uri entity for transform script script assumed local unless downloadable 
WITHOUT_CLASSIFICATION	 tablecat tableschem tablename columnname datatype typename columnsize bufferlength unused decimaldigits numprecradix 
WITHOUT_CLASSIFICATION	 todo could actually store bit flag ref indicating whether this hash match probe and the former case use hash bits for first few resizes int hashcodeorpart oldslot newhashbitcount 
WITHOUT_CLASSIFICATION	 input aliases this for join used for ppd 
WITHOUT_CLASSIFICATION	 close the previous fsp longer needed 
WITHOUT_CLASSIFICATION	 decide skewed value directory selection 
WITHOUT_CLASSIFICATION	 minidfscluster litters files and folders all over the place 
WITHOUT_CLASSIFICATION	 new logic 
WITHOUT_CLASSIFICATION	 sessionscope compile lock 
WITHOUT_CLASSIFICATION	 for addition for subtraction 
WITHOUT_CLASSIFICATION	 helper class set chunkedinputoutput stream for testing 
WITHOUT_CLASSIFICATION	 merge currtask from multiple topops 
WITHOUT_CLASSIFICATION	 tblname can null cases helper being used higher abstraction level such with datbases 
WITHOUT_CLASSIFICATION	 one cannot simply reuse the session there are other queries waiting maintain fairness well try take query slot instantly and that fails well return this session back the pool and give the user new session later 
WITHOUT_CLASSIFICATION	 array level 
WITHOUT_CLASSIFICATION	 needs set these values should the work detangle this 
WITHOUT_CLASSIFICATION	 test for aborted transactions 
WITHOUT_CLASSIFICATION	 map from new tags indices children demuxoperator the first operator the 
WITHOUT_CLASSIFICATION	 helper function create vertex from mapwork 
WITHOUT_CLASSIFICATION	 bgenjjtree constmapcontents 
WITHOUT_CLASSIFICATION	 the table has implemented the project the obvious way creating project with fields strip away and create our own project with one field 
WITHOUT_CLASSIFICATION	 escaped byte unescape 
WITHOUT_CLASSIFICATION	 new merge 
WITHOUT_CLASSIFICATION	 are going use this counter pseudorandom number for the start the search this avoid churning the beginning the arena all the time 
WITHOUT_CLASSIFICATION	 cascade only occurs table level then cascade partition level 
WITHOUT_CLASSIFICATION	 remember any matching rows matchs matchsize the end the loop selected batchsize will represent both matching and nonmatching rows for outer join only deferred rows will have been removed from selected 
WITHOUT_CLASSIFICATION	 pool min 
WITHOUT_CLASSIFICATION	 check for nulls just safe 
WITHOUT_CLASSIFICATION	 bgenjjtree function 
WITHOUT_CLASSIFICATION	 todo check view references too 
WITHOUT_CLASSIFICATION	 special handling for count similar 
WITHOUT_CLASSIFICATION	 index has primitive 
WITHOUT_CLASSIFICATION	 get all partitions 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 parent current pipeline 
WITHOUT_CLASSIFICATION	 generate the mapside groupbyoperator for the query block the new groupbyoperator will child the inputoperatorinfo param mode the mode the aggregation hash param not null this function will store the mapping from aggregation stringtree the this parameter can used the nextstage groupby aggregations return the new groupbyoperator 
WITHOUT_CLASSIFICATION	 meanwhile the init succeeds 
WITHOUT_CLASSIFICATION	 save away the original ast 
WITHOUT_CLASSIFICATION	 end serves final separator 
WITHOUT_CLASSIFICATION	 safely sorting 
WITHOUT_CLASSIFICATION	 logger with int base 
WITHOUT_CLASSIFICATION	 not should value 
WITHOUT_CLASSIFICATION	 dont try operate with less than minsize allocator space will just give you grief 
WITHOUT_CLASSIFICATION	 get the number retries 
WITHOUT_CLASSIFICATION	 isexternal set false here can overwritten the import stmt 
WITHOUT_CLASSIFICATION	 this map type 
WITHOUT_CLASSIFICATION	 truncate table the wrong catalog 
WITHOUT_CLASSIFICATION	 zero special case 
WITHOUT_CLASSIFICATION	 unique key the filterinputrel 
WITHOUT_CLASSIFICATION	 already visited 
WITHOUT_CLASSIFICATION	 allocate map bucket grouped splits 
WITHOUT_CLASSIFICATION	 see the javadoc and 
WITHOUT_CLASSIFICATION	 when comparing the compressedowid the one with the lesser value smaller 
WITHOUT_CLASSIFICATION	 materialized views 
WITHOUT_CLASSIFICATION	 the name the hive column 
WITHOUT_CLASSIFICATION	 update partition column info descriptor 
WITHOUT_CLASSIFICATION	 current code version datas version datas version 
WITHOUT_CLASSIFICATION	 need flatten 
WITHOUT_CLASSIFICATION	 this can reasonable for empty txn startcommit readonly txn also iud with that didnt match any rows 
WITHOUT_CLASSIFICATION	 impossible get ranges for row aaa and row bbb 
WITHOUT_CLASSIFICATION	 whether the subdirectory has any file 
WITHOUT_CLASSIFICATION	 rename the partition directory not external table 
WITHOUT_CLASSIFICATION	 create the mapping between the output the old correlation rel 
WITHOUT_CLASSIFICATION	 scale length value 
WITHOUT_CLASSIFICATION	 have connection error the jdo connection url hook might 
WITHOUT_CLASSIFICATION	 with this option were assuming that the external application using the jdbc driver has done jaas kerberos login already 
WITHOUT_CLASSIFICATION	 naked 
WITHOUT_CLASSIFICATION	 setting the comparison greater the search should use the block 
WITHOUT_CLASSIFICATION	 output type hiveintervaldaytime 
WITHOUT_CLASSIFICATION	 these members hold the current value that was read when readnextfield return false 
WITHOUT_CLASSIFICATION	 add filters for each the uris supported templeton added the entire substructure using the mapreduce notification cannot give the callback templeton secure mode this because mapreduce does not use secure credentials for callbacks jetty would fail the request unauthorized 
WITHOUT_CLASSIFICATION	 turn off clientside authorization 
WITHOUT_CLASSIFICATION	 required required required required required required required required required required 
WITHOUT_CLASSIFICATION	 foreigntablename 
WITHOUT_CLASSIFICATION	 handled later only struct will supported 
WITHOUT_CLASSIFICATION	 this starts the reader the background 
WITHOUT_CLASSIFICATION	 nonjavadoc serializes one int part into the given link bytebuffer considering twos complement for negatives 
WITHOUT_CLASSIFICATION	 the sorted columns cant all found the values then the data only sorted the columns seen until now 
WITHOUT_CLASSIFICATION	 for each bucket file only keep its base files and store into new list 
WITHOUT_CLASSIFICATION	 wait until either all messages are processed maximum time limit reached 
WITHOUT_CLASSIFICATION	 this time fails when try load the foreign key constraints all other constraints are loaded 
WITHOUT_CLASSIFICATION	 create the additional vectorization ptf information needed the vectorptfoperator during execution 
WITHOUT_CLASSIFICATION	 there anything check here 
WITHOUT_CLASSIFICATION	 combine all predicates into single expression 
WITHOUT_CLASSIFICATION	 lru cache using linked hash map 
WITHOUT_CLASSIFICATION	 throwexception 
WITHOUT_CLASSIFICATION	 get groupby keys and store reducekeys 
WITHOUT_CLASSIFICATION	 bgenjjtree async 
WITHOUT_CLASSIFICATION	 todo make work for jdk use cleanerutil from 
WITHOUT_CLASSIFICATION	 serdeinfo 
WITHOUT_CLASSIFICATION	 timeout before reset 
WITHOUT_CLASSIFICATION	 probably the app does not exist 
WITHOUT_CLASSIFICATION	 this should not happen cannot merge 
WITHOUT_CLASSIFICATION	 apply this optimization the input query there cannot exist any order bysort clause thus existsordering should false there cannot exist any distribute clause thus existspartitioning should false there cannot exist any cluster clause thus 
WITHOUT_CLASSIFICATION	 create table with smallinttinyint columns load data and query from hive 
WITHOUT_CLASSIFICATION	 load conf files 
WITHOUT_CLASSIFICATION	 construct pattern the form where partval either the escaped partition value given input regex the form this works because the and separating key names and partition keyvalues are not escaped 
WITHOUT_CLASSIFICATION	 transport would have got closed via clientshutdown dont need this but just case make this call 
WITHOUT_CLASSIFICATION	 fetchfirst fetch again from the same operation handle with fetchfirst orientation 
WITHOUT_CLASSIFICATION	 guaranteed flag inconsistent based heartbeat another message should send 
WITHOUT_CLASSIFICATION	 note for now dont actually pass the queryforcbo cbo because accepts not ast and can also access all the private stuff rely the fact that cbo ignores the unknown tokens create table destination the query otherwise 
WITHOUT_CLASSIFICATION	 match multijoin join 
WITHOUT_CLASSIFICATION	 missing database the query 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 case the servers idletimeout set lower value might close its side 
WITHOUT_CLASSIFICATION	 init closeable utils case register not called see hive 
WITHOUT_CLASSIFICATION	 where null same where false 
WITHOUT_CLASSIFICATION	 the dispatcher fires the processor corresponding the closest matching rule and passes the context along 
WITHOUT_CLASSIFICATION	 typically targettmp typically target 
WITHOUT_CLASSIFICATION	 return further processing needed 
WITHOUT_CLASSIFICATION	 once the lines the log file have been fed into the errorheuristics see they have detected anything any has record what errorandsolution gave can later return the most 
WITHOUT_CLASSIFICATION	 type checking and implicit type conversion for join keys 
WITHOUT_CLASSIFICATION	 supported 
WITHOUT_CLASSIFICATION	 recursively remove task from its children tasks this task doesnt have any parent task 
WITHOUT_CLASSIFICATION	 this plugable policy chose the candidate mapjoin table for converting join sort merge join the policy can decide the big table position some the existing policies decide the big table based size position the tables 
WITHOUT_CLASSIFICATION	 true prune 
WITHOUT_CLASSIFICATION	 this will return null the metastore not being accessed from metastore thrift server the ttransport being used connect not instance tsocket kereberos 
WITHOUT_CLASSIFICATION	 filter 
WITHOUT_CLASSIFICATION	 adjust right collation 
WITHOUT_CLASSIFICATION	 aggregation buffer methods wrap aggregation buffer 
WITHOUT_CLASSIFICATION	 the partitions used 
WITHOUT_CLASSIFICATION	 write delta file partition 
WITHOUT_CLASSIFICATION	 set hivelockmgr null just case this invalid manager got set next querys ctx 
WITHOUT_CLASSIFICATION	 session string supposed unique its got some reasonable size 
WITHOUT_CLASSIFICATION	 put records into compactionqueue and nothing 
WITHOUT_CLASSIFICATION	 last split 
WITHOUT_CLASSIFICATION	 location json file 
WITHOUT_CLASSIFICATION	 closed state not interesting state before finished error 
WITHOUT_CLASSIFICATION	 distributed attribute with distinct values 
WITHOUT_CLASSIFICATION	 case failure send back whatever constructed far which would from the appreport 
WITHOUT_CLASSIFICATION	 load the version stored the metastore 
WITHOUT_CLASSIFICATION	 mapjoin 
WITHOUT_CLASSIFICATION	 create the parents first 
WITHOUT_CLASSIFICATION	 set the default 
WITHOUT_CLASSIFICATION	 store partition key expr mapwork 
WITHOUT_CLASSIFICATION	 set the extra fields null 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 are doing this both load table and load partitions 
WITHOUT_CLASSIFICATION	 using vint instead bytes parse the first byte vintvlong determine the number bytes 
WITHOUT_CLASSIFICATION	 poll the tasks see which one completed 
WITHOUT_CLASSIFICATION	 generate column access stats required wait until column pruning 
WITHOUT_CLASSIFICATION	 use ddlexclusive cause lock prevent races between concurrent add partition calls with not exists this concurrent calls add the same partition may both add data since for transactional tables creating partition metadata and moving data there are separate actions 
WITHOUT_CLASSIFICATION	 null out the remaining columns 
WITHOUT_CLASSIFICATION	 endreason shows other for containertimeout 
WITHOUT_CLASSIFICATION	 first calls resultsetnext should return true 
WITHOUT_CLASSIFICATION	 parent statistics null then that branch the tree not walked yet dont update the stats until all branches are walked 
WITHOUT_CLASSIFICATION	 bgenjjtree 
WITHOUT_CLASSIFICATION	 retrieve stats from metastore 
WITHOUT_CLASSIFICATION	 aborted terminal state nothing about the txn can change after that read committed sufficient 
WITHOUT_CLASSIFICATION	 while scan the css also get the densityavg lowerbound and 
WITHOUT_CLASSIFICATION	 processing directories 
WITHOUT_CLASSIFICATION	 test set random multiplications high precision 
WITHOUT_CLASSIFICATION	 enable read authorization metastore 
WITHOUT_CLASSIFICATION	 fetch the xml tag dogcxxxdogc 
WITHOUT_CLASSIFICATION	 destination hash partition has just spilled 
WITHOUT_CLASSIFICATION	 validate mainly for includes excludes working they should 
WITHOUT_CLASSIFICATION	 could rewrite into subquery 
WITHOUT_CLASSIFICATION	 check there are ioexceptions 
WITHOUT_CLASSIFICATION	 rightinputrel has this shape filter references corvar 
WITHOUT_CLASSIFICATION	 total free memory maxmemory used memory 
WITHOUT_CLASSIFICATION	 notice that command line options take precedence over the deprecated old style naked args 
WITHOUT_CLASSIFICATION	 update top project positions 
WITHOUT_CLASSIFICATION	 second input parameter but column 
WITHOUT_CLASSIFICATION	 refer flajoletmartin for the value phi 
WITHOUT_CLASSIFICATION	 doublecheck 
WITHOUT_CLASSIFICATION	 process singlecolumn long leftsemi join vectorized row batch 
WITHOUT_CLASSIFICATION	 range register index bits 
WITHOUT_CLASSIFICATION	 empty keyset basically 
WITHOUT_CLASSIFICATION	 from zookeepermain private method 
WITHOUT_CLASSIFICATION	 lump all partitions outside the tablepath into one partspec 
WITHOUT_CLASSIFICATION	 drop table from the wrong catalog 
WITHOUT_CLASSIFICATION	 the table name can potentially dotformat one with column names specified part the table name abc where column and field the objectcolumn etc for authorization purposes should use only the first part the dotted name format 
WITHOUT_CLASSIFICATION	 the following fields specify the criteria objects for this priv required 
WITHOUT_CLASSIFICATION	 the session will with the new mapping check 
WITHOUT_CLASSIFICATION	 first lets try connecting using the last successful url that fails then error out 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 set alias size mapping this can used determine one table chosen big table whats the total size left tables which 
WITHOUT_CLASSIFICATION	 set some conf parameters 
WITHOUT_CLASSIFICATION	 empty regex should one warn message 
WITHOUT_CLASSIFICATION	 invariant ksize ksize 
WITHOUT_CLASSIFICATION	 generate split strategy for nonacid schema original files any 
WITHOUT_CLASSIFICATION	 add the group expressions 
WITHOUT_CLASSIFICATION	 delete unneeded directories that were replaced other ones via reopen 
WITHOUT_CLASSIFICATION	 verify output 
WITHOUT_CLASSIFICATION	 class 
WITHOUT_CLASSIFICATION	 which affects the locality matching 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 return false any input column nonrepeating otherwise true this returns false all the arguments are constant there are zero arguments possible future optimization set the output isrepeating for cases allconstant arguments for deterministic functions 
WITHOUT_CLASSIFICATION	 modifying the meastoreuri property 
WITHOUT_CLASSIFICATION	 output job properties 
WITHOUT_CLASSIFICATION	 start with the destination bucketedsorted position the source the column corresponding that position key which maps column which also bucketedsorted into the same 
WITHOUT_CLASSIFICATION	 vectorization should the last optimization because doesnt modify the plan any operators makes very low level transformation the expressions 
WITHOUT_CLASSIFICATION	 analyze create view command 
WITHOUT_CLASSIFICATION	 matches only groupbyoperators which are reducers rather than map group operators 
WITHOUT_CLASSIFICATION	 check the local work 
WITHOUT_CLASSIFICATION	 this changed under tmp dir not sure this will have any effect 
WITHOUT_CLASSIFICATION	 just pass everything that syntax supports 
WITHOUT_CLASSIFICATION	 spill 
WITHOUT_CLASSIFICATION	 the lock may have multiple components dbhivelock hence need check for each them 
WITHOUT_CLASSIFICATION	 check that the error code present the error description 
WITHOUT_CLASSIFICATION	 encountered column field which cannot found sources 
WITHOUT_CLASSIFICATION	 todo hive 
WITHOUT_CLASSIFICATION	 txnid 
WITHOUT_CLASSIFICATION	 mgby already contains key remove distinct and change all the others 
WITHOUT_CLASSIFICATION	 todo allow defaults for scheduling policy 
WITHOUT_CLASSIFICATION	 someone using this buffer eventually will evicted 
WITHOUT_CLASSIFICATION	 cost cost writing intermediary results local cost reading from local for transferring join 
WITHOUT_CLASSIFICATION	 this time completes adding just constraints for table 
WITHOUT_CLASSIFICATION	 for uncompressed case need some special processing before read basically are trying create artificial consistent ranges cache there are cbs uncompressed file the end this processing the list would contain either cache buffers buffers allocated and not cached are only reading parts the data for some ranges and dont want cache both are represented 
WITHOUT_CLASSIFICATION	 recursively going into objectinspector structure 
WITHOUT_CLASSIFICATION	 set global separator member next level 
WITHOUT_CLASSIFICATION	 generate split for any buckets that werent covered this happens the case where bucket just has deltas and 
WITHOUT_CLASSIFICATION	 introduce some randomness and avoid hammering the metastore the same time same logic dbtxnmanager 
WITHOUT_CLASSIFICATION	 convert everything writable types arguments are the same but objectinspectors are different 
WITHOUT_CLASSIFICATION	 first separated substring would txnid and the rest are 
WITHOUT_CLASSIFICATION	 rerun insert into but this time new partitions will created there will violation 
WITHOUT_CLASSIFICATION	 object that added the cache 
WITHOUT_CLASSIFICATION	 multi group optimization specific operators 
WITHOUT_CLASSIFICATION	 drop case leftover from unsuccessful run 
WITHOUT_CLASSIFICATION	 string 
WITHOUT_CLASSIFICATION	 because null literal doesnt work for all dbs 
WITHOUT_CLASSIFICATION	 get new location 
WITHOUT_CLASSIFICATION	 were looking for the udf with the smallest maximum numeric type 
WITHOUT_CLASSIFICATION	 partition can archived during recovery 
WITHOUT_CLASSIFICATION	 encoding still sparse use linear counting with increase accuracy use pprime bits for register index 
WITHOUT_CLASSIFICATION	 test the udf adaptor for generic udf opposed legacy udf 
WITHOUT_CLASSIFICATION	 annotation tree with statistics 
WITHOUT_CLASSIFICATION	 add the reducer 
WITHOUT_CLASSIFICATION	 flush here the memory usage too high after that have the entire 
WITHOUT_CLASSIFICATION	 are provided with prefix 
WITHOUT_CLASSIFICATION	 create external table 
WITHOUT_CLASSIFICATION	 test that adding jar the remote context makes show the classpath 
WITHOUT_CLASSIFICATION	 open txn which allocate write and remain open state 
WITHOUT_CLASSIFICATION	 map mode run iff work map work 
WITHOUT_CLASSIFICATION	 xmx specified 
WITHOUT_CLASSIFICATION	 the current txn either open aborted state 
WITHOUT_CLASSIFICATION	 stub out mocked helper instance 
WITHOUT_CLASSIFICATION	 prep 
WITHOUT_CLASSIFICATION	 call reducesinkoperator with new input inspector 
WITHOUT_CLASSIFICATION	 set timezone based user timezone origin not already set default hive time semantics consider user timezone 
WITHOUT_CLASSIFICATION	 reconstruct the sparse map from delta encoded and varint input stream 
WITHOUT_CLASSIFICATION	 for each field 
WITHOUT_CLASSIFICATION	 stats are not available just assume its useful edge 
WITHOUT_CLASSIFICATION	 lets see doubles work 
WITHOUT_CLASSIFICATION	 java 
WITHOUT_CLASSIFICATION	 currrecord numrecords have already fetched the top numrecords 
WITHOUT_CLASSIFICATION	 alias operator map from the semantic analyzer 
WITHOUT_CLASSIFICATION	 output batch scratch columns for the small table portion 
WITHOUT_CLASSIFICATION	 all the parents are locked shared mode 
WITHOUT_CLASSIFICATION	 backup task 
WITHOUT_CLASSIFICATION	 connect using token via beeline with inputstream 
WITHOUT_CLASSIFICATION	 this test session temporary files are cleaned after hive 
WITHOUT_CLASSIFICATION	 tokdestination toktab toktabname materializationname 
WITHOUT_CLASSIFICATION	 the caller needs gurrantee that they are the same type based numbitvectors 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 action 
WITHOUT_CLASSIFICATION	 once sessionstate for thread set clidriver picks conf from 
WITHOUT_CLASSIFICATION	 minute granularity 
WITHOUT_CLASSIFICATION	 max 
WITHOUT_CLASSIFICATION	 only first call throws exception 
WITHOUT_CLASSIFICATION	 isdead only set internally this class link markdeadboolean will abort all remaining txns make this noop make sure that wellbehaved client that calls aborttransaction error doesnt get misleading errors 
WITHOUT_CLASSIFICATION	 handle secure connection specified 
WITHOUT_CLASSIFICATION	 test normal retriable client 
WITHOUT_CLASSIFICATION	 walk relnode graph note from where gby nodes 
WITHOUT_CLASSIFICATION	 take look see escaped 
WITHOUT_CLASSIFICATION	 updating several local structures 
WITHOUT_CLASSIFICATION	 unwrap the tuple 
WITHOUT_CLASSIFICATION	 now rewrite the plan projecta all lhs plus transformed original projections replacing references count with case statement correlatorleft correlation condition true leftinputrel aggregate groupby agg agg 
WITHOUT_CLASSIFICATION	 all the inputs for the tez processor 
WITHOUT_CLASSIFICATION	 write fourth byte header 
WITHOUT_CLASSIFICATION	 deserializes bit decimals the maximum bit precision decimal digits note major assumption the input decimal has already been bounds checked and least has precision not bounds check here for better performance 
WITHOUT_CLASSIFICATION	 unregister from the amreporter since the task now running 
WITHOUT_CLASSIFICATION	 just trigger auto creation needed metastore tables 
WITHOUT_CLASSIFICATION	 not partitioned 
WITHOUT_CLASSIFICATION	 txnid 
WITHOUT_CLASSIFICATION	 make sure not the default that are testing tblproperties indeed propagate 
WITHOUT_CLASSIFICATION	 this assumes that the llap cluster and session are both running under user 
WITHOUT_CLASSIFICATION	 column stats will inaccurate 
WITHOUT_CLASSIFICATION	 remove map extra level 
WITHOUT_CLASSIFICATION	 make sure reduce task environment points 
WITHOUT_CLASSIFICATION	 this what expect disk ekoifmanwarehouse ekoifman tree nonacidpart nonacidpart hiveunionsubdir hiveunionsubdir hiveunionsubdir directories files 
WITHOUT_CLASSIFICATION	 sum input and output are decimal any mode partial partial final complete 
WITHOUT_CLASSIFICATION	 finally add the event broadcast operator 
WITHOUT_CLASSIFICATION	 means that expression cant pushed either because value group 
WITHOUT_CLASSIFICATION	 convert input arguments text necessary 
WITHOUT_CLASSIFICATION	 table write already allocated for the given transaction then just use 
WITHOUT_CLASSIFICATION	 once have decided the map join the tree would transform from join mapjoin big table small table for spark 
WITHOUT_CLASSIFICATION	 read from the new table 
WITHOUT_CLASSIFICATION	 rest the types date char varchar etc are already registered 
WITHOUT_CLASSIFICATION	 traverse the leaf nodes the tree the stack entries indicate the existing leaf 
WITHOUT_CLASSIFICATION	 the byte length the scratch byte array that needs passed 
WITHOUT_CLASSIFICATION	 event 
WITHOUT_CLASSIFICATION	 reset the aggregations for distincts optimization with sortingbucketing perform partial aggregation 
WITHOUT_CLASSIFICATION	 config logj with customized files 
WITHOUT_CLASSIFICATION	 the new session will also now 
WITHOUT_CLASSIFICATION	 get the serialization object and the class being deserialized 
WITHOUT_CLASSIFICATION	 kill the second ctrlc 
WITHOUT_CLASSIFICATION	 let the job retry several times which eventually lead failure 
WITHOUT_CLASSIFICATION	 prscrscgbyr map aggregation prscgbyrcomplete revert expressions cgbyr that crs 
WITHOUT_CLASSIFICATION	 check dpp branches are equal 
WITHOUT_CLASSIFICATION	 configure http client for kerberospassword based authentication 
WITHOUT_CLASSIFICATION	 allow for improved schemes 
WITHOUT_CLASSIFICATION	 already called verify there input split thus for groupbyoperator summary row set finaldirs and add dummy split here 
WITHOUT_CLASSIFICATION	 discard all the locked blocks 
WITHOUT_CLASSIFICATION	 closeop can overriden 
WITHOUT_CLASSIFICATION	 triggername 
WITHOUT_CLASSIFICATION	 its possible that user session closed while creating spark client 
WITHOUT_CLASSIFICATION	 zone part 
WITHOUT_CLASSIFICATION	 acid and tables support load data with transactional semantics this will allow load data txn assuming can determine the target suitable table type 
WITHOUT_CLASSIFICATION	 all the operators need initialized before process 
WITHOUT_CLASSIFICATION	 bug throws exception 
WITHOUT_CLASSIFICATION	 reduce side gby dont know the grouping set was present not get from map side gby 
WITHOUT_CLASSIFICATION	 secure the web server with kerberos 
WITHOUT_CLASSIFICATION	 use the new faster hash code since are hashing memory objects 
WITHOUT_CLASSIFICATION	 should not allowed after query complete received 
WITHOUT_CLASSIFICATION	 whether pattern sel gby dpp 
WITHOUT_CLASSIFICATION	 float 
WITHOUT_CLASSIFICATION	 otherwise return the expression 
WITHOUT_CLASSIFICATION	 txnid 
WITHOUT_CLASSIFICATION	 open the original path weve been given and find the list original buckets 
WITHOUT_CLASSIFICATION	 determine input type info 
WITHOUT_CLASSIFICATION	 first get the appropriate field schema for this field 
WITHOUT_CLASSIFICATION	 call the actual operator initialization function 
WITHOUT_CLASSIFICATION	 from 
WITHOUT_CLASSIFICATION	 the set virtual columns that vectorized readers may support 
WITHOUT_CLASSIFICATION	 verify that environment context has statsgenerated set task 
WITHOUT_CLASSIFICATION	 extract innerrecord field refs 
WITHOUT_CLASSIFICATION	 pmod calculation can overflow based the type arguments casting the arguments according outputtypeinfo that the results match with genericudfposmod implementation 
WITHOUT_CLASSIFICATION	 change connector ssl used 
WITHOUT_CLASSIFICATION	 skip leading zeroes word 
WITHOUT_CLASSIFICATION	 need rollback because did select that acquired locks but didnt actually update anything also may have locked some locks acquired that now want not acquire its rollback because once see one wait were done wont look for more only rollback savepoint because want commit our heartbeat changes 
WITHOUT_CLASSIFICATION	 teardown the cluster 
WITHOUT_CLASSIFICATION	 since may split current task use preorder walker 
WITHOUT_CLASSIFICATION	 setstring can override this 
WITHOUT_CLASSIFICATION	 check whether the have the same schema 
WITHOUT_CLASSIFICATION	 put the exe context into all the operators 
WITHOUT_CLASSIFICATION	 first row determines isgroupresultnull and decimal firstvalue stream fill result repeated 
WITHOUT_CLASSIFICATION	 first get the utc midnight for that day which always exists small island sanity 
WITHOUT_CLASSIFICATION	 since old orc format doesnt support binary statistics 
WITHOUT_CLASSIFICATION	 bytes are same case was longer 
WITHOUT_CLASSIFICATION	 create test tables with partitions 
WITHOUT_CLASSIFICATION	 delete delta file with delete events 
WITHOUT_CLASSIFICATION	 save 
WITHOUT_CLASSIFICATION	 failures will not retried avoid fork exec running sysctl command 
WITHOUT_CLASSIFICATION	 decompose the incoming text row into fields 
WITHOUT_CLASSIFICATION	 was internal column lets try get name from columnexprmap 
WITHOUT_CLASSIFICATION	 deserializes from string fastbitset creates object and returns 
WITHOUT_CLASSIFICATION	 case column stats grouping sets 
WITHOUT_CLASSIFICATION	 read the relative offset word the beginning and beyond records 
WITHOUT_CLASSIFICATION	 disable backtracking 
WITHOUT_CLASSIFICATION	 operator file sink reduce sink something that forces 
WITHOUT_CLASSIFICATION	 obtain table props query 
WITHOUT_CLASSIFICATION	 this stream for entire stripe and needed for every uncompress once and reuse 
WITHOUT_CLASSIFICATION	 coming from reducesink the aggregations would the form valuecol valuecol 
WITHOUT_CLASSIFICATION	 construct setop output using original left right input 
WITHOUT_CLASSIFICATION	 preallocated member for storing index into the hashsetresults for each spilled row 
WITHOUT_CLASSIFICATION	 topn will cause shortcircuit dont need any initialization 
WITHOUT_CLASSIFICATION	 doublevalue 
WITHOUT_CLASSIFICATION	 whether this rebuild rewritten expression 
WITHOUT_CLASSIFICATION	 add the output dir the watch set scan and cancel current watch 
WITHOUT_CLASSIFICATION	 its corresponding tablescanoperator 
WITHOUT_CLASSIFICATION	 logically each bucket consists copy copyn etc dont know priori this true then the current split from copyn file its needed correctly set maxkey particular set maxkeynull this split the tail the last file for this logical bucket include all deltas written after nonacid acid table conversion todo hive also see comments link originalreaderpair about unbucketed tables 
WITHOUT_CLASSIFICATION	 set and parse the row 
WITHOUT_CLASSIFICATION	 finally are going use 
WITHOUT_CLASSIFICATION	 retry once 
WITHOUT_CLASSIFICATION	 propagate the cluster name the script 
WITHOUT_CLASSIFICATION	 craete table and check dir ownership 
WITHOUT_CLASSIFICATION	 check can merge mapjointask into that child 
WITHOUT_CLASSIFICATION	 making sure this not initialized unless needed 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 authorize the grant 
WITHOUT_CLASSIFICATION	 same idea only set for nonnative tables 
WITHOUT_CLASSIFICATION	 the next parseddelta may have everything equal the prev parseddelta except the path this may happen when have split update and have two types delta directories deltaxy and deletedeltaxy for the same txn range 
WITHOUT_CLASSIFICATION	 nonjavadoc see int int int 
WITHOUT_CLASSIFICATION	 increment the createdfiles counter 
WITHOUT_CLASSIFICATION	 update has failed should try task 
WITHOUT_CLASSIFICATION	 may statementid 
WITHOUT_CLASSIFICATION	 does not change the output ordering from the inputs 
WITHOUT_CLASSIFICATION	 process column constraint 
WITHOUT_CLASSIFICATION	 this input the big table contained the big candidates set and either have not chosen big table yet has been chosen the big table above the cumulative cardinality for this input higher 
WITHOUT_CLASSIFICATION	 assumes stored data schema acid 
WITHOUT_CLASSIFICATION	 process the current data point 
WITHOUT_CLASSIFICATION	 and 
WITHOUT_CLASSIFICATION	 have essentially deallocated this 
WITHOUT_CLASSIFICATION	 highvalue 
WITHOUT_CLASSIFICATION	 default not wait 
WITHOUT_CLASSIFICATION	 create acid table with dbtxnmanager 
WITHOUT_CLASSIFICATION	 data stream could empty stream already reached end stream before present stream 
WITHOUT_CLASSIFICATION	 only populate corrupt ids for the things couldnt deserialize are not using ppd assume that ppd makes sure the cached values are correct fails otherwise also dont use the footers ppd case 
WITHOUT_CLASSIFICATION	 bitvectors 
WITHOUT_CLASSIFICATION	 how handle different scales 
WITHOUT_CLASSIFICATION	 skip word also 
WITHOUT_CLASSIFICATION	 try rewrite countx into count not nullable remove duplicate aggregate calls well 
WITHOUT_CLASSIFICATION	 replace the partitions dfs with the tables dfs 
WITHOUT_CLASSIFICATION	 cant use equals because the walker depends them being object equal the default graph walker processes node after its kids have been processed that comparison needs 
WITHOUT_CLASSIFICATION	 lock operations not controlled for now 
WITHOUT_CLASSIFICATION	 import will mark the parent writeentity thus ensuring that check for table creation privileges 
WITHOUT_CLASSIFICATION	 assume here wont lower maybe should just read and not guess 
WITHOUT_CLASSIFICATION	 conflict 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 catchall call cases like those with ctas onto unpartitioned table see hive 
WITHOUT_CLASSIFICATION	 dont bust existing setups 
WITHOUT_CLASSIFICATION	 interrupt the cli thread stop the current statement and return 
WITHOUT_CLASSIFICATION	 call when nextreadindex nextreadcount 
WITHOUT_CLASSIFICATION	 orc table restrict reordering columns will break schema evolution 
WITHOUT_CLASSIFICATION	 direct access 
WITHOUT_CLASSIFICATION	 increment dropkey get new key for hash map 
WITHOUT_CLASSIFICATION	 lowest word gets integer rounding 
WITHOUT_CLASSIFICATION	 length utf string fixed width bytes serializing binary format 
WITHOUT_CLASSIFICATION	 note for now llap only supported tez tasks will never come others may added here although this only necessary have extra debug information 
WITHOUT_CLASSIFICATION	 expand list correct size 
WITHOUT_CLASSIFICATION	 filter didnt anything 
WITHOUT_CLASSIFICATION	 when getpos called should return the same value signaling the end the search the search should continue linearly and should sync the beginning the block 
WITHOUT_CLASSIFICATION	 hive join tests fail tez when have more than join the same key and there outer join down the join tree that requires filtertag disable this conversion map join here now need emulate the behavior create new operation able support this this seems like corner case enough special case this for now 
WITHOUT_CLASSIFICATION	 use thrift transportable formatter 
WITHOUT_CLASSIFICATION	 for metadataonly empty rows optimizations nullonerow input format can selected 
WITHOUT_CLASSIFICATION	 not supported 
WITHOUT_CLASSIFICATION	 because rexinputrefs represent ref expr corresponding value inputrefs used get corresponding index 
WITHOUT_CLASSIFICATION	 use clip the name because this method will return corrupted value when 
WITHOUT_CLASSIFICATION	 the user didnt specify serde use the default 
WITHOUT_CLASSIFICATION	 optional optional optional optional 
WITHOUT_CLASSIFICATION	 required required required required required optional 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 wrong type here 
WITHOUT_CLASSIFICATION	 viewexpandedtext 
WITHOUT_CLASSIFICATION	 production listfieldtype 
WITHOUT_CLASSIFICATION	 nothing stop 
WITHOUT_CLASSIFICATION	 technically methods run threadpool that created externally with the ugi however that brittle wed save the ugi explicitly here 
WITHOUT_CLASSIFICATION	 try with types that have type params 
WITHOUT_CLASSIFICATION	 drop the table but not its data 
WITHOUT_CLASSIFICATION	 build calcite rel node for project using converted projections col 
WITHOUT_CLASSIFICATION	 perform conversion null map values 
WITHOUT_CLASSIFICATION	 show locks 
WITHOUT_CLASSIFICATION	 initialize hcatoutputformat 
WITHOUT_CLASSIFICATION	 for backwards compatibility with old metastore persistence 
WITHOUT_CLASSIFICATION	 get the counters for the task 
WITHOUT_CLASSIFICATION	 element map 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 get the lru key value 
WITHOUT_CLASSIFICATION	 get new client 
WITHOUT_CLASSIFICATION	 bgenjjtree enum 
WITHOUT_CLASSIFICATION	 the path string contains the dag identifier 
WITHOUT_CLASSIFICATION	 used for buffer columns values 
WITHOUT_CLASSIFICATION	 drop tablepartition corresponding records compactionqueue and should disappear 
WITHOUT_CLASSIFICATION	 get column name from custom path matcher and column value from dynamic path matcher 
WITHOUT_CLASSIFICATION	 private static minicluster cluster 
WITHOUT_CLASSIFICATION	 now add cache the dummy colstats for these partitions 
WITHOUT_CLASSIFICATION	 varchar 
WITHOUT_CLASSIFICATION	 show locks 
WITHOUT_CLASSIFICATION	 join which takes place separate task 
WITHOUT_CLASSIFICATION	 tasklist 
WITHOUT_CLASSIFICATION	 comparesupported returns false because union can contain 
WITHOUT_CLASSIFICATION	 required optional optional optional optional optional optional 
WITHOUT_CLASSIFICATION	 thus use flag identify have finished pushing the sort past union 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 create table select should not return resultset 
WITHOUT_CLASSIFICATION	 map operators only all operators launch containers user code etc prevents running inside llap operators try running everything llap fail that not possible non blessed user code script etc please hive choose for 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 this not the first table and are not using big table 
WITHOUT_CLASSIFICATION	 are getting session from tezsessionpool have the session but doesnt have registry info yet have the session with registry info have failed the master thread has canceled this and will never look again 
WITHOUT_CLASSIFICATION	 bgenjjtree definition 
WITHOUT_CLASSIFICATION	 always configure storage handler with before calling any methods 
WITHOUT_CLASSIFICATION	 get the ast tree 
WITHOUT_CLASSIFICATION	 cleanup method called run cleanup tasks job state failed default cleanup provided 
WITHOUT_CLASSIFICATION	 changing the sortmerge join mapjoin 
WITHOUT_CLASSIFICATION	 finalize the headers 
WITHOUT_CLASSIFICATION	 check whether exception thrown when fetching log from closed operation 
WITHOUT_CLASSIFICATION	 ratio greater than then number rows increases this can happen when some operators like groupby duplicates the input rows which case number distincts should not change update the distinct count only when the output number rows less than input number rows 
WITHOUT_CLASSIFICATION	 load into existing empty table 
WITHOUT_CLASSIFICATION	 not update metrics see above 
WITHOUT_CLASSIFICATION	 validate resource plan 
WITHOUT_CLASSIFICATION	 segments load still need honer overwrite 
WITHOUT_CLASSIFICATION	 list operation states measure duration 
WITHOUT_CLASSIFICATION	 this semijoin need add the condition 
WITHOUT_CLASSIFICATION	 execute select statement and verify that aborted insert statement not counted 
WITHOUT_CLASSIFICATION	 add these values red green null 
WITHOUT_CLASSIFICATION	 the size the array equal the number selected columns 
WITHOUT_CLASSIFICATION	 loop through the partitions and form the expression 
WITHOUT_CLASSIFICATION	 test longlong version 
WITHOUT_CLASSIFICATION	 nonjavadoc see javaioreader long 
WITHOUT_CLASSIFICATION	 the key column not column then dont apply this optimization this will fixed part for type conversion udfs 
WITHOUT_CLASSIFICATION	 txinid 
WITHOUT_CLASSIFICATION	 additional information about stats virtual column number 
WITHOUT_CLASSIFICATION	 example from hivedecimalsubtract header comments 
WITHOUT_CLASSIFICATION	 initialize the transaction manager this must done before analyze called 
WITHOUT_CLASSIFICATION	 after dedup should left with locks path exclusive 
WITHOUT_CLASSIFICATION	 load the expected results 
WITHOUT_CLASSIFICATION	 test read 
WITHOUT_CLASSIFICATION	 set fetch size session conf map 
WITHOUT_CLASSIFICATION	 write the data type 
WITHOUT_CLASSIFICATION	 table directory which includes the partition directory has already been moved just update the partition location the metastore 
WITHOUT_CLASSIFICATION	 this only executed for outer joins with residual filters 
WITHOUT_CLASSIFICATION	 retrieve information about cache usage for the query 
WITHOUT_CLASSIFICATION	 believe not some tools generate queries with limit and than expect query run quickly lets meet their requirement 
WITHOUT_CLASSIFICATION	 determine which columns requires cast leftright input calcite 
WITHOUT_CLASSIFICATION	 currently acid requires table bucketed 
WITHOUT_CLASSIFICATION	 the vectorization context for creating the vectorizedrowbatch for the node 
WITHOUT_CLASSIFICATION	 referencing correlated variables 
WITHOUT_CLASSIFICATION	 update existing stat objects field 
WITHOUT_CLASSIFICATION	 for tables other than the big table need fetch more data until reach new group done 
WITHOUT_CLASSIFICATION	 first incremental load 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 short year should work 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 separator for multiple tables validwriteidlist also skip for last entry 
WITHOUT_CLASSIFICATION	 check the ouput specs only storage handler native tabless outputformats does not set the jobs output properties correctly 
WITHOUT_CLASSIFICATION	 and thus valid both times flowed the same pace congratulate ourselves and bail 
WITHOUT_CLASSIFICATION	 longvalue 
WITHOUT_CLASSIFICATION	 for incremental repl will have individual events which can other things like roles and fns well this point all dump dirs should contain dumpmetadata file that tells what inside that dumpdir 
WITHOUT_CLASSIFICATION	 fill the prefix bytes with deterministic data based the actual meaningful data 
WITHOUT_CLASSIFICATION	 including parameters passed the query 
WITHOUT_CLASSIFICATION	 reset the behaviour 
WITHOUT_CLASSIFICATION	 when have operators that have multiple parents not clear which parents traits need propagate forward 
WITHOUT_CLASSIFICATION	 sum input timestamp and output double just modes partial complete 
WITHOUT_CLASSIFICATION	 get view column authorization 
WITHOUT_CLASSIFICATION	 note deos not allow upgrading read lock write lock care must taken while under read lock make sure not perform any actions which attempt take write lock 
WITHOUT_CLASSIFICATION	 private final mapstring integer columnmap 
WITHOUT_CLASSIFICATION	 serdeinfo 
WITHOUT_CLASSIFICATION	 all index 
WITHOUT_CLASSIFICATION	 filterg does date parsing for quoted strings wed need verify theres 
WITHOUT_CLASSIFICATION	 for each column are converting the row column object 
WITHOUT_CLASSIFICATION	 verify can get from cache 
WITHOUT_CLASSIFICATION	 array hash set results can lookups the whole batch before output result 
WITHOUT_CLASSIFICATION	 extract the integer portion get the quotient 
WITHOUT_CLASSIFICATION	 for static partition may not exist when set true 
WITHOUT_CLASSIFICATION	 extract information about the old value 
WITHOUT_CLASSIFICATION	 find theres any dpp sink branch the branchingop that equivalent 
WITHOUT_CLASSIFICATION	 use scratch buffer with the hivedecimalwritable tobytes method dont incur poor performance creating string result 
WITHOUT_CLASSIFICATION	 there are some partitions with state didnt fetch any state update the stats with empty list reflect that the stateinitialize structures 
WITHOUT_CLASSIFICATION	 use default configuration for noauth mode 
WITHOUT_CLASSIFICATION	 skip leading zeros and compute number digits magnitude 
WITHOUT_CLASSIFICATION	 indicates read buffer has data number rows the temporary read buffer cursor during reading total number pairs output 
WITHOUT_CLASSIFICATION	 system include path 
WITHOUT_CLASSIFICATION	 add two decimals 
WITHOUT_CLASSIFICATION	 ctxgetcurrtask roottasks should removed 
WITHOUT_CLASSIFICATION	 below method returns the dependencies for the passed query explain the dependencies are the set input tables and partitions and are provided back json output for the explain command example output defaulttestsambaviv tabletype table input 
WITHOUT_CLASSIFICATION	 obtain delegation token from accumulo 
WITHOUT_CLASSIFICATION	 check hiveconfdir defined 
WITHOUT_CLASSIFICATION	 but can backported disable setupcleanup all versions 
WITHOUT_CLASSIFICATION	 have received new directory information make split strategies 
WITHOUT_CLASSIFICATION	 mapoperator out sparkwork use bridge spark transformation and hive operators sparkwork 
WITHOUT_CLASSIFICATION	 alter the table missing either due droprename which follows the alter the existing table newer than our update 
WITHOUT_CLASSIFICATION	 serializer then buffers rows certain limit and serializes the whole batch when the buffer full the serialize returns null the buffer not full the size buffer kept track the 
WITHOUT_CLASSIFICATION	 based whitelistblacklist 
WITHOUT_CLASSIFICATION	 the same server 
WITHOUT_CLASSIFICATION	 todo try using set 
WITHOUT_CLASSIFICATION	 close also calls flush 
WITHOUT_CLASSIFICATION	 failure occurs here the directory containing the original files 
WITHOUT_CLASSIFICATION	 remember case need connect additional work later 
WITHOUT_CLASSIFICATION	 its not exception caused auth check ignore 
WITHOUT_CLASSIFICATION	 generate the second reducesinkoperator for the group plan parseinfogetxxxdest the new reducesinkoperator will child groupbyoperatorinfo the second reducesinkoperator will put the group keys the mapreduce sort key and put the partial aggregation results the mapreduce value param numpartitionfields the number fields the mapreduce partition key this should always the same the number group keys should able remove this parameter since this phase there distinct any more return the new reducesinkoperator throws semanticexception 
WITHOUT_CLASSIFICATION	 sort state acquired waiting and then locktype then 
WITHOUT_CLASSIFICATION	 some prime numbers spaced about powers magnitude 
WITHOUT_CLASSIFICATION	 the output this udf constant dont even bother evaluating 
WITHOUT_CLASSIFICATION	 int offset outputgetlength 
WITHOUT_CLASSIFICATION	 break immediately timeout 
WITHOUT_CLASSIFICATION	 final preds 
WITHOUT_CLASSIFICATION	 only right input repeating and has nulls 
WITHOUT_CLASSIFICATION	 time part 
WITHOUT_CLASSIFICATION	 the delta directory should also have only bucket file bucket 
WITHOUT_CLASSIFICATION	 everything prefixed unittests everything prefixed 
WITHOUT_CLASSIFICATION	 enforce uniqueness column names 
WITHOUT_CLASSIFICATION	 hive history disabled create noop proxy 
WITHOUT_CLASSIFICATION	 null means all for show grants global for grantrevoke 
WITHOUT_CLASSIFICATION	 verify tables and partitions destination for equivalence 
WITHOUT_CLASSIFICATION	 required 
WITHOUT_CLASSIFICATION	 clear existing cookie 
WITHOUT_CLASSIFICATION	 cast timestamp 
WITHOUT_CLASSIFICATION	 constant string projection select hello from table 
WITHOUT_CLASSIFICATION	 the produces object due the limitations the traversal interface which requires interpretation that object into ranges changes the return object from the must also represent change the 
WITHOUT_CLASSIFICATION	 this setups auth filtering build 
WITHOUT_CLASSIFICATION	 the result null throw exception this can caught calling code the vectorized code path and made yield sql null value 
WITHOUT_CLASSIFICATION	 only one result column verify the system generated column name 
WITHOUT_CLASSIFICATION	 constants and nulls are 
WITHOUT_CLASSIFICATION	 for now dont know which virtual columns are going included well add them 
WITHOUT_CLASSIFICATION	 need preserve currentgroups 
WITHOUT_CLASSIFICATION	 metrics will have already been initialized were using them since hmshandler 
WITHOUT_CLASSIFICATION	 are not calling superseek since handle the present stream differently 
WITHOUT_CLASSIFICATION	 add row chain except case unb preceding only max needs tracked current max will never become out range can only replaced larger max 
WITHOUT_CLASSIFICATION	 need make sure that null operator lim fil present all branches multiinsert query before applying the optimization this method does full tree traversal starting from and will return true only finds target null operator each branch 
WITHOUT_CLASSIFICATION	 need set null data entries because the input nan values will automatically propagate the output 
WITHOUT_CLASSIFICATION	 hashmap javafieldref primitives hashmapentry javafieldref 
WITHOUT_CLASSIFICATION	 create path hivecontrib jar local filesystem 
WITHOUT_CLASSIFICATION	 the final move 
WITHOUT_CLASSIFICATION	 column qualifier with colon 
WITHOUT_CLASSIFICATION	 adjust the number reducers this correlation based 
WITHOUT_CLASSIFICATION	 disconnect the reduce work from its child this should produce two isolated sub graphs 
WITHOUT_CLASSIFICATION	 wait seconds too case exception not end busy waiting for the solution for this exception 
WITHOUT_CLASSIFICATION	 load data local inpath doesnt delete source files clean here 
WITHOUT_CLASSIFICATION	 element for key long hash table hashmap 
WITHOUT_CLASSIFICATION	 derived from the alias vvvt 
WITHOUT_CLASSIFICATION	 lock are trying acquire shared write 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 the reducer needs restored consider query like select count from bucketbig join bucketsmall akey bkey 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqlblob 
WITHOUT_CLASSIFICATION	 always start the running state requests for state updates will sent out after registration 
WITHOUT_CLASSIFICATION	 optional int fragmentnumber 
WITHOUT_CLASSIFICATION	 nway join 
WITHOUT_CLASSIFICATION	 now change its child 
WITHOUT_CLASSIFICATION	 input expression for count 
WITHOUT_CLASSIFICATION	 expect query completed now 
WITHOUT_CLASSIFICATION	 for partial and final objectinspectors for partial aggregations list doubles 
WITHOUT_CLASSIFICATION	 will transform gbrsgby 
WITHOUT_CLASSIFICATION	 for backward compatibility 
WITHOUT_CLASSIFICATION	 grouping same but category not 
WITHOUT_CLASSIFICATION	 decrease check that the pool shrinks incl killing the unused and returned sessions 
WITHOUT_CLASSIFICATION	 case custom dynamic partitions cant just move the subtree partition root directory since the partitions location contain regex pattern need first find the final destination each partition and move its output 
WITHOUT_CLASSIFICATION	 verify true have acid table are producing the table schema from orc the vectorizer class assures this 
WITHOUT_CLASSIFICATION	 store the results produced the dispatcher 
WITHOUT_CLASSIFICATION	 this assumes all paths are bucket names which means lookup needed 
WITHOUT_CLASSIFICATION	 according hivetypetosqltype possible options are 
WITHOUT_CLASSIFICATION	 more batches read exhausted the reader 
WITHOUT_CLASSIFICATION	 the record with valid cqid has disappeared this sign something wrong 
WITHOUT_CLASSIFICATION	 detect udtfs nested select group etc they arent supported 
WITHOUT_CLASSIFICATION	 estimate needed underlying aggbuffer for results for intermediates results underlying wdwsz intermediates underlying wdwsz 
WITHOUT_CLASSIFICATION	 surprisingly these privs are already granted 
WITHOUT_CLASSIFICATION	 lower case role names for case insensitive behavior 
WITHOUT_CLASSIFICATION	 check log files look 
WITHOUT_CLASSIFICATION	 set the range bytes deserialized 
WITHOUT_CLASSIFICATION	 the has access method can try defaultfileaccess 
WITHOUT_CLASSIFICATION	 bgenjjtree typeset 
WITHOUT_CLASSIFICATION	 there more than argument specified different natural language locale being specified 
WITHOUT_CLASSIFICATION	 internal input format class for 
WITHOUT_CLASSIFICATION	 specified the query 
WITHOUT_CLASSIFICATION	 row 
WITHOUT_CLASSIFICATION	 expected failure 
WITHOUT_CLASSIFICATION	 assume all columns are null except the dummy column always nonnull 
WITHOUT_CLASSIFICATION	 compare the field types 
WITHOUT_CLASSIFICATION	 arithmetic two type intervaldaytime storing nanosecond interval 
WITHOUT_CLASSIFICATION	 todo currently way test alter table this interface doesnt support alter table 
WITHOUT_CLASSIFICATION	 sanity check 
WITHOUT_CLASSIFICATION	 empty 
WITHOUT_CLASSIFICATION	 since cannot directly set the private byte field inside text 
WITHOUT_CLASSIFICATION	 did not reduce check the children nodes 
WITHOUT_CLASSIFICATION	 test the sequence validation functionality 
WITHOUT_CLASSIFICATION	 binary search only works know the size the split and the recordreader rcfilerecordreader 
WITHOUT_CLASSIFICATION	 show user level privileges 
WITHOUT_CLASSIFICATION	 must release the connection before call other methods 
WITHOUT_CLASSIFICATION	 adopted hadoop calling new sequencefilereader leaves 
WITHOUT_CLASSIFICATION	 todo 
WITHOUT_CLASSIFICATION	 operator tree now done 
WITHOUT_CLASSIFICATION	 get the connection properties from user specific config file 
WITHOUT_CLASSIFICATION	 parent guaranteed have single list because reduce sink 
WITHOUT_CLASSIFICATION	 check whether session log dir deleted after session closed 
WITHOUT_CLASSIFICATION	 nothing aggregate 
WITHOUT_CLASSIFICATION	 read event from notification 
WITHOUT_CLASSIFICATION	 just return stats gathering should not block the main query 
WITHOUT_CLASSIFICATION	 greater than equal and less than equal 
WITHOUT_CLASSIFICATION	 should return all 
WITHOUT_CLASSIFICATION	 test select absrootcolb from table testroot 
WITHOUT_CLASSIFICATION	 ignore exception 
WITHOUT_CLASSIFICATION	 succeed transactional set true and the table bucketed and uses orc 
WITHOUT_CLASSIFICATION	 the current position the key series 
WITHOUT_CLASSIFICATION	 partition column 
WITHOUT_CLASSIFICATION	 predicates without field references can pushed both inputs 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 check standard out terminal 
WITHOUT_CLASSIFICATION	 get the session specified class loader from sessionstate 
WITHOUT_CLASSIFICATION	 confusing directories export 
WITHOUT_CLASSIFICATION	 get the key store map 
WITHOUT_CLASSIFICATION	 match 
WITHOUT_CLASSIFICATION	 construct using 
WITHOUT_CLASSIFICATION	 adds databasename dbname the filter 
WITHOUT_CLASSIFICATION	 specialcasing for adminlevel operations that not require object checking 
WITHOUT_CLASSIFICATION	 need attempt merge the files again 
WITHOUT_CLASSIFICATION	 check there are privileges filtered 
WITHOUT_CLASSIFICATION	 free list indices each unallocated block for quick lookup 
WITHOUT_CLASSIFICATION	 add hive operator level statistics 
WITHOUT_CLASSIFICATION	 appropriate operators the 
WITHOUT_CLASSIFICATION	 hbase stuff 
WITHOUT_CLASSIFICATION	 isrepeating and there are nulls 
WITHOUT_CLASSIFICATION	 remove the dead session dir 
WITHOUT_CLASSIFICATION	 entire batch filtered out 
WITHOUT_CLASSIFICATION	 check column number 
WITHOUT_CLASSIFICATION	 set the output string entry the contents text object null object reference record that the value sql null 
WITHOUT_CLASSIFICATION	 walkaround tez 
WITHOUT_CLASSIFICATION	 initialize input 
WITHOUT_CLASSIFICATION	 all values are null none qualify 
WITHOUT_CLASSIFICATION	 since schedule can called from multiple threads peek the wait queue try scheduling the task and then remove the task scheduling successful this 
WITHOUT_CLASSIFICATION	 skip leading zeroes word 
WITHOUT_CLASSIFICATION	 replace all toktabref with fully qualified table name not already fully qualified 
WITHOUT_CLASSIFICATION	 nodes that need see siblings for and sibling levels 
WITHOUT_CLASSIFICATION	 alteroptype null case stats update 
WITHOUT_CLASSIFICATION	 just need initialize the proxyusers for the first time given that the conf will not change the fly 
WITHOUT_CLASSIFICATION	 contains functionality that helps with understanding how subquery was rewritten 
WITHOUT_CLASSIFICATION	 could also have one metricssource for all the pools and add all the pools the collector its getmetrics call separate records not clear thats supported also wed have initialize the metrics ourselves instead using metric annotation 
WITHOUT_CLASSIFICATION	 set max possible value 
WITHOUT_CLASSIFICATION	 set the thread local username used for doas true 
WITHOUT_CLASSIFICATION	 check the original partitions the dest table 
WITHOUT_CLASSIFICATION	 for direct connections dont yet support reestablishing connections 
WITHOUT_CLASSIFICATION	 childtask merge nothing there are more than one childtasks which case dont want anything 
WITHOUT_CLASSIFICATION	 valid events this batch but were still not done processing events 
WITHOUT_CLASSIFICATION	 split work into multiple branches one for each childwork childworks 
WITHOUT_CLASSIFICATION	 generate types for column mapping 
WITHOUT_CLASSIFICATION	 table descriptor the final 
WITHOUT_CLASSIFICATION	 the verifyandset this case expected fail with the 
WITHOUT_CLASSIFICATION	 fetchfirst execute sql and fetch its sql operation log expected value 
WITHOUT_CLASSIFICATION	 validate after compaction 
WITHOUT_CLASSIFICATION	 fixed doesnt exist hive fixeds lists bytes out 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 the code should account for the bug and update the iterators the split 
WITHOUT_CLASSIFICATION	 files size for splits 
WITHOUT_CLASSIFICATION	 should not happen default value set 
WITHOUT_CLASSIFICATION	 the scaling down during multiplication avoid unnecessary overflow note that even this could overflow newscale too small 
WITHOUT_CLASSIFICATION	 note some code uses this list correlate with column names and yet these lists may contain duplicates which this call will remove and the other wont far can tell code will actually use these two methods together all good the code gets the list without relying this method maybe just works magic 
WITHOUT_CLASSIFICATION	 output will same both partial full aggregation modes 
WITHOUT_CLASSIFICATION	 flag control that always threads are initialized only once 
WITHOUT_CLASSIFICATION	 this materialized view this stores the view descriptor 
WITHOUT_CLASSIFICATION	 setup connection information 
WITHOUT_CLASSIFICATION	 add the index expr reducekeys distinctindices 
WITHOUT_CLASSIFICATION	 null evicted task means offer accepted evictedtask not equal taskwrapper means current task accepted and evicted 
WITHOUT_CLASSIFICATION	 true the logs should removed after the operation should used only test mode 
WITHOUT_CLASSIFICATION	 see this explicit cast 
WITHOUT_CLASSIFICATION	 nonimpersonation mode map scheduler queue current user fair scheduler configured 
WITHOUT_CLASSIFICATION	 column constant 
WITHOUT_CLASSIFICATION	 nonjavadoc see long 
WITHOUT_CLASSIFICATION	 get completed attempts from jobtasksjsp 
WITHOUT_CLASSIFICATION	 already set 
WITHOUT_CLASSIFICATION	 clean 
WITHOUT_CLASSIFICATION	 scale down right and compare 
WITHOUT_CLASSIFICATION	 returns map 
WITHOUT_CLASSIFICATION	 add field with comment 
WITHOUT_CLASSIFICATION	 can have unaliased and one aliased mapping column 
WITHOUT_CLASSIFICATION	 test that can search correctly using buffer and pulling sequence bytes out the middle this case 
WITHOUT_CLASSIFICATION	 test null right 
WITHOUT_CLASSIFICATION	 increment count values seen far 
WITHOUT_CLASSIFICATION	 the directory might dbtablepartition 
WITHOUT_CLASSIFICATION	 get our singlecolumn long hash set information for this specialized class 
WITHOUT_CLASSIFICATION	 groupbyoperators 
WITHOUT_CLASSIFICATION	 after the timeout just force abort the open txns 
WITHOUT_CLASSIFICATION	 fastscale 
WITHOUT_CLASSIFICATION	 the processing thread will switch between these two objects 
WITHOUT_CLASSIFICATION	 set true the operator tree below 
WITHOUT_CLASSIFICATION	 this struct type 
WITHOUT_CLASSIFICATION	 param statusdir directory statusdir defined for the webhcat job supposed contain stdoutstderrsyslog for the webhcat controller job param jobtype currently support mapreduce the specific parser will parse the log the controller job and retrieve jobid all mapreduce jobs launches the generic mapreduce parser works when the program use jobclientrunjob submit the job but the program use other api generic mapreduce parser not guaranteed find the jobid param conf configuration for webhcat 
WITHOUT_CLASSIFICATION	 nothing with nulls 
WITHOUT_CLASSIFICATION	 partition columns the table level schema 
WITHOUT_CLASSIFICATION	 insert event partitioned table existing partition 
WITHOUT_CLASSIFICATION	 must have already been called 
WITHOUT_CLASSIFICATION	 set invoking hmshandler threadlocal this will used later notify 
WITHOUT_CLASSIFICATION	 the state for guaranteed task tracking synchronized this addition isguaranteed only modified under the epic lock because involves modifying the corresponding structures that contain the task objects the same time 
WITHOUT_CLASSIFICATION	 nonjavadoc see javasqltime javautilcalendar 
WITHOUT_CLASSIFICATION	 dump and load only first insert record 
WITHOUT_CLASSIFICATION	 try populate correlation variables using local fields 
WITHOUT_CLASSIFICATION	 ctas cannot part multitxn stmt 
WITHOUT_CLASSIFICATION	 now the job initialized reason rjgetjobstate again and not want extra rpc call 
WITHOUT_CLASSIFICATION	 know how handle dpp sinks 
WITHOUT_CLASSIFICATION	 nothing implicitconversions 
WITHOUT_CLASSIFICATION	 note critical this prior initializing logj otherwise 
WITHOUT_CLASSIFICATION	 check basic operation 
WITHOUT_CLASSIFICATION	 extract all the inputformatclass names for each chunk the combinedsplit 
WITHOUT_CLASSIFICATION	 from script need load history and need completer either 
WITHOUT_CLASSIFICATION	 check the partitions dont exist the desttable 
WITHOUT_CLASSIFICATION	 need convert 
WITHOUT_CLASSIFICATION	 load required jdbc driver 
WITHOUT_CLASSIFICATION	 change the resource plan use fifo policy 
WITHOUT_CLASSIFICATION	 columnvector entry byte array representing serialized bloomfilter does simple byte oring which should faster than deserializemerge 
WITHOUT_CLASSIFICATION	 allocate free and optionally allocate 
WITHOUT_CLASSIFICATION	 because percentile really quantile values should generally strictly between and 
WITHOUT_CLASSIFICATION	 the user may have passed list files comma separated 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 heap not full add the buffer the heap and restore heap property 
WITHOUT_CLASSIFICATION	 public string rid specific zemanta use 
WITHOUT_CLASSIFICATION	 put the jks file the current test path only for test purpose 
WITHOUT_CLASSIFICATION	 found lazysimpleserdes nullsequence 
WITHOUT_CLASSIFICATION	 serializer then recordvalue null the buffer not full the size buffer kept track the serde 
WITHOUT_CLASSIFICATION	 once have read the from the file there are two kinds cases for which might have discard rows from the batch case when the row created transaction that not valid case when the row has been deleted will through the batch discover rows which match any the cases and specifically remove them from the selected vector course selectedinuse should also set 
WITHOUT_CLASSIFICATION	 createtable truncate insert the result just one record 
WITHOUT_CLASSIFICATION	 test char literal string column comparison 
WITHOUT_CLASSIFICATION	 there conflicting lock the same object with lower sequence number 
WITHOUT_CLASSIFICATION	 final final mixing bit values abc into pairs abc values differing only few bits will usually produce values that look totally different this was tested for pairs that differed one bit two bits any combination top bits abc any combination bottom bits abc differ defined for and transformed the output delta gray code string commonly produced subtraction look like single bit difference the base values were pseudorandom all zero but one bit set all zero plus counter that starts zero these constants passed and these came close define finalabc rotb rotc rota rotb rotc rota rotb 
WITHOUT_CLASSIFICATION	 tracks total pending preemptions 
WITHOUT_CLASSIFICATION	 this one has the mixedsize chars 
WITHOUT_CLASSIFICATION	 update the table column stats for table cache 
WITHOUT_CLASSIFICATION	 write header 
WITHOUT_CLASSIFICATION	 make sure escape separator char prop values 
WITHOUT_CLASSIFICATION	 checking for status table 
WITHOUT_CLASSIFICATION	 test few field names 
WITHOUT_CLASSIFICATION	 unique identity for this instance 
WITHOUT_CLASSIFICATION	 were here proxy user set 
WITHOUT_CLASSIFICATION	 recheck got verified another thread while were waiting 
WITHOUT_CLASSIFICATION	 convert input row standard objects 
WITHOUT_CLASSIFICATION	 the rel which being visited 
WITHOUT_CLASSIFICATION	 whatever 
WITHOUT_CLASSIFICATION	 check the rest command specified explicitly use hcatalog says that implicitly using the pig usehcatalog arg 
WITHOUT_CLASSIFICATION	 there was error adding partitions rollback copy and rethrow 
WITHOUT_CLASSIFICATION	 the volcanoplanner from apply both planners need use the correct executor 
WITHOUT_CLASSIFICATION	 newpath the basedelta dir 
WITHOUT_CLASSIFICATION	 update the metadata for the materialized view 
WITHOUT_CLASSIFICATION	 the partitioning columns the parent are more specific than those the child 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 longer part the hivedecimal representation anymore string then bytes 
WITHOUT_CLASSIFICATION	 
WITHOUT_CLASSIFICATION	 pass through group key indicators present 
WITHOUT_CLASSIFICATION	 year month day hour minute second 
WITHOUT_CLASSIFICATION	 periodic task time out submitted tasks that have not been updated with umbilical heartbeat 
WITHOUT_CLASSIFICATION	 found old valid block for this key the cache 
WITHOUT_CLASSIFICATION	 continue reading from the input stream until the desired number byte has been read 
WITHOUT_CLASSIFICATION	 test that the server code exists 
WITHOUT_CLASSIFICATION	 modifiedrowcount 
WITHOUT_CLASSIFICATION	 this will also invoked for tasks which have been killed rejected the daemon informing the daemon becomes necessary once the llapscheduler supports preemption andor starts attempting kill tasks which may running node 
WITHOUT_CLASSIFICATION	 get base type since type string may parameterized 
WITHOUT_CLASSIFICATION	 once enough rows have been output there need generate more output 
WITHOUT_CLASSIFICATION	 twice returns not cleaned cache 
WITHOUT_CLASSIFICATION	 add select expression 
WITHOUT_CLASSIFICATION	 all accesses synchronized the object itself could replaced with cas 
WITHOUT_CLASSIFICATION	 using minidfs the permissions dont work properly because the current user gets treated superuser for this test specify different nonsuper user 
WITHOUT_CLASSIFICATION	 break out the loop fast watchmode disabled 
WITHOUT_CLASSIFICATION	 bail out first missed column 
WITHOUT_CLASSIFICATION	 stage waiting for inputslots complete 
WITHOUT_CLASSIFICATION	 remove the last 
WITHOUT_CLASSIFICATION	 block semantic analysis check activecalls 
WITHOUT_CLASSIFICATION	 create schema object containing the give column 
WITHOUT_CLASSIFICATION	 the final match intend return 
WITHOUT_CLASSIFICATION	 limit below milliseconds only 
WITHOUT_CLASSIFICATION	 match found match found and the current row will dropped the current row has been spilled disk the join postponed 
WITHOUT_CLASSIFICATION	 construct without map field 
WITHOUT_CLASSIFICATION	 store the given version and comment the metastore 
WITHOUT_CLASSIFICATION	 just digits 
WITHOUT_CLASSIFICATION	 convert seconds since the epoch with fraction nanoseconds long integer 
WITHOUT_CLASSIFICATION	 can fail with 
WITHOUT_CLASSIFICATION	 the delta dirs should have been cleaned 
WITHOUT_CLASSIFICATION	 prepare data for the source table 
WITHOUT_CLASSIFICATION	 get the submap 
WITHOUT_CLASSIFICATION	 seqnumber 
WITHOUT_CLASSIFICATION	 now add any scratch columns needed for children operators 
WITHOUT_CLASSIFICATION	 because its using write 
WITHOUT_CLASSIFICATION	 restore the old out stream 
WITHOUT_CLASSIFICATION	 initialize listcolumnvector for keys and values 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 request accepted request rejected wait queue full request accepted but evicted other low priority task 
WITHOUT_CLASSIFICATION	 the tail each task chain 
WITHOUT_CLASSIFICATION	 can the join operator converted bucket mapmerge join operator 
WITHOUT_CLASSIFICATION	 dont measure data generation execution cost generate the big table into memory first 
WITHOUT_CLASSIFICATION	 load the list partitions and return the list partition specs 
WITHOUT_CLASSIFICATION	 save results the cache for future queries use 
WITHOUT_CLASSIFICATION	 use construct 
WITHOUT_CLASSIFICATION	 drop messages for the dropped partitions 
WITHOUT_CLASSIFICATION	 link filesink that will write the same final location 
WITHOUT_CLASSIFICATION	 this class populates the following operator traits for the entire operator tree bucketing columns table pruned partitions bucketing columns refer not the bucketing columns from the table object but instead the dynamic bucketing done operators such reduce sinks and groupbys all the operators have translation from their input names the output names corresponding the bucketing column the colexprmap that part every operator used this transformation the table object used for the basecase mapreduce when deciding perform bucket map join this object used the bucketmapjoinproc find number files for the table correspond the number buckets specified the meta data the pruned partition information has the same purpose the table object the moment the traits sortedness etc can populated well for future optimizations make use 
WITHOUT_CLASSIFICATION	 this makes sure can use the same formula compute the 
WITHOUT_CLASSIFICATION	 this should validated change time lets fall back default here 
WITHOUT_CLASSIFICATION	 this set all move tasks the end multiinsert query will only begin once all 
WITHOUT_CLASSIFICATION	 not able push anything down 
WITHOUT_CLASSIFICATION	 test with table name which does not exists 
WITHOUT_CLASSIFICATION	 belongs target table strictly speaking there maybe ambiguous ref but this will caught later when multiinsert parsed 
WITHOUT_CLASSIFICATION	 need prune the select operator 
WITHOUT_CLASSIFICATION	 for hdfs could avoid serializing file and just replace the path with inodebased path however that breaks bunch stuff because hive later looks things split path 
WITHOUT_CLASSIFICATION	 now consolidate all the events that happenned during the objdump into the objdump 
WITHOUT_CLASSIFICATION	 not adding the registry service since need control when initialized conf used pickup properties 
WITHOUT_CLASSIFICATION	 prepare the expression filter the columns 
WITHOUT_CLASSIFICATION	 need process here 
WITHOUT_CLASSIFICATION	 create new session ctx object with client type 
WITHOUT_CLASSIFICATION	 corvar offset should point the leftinput currentrel 
WITHOUT_CLASSIFICATION	 reducer autoparallelism unset fixed uniform parallel 
WITHOUT_CLASSIFICATION	 null constant could typed need check the value 
WITHOUT_CLASSIFICATION	 remove parent for the big table branch 
WITHOUT_CLASSIFICATION	 otherwise recurse 
WITHOUT_CLASSIFICATION	 end 
WITHOUT_CLASSIFICATION	 release them the same time 
WITHOUT_CLASSIFICATION	 hosts per host requests the same priority first host next host last with host third request host should not allocated immediately 
WITHOUT_CLASSIFICATION	 tests returns the first file present the lookup order when files are present the lookup order 
WITHOUT_CLASSIFICATION	 are going small tables 
WITHOUT_CLASSIFICATION	 read result over ssl 
WITHOUT_CLASSIFICATION	 split the partition predicate identify column and value 
WITHOUT_CLASSIFICATION	 recursively through expression and make sure the following 
WITHOUT_CLASSIFICATION	 this exception indicates that code record could not parsed and the caller can decide whether drop send dead letter queue rolling back the txn and retrying wontable help since the tuple will exactly the same when its replayed 
WITHOUT_CLASSIFICATION	 the reader that currently has the lowest key 
WITHOUT_CLASSIFICATION	 since integer always some products here are not included 
WITHOUT_CLASSIFICATION	 todo verify that this works for systems using ugidoas oozie 
WITHOUT_CLASSIFICATION	 need update event operators with the cloned table scan 
WITHOUT_CLASSIFICATION	 can trick the inputformat into using mockinstance 
WITHOUT_CLASSIFICATION	 otherwise just removed lockedinvalid item from heap continue 
WITHOUT_CLASSIFICATION	 ideally test like this should qfile test however the explain output from qfile always 
WITHOUT_CLASSIFICATION	 nothing changes cache 
WITHOUT_CLASSIFICATION	 network cost dphj 
WITHOUT_CLASSIFICATION	 new part 
WITHOUT_CLASSIFICATION	 the lock has single components simplehivelock zookeeperhivelock pos lock paths array contains dbname pos contains tblname 
WITHOUT_CLASSIFICATION	 nothing intern 
WITHOUT_CLASSIFICATION	 just check for writing permissions fails with then means the location may readonly 
WITHOUT_CLASSIFICATION	 the parser should not allow this 
WITHOUT_CLASSIFICATION	 first data dir contains files 
WITHOUT_CLASSIFICATION	 set some reasonable defaults 
WITHOUT_CLASSIFICATION	 since are going creating new table should mark that write entity that the auth framework can work there 
WITHOUT_CLASSIFICATION	 remove from the residual predicate 
WITHOUT_CLASSIFICATION	 the max age task allowed 
WITHOUT_CLASSIFICATION	 build reloptabstracttable 
WITHOUT_CLASSIFICATION	 signal new failure mapreduce 
WITHOUT_CLASSIFICATION	 for special modes that case sessionstateget empty 
WITHOUT_CLASSIFICATION	 create new ugi and add map 
WITHOUT_CLASSIFICATION	 set job 
WITHOUT_CLASSIFICATION	 print out the vertex dependency root stage 
WITHOUT_CLASSIFICATION	 the join key 
WITHOUT_CLASSIFICATION	 this used multiple places among others 
WITHOUT_CLASSIFICATION	 loop through the bits that are set true and mark those rows false their 
WITHOUT_CLASSIFICATION	 use lazysimpleserde for note lazysimpleserde does not support tables with single column col type arraystring this happens when the table created using 
WITHOUT_CLASSIFICATION	 note columnstypes missing all columns will string type 
WITHOUT_CLASSIFICATION	 new input inspector 
WITHOUT_CLASSIFICATION	 template classname valuetype ifdefined 
WITHOUT_CLASSIFICATION	 user supplied data for that object 
WITHOUT_CLASSIFICATION	 middle word gets integer rounding lower longword cleared 
WITHOUT_CLASSIFICATION	 job 
WITHOUT_CLASSIFICATION	 create another dynamicpartitionctx which has different inputtodp column mapping 
WITHOUT_CLASSIFICATION	 also allow constraint creation only and foreign key creation fails 
WITHOUT_CLASSIFICATION	 clean history 
WITHOUT_CLASSIFICATION	 check each mapjoin and shufflejoin operator see they are performing cross product yes output warning the sessions console the checks made are the following shuffle join check the parent reducesinkop the joinop its keys list size then this cross product the parent reducesinkop the mapwork for the same stage mapjoin the keys expr list the mapjoin desc empty list for any input this implies cross product tez shuffle join check the parent reducesinkop the joinop its keys list size then this cross product the parent reducesinkop checked based the map the reducework that contains the joinop tez map join the keys expr list the mapjoin desc empty list for any input this implies cross product 
WITHOUT_CLASSIFICATION	 applied the table 
WITHOUT_CLASSIFICATION	 tablecolumn alias 
WITHOUT_CLASSIFICATION	 move unprocessed remainder beginning buffer 
WITHOUT_CLASSIFICATION	 nonjavadoc see 
WITHOUT_CLASSIFICATION	 create too many partitions just enough validate over limit requests 
WITHOUT_CLASSIFICATION	 inscriptional yodh bytes 
WITHOUT_CLASSIFICATION	 this may happen when enablebitvector false 
WITHOUT_CLASSIFICATION	 importing into existing table fileformat checked checktable 
WITHOUT_CLASSIFICATION	 row mode will not catch this input file format then not enabled 
WITHOUT_CLASSIFICATION	 map aggregation 
WITHOUT_CLASSIFICATION	 send failover request minihs and make sure minihs takes over returning back leader test listeners 
WITHOUT_CLASSIFICATION	 test query where timeout kicks 
WITHOUT_CLASSIFICATION	 lookup udf class failed 
WITHOUT_CLASSIFICATION	 notify the master thread and the user 
WITHOUT_CLASSIFICATION	 try read the dropped after cache update 
WITHOUT_CLASSIFICATION	 set clear the rest the reading variables based vectorrow deserialization 
WITHOUT_CLASSIFICATION	 generics this how vectorization currently works 
WITHOUT_CLASSIFICATION	 store the differing configuration for each alias the job 
WITHOUT_CLASSIFICATION	 error crosses threshold inside close want 
WITHOUT_CLASSIFICATION	 this unnecessary check and forced configuration the property file maybe replace with enforced empty value string 
WITHOUT_CLASSIFICATION	 reverse the list since checked the part from leaf dir tables base dir 
WITHOUT_CLASSIFICATION	 user hasnt specify partition spec generate from tables partition spec this only insertinsert intoinsert overwriteanalyze 
WITHOUT_CLASSIFICATION	 see comment dumping rows via sql for why this doesnt work for all types 
WITHOUT_CLASSIFICATION	 copy without retry 
WITHOUT_CLASSIFICATION	 repeat and drop partition without purge 
WITHOUT_CLASSIFICATION	 obsolete list should include the two original bucket files and the old base dir 
WITHOUT_CLASSIFICATION	 replace the edge manager for all vertices which have routing type custom 
WITHOUT_CLASSIFICATION	 not included the input collations but can propagated this join might enforce 
WITHOUT_CLASSIFICATION	 will download into fnscoped subdirectories avoid name collisions assume there are collisions within the same that doesnt mean download for every 
WITHOUT_CLASSIFICATION	 get all the sessions validate cluster fractions 
WITHOUT_CLASSIFICATION	 mapping from task the number failures 
WITHOUT_CLASSIFICATION	 default file system which may may not online during pure metadata operations 
WITHOUT_CLASSIFICATION	 deterministic ease testing 
WITHOUT_CLASSIFICATION	 this unionoperator inside the reduce side job generated correlation optimizer which means all inputs this unionoperator are from demuxoperator should not touch this unionoperator genmapredtasks 
WITHOUT_CLASSIFICATION	 ambiguous 
